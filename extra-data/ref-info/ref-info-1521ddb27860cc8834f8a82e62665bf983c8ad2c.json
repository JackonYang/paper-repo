{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5139774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1a3b5a20e77d8ac94967dc48173c48af3012eaf",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations."
            },
            "slug": "Constructing-Semantic-Space-Models-from-Parsed-Pad\u00f3-Lapata",
            "title": {
                "fragments": [],
                "text": "Constructing Semantic Space Models from Parsed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel approach for constructing semantic spaces that takes syntactic relations into account is presented, which is a formalisation for this class of models and their adequacy on two modelling tasks is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15698938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd1901f34cc3673072264104885d70555b1a4cdc",
            "isKey": false,
            "numCitedBy": 1928,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is."
            },
            "slug": "Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval and Clustering of Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A word similarity measure based on the distributional pattern of words allows the automatically constructed thesaurus to be significantly closer to WordNet than Roget Thesaurus is."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 191
                            }
                        ],
                        "text": "Additionally, Gunnar Eriksson has read this text carefully and provided numerous invaluable comments; Magnus Boman, Martin Volk and Arne Jo\u0308nsson have read and commented on parts of the text; Hinrich Schu\u0308tze, Susan Dumais, Thomas Landauer and David Waltz have generously answered any questions I have had.\nv\nI salute Fredrik Olsson for being a good colleague, a good training buddy, and more than anything for being a good friend; Rickard Co\u0308ster for exciting collaborations, inspiring discussions, and most of all for all the fun; Ola Knutsson for invigorating philosophical excursions and for all the laughs; David Swanberg, who I started this word-space odyssey with, and who have remained a good friend."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 18
                            }
                        ],
                        "text": "A good example is Sch\u00fctze & Pedersen (1995), who state that \u201cwords with similar meanings will occur with similar neighbors if enough text material is available,\u201d and Rubenstein & Goodenough (1965) \u2014 one of the very first studies to explicitly formulate and investigate the distributional hypothesis \u2014 who state that \u201cwords which are similar in meaning occur in similar contexts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 3
                            }
                        ],
                        "text": "As Sch\u00fctze (1992) notes, it is unnecessary to apply SVD (or any other related dimensionality-reduction technique) when using paradigmatic contexts, since they are already dense, and of course already contain paradigmatic relations between words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 75
                            }
                        ],
                        "text": "Several researchers have noted this difference between the two approaches; Sch\u00fctze & Pedersen (1997) argue that a paradigmatic use of context (what they call lexical co-occurrences) is both quantitatively (because it provides a better statistical foundation) and qualitatively (because the fact that two words occur close to each other is, they argue, likely to be more significant than the fact that they"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 28
                            }
                        ],
                        "text": "This term is due to Hinrich Sch\u00fctze (1993), who defines the model as follows:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3211177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d24afe3a62331ebfad400c3fec77c836d2b99db",
            "isKey": true,
            "numCitedBy": 307,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Representations for semantic information about words are necessary for many applications of neural networks in natural language processing. This paper describes an efficient, corpus-based method for inducing distributed semantic representations for a large number of words (50,000) from lexical coccurrence statistics by means of a large-scale linear regression. The representations are successfully applied to word sense disambiguation using a nearest neighbor method."
            },
            "slug": "Word-Space-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Word Space"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An efficient, corpus-based method for inducing distributed semantic representations for a large number of words from lexical coccurrence statistics by means of a large-scale linear regression is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1528142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44937535d0849412e8042f4fddca5c9dde7da3e",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces context digests, high-dimensional real-valued representations for the typical left and right contexts of a word. Initial entries for the context digests are formed from the word\u2019s close left and right neighbors. A singular value decomposition reduces the dimensionality of the space to enable subsequent efficient processing. In contrast to similar techniques, no preprocessor such as a parser is required. Context digests summarize both syntagmatic and paradigmatic relations between words: how typical they are as neighbors and how well they are substitutable for each other. We apply context digests to identifying collocations, to assessing the similarity of the arguments of different verbs, and to clustering occurrences of adjectives and verbs according to the words they modify in context."
            },
            "slug": "A-Vector-Model-for-Syntagmatic-and-Paradigmatic-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "A Vector Model for Syntagmatic and Paradigmatic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This paper introduces context digests, high-dimensional real-valued representations for the typical left and right contexts of a word that apply to identifying collocations, assessing the similarity of the arguments of different verbs, and to clustering occurrences of adjectives and verbs according to the words they modify in context."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2500077"
                        ],
                        "name": "Julie Weeds",
                        "slug": "Julie-Weeds",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Weeds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julie Weeds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35258592"
                        ],
                        "name": "David J. Weir",
                        "slug": "David-J.-Weir",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Weir",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Weir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3016990,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "527eb9c939801f1edcedace66eff7bbc02f74e80",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This work investigates the variation in a word's distributionally nearest neighbours with respect to the similarity measure used. We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations)."
            },
            "slug": "Characterising-Measures-of-Lexical-Distributional-Weeds-Weir",
            "title": {
                "fragments": [],
                "text": "Characterising Measures of Lexical Distributional Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A three-way connection is demonstrated between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy in a word's distributionally nearest neighbours with respect to the similarity measure."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7845331,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "c28eb618304ad717a5a173fdcbd6ebe9e176aebd",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Random Indexing is a vector-based technique for extracting semantically similar words from the co-occurrence statistics of words in large text data. We have applied the technique on aligned bilingual corpora, producing French-English and Swedish-English thesauri that we have used for cross-lingual query expansion. In this paper, we report on our CLEF 2001 experiments on French-to-English and Swedish-to-English query expansion."
            },
            "slug": "Vector-Based-Semantic-Analysis-Using-Random-for-Sahlgren-Karlgren",
            "title": {
                "fragments": [],
                "text": "Vector-Based Semantic Analysis Using Random Indexing for Cross-Lingual Query Expansion"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Random Indexing is applied on aligned bilingual corpora, producing French-English and Swedish-English thesauri that are used for cross-lingual query expansion in CLEF 2001."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15862538,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "f3f3dcfcaa960ec201e0381f4d026e57e64bea76",
            "isKey": false,
            "numCitedBy": 689,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification."
            },
            "slug": "Noun-Classification-from-Predicate-Argument-Hindle",
            "title": {
                "fragments": [],
                "text": "Noun Classification from Predicate-Argument Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2308434"
                        ],
                        "name": "W. R. Caid",
                        "slug": "W.-R.-Caid",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Caid",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. R. Caid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889692"
                        ],
                        "name": "S. I. Gallant",
                        "slug": "S.-I.-Gallant",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gallant",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. I. Gallant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15534242,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bf8a525932700e865e03f14c22e24c5bb24d1f8",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learned-Vector-Space-Models-for-Document-Retrieval-Caid-Dumais",
            "title": {
                "fragments": [],
                "text": "Learned Vector-Space Models for Document Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 43
                            }
                        ],
                        "text": "A more sophisticated approach to utilizing linguistic information is Pad\u00f3 & Lapata (2003), who uses syntactically parsed data to build contexts that reflect the dependency relations between the words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 43
                            }
                        ],
                        "text": "A more sophisticated approach to utilizing linguistic information is Pad\u00f3 & Lapata (2003), who uses syntactically parsed data to build contexts that reflect the dependency relations between the words. Their approach is inspired by the works of Strzalkowski (1994) and Lin (1997, 1998a, 1998b), who also used parsed data to compute distributional similarity between words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1394805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30a8f7577c1ccdbd14dcea85efdfae5420e1ccb6",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word. Separate classifiers have to be trained for different words. We present an algorithm that uses the same knowledge sources to disambiguate different words. The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts."
            },
            "slug": "Using-Syntactic-Dependency-as-Local-Context-to-Word-Lin",
            "title": {
                "fragments": [],
                "text": "Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An algorithm is presented that uses the same knowledge sources to disambiguate different words and does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15829786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e569d99f3a0fcfa038631dda2b44c73a6e8e97b8",
            "isKey": false,
            "numCitedBy": 454,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The representation of documents and queries as vectors in a high-dimensional space is well-established in information retrieval. The author proposes that the semantics of words and contexts in a text be represented as vectors. The dimensions of the space are words and the initial vectors are determined by the words occurring close to the entity to be represented, which implies that the space has several thousand dimensions (words). This makes the vector representations (which are dense) too cumbersome to use directly. Therefore, dimensionality reduction by means of a singular value decomposition is employed. The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction. >"
            },
            "slug": "Dimensions-of-meaning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Dimensions of meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction and finds that dimensionality reduction by means of a singular value decomposition is employed."
            },
            "venue": {
                "fragments": [],
                "text": "Supercomputing '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29249810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "244ac8d99def8e6238f318e5a4cdcec8023970e1",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Cooccurrence-Based-Thesaurus-and-Two-Applications-Sch\u00fctze-Pedersen",
            "title": {
                "fragments": [],
                "text": "A Cooccurrence-Based Thesaurus and Two Applications to Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153584694"
                        ],
                        "name": "R. Rapp",
                        "slug": "R.-Rapp",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Rapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rapp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8736393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbec1c3e45237b0cd7e6e1f7daf928fecef2d2a3",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora. According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts. The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics. The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. Both approaches are systematically compared and are validated on empirical data. It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects."
            },
            "slug": "The-Computation-of-Word-Associations:-Comparing-and-Rapp",
            "title": {
                "fragments": [],
                "text": "The Computation of Word Associations: Comparing Syntagmatic and Paradigmatic Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14842061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ff0a2bf7bfc077ddea9f91552d8b59e1d386116",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an algorithm for word sense disambiguation based on a vector representation of word similarity derived from lexical co-occurrence. It diiers from standard approaches by allowing for as ne grained distinctions as is warranted by the information at hand, rather than supposing a xed number of senses per word, and by allowing for more than one sense to be assigned to a given word occurrence. The algorithm is applied to the standard vector-space information retrieval model and an evaluation is performed over the Category B TREC-1 corpus (WSJ subcollection). Results show that this sense disambiguation algorithm improves performance by between 7% and 14% on average ."
            },
            "slug": "Information-Retrieval-Based-on-Word-Senses-Pedersen",
            "title": {
                "fragments": [],
                "text": "Information Retrieval Based on Word Senses"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An algorithm for word sense disambiguation based on a vector representation of word similarity derived from lexical co-occurrence is proposed and an evaluation is performed over the Category B TREC-1 corpus (WSJ subcollection)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400286782"
                        ],
                        "name": "P. Wiemer-Hastings",
                        "slug": "P.-Wiemer-Hastings",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Wiemer-Hastings",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wiemer-Hastings"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 291794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "605527f92a5f8ebaae7405d7ae78ea03f830fb5a",
            "isKey": false,
            "numCitedBy": 561,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Semantic-Analysis-Wiemer-Hastings",
            "title": {
                "fragments": [],
                "text": "Latent Semantic Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143721588"
                        ],
                        "name": "P. Hansen",
                        "slug": "P.-Hansen",
                        "structuredName": {
                            "firstName": "Preben",
                            "lastName": "Hansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 188
                            }
                        ],
                        "text": "In bilingual settings, we have used word-space methodology for query expansion and query translation using French\u2013English and Swedish\u2013English (Sahlgren & Karlgren, 2002), English\u2013Japanese (Sahlgren et al., 2002), and Swedish\u2013French (Karlgren, Sahlgren, J\u00e4rvinen, & C\u00f6ster, 2005) data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 36
                            }
                        ],
                        "text": ", 2003) to \u201cdownright catastrophic\u201d (Sahlgren et al., 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 555883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3b0a00360283f9ac90a9794067dd3df36e36a93",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector-space techniques can be used for extracting semantically similar words from the co-occurrence statistics of words in large text data. In this paper, we report on experiments with using the Random Indexing vector-space technique for extracting a cross-lingual thesaurus from aligned English-Japanese bilingual data. The cross-lingual thesaurus has been used for automatic cross-lingual query expansion in the NTCIR patent retrieval task."
            },
            "slug": "English-Japanese-Cross-lingual-Query-Expansion-of-Sahlgren-Hansen",
            "title": {
                "fragments": [],
                "text": "English-Japanese Cross-lingual Query Expansion Using Random Indexing of Aligned Bilingual Text Data"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experiments with using the Random Indexing vector-space technique for extracting a cross-lingual thesaurus from aligned English-Japanese bilingual data are reported on."
            },
            "venue": {
                "fragments": [],
                "text": "NTCIR"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8523268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b132192076c65ee9c16c851728827634991d6868",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "For a very long time, it has been considered that the only way of automatically extracting similar groups of words from a text collection for which no semantic information exists is to use document co-occurrence data. But, with robust syntactic parsers that are becoming more frequently available, syntactically recognizable phenomena about word usage can be confidently noted in large collections of texts. We present here a new system called SEXTANT which uses these parsers and the finer-grained contexts they produce to judge word similarity."
            },
            "slug": "SEXTANT:-Exploring-Unexplored-Contexts-for-Semantic-Grefenstette",
            "title": {
                "fragments": [],
                "text": "SEXTANT: Exploring Unexplored Contexts for Semantic Extraction from Syntactic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new system called SEXTANT is presented which uses robust syntactic parsers and the finer-grained contexts they produce to judge word similarity."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 13
                            }
                        ],
                        "text": "For example, Sch\u00fctze (1992) uses a window size of 1 000 characters, with the argument that a few long words are possibly better than many short words, which tend to be high-frequency function words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 13
                            }
                        ],
                        "text": "For example, Sch\u00fctze (1992) uses a window size of 1 000 characters, with the argument that a few long words are possibly better than many short words, which tend to be high-frequency function words. Yarowsky (1992) uses 100 words, while Gale et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12787230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a63c71ab4fd81fd8acd861266482f49ec938ccaf",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a measure of corpus homogeneity that indicates the amount of topical dispersion in a corpus. The measure is based on the density of neighborhoods in semantic word spaces. We evaluate the measure by comparing the results for five different corpora. Our initial results indicate that the proposed density measure can indeed identify differences in topical dispersion."
            },
            "slug": "Counting-Lumps-in-Word-Space:-Density-as-a-Measure-Sahlgren-Karlgren",
            "title": {
                "fragments": [],
                "text": "Counting Lumps in Word Space: Density as a Measure of Corpus Homogeneity"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The initial results indicate that the proposed density measure can indeed identify differences in topical dispersion and is based on the density of neighborhoods in semantic word spaces."
            },
            "venue": {
                "fragments": [],
                "text": "SPIRE"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2325236"
                        ],
                        "name": "Y. Niwa",
                        "slug": "Y.-Niwa",
                        "structuredName": {
                            "firstName": "Yoshiki",
                            "lastName": "Niwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niwa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806193"
                        ],
                        "name": "Y. Nitta",
                        "slug": "Y.-Nitta",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Nitta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nitta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2646329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c989e8aa08b24345419e4528198fe5ea17cc0160",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the interword distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary (60K head words + 1.6M definition words). However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors."
            },
            "slug": "Co-Occurrence-Vectors-From-Corpora-vs.-Distance-Niwa-Nitta",
            "title": {
                "fragments": [],
                "text": "Co-Occurrence Vectors From Corpora vs. Distance Vectors From Dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors, compared with other experimental results, which suggest that word sense disambiguation is affected by distance vectors."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Additionally, Gunnar Eriksson has read this text carefully and provided numerous invaluable comments; Magnus Boman, Martin Volk and Arne Jo\u0308nsson have read and commented on parts of the text; Hinrich Schu\u0308tze, Susan Dumais, Thomas Landauer and David Waltz have generously answered any questions I have had.\nv\nI salute Fredrik Olsson for being a good colleague, a good training buddy, and more than anything for being a good friend; Rickard Co\u0308ster for exciting collaborations, inspiring discussions, and most of all for all the fun; Ola Knutsson for invigorating philosophical excursions and for all the laughs; David Swanberg, who I started this word-space odyssey with, and who have remained a good friend."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 442,
                                "start": 438
                            }
                        ],
                        "text": "My sincere thanks go to Bjo\u0308rn Gamba\u0308ck (LATEXguru), Preben Hansen, Kristofer Franzen, Martin Svensson, and the rest of the lab at SICS; to Vicki Carleson for always hunting down the articles and books I fail to find on my own; to Mikael Nehlsen for sysadmin at SICS; to Robert Andersson for sysadmin at GSLT; to Marianne Rosenqvist for always keeping track of my coffee jug; to Joakim Nivre, Jens Allwood, and everyone at GSLT; to Martin, Magnus, Jonas, and Johnny (formerly) at KTH; and to the computational linguistics group at SU."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 276
                            }
                        ],
                        "text": "\u2026Computational Linguistics Gothenburg University Userware Laboratory Stockholm, Sweden Gothenburg, Sweden Kista, Sweden\nISBN 91-7155-281-2 ISSN 1101-1335\nISRN SICS-D\u201344\u2013SE SICS Dissertation Series 44\nDoctoral Dissertation Department of Linguistics Stockholm University c\u00a9 Magnus Sahlgren, 2006."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "The Word-Space Model\nUsing distributional analysis to represent\nsyntagmatic and paradigmatic relations between words\nin high-dimensional vector spaces\nMagnus Sahlgren\nA Dissertation submitted to Stockholm University\nin partial fulfillment of the requirements\nfor the degree of Doctor of Philosophy\n2006\nStockholm University National Graduate School Swedish Institute Department of Linguistics of Language Technology of Computer Science Computational Linguistics Gothenburg University Userware Laboratory Stockholm, Sweden Gothenburg, Sweden Kista, Sweden\nISBN 91-7155-281-2 ISSN 1101-1335\nISRN SICS-D\u201344\u2013SE SICS Dissertation Series 44\nDoctoral Dissertation Department of Linguistics Stockholm University c\u00a9 Magnus Sahlgren, 2006."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3159919,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "373305b3b529f874e3cddc4fdc0081ed6709b567",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses evaluation methodologies for a particular kind of meaning models known as word-space models, which use distributional information to assemble geometric representations of meaning similarities. Word-space models have received considerable attention in recent years, and have begun to see employment outside the walls of computational linguistics laboratories. However, the evaluation methodologies of such models remain infantile, and lack efforts at standardization. Very few studies have critically assessed the methodologies used to evaluate word spaces. This paper attempts to fill some of this void. It is the central goal of this paper to answer the question \u0093how can we determine whether a given word space is a good word space?\u0094"
            },
            "slug": "Towards-pertinent-evaluation-methodologies-for-Sahlgren",
            "title": {
                "fragments": [],
                "text": "Towards pertinent evaluation methodologies for word-space models"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "Evaluation methodologies for a particular kind of meaning models known as word-space models, which use distributional information to assemble geometric representations of meaning similarities, are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144053576"
                        ],
                        "name": "J. Levy",
                        "slug": "J.-Levy",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948254"
                        ],
                        "name": "J. Bullinaria",
                        "slug": "J.-Bullinaria",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bullinaria",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bullinaria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121307140"
                        ],
                        "name": "Malti Patel",
                        "slug": "Malti-Patel",
                        "structuredName": {
                            "firstName": "Malti",
                            "lastName": "Patel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Malti Patel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 157
                            }
                        ],
                        "text": "In addition to this, most published experiments tend to use the exact same test set \u2014 the 80 multiple-choice TOEFL items that were used by Landauer & Dumais (Landauer & Dumais, 1997; Levy et al., 1998; Karlgren & Sahlgren, 2001; Turney, 2001; Rapp, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 214
                            }
                        ],
                        "text": "Additionally, Gunnar Eriksson has read this text carefully and provided numerous invaluable comments; Magnus Boman, Martin Volk and Arne Jo\u0308nsson have read and commented on parts of the text; Hinrich Schu\u0308tze, Susan Dumais, Thomas Landauer and David Waltz have generously answered any questions I have had.\nv\nI salute Fredrik Olsson for being a good colleague, a good training buddy, and more than anything for being a good friend; Rickard Co\u0308ster for exciting collaborations, inspiring discussions, and most of all for all the fun; Ola Knutsson for invigorating philosophical excursions and for all the laughs; David Swanberg, who I started this word-space odyssey with, and who have remained a good friend."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 70
                            }
                        ],
                        "text": "75% using a probabilistic algorithm called PMI-IR (Turney, 2001), and Levy et al. (1998) reached 76% using the Hellinger distance metric,(2) and a small context window with probabilistic weights for the BNC."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 70
                            }
                        ],
                        "text": "75% using a probabilistic algorithm called PMI-IR (Turney, 2001), and Levy et al. (1998) reached 76% using the Hellinger distance metric,(2) and a small context window with probabilistic weights for the BNC. Rapp (2003) reports an astounding result of 92."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 676,
                                "start": 3
                            }
                        ],
                        "text": "(5)Linear independence means that none of a set of vectors can be written as a linear combination of the other vectors. As an example, the vectors (0,0,1), (0,1,0) and (1,0,0) are linearly independent, while the vectors (1,0,0), (0,0,1) and (1,0,1) are not, since the third of these last vectors can be written as a combination of the other two. (6)It is interesting to note that Osgood and colleagues already in 1957 (Osgood et al., 1957) \u2014 roughly 30 years before the advent of LSA \u2014 mentioned the use of factor analysis to uncover orthogonal dimensions in the semantic space. (7)It should be noted that there are critical voices as well. For example, Isbell & Viola (1998) demonstrate experimentally that truncated SVD generates an approximation that is sub-optimal with regard to a given information-retrieval problem, and Husbands et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 40
                            }
                        ],
                        "text": "This concurs in the results reported by Levy et al. (1998). The results decrease for both corpora when the windows grow wider."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 579,
                                "start": 24
                            }
                        ],
                        "text": "So what would be a more linguistically justified definition of context in which to collect syntagmatic information? Perhaps a clause or a sentence, since they seem to be linguistic universals; clauses and sentences, or at least the functional equivalent to such entities (i.e. some sequence delimited by some kind of delimiter), seem to exist in every language \u2014 spoken as well as written or signalled. Thus, it would be possible to argue for its apparent linguistic reality as context. Sentences have been used to harvest co-occurrences by, e.g., Rubenstein & Goodenough (1965), Miller & Charles (1991), and Leacock et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Levy et al. (1998) performed a similar experiment where they computed centroid vectors(2) for 12 different parts-of-speech by combining the context vectors \u201cfrom a very large number of different words\u201d belonging to the parts-of-speech."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119756963,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "61fc06dcc8440e86d42daf395f122a9054077cb6",
            "isKey": true,
            "numCitedBy": 29,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has demonstrated that counts of which other words co-occur with a word of interest can reflect interesting properties of that word. We have studied aspects of this kind of methodology by systematically examining the effects of different combinations of parameters used in the preparation of co-occurrence statistics. Several psychologically relevant evaluation measures are used. We have found that successful performance on the evaluation tasks depends on the correct selection of parameters such as window size and distance metric."
            },
            "slug": "Explorations-in-the-derivation-of-word-statistics-Levy-Bullinaria",
            "title": {
                "fragments": [],
                "text": "Explorations in the derivation of word co-occurrence statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work has studied aspects of this kind of methodology by systematically examining the effects of different combinations of parameters used in the preparation of co-occurrence statistics by systematically analyzing the results of several psychologically relevant evaluation measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47709773"
                        ],
                        "name": "H. Rubenstein",
                        "slug": "H.-Rubenstein",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Rubenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rubenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898344"
                        ],
                        "name": "J. Goodenough",
                        "slug": "J.-Goodenough",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Goodenough",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodenough"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 3
                            }
                        ],
                        "text": "In Sahlgren (2004) and Sahlgren & Karlgren (2005a), we used RI to acquire bilingual lexica from aligned parallel data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18309234,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7ef3ac14cdb484aaa2b039850093febd5cf73a21",
            "isKey": false,
            "numCitedBy": 1460,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "slug": "Contextual-correlates-of-synonymy-Rubenstein-Goodenough",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of synonymy"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The shapes of the functions indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34729490"
                        ],
                        "name": "W. Charles",
                        "slug": "W.-Charles",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Charles",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Charles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 17
                            }
                        ],
                        "text": "Examples include Nakov et al. (2001), who studied the effects of using different weighting schemes in LSA; Bingham and Mannila (2001), who investigated the effects of using different dimensionality-reduction techniques; and Weeds et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 145580646,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "402627e4eb8c95e4aae3026fd921aa08cd792006",
            "isKey": false,
            "numCitedBy": 1678,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be."
            },
            "slug": "Contextual-correlates-of-semantic-similarity-Miller-Charles",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of semantic similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153584694"
                        ],
                        "name": "R. Rapp",
                        "slug": "R.-Rapp",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Rapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rapp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 214
                            }
                        ],
                        "text": "Additionally, Gunnar Eriksson has read this text carefully and provided numerous invaluable comments; Magnus Boman, Martin Volk and Arne Jo\u0308nsson have read and commented on parts of the text; Hinrich Schu\u0308tze, Susan Dumais, Thomas Landauer and David Waltz have generously answered any questions I have had.\nv\nI salute Fredrik Olsson for being a good colleague, a good training buddy, and more than anything for being a good friend; Rickard Co\u0308ster for exciting collaborations, inspiring discussions, and most of all for all the fun; Ola Knutsson for invigorating philosophical excursions and for all the laughs; David Swanberg, who I started this word-space odyssey with, and who have remained a good friend."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 157
                            }
                        ],
                        "text": "In addition to this, most published experiments tend to use the exact same test set \u2014 the 80 multiple-choice TOEFL items that were used by Landauer & Dumais (Landauer & Dumais, 1997; Levy et al., 1998; Karlgren & Sahlgren, 2001; Turney, 2001; Rapp, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1171753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b62fce4fc3476e36545acff0a4d0627326959a53",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In machine translation, information on word ambiguities is usually provided by the lexicographers who construct the lexicon. In this paper we propose an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word. The approach is based on the statistics of the distributional similarity between the words in a corpus. Our algorithm works as follows: The 20 strongest first-order associations to the ambiguous word are considered as sense descriptor candidates. All pairs of these candidates are ranked according to the following two criteria: First, the two words in a pair should be as dissimilar as possible. Second, although being dissimilar their co-occurrence vectors should add up to the co-occurrence vector of the ambiguous word scaled by two. Both conditions together have the effect that preference is given to pairs whose co-occurring words are complementary. For best results, our implementation uses singular value decomposition, entropy-based weights, and second-order similarity metrics."
            },
            "slug": "Word-sense-discovery-based-on-sense-descriptor-Rapp",
            "title": {
                "fragments": [],
                "text": "Word sense discovery based on sense descriptor dissimilarity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word, based on the statistics of the distributional similarity between the words in a corpus."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055976119"
                        ],
                        "name": "S. Marcus",
                        "slug": "S.-Marcus",
                        "structuredName": {
                            "firstName": "Shaul",
                            "lastName": "Marcus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309269"
                        ],
                        "name": "Shaul Markovitch",
                        "slug": "Shaul-Markovitch",
                        "structuredName": {
                            "firstName": "Shaul",
                            "lastName": "Markovitch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaul Markovitch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 697,
                                "start": 221
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999). Although these probabilistic approaches do rely on the distributional methodology as discovery procedure, they do not utilize the geometric metaphor of meaning as representational basis, and thus fall outside the scope of this venture. A good explanation of the difference between the geometric and the probabilistic approaches is the distinction made by Ruge (1992):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1154960,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "8310a3f4ec3d6a37958c1c2aefe80b7b7badbca0",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In recent years there is much interest in word co-occurrence relations, such as n-grams, verb\u2013object combinations, or co-occurrence within a limited context. This paper discusses how to estimate the likelihood of co-occurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved co-occurrence and other co-occurrences that contain similar words. These analogies are based on the assumption that similar word co-occurrences have similar values of mutual information. Accordingly, the word similarity metric captures similarities between vectors of mutual information values. Our evaluation suggests that this method performs better than existing, frequency-based, smoothing methods, and may provide an alternative to class-based models. A background survey is included, covering issues of lexical co-occurrence, data sparseness and smoothing, word similarity and clustering, and mutual information."
            },
            "slug": "Contextual-Word-Similarity-and-Estimation-From-Data-Dagan-Marcus",
            "title": {
                "fragments": [],
                "text": "Contextual Word Similarity and Estimation From Sparse Data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The evaluation suggests that this method performs better than existing, frequency-based, smoothing methods, and may provide an alternative to class-based models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226331"
                        ],
                        "name": "C. Leacock",
                        "slug": "C.-Leacock",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Leacock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leacock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804836"
                        ],
                        "name": "G. Towell",
                        "slug": "G.-Towell",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Towell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Towell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746656"
                        ],
                        "name": "E. Voorhees",
                        "slug": "E.-Voorhees",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Voorhees",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Voorhees"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6830876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6520da982493225779ef2cac3411d10de50f5a7",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A b s t r a c t Automatic corpus-based sense resolution, or sense dlsambiguation, techniques tend to focus either on very local context or on topical context. Both components axe needed for word sense resolution. A contextual representation of a word sense consists of topical context and local context. Our goal is to construct contextual representations by automatically extracting topical and local information from textual corpora. We review an experiment evaluating three statistical classifiers that automatically extract topical context. An experiment designed to examine human subject performance with similar input is described. Finally, we investigate a method for automatically extracting local context from a corpus. Preliminary results show improved perfor-"
            },
            "slug": "Towards-Building-Contextual-Representations-of-Word-Leacock-Towell",
            "title": {
                "fragments": [],
                "text": "Towards Building Contextual Representations of Word Senses Using Statistical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The goal is to construct contextual representations by automatically extracting topical and local information from textual corpora by investigating a method for automatically extracting local context from a corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 24
                            }
                        ],
                        "text": "(1990), Sch\u00fctze (1992), Pereira et al. (1993), and Niwa & Nitta (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 678,
                                "start": 24
                            }
                        ],
                        "text": "(1990), Sch\u00fctze (1992), Pereira et al. (1993), and Niwa & Nitta (1994). The arguably most influential work from this period comes from Hinrich Sch\u00fctze (1992, 1993), who builds context vectors (which he calls \u201cterm vectors\u201d or \u201cword vectors\u201d) in precisely the manner described in Section 3.1 above: co-occurrence counts are collected in a words-by-words matrix, in which the elements record the number of times two words co-occur within a set window of word tokens. Context vectors are then defined as the rows or the columns of the matrix (the matrix is symmetric, so it does not matter if the rows or the columns are used). A similar approach is described by Qiu & Frei (1993), with the difference that they use a words-by-documents matrix to collect the cooccurrence counts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 24
                            }
                        ],
                        "text": "(1990), Sch\u00fctze (1992), Pereira et al. (1993), and Niwa & Nitta (1994). The arguably most influential work from this period comes from Hinrich Sch\u00fctze (1992, 1993), who builds context vectors (which he calls \u201cterm vectors\u201d or \u201cword vectors\u201d) in precisely the manner described in Section 3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6713452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5",
            "isKey": true,
            "numCitedBy": 1193,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
            },
            "slug": "Distributional-Clustering-of-English-Words-Pereira-Tishby",
            "title": {
                "fragments": [],
                "text": "Distributional Clustering of English Words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the annealed parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996667"
                        ],
                        "name": "Beno\u00eet Lemaire",
                        "slug": "Beno\u00eet-Lemaire",
                        "structuredName": {
                            "firstName": "Beno\u00eet",
                            "lastName": "Lemaire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beno\u00eet Lemaire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2496651"
                        ],
                        "name": "G. Denhi\u00e8re",
                        "slug": "G.-Denhi\u00e8re",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Denhi\u00e8re",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Denhi\u00e8re"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 579,
                                "start": 24
                            }
                        ],
                        "text": "So what would be a more linguistically justified definition of context in which to collect syntagmatic information? Perhaps a clause or a sentence, since they seem to be linguistic universals; clauses and sentences, or at least the functional equivalent to such entities (i.e. some sequence delimited by some kind of delimiter), seem to exist in every language \u2014 spoken as well as written or signalled. Thus, it would be possible to argue for its apparent linguistic reality as context. Sentences have been used to harvest co-occurrences by, e.g., Rubenstein & Goodenough (1965), Miller & Charles (1991), and Leacock et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8933441,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f992e17cd614836ba148ed172044bcfe7601d9c1",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a computational model of the incremental construction of an associative network from a corpus. It is aimed at modeling the development of the human semantic memory. It is not based on a vector representation, which does not well reproduce the asymmetrical property of word similarity, but rather on a network representation. Compared to Latent Semantic Analysis, it is incremental which is cognitively more plausible. It is also an attempt to take into account higher-order co-occurrences in the construction of word similarities. This model was compared to children association norms. A good correlation as well as a similar gradient of similarity were found."
            },
            "slug": "Incremental-Construction-of-an-Associative-Network-Lemaire-Denhi\u00e8re",
            "title": {
                "fragments": [],
                "text": "Incremental Construction of an Associative Network from a Corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A computational model of the incremental construction of an associative network from a corpus aimed at modeling the development of the human semantic memory is presented, not based on a vector representation, but rather on a network representation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828241"
                        ],
                        "name": "Rickard C\u00f6ster",
                        "slug": "Rickard-C\u00f6ster",
                        "structuredName": {
                            "firstName": "Rickard",
                            "lastName": "C\u00f6ster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rickard C\u00f6ster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15794465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ed5291b6746a6420db79e3706c6b70aa819dd8f",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the use of concept-based representations for text categorization. We introduce a new approach to create concept-based text representations, and apply it to a standard text categorization collection. The representations are used as input to a Support Vector Machine classifier, and the results show that there are certain categories for which concept-based representations constitute a viable supplement to word-based ones. We also demonstrate how the performance of the Support Vector Machine can be improved by combining representations."
            },
            "slug": "Using-Bag-of-Concepts-to-Improve-the-Performance-of-Sahlgren-C\u00f6ster",
            "title": {
                "fragments": [],
                "text": "Using Bag-of-Concepts to Improve the Performance of Support Vector Machines in Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A new approach is introduced to create concept-based text representations, and they are applied to a standard text categorization collection and it is demonstrated how the performance of the Support Vector Machine can be improved by combining representations."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119515866"
                        ],
                        "name": "G. Ruge",
                        "slug": "G.-Ruge",
                        "structuredName": {
                            "firstName": "Gerd",
                            "lastName": "Ruge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ruge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5517932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4966f2d75734b4abd4ad105b85eff675cb781b5d",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Experiments-on-Linguistically-Based-Term-Ruge",
            "title": {
                "fragments": [],
                "text": "Experiments on Linguistically-Based Term Associations"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791072"
                        ],
                        "name": "T. Strzalkowski",
                        "slug": "T.-Strzalkowski",
                        "structuredName": {
                            "firstName": "Tomek",
                            "lastName": "Strzalkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Strzalkowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16572215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a490bd284d0bdef19dea5c3d59ad8fc279c5301d",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In information retrieval the task is to extract from the database ~dl ,and only the documents which are relevant to a user query, even when the query and the documents use little common vocabul~u'y. In this paper we discuss the problem of automatic generation of lexical relations between words ,and phrltses from large text corpora :rod their application to automatic query expansion ill information retrieval. Reported here ,are some preliminary resuhs and observations from the experiments with a 85 million word Wall Street Journal dalabase and a 45 million word San Jose Mercury News database (piu'ts of 0.5 billion word TIPSTER/TREC datab`ase)."
            },
            "slug": "Building-a-Lexical-Domain-Map-From-Text-Corpora-Strzalkowski",
            "title": {
                "fragments": [],
                "text": "Building a Lexical Domain Map From Text Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The problem of automatic generation of lexical relations between words, and phrltses from large text corpora is discussed, and their application to automatic query expansion ill information retrieval is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15763200,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "dbfd191afbbc8317577cbc44afe7156df546e143",
            "isKey": false,
            "numCitedBy": 3648,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested."
            },
            "slug": "Automatic-Acquisition-of-Hyponyms-from-Large-Text-Hearst",
            "title": {
                "fragments": [],
                "text": "Automatic Acquisition of Hyponyms from Large Text Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest are identified."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889692"
                        ],
                        "name": "S. I. Gallant",
                        "slug": "S.-I.-Gallant",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gallant",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. I. Gallant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19596937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eac60aeb9a17517a3e92fda4b4ba71051959b85f",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Representing and manipulating context information is one of the hardest problems in natural language processing. This paper proposes a method for representing some context information so that the correct meaning for a word in a sentence can be selected. The approach is primarily based on work by Waltz and Pollack (1985, 1984), who emphasized neutrally plausible systems. By contrast this paper focuses on computationally feasible methods applicable to full-scale natural language processing systems. There are two key elements: a collection of context vectors defined for every word used by a natural language processing system, and a context algorithm that computes a dynamic context vector at any position in a body of text. Once the dynamic context vector has been computed it is easy to choose among competing meanings for a word. This choice of definitions is essentially a neural network computation, and neural network learning algorithms should be able to improve such choices. Although context vectors do not represent all context information, their use should improve those full-scale systems that have avoided context as being too difficult to deal with. Good candidates for full-scale context vector implementations are machine translation systems and Japanese word processors. A main goal of this paper is to encourage such large-scale implementations and tests of context vector approaches. A variety of interesting directions for research in natural language processing and machine learning will be possible once a full set of context vectors has been created. In particular the development of more powerful context algorithms will be an important topic for future research."
            },
            "slug": "A-Practical-Approach-for-Representing-Context-and-Gallant",
            "title": {
                "fragments": [],
                "text": "A Practical Approach for Representing Context and for Performing Word Sense Disambiguation Using Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A method for representing some context information so that the correct meaning for a word in a sentence can be selected and the development of more powerful context algorithms will be an important topic for future research."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8754851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c",
            "isKey": false,
            "numCitedBy": 1401,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words."
            },
            "slug": "Automatic-Word-Sense-Discrimination-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Automatic Word Sense Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering that demonstrates good performance of context- group discrimination for a sample of natural and artificial ambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1693468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d922631a6bf8361d7602e12cafb9e15d421c827",
            "isKey": false,
            "numCitedBy": 836,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories. Roget's categories serve as approximations of conceptual classes. The categories listed for a word in Roget's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework.Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention. Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature."
            },
            "slug": "Word-Sense-Disambiguation-Using-Statistical-Models-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Word-Sense Disambiguation Using Statistical Models of Roget\u2019s Categories Trained on Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories, enabling training on unrestricted monolingual text without human intervention."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828241"
                        ],
                        "name": "Rickard C\u00f6ster",
                        "slug": "Rickard-C\u00f6ster",
                        "structuredName": {
                            "firstName": "Rickard",
                            "lastName": "C\u00f6ster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rickard C\u00f6ster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7990402"
                        ],
                        "name": "T. J\u00e4rvinen",
                        "slug": "T.-J\u00e4rvinen",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "J\u00e4rvinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J\u00e4rvinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26632181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78bc527bea33ca3a898ef6546c18c93f4448c29f",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector space techniques can be used for extracting semantically similar words from the co-occurrence statistics of words in large text data collections. We have used a technique called Random Indexing to accumulate context vectors for Swedish, French and Italian. We have then used the context vectors to perform automatic query expansion. In this paper, we report on our CLEF 2002 experiments on Swedish, French and Italian monolingual query expansion."
            },
            "slug": "SICS-at-CLEF-2002:-Automatic-Query-Expansion-using-Sahlgren-Karlgren",
            "title": {
                "fragments": [],
                "text": "SICS at CLEF 2002: Automatic Query Expansion using Random Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper uses a technique called Random Indexing to accumulate context vectors and uses the context vectors to perform automatic query expansion on Swedish, French and Italian monolingual query expansion in CLEF 2002."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50611682"
                        ],
                        "name": "C. Burgess",
                        "slug": "C.-Burgess",
                        "structuredName": {
                            "firstName": "Curt",
                            "lastName": "Burgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burgess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51933584"
                        ],
                        "name": "Kay Livesay",
                        "slug": "Kay-Livesay",
                        "structuredName": {
                            "firstName": "Kay",
                            "lastName": "Livesay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kay Livesay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40591835"
                        ],
                        "name": "K. Lund",
                        "slug": "K.-Lund",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lund"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 85
                            }
                        ],
                        "text": "essential to HAL; it is only used whenever computational efficiency becomes an issue (Burgess et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62155829,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b91e3a1ac98c84b4396faae4f8b0bd79465f60a6",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Deriving representations of meaning, whether at the word, the sentence, or the discourse level, is a problem with a long history in cognitive psychology and psycholinguistics. In this article, we describe a computational model of high\u2010dimensional context space, the Hyperspace Analog to Language (HAL), and present simulation evidence that HAL's vector representations can provide sufficient information to make semantic, grammatical, and abstract distinctions. Human participants were able to use the context neighborhoods that HAL generates to match words with similar items and to derive the word (or a similar word) from the neighborhood, thus demonstrating the cognitive compatibility of the representations with human processing. An experiment exploring the meaning of agent\u2010 and patient\u2010oriented verbs provided the context for discussing how the connotative aspects of word neighborhoods could provide cues in establishing a discourse model. Using the vector representations to build a sentence\u2010level representati..."
            },
            "slug": "Explorations-in-context-space:-Words,-sentences,-Burgess-Livesay",
            "title": {
                "fragments": [],
                "text": "Explorations in context space: Words, sentences, discourse"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A computational model of high\u2010dimensional context space, the Hyperspace Analog to Language (HAL), is described and simulation evidence that HAL's vector representations can provide sufficient information to make semantic, grammatical, and abstract distinctions is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 105
                            }
                        ],
                        "text": "In order to test the ability of the word spaces to capture antonymy, I follow the procedure described by Grefenstette (1992a), who uses a list of antonym pairs"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17908075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d7252950d51f67fb4613c6a364be48baa709b5",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "As more and more text becomes readily available in electronic form, much interest is being generated by finding ways of automatically extracting information from subsets of this text. While manual indexing and automatic keyword indexing are well known, both have drawbacks. Recent research on robust syntactic analysis and statistical correlations promises that some of the intuitive advantages of manual indexing can be retained in a fully automatic system. Here I present an experiment performed with my system SEXTANT which extracts semantically similar words from raw text. Using statistical methods combined with robust syntactic analysis, SEXTANT was able to find many of the intuitive pairings between semantically similar words studied by Deese [Deese, 1954]."
            },
            "slug": "Finding-Semantic-Similarity-in-Raw-Text:-the-Deese-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Finding Semantic Similarity in Raw Text: the Deese Antonyms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Using statistical methods combined with robust syntactic analysis, SEXTANT was able to find many of the intuitive pairings between semantically similar words studied by Deese [Deese, 1954]."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 124
                            }
                        ],
                        "text": "More sophisticated statistical criteria includes the tfidf, and different variants and mixtures of the Poisson distribution (Damerau, 1965; Harter, 1975; Katz, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 157
                            }
                        ],
                        "text": "The idea is that a single occurrence of a word in a large context region can be duly treated as a chance occurrence, and thus need not be taken into account (Katz, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7423683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b72c3fddfb70920c2a3be8adfbb3d505120f3f5",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of distribution of words and phrases in text, a problem of great general interest and of importance for many practical applications. The existing models for word distribution present observed sequences of words in text documents as an outcome of some stochastic processes; the corresponding distributions of numbers of word occurrences in the documents are modelled as mixtures of Poisson distributions whose parameter values are fitted to the data. We pursue a linguistically motivated approach to statistical language modelling and use observable text characteristics as model parameters. Multi-word technical terms, intrinsically content entities, are chosen for experimentation. Their occurrence and the occurrence dynamics are investigated using a 100-million word data collection consisting of a variety of about 13,000 technical documents. The derivation of models describing word distribution in text is based on a linguistic interpretation of the process of text formation, with the probabilities of word occurrence being functions of observable and linguistically meaningful text characteristics. The adequacy of the proposed models for the description of actually observed distributions of words and phrases in text is confirmed experimentally. The paper has two focuses: one is modelling of the distributions of content words and phrases among different documents; and another is word occurrence dynamics within documents and estimation of corresponding probabilities. Accordingly, among the application areas for the new modelling paradigm are information retrieval and speech recognition."
            },
            "slug": "Distribution-of-content-words-and-phrases-in-text-Katz",
            "title": {
                "fragments": [],
                "text": "Distribution of content words and phrases in text and language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The derivation of models describing word distribution in text is based on a linguistic interpretation of the process of text formation, with the probabilities of word occurrence being functions of observable and linguistically meaningful text characteristics."
            },
            "venue": {
                "fragments": [],
                "text": "Nat. Lang. Eng."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Black et al. (1988) uses narrow windows spanning 3\u20136 words, Church & Hanks (1989) used 5 words, and Dagan et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 461,
                                "start": 0
                            }
                        ],
                        "text": "Black et al. (1988) uses narrow windows spanning 3\u20136 words, Church & Hanks (1989) used 5 words, and Dagan et al. (1993) uses a window spanning 3 words, when ignoring function words. As we can see, examples of window sizes range from 100 words to just a couple of words. There is very seldom a theoretical motivation for a particular window size. Rather, the context window is often seen as just another experimentally determinable parameter. Levy et al. (1998) is a good example of this viewpoint:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 0
                            }
                        ],
                        "text": "Black et al. (1988) uses narrow windows spanning 3\u20136 words, Church & Hanks (1989) used 5 words, and Dagan et al. (1993) uses a window spanning 3 words, when ignoring function words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19859071,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "806f47862549c51f12c5539451abce2f6573236d",
            "isKey": true,
            "numCitedBy": 91,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of researchers in text processing have independently observed that people can consistently determine in which of several given senses a word is being used in text, simply by examining the half dozen or so words just before and just after the word in focus. The question arises whether the same task can be accomplished by mechanical means. Experimental results are presented which suggest an affirmative answer to this query. Three separate methods of discriminating English word senses are compared information-theoretically. Findings include a strong indication of the power of domain-specific content analysis of text, as opposed to domain-general approaches."
            },
            "slug": "An-Experiment-in-Computational-Discrimination-of-Black",
            "title": {
                "fragments": [],
                "text": "An Experiment in Computational Discrimination of English Word Senses"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "Experimental results are presented which suggest that people can consistently determine in which of several given senses a word is being used in text, simply by examining the half dozen or so words just before and just after the word in focus."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9558665,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
            "isKey": false,
            "numCitedBy": 4363,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "slug": "Word-Association-Norms,-Mutual-Information-and-Church-Hanks",
            "title": {
                "fragments": [],
                "text": "Word Association Norms, Mutual Information and Lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10688737,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "dcdf04ce36cb06db8976bba1f1b41f4aca1e9089",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a very simple and effective approach to automatic bilingual lexicon acquisition. The approach is cooccurrence-based, and uses the Random Indexing vector space methodology applied to aligned bilingual data. The approach is simple, efficient and scalable, and generate promising results when compared to a manually compiled lexicon. The paper also discusses some of the methodological problems with the prefered evaluation procedure."
            },
            "slug": "Automatic-Bilingual-Lexicon-Acquisition-Using-of-Sahlgren",
            "title": {
                "fragments": [],
                "text": "Automatic Bilingual Lexicon Acquisition Using Random Indexing of Aligned Bilingual Data"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The approach is cooccurrence-based, and uses the Random Indexing vector space methodology applied to aligned bilingual data, and generate promising results when compared to a manually compiled lexicon."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271549"
                        ],
                        "name": "M. Berry",
                        "slug": "M.-Berry",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berry",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Berry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401512392"
                        ],
                        "name": "G. O'Brien",
                        "slug": "G.-O'Brien",
                        "structuredName": {
                            "firstName": "Gavin",
                            "lastName": "O'Brien",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. O'Brien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7580761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06871028d4e71ceefe879853e9dc2183ea81bb32",
            "isKey": false,
            "numCitedBy": 1755,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Currently, most approaches to retrieving textual materials from scientific databases depend on a lexical match between words in users\u2019 requests and those in or assigned to documents in a database. ..."
            },
            "slug": "Using-Linear-Algebra-for-Intelligent-Information-Berry-Dumais",
            "title": {
                "fragments": [],
                "text": "Using Linear Algebra for Intelligent Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A lexical match between words in users\u2019 requests and those in or assigned to documents in a database helps retrieve textual materials from scientific databases."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 38
                            }
                        ],
                        "text": "Yarowsky (1992) uses 100 words, while Gale et al. (1994) uses 50 words to the left and 50 words to the right, with the argument that this kind of large context is useful for \u201cbroad topic classification.\u201d Sch\u00fctze (1998) uses a 50-word window, whereas Sch\u00fctze & Pedersen (1997) uses a context window spanning 40 words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 38
                            }
                        ],
                        "text": "Yarowsky (1992) uses 100 words, while Gale et al. (1994) uses 50 words to the left and 50 words to the right, with the argument that this kind of large context is useful for \u201cbroad topic classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 337,
                                "start": 38
                            }
                        ],
                        "text": "Yarowsky (1992) uses 100 words, while Gale et al. (1994) uses 50 words to the left and 50 words to the right, with the argument that this kind of large context is useful for \u201cbroad topic classification.\u201d Sch\u00fctze (1998) uses a 50-word window, whereas Sch\u00fctze & Pedersen (1997) uses a context window spanning 40 words. Niwa & Nitta (1994) uses a 10+10-sized window, and as we saw in Chapter 3, the HAL algorithm uses a directional 10-word window."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 64
                            }
                        ],
                        "text": "LSA was developed under the name Latent Semantic Indexing (LSI) (Dumais et al., 1988; Deerwester et al., 1990) in the late 1980s as an extension to the traditional vector-space model in information retrieval."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15273399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11d0506eb9739cfbcc5742d0f5c38cd6f4d96f5f",
            "isKey": true,
            "numCitedBy": 603,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new approach for dealing with the vocabulary problem in human-computer interaction. Most approaches to retrieving textual materials depend on a lexical match between words in users' requests and those in or assigned to database objects. Because of the tremendous diversity in the words people use to describe the same object, lexical matching methods are necessarily incomplete and imprecise [5]. The latent semantic indexing approach tries to overcome these problems by automatically organizing text objects into a semantic structure more appropriate for matching user requests. This is done by taking advantage of implicit higher-order structure in the association of terms with text objects. The particular technique used is singular-value decomposition, in which a large term by text-object matrix is decomposed into a set of about 50 to 150 orthogonal factors from which the original matrix can be approximated by linear combination. Terms and objects are represented by 50 to 150 dimensional vectors and matched against user queries in this \u201csemantic\u201d space. Initial tests find this completely automatic method widely applicable and a promising way to improve users' access to many kinds of textual materials, or to objects and services for which textual descriptions are available."
            },
            "slug": "Using-latent-semantic-analysis-to-improve-access-to-Dumais-Furnas",
            "title": {
                "fragments": [],
                "text": "Using latent semantic analysis to improve access to textual information"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Initial tests find this completely automatic method widely applicable and a promising way to improve users' access to many kinds of textual materials, or to objects and services for which textual descriptions are available."
            },
            "venue": {
                "fragments": [],
                "text": "CHI '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400286782"
                        ],
                        "name": "P. Wiemer-Hastings",
                        "slug": "P.-Wiemer-Hastings",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Wiemer-Hastings",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wiemer-Hastings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331564"
                        ],
                        "name": "Iraide Zipitria",
                        "slug": "Iraide-Zipitria",
                        "structuredName": {
                            "firstName": "Iraide",
                            "lastName": "Zipitria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iraide Zipitria"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 764162,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4c46ac28ac0a23af428437afcb443730c5c3b58",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Rules for Syntax, Vectors for Semantics Peter Wiemer-Hastings (Peter.Wiemer-Hastings@ed.ac.uk) Iraide Zipitria (iraidez@cogsci.ed.ac.uk) University of Edinburgh Division of Informatics 2 Buccleuch Place Edinburgh EH8 9LW Scotland Abstract Latent Semantic Analysis (LSA) has been shown to perform many linguistic tasks as well as humans do, and has been put forward as a model of human linguistic competence. But LSA pays no attention to word order, much less sentence structure. Researchers in Natural Language Processing have made significant progress in quickly and accurately deriv- ing the syntactic structure of texts. But there is little agree- ment on how best to represent meaning, and the representa- tions are brittle and difficult to build. This paper evaluates a model of language understanding that combines information from rule-based syntactic processing with a vector-based se- mantic representation which is learned from a corpus. The model is evaluated as a cognitive model, and as a potential technique for natural language understanding. Motivations Latent Semantic Analysis (LSA) was originally developed for the task of information retrieval, selecting a text which matches a query from a large database (Deerwester, Du- mais, Furnas, Landauer, & Harshman, 1990) 1 . More re- cently, LSA has been evaluated by psychologists as a model for human lexical acquisition (Landauer & Dumais, 1997). It has been applied to other textual tasks and found to gen- erally perform at levels matching human performance. All this despite the fact that LSA pays no attention to word or- der, let alone syntax. This led Landauer to claim that syntax apparently has no contribution to the meaning of a sentence, and may only serve as a working memory crutch for sen- tence processing, or in a stylistic role (Landauer, Laham, Rehder, & Schreiner, 1997). The tasks that LSA has been shown to perform well on can be separated into two groups: those that deal with sin- gle words and those that deal with longer texts. For exam- ple, on the synonym selection part of the TOEFL (Test of English as a Foreign Language), LSA was as accurate at choosing the correct synonym (out of 4 choices) as were successful foreign applicants to US universities (Landauer et al., 1997). For longer texts, Rehder et al (1998) showed that for evaluating author knowledge, LSA does steadily worse for texts shorter than 200 words. More specifically, 1 We do not describe the functioning of the LSA mechanism here. For a complete description, see (Deerwester et al., 1990; Landauer & Dumais, 1997) for 200-word essay segments, LSA accounted for 60% of the variance in human scores. For 60-word essay segments, LSA scores accounted for only 10% of the variance. In work on judging the quality of single-sentence student answers in an intelligent tutoring context, we have shown in previous work that although LSA nears the performance of intermediate-knowledge human raters, it lags far behind expert performance (Wiemer-Hastings, Wiemer-Hastings, & Graesser, 1999b). Furthermore, when we compared LSA to a keyword-based approach, LSA performed only marginally better (Wiemer-Hastings, Wiemer-Hastings, & Graesser, 1999a). This accords with unpublished results on short answer sentences from Walter Kintsch, personal com- munication, January 1999. In the field of Natural Language Processing, the eras of excessive optimism and ensuing disappointment have been followed by study increases in the systems\u2019 ability to pro- cess the syntactic structure of texts with rule-based mecha- nisms. The biggest recent developments have been due to the augmentation of the rules with corpus-derived proba- bilities for when they should be applied (Charniak, 1997; Collins, 1996, 1998, for example). Unfortunately, progress in the area of computing the se- mantic content of texts has not been so successful. Two ba- sic variants of semantic theories have been developed. One is based on some form of logic. The other is represented by connections within semantic networks. In fact, the latter can be simply converted into a logic-based representation. Such theories are brittle in two ways. First, they require every concept and every connection between concepts to be defined by a human knowledge engineer. Multi-purpose representations are not feasible because of the many techni- cal senses of words in every different domain. Second, such representations can not naturally make the graded judge- ments that humans do. Humans can compare any two things (even apples and oranges!), but aside from count- ing feature overlap, logic-based representations have diffi- culty with relationships other than subsumption and \u201chas- as-part\u201d. Due to these various motivations, we are pursuing a two- pronged research project. First, we want to evaluate the combination of a syntactic processing mechanism with an LSA-based semantic representation as a cognitive model of human sentence similarity judgements. Second, we are"
            },
            "slug": "Rules-for-Syntax,-Vectors-for-Semantics-Wiemer-Hastings-Zipitria",
            "title": {
                "fragments": [],
                "text": "Rules for Syntax, Vectors for Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A model of language understanding that combines information from rule-based syntactic processing with a vector-based se- mantic representation which is learned from a corpus is evaluated as a cognitive model, and as a potential technique for natural language understanding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6305097,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6f3250ba47fdb413a0c113cc16d274517864f8ab",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions."
            },
            "slug": "Measures-of-Distributional-Similarity-Lee",
            "title": {
                "fragments": [],
                "text": "Measures of Distributional Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59167516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4471e3117cdac2fae74d305d54b237bb3addd749",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1. Introduction. 2. Semantic Extraction. 3. Sextant. 4. Evaluation. 5. Applications. 6. Conclusion. 1: Preprocesors. 2. Webster Stopword List. 3: Similarity List. 4: Semantic Clustering. 5: Automatic Thesaurus Generation. 6. Corpora Treated. Index."
            },
            "slug": "Explorations-in-automatic-thesaurus-discovery-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Explorations in automatic thesaurus discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The aim of this monograph is to provide a catalog of words and phrases used in ThesaurusGeneration, as well as some examples of other writers' work, which have been used in similar contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889692"
                        ],
                        "name": "S. I. Gallant",
                        "slug": "S.-I.-Gallant",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gallant",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. I. Gallant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43641558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f53c040f8f7ec97fc7cbceeb64550a590d3c705",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Context Vectors are fixed-length vector representations useful for document retrieval and word sense disambiguation. Context vectors were motivated by four goals: \n \n1 \n \nCapture \u201csimilarity of use\u201d among words (\u201ccar\u201d is similar to \u201cauto\u201d, but not similar to \u201chippopotamus\u201d). \n \n \n \n \n2 \n \nQuickly find constituent objects (eg., documents that contain specified words). \n \n \n \n \n3 \n \nGenerate context vectors automatically from an unlabeled corpus. \n \n \n \n \n4 \n \nUse context vectors as input to standard learning algorithms. \n \n \n \n \n \n \nContext Vectors lack, however, a natural way to represent syntax, discourse, or logic. Accommodating all these capabilities into a \u201cGrand Unified Representation\u201d is, we maintain, a prerequisite for solving the most difficult problems in Artificial Intelligence, including natural language understanding."
            },
            "slug": "Context-Vectors:-A-Step-Toward-a-\"Grand-Unified-Gallant",
            "title": {
                "fragments": [],
                "text": "Context Vectors: A Step Toward a \"Grand Unified Representation\""
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Accommodating all these capabilities into a \u201cGrand Unified Representation\u201d is, it is maintained, a prerequisite for solving the most difficult problems in Artificial Intelligence, including natural language understanding."
            },
            "venue": {
                "fragments": [],
                "text": "Hybrid Neural Systems"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192424"
                        ],
                        "name": "A. Lavelli",
                        "slug": "A.-Lavelli",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Lavelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lavelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145077269"
                        ],
                        "name": "F. Sebastiani",
                        "slug": "F.-Sebastiani",
                        "structuredName": {
                            "firstName": "Fabrizio",
                            "lastName": "Sebastiani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Sebastiani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36024018"
                        ],
                        "name": "Roberto Zanoli",
                        "slug": "Roberto-Zanoli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Zanoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Zanoli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 121
                            }
                        ],
                        "text": "The only other experimental investigation of how different contexts influence the word-space model that I am aware of is Lavelli et al. (2004), who compare what they call document occurrence representation (DOR) and term co-occurrence representation (TCOR)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 133
                            }
                        ],
                        "text": "However, word-space algorithms that use a words-by-words co-occurrence matrix normally do not use tfidf-weights (the exception being Lavelli et al. (2004), who I will return to in Chapter 15)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15255229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf7bf55a5307726aecf6753a951661268cf3f5a9",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of content management tasks, including term categorization, term clustering, and automated thesaurus generation, view natural language <i>terms</i> (e.g. words, noun phrases) as first-class objects, i.e. as objects endowed with an internal representation which makes them suitable for explicit manipulation by the corresponding algorithms. The information retrieval (IR) literature has traditionally used an extensional (aka <i>distributional</i>) representation for terms according to which a term is represented by the \"bag of documents\" in which the term occurs. The computational linguistics (CL) literature has independently developed an alternative distributional representation for terms, according to which a term is represented by the \"bag of terms\" that co-occur with it in some document. This paper aims at discovering which of the two representations is most effective, i.e. brings about higher effectiveness once used in tasks that require terms to be explicitly represented and manipulated. We carry out experiments on (i) a term categorization task, and (ii) a term clustering task; this allows us to compare the two different representations in closely controlled experimental conditions. We report the results of experiments in which we categorize/cluster under 42 different classes the terms extracted from a corpus of more than 65,000 documents. Our results show a substantial difference in effectiveness between the two representation styles; we give both an intuitive explanation and an information-theoretic justification for these different behaviours."
            },
            "slug": "Distributional-term-representations:-an-comparison-Lavelli-Sebastiani",
            "title": {
                "fragments": [],
                "text": "Distributional term representations: an experimental comparison"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results show a substantial difference in effectiveness between the two representation styles; this paper gives both an intuitive explanation and an information-theoretic justification for these different behaviours."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17836106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c96fe25817c5fca96719cfa56cdaeeb2d17c93d7",
            "isKey": false,
            "numCitedBy": 518,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a description of research in developing a natural language processing system with modular knowledge sources but strongly interactive processing. The system offers insights into a variety of linguistic phenomena and allows easy testing of a variety of hypotheses. Language interpretation takes place on a activation network which is dynamically created from input, recent context, and long-term knowledge. Initially ambiguous and unstable, the network settles on a single interpretation, using a parallel, analog relaxation process. We also describe a parallel model for the representation of context and of the priming of concepts. Examples illustrating contextual influence on meaning interpretation and \u201csemantic garden path\u201d sentence processing, among other issues, are included."
            },
            "slug": "Massively-Parallel-Parsing:-A-Strongly-Interactive-Waltz-Pollack",
            "title": {
                "fragments": [],
                "text": "Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work describes a parallel model for the representation of context and of the priming of concepts in a natural language processing system with modular knowledge sources but strongly interactive processing."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34729490"
                        ],
                        "name": "W. Charles",
                        "slug": "W.-Charles",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Charles",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Charles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 32
                            }
                        ],
                        "text": "Almost 30 years later, Miller & Charles (1991) repeated Rubenstein\u2019s & Goodenough\u2019s experiment using 30 of the original 65 noun pairs, and reported remarkably similar results."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 43
                            }
                        ],
                        "text": ", Rubenstein & Goodenough (1965), Miller & Charles (1991), and Leacock et al. (1996). Another possibility would be to use a smaller context region consisting of only a couple of consecutive words, as in the example with collocations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "For example, Miller & Charles (1991) point out that people instinctively make judgments about semantic similarity when asked to do so, without the need for further explanations of the concept; people appear to instinctively understand what semantic similarity is, and they make their judgments quickly and without difficulties."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 87
                            }
                        ],
                        "text": "\u201d Similar claims are made by Rubenstein & Goodenough (1965), Grefenstette (1992b), and Charles (2000). These claims that paradigmatic uses of context capture semantic similarity better than syntagmatic ones do not seem compatible with the results presented in the current investigation."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 462,
                                "start": 43
                            }
                        ],
                        "text": ", Rubenstein & Goodenough (1965), Miller & Charles (1991), and Leacock et al. (1996). Another possibility would be to use a smaller context region consisting of only a couple of consecutive words, as in the example with collocations. However, a serious problem with using such a small context to collect syntagmatic information is that very few words \u2014 basically only collocations \u2014 co-occur often within a small context region. In fact, as, e.g., Picard (1999) points out, the majority of terms never co-occur."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 43
                            }
                        ],
                        "text": ", Rubenstein & Goodenough (1965), Miller & Charles (1991), and Leacock et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 152
                            }
                        ],
                        "text": "However, it is perfectly feasible to assume that any sufficiently literate person can learn the meaning of a new word through reading only, as Miller & Charles (1991) observe."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 81
                            }
                        ],
                        "text": "Other experimental validations of the distributional hypothesis include Miller & Charles (2000) and McDonald & Ramscar (2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143099365,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "247c1aa552399767d39a451bcb8a82ca6071040c",
            "isKey": true,
            "numCitedBy": 50,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The relation between similarity and dissimilarity of meaning and similarity of context was analyzed for synonymous nouns. New semantic similarity and dissimilarity rating tests with an empirically determined series of linguistic anchors and conventional, arbitrarily anchored semantic similarity ratings were compared. Contextual similarity was elicited by a sorting test based on substitution and yielding d-primes. The study found reliable correlations between the d-primes and the different ratings for semantic similarity and dissimilarity of the synonymous nouns across a wide continuum of meaning. The data strongly supported a contextual hypothesis of meaning. The data endorsed the claim that people abstract a contextual representation from experiencing the multiple natural linguistic contexts of a word. Semantic similarity and dissimilarity rating formats with an empirically chosen series of linguistic anchors and a sorting test of contextual similarity yielded stronger support for a contextual hypothesis than did alternative methods of eliciting lexical and contextual similarity."
            },
            "slug": "Contextual-correlates-of-meaning-Charles",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of meaning"
            },
            "venue": {
                "fragments": [],
                "text": "Applied Psycholinguistics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 326720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75a2a3ae140b86365687453a8a8ab10cb4e1db63",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a very simple and effective approach to using parallel corpora for automatic bilingual lexicon acquisition. The approach, which uses the Random Indexing vector space methodology, is based on finding correlations between terms based on their distributional characteristics. The approach requires a minimum of preprocessing and linguistic knowledge, and is efficient, fast and scalable. In this paper, we explain how our approach differs from traditional cooccurrence-based word alignment algorithms, and we demonstrate how to extract bilingual lexica using the Random Indexing approach applied to aligned parallel data. The acquired lexica are evaluated by comparing them to manually compiled gold standards, and we report overlap of around 60%. We also discuss methodological problems with evaluating lexical resources of this kind."
            },
            "slug": "Automatic-bilingual-lexicon-acquisition-using-of-Sahlgren-Karlgren",
            "title": {
                "fragments": [],
                "text": "Automatic bilingual lexicon acquisition using random indexing of parallel corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper presents a very simple and effective approach to using parallel corpora for automatic bilingual lexicon acquisition, which uses the Random Indexing vector space methodology, and explains how this approach differs from traditional cooccurrence-based word alignment algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2172308"
                        ],
                        "name": "Todd A. Letsche",
                        "slug": "Todd-A.-Letsche",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Letsche",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Todd A. Letsche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8291212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6936964a235b30fd5970e7d7663c8a27a429411",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for fully automated cross-language document retrieval in which no query translation is required. Queries in one language can retrieve documents in other languages (as well as the original language). This is accomplished by a method that automatically constructs a multilingual semantic space using Latent Semantic Indexing (LSI). Strong test results for the cross-language LSI (CLLSI) method are presented for a new French-English collection. We also provide evidence that this automatic method performs comparably to a retrieval method based on machine translation (MT-LSI), and explore several practical training methods. By all available measures, CL-LSI performs quite well and is widely applicable."
            },
            "slug": "Automatic-Cross-Language-Retrieval-Using-Latent-Dumais-Letsche",
            "title": {
                "fragments": [],
                "text": "Automatic Cross-Language Retrieval Using Latent Semantic Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A method for fully automated cross-language document retrieval in which no query translation is required and this automatic method performs comparably to a retrieval method based on machine translation (MT-LSI)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787816"
                        ],
                        "name": "C. Isbell",
                        "slug": "C.-Isbell",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Isbell",
                            "middleNames": [
                                "Lee"
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Isbell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10099769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f91d7674731a88200bad54ff0223f7d60ceb6bc2",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The task in text retrieval is to find the subset of a collection of documents relevant to a user's information request, usually expressed as a set of words. Classically, documents and queries are represented as vectors of word counts. In its simplest form, relevance is defined to be the dot product between a document and a query vector-a measure of the number of common terms. A central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query. Linear dimensionality reduction has been proposed as a technique for extracting underlying structure from the document collection. In some domains (such as vision) dimensionality reduction reduces computational complexity. In text retrieval it is more often used to improve retrieval performance. We propose an alternative and novel technique that produces sparse representations constructed from sets of highly-related words. Documents and queries are represented by their distance to these sets, and relevance is measured by the number of common clusters. This technique significantly improves retrieval performance, is efficient to compute and shares properties with the optimal linear projection operator and the independent components of documents."
            },
            "slug": "Restructuring-Sparse-High-Dimensional-Data-for-Isbell-Viola",
            "title": {
                "fragments": [],
                "text": "Restructuring Sparse High Dimensional Data for Effective Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes an alternative and novel technique that produces sparse representations constructed from sets of highly-related words that significantly improves retrieval performance, is efficient to compute and shares properties with the optimal linear projection operator and the independent components of documents."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 200
                            }
                        ],
                        "text": "Other attempts at using linguistic information for computing distributional similarity between words include Hindle (1990), who used predicate\u2013argument structure to determine the similarity of nouns; Hearst (1992), who extracted hyponyms by using lexical\u2013syntactic templates; Ruge (1992), who used head\u2013modifier relations for extracting similar words; and Grefenstette (1992a, 1992b, 1993), who also used syntactic context to measure similarity between words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14558325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fed3002240ebfcfbad4ff472748f46191e17e4e0",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words."
            },
            "slug": "Evaluation-Techniques-for-Automatic-Semantic-and-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Evaluation Techniques for Automatic Semantic Extraction: Comparing Syntactic and Window Based Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An evaluation method using gold standards, i.e., pre-existing hand-compiled resources, is proposed as a means of comparing extraction techniques, which compare two semantic extraction techniques which produce similar word lists."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop On The Acquisition Of Lexical Knowledge From Text"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72537573"
                        ],
                        "name": "Gregory Grefenstetti",
                        "slug": "Gregory-Grefenstetti",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Grefenstetti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61937302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ded8f7265b7208d4fbca21c1fdaf2fc01ac3c512",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words."
            },
            "slug": "Evaluation-techniques-for-automatic-semantic-and-Grefenstetti",
            "title": {
                "fragments": [],
                "text": "Evaluation techniques for automatic semantic extraction: comparing syntactic and window based approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An evaluation method using gold standards, i.e., pre-existing hand-compiled resources, is proposed as a means of comparing extraction techniques, which compare two semantic extraction techniques which produce similar word lists."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5509836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e517e1645708e7b050787bb4734002ea194a1958",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing)."
            },
            "slug": "Mining-the-Web-for-Synonyms:-PMI-IR-versus-LSA-on-Turney",
            "title": {
                "fragments": [],
                "text": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL"
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144514944"
                        ],
                        "name": "W. Lowe",
                        "slug": "W.-Lowe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2260968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e43f8a9a1ff712bdb3192d6e66b04effaf56597f",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "McDonald and Lowe [15] showed that cosines in a semantic space of several hundred dimensions reflect human priming results for a wide range of semantic and associatively related words [16, Exp.2]. Previously, Lowe [II, 10] argued that the intrinsic dimensionality of semantic space is much lower, and that high-dimensional structure can be effectively captured in just two dimensions as the surface of a neural map. This paper provides a replication of McDonald and Lowe\u2019s results in two dimensions using the Generative Topographic Mapping [2], a statistically motivated neural network architecture for topographic maps."
            },
            "slug": "What-is-the-Dimensionality-of-Human-Semantic-Space-Lowe",
            "title": {
                "fragments": [],
                "text": "What is the Dimensionality of Human Semantic Space?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper provides a replication of McDonald and Lowe\u2019s results in two dimensions using the Generative Topographic Mapping [2], a statistically motivated neural network architecture for topographic maps."
            },
            "venue": {
                "fragments": [],
                "text": "NCPW"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 110
                            }
                        ],
                        "text": "Other early attempts at deriving context vectors automatically from the contexts in which words occur include Wilks et al. (1990), Sch\u00fctze (1992), Pereira et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6987562,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c2195bdf698caba4c3b9753cb0ce5acddff5b73",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an unsupervised algorithm for placing unknown words into a taxonomy and evaluates its accuracy on a large and varied sample of words. The algorithm works by first using a large corpus to find semantic neighbors of the unknown word, which we accomplish by combining latent semantic analysis with part-of-speech information. We then place the unknown word in the part of the taxonomy where these neighbors are most concentrated, using a class-labelling algorithm developed especially for this task. This method is used to reconstruct parts of the existing Word-Net database, obtaining results for common nouns, proper nouns and verbs. We evaluate the contribution made by part-of-speech tagging and show that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy."
            },
            "slug": "Unsupervised-methods-for-developing-taxonomies-by-Widdows",
            "title": {
                "fragments": [],
                "text": "Unsupervised methods for developing taxonomies by combining syntactic and statistical information"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An unsupervised algorithm for placing unknown words into a taxonomy and its accuracy on a large and varied sample of words is evaluated and it is shown that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144102674"
                        ],
                        "name": "C. Papadimitriou",
                        "slug": "C.-Papadimitriou",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadimitriou",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadimitriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145503401"
                        ],
                        "name": "P. Raghavan",
                        "slug": "P.-Raghavan",
                        "structuredName": {
                            "firstName": "Prabhakar",
                            "lastName": "Raghavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Raghavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145955092"
                        ],
                        "name": "H. Tamaki",
                        "slug": "H.-Tamaki",
                        "structuredName": {
                            "firstName": "Hisao",
                            "lastName": "Tamaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Tamaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 183
                            }
                        ],
                        "text": "The near-orthogonality of random directions in high-dimensional spaces is exploited by a number of dimensionality-reduction techniques that includes methods such as Random Projection (Papadimitriou et al., 1998), Random Mapping (Kaski,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 92
                            }
                        ],
                        "text": "forming a w\u00d7 r random matrix and projecting the original w\u00d7 d matrix through it \u2014 is O(zrw) (Papadimitriou et al., 1998; Bingham & Mannila, 2001), where r is the dimensionality of the vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1479546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "546fdb984bd63214dac8f552ef8d8ae6fa7c7d1a",
            "isKey": false,
            "numCitedBy": 1050,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent semantic indexing LSI is an information retrieval technique based on the spectral analysis of the term document matrix whose empirical success had heretofore been without rigorous prediction and explanation We prove that under certain conditions LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance We also propose the technique of random projection as a way of speeding up LSI We complement our theorems with encouraging experimental results We also argue that our results may be viewed in a more general framework as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative ltering"
            },
            "slug": "Latent-semantic-indexing:-a-probabilistic-analysis-Papadimitriou-Raghavan",
            "title": {
                "fragments": [],
                "text": "Latent semantic indexing: a probabilistic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that under certain conditions LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance."
            },
            "venue": {
                "fragments": [],
                "text": "PODS '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 12
                            }
                        ],
                        "text": "In fact, as Harris (2001) points out, the term \u201cstructure\u201d only occurs three times in the Cours, and never in a structuralist sense."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 109
                            }
                        ],
                        "text": "Other attempts at using linguistic information for computing distributional similarity between words include Hindle (1990), who used predicate\u2013argument structure to determine the similarity of nouns; Hearst (1992), who extracted hyponyms by using lexical\u2013syntactic templates; Ruge (1992), who used head\u2013modifier relations for extracting similar words; and Grefenstette (1992a, 1992b, 1993), who also used syntactic context to measure similarity between words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 15
                            }
                        ],
                        "text": "\u201d According to Harris (2001), it was Hjelmslev who coined the term paradigmatic relations as a substitute for Saussure\u2019s \u201crapports associatifs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 86680084,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "decd9bc0385612bdf936928206d83730718e737e",
            "isKey": false,
            "numCitedBy": 2644,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "For the purposes of the present discussion, the term structure will be used in the following non-rigorous sense: A set of phonemes or a set of data is structured in respect to some feature, to the extent that we can form in terms of that feature some organized system of statements which describes the members of the set and their interrelations (at least up to some limit of complexity). In this sense, language can be structured in respect to various independent features. And whether it is structured (to more than a trivial extent) in respect to, say, regular historical change, social intercourse, meaning, or distribution - or to what extent it is structured in any of these respects - is a matter decidable by investigation. Here we will discuss how each language can be described in terms of a distributional structure, i.e. in terms of the occurrence of parts (ultimately sounds) relative to other parts, and how this description is complete without intrusion of other features such as history or meaning. It goes without saying that other studies of language - historical, psychological, etc.-are also possible, both in relation to distributional structure and independently of it."
            },
            "slug": "Distributional-Structure-Harris",
            "title": {
                "fragments": [],
                "text": "Distributional Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This discussion will discuss how each language can be described in terms of a distributional structure, i.e. in Terms of the occurrence of parts relative to other parts, and how this description is complete without intrusion of other features such as history or meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7990402"
                        ],
                        "name": "T. J\u00e4rvinen",
                        "slug": "T.-J\u00e4rvinen",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "J\u00e4rvinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J\u00e4rvinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828241"
                        ],
                        "name": "Rickard C\u00f6ster",
                        "slug": "Rickard-C\u00f6ster",
                        "structuredName": {
                            "firstName": "Rickard",
                            "lastName": "C\u00f6ster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rickard C\u00f6ster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 574457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4158fd59f90cfe04b125daa1374893222fbaa00",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This experiment tests a simple, scalable, and effective approach to building a domain-specific translation lexicon using distributional statistics over parallellized bilingual corpora. A bilingual lexicon is extracted from aligned Swedish-French data, used to translate CLEF topics from Swedish to French, which resulting French queries are then in turn used to retrieve documents from the French language CLEF collection. The results give 34 of fifty queries on or above median for the \u201cprecision at 1000 documents\u201d recall oriented score; with many of the errors possible to handle by the use of string-matching and cognate search. We conclude that the approach presented here is a simple and efficient component in an automatic query translation system."
            },
            "slug": "Dynamic-Lexica-for-Query-Translation-Karlgren-Sahlgren",
            "title": {
                "fragments": [],
                "text": "Dynamic Lexica for Query Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This experiment tests a simple, scalable, and effective approach to building a domain-specific translation lexicon using distributional statistics over parallellized bilingual corpora and concludes that the approach presented here is a simple and efficient component in an automatic query translation system."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756977"
                        ],
                        "name": "S. Small",
                        "slug": "S.-Small",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Small",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Small"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524582"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69911446"
                        ],
                        "name": "Michael Tanennaus",
                        "slug": "Michael-Tanennaus",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tanennaus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Tanennaus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 138
                            }
                        ],
                        "text": "Osgood\u2019s feature-space approach was the major influence for early connectionist research that used distributed representations of meaning (Smith & Medin, 1981; Cottrell & Small, 1983; Small et al., 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60652039,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fe85b7b4d9bf729ffe6958d24f9c53bac9d8ca2f",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThe most frequently used words in English are highly ambiguous; for example, Webster's Ninth New Collegiate Dictionary lists 94 meanings for the word \"run\" as a verb alone. Yet people rarely notice this ambiguity. Solving this puzzle has commanded the efforts of cognitive scientists for many years. The solution most often identified is \"context\": we use the context of utterance to determine the proper meanings of words and sentences. The problem then becomes specifying the nature of context and how it interacts with the rest of an understanding system. The difficulty becomes especially apparent in the attempt to write a computer program to understand natural language. Lexical ambiguity resolution (LAR), then, is one of the central problems in natural language and computational semantics research. \n \nA collection of the best research on LAR available, this volume offers eighteen original papers by leading scientists. Part I, Computer Models, describes nine attempts to discover the processes necessary for disambiguation by implementing programs to do the job. Part II, Empirical Studies, goes into the laboratory setting to examine the nature of the human disambiguation mechanism and the structure of ambiguity itself. \n \nA primary goal of this volume is to propose a cognitive science perspective arising out of the conjunction of work and approaches from neuropsychology, psycholinguistics, and artificial intelligence--thereby encouraging a closer cooperation and collaboration among these fields. \n \nLexical Ambiguity Resolution is a valuable and accessible source book for students and cognitive scientists in AI, psycholinguistics, neuropsychology, or theoretical linguistics."
            },
            "slug": "Lexical-Ambiguity-Resolution:-Perspectives-from-and-Small-Cottrell",
            "title": {
                "fragments": [],
                "text": "Lexical Ambiguity Resolution: Perspectives from Psycholinguistics, Neuropsychology, and Artificial Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A cognitive science perspective arising out of the conjunction of work and approaches from neuropsychology, psycholinguistics, and artificial intelligence is proposed, encouraging a closer cooperation and collaboration among these fields."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380069640"
                        ],
                        "name": "L. Baker",
                        "slug": "L.-Baker",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Baker",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6146974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e733226b881f11f25c87e8bac8d602ba3d9c220e",
            "isKey": false,
            "numCitedBy": 843,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the application of Distributional Clustering [20] to document classification. This approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionalityreduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensional&y by three orders of magnitude and lose only 2% accuracy-significantly better than Latent Semantic Indexing [6], class-based clustering [l], feature selection by mutual information [23], or Markov-blanket-based feature selection [13]. We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering."
            },
            "slug": "Distributional-clustering-of-words-for-text-Baker-McCallum",
            "title": {
                "fragments": [],
                "text": "Distributional clustering of words for text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This paper describes the application of Distributional Clustering to document classification and shows that it can reduce the feature dimensional&y by three orders of magnitude and lose only 2% accuracy-significantly better than Latent Semantic Indexing, class-based clustering, feature selection by mutual information, or Markov-blanket-based feature selection."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13553551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8ef47eec25e3d80574b5090054c30ead6d0b678",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes the Latent Semantic Indexing (LSI) approach, an extension of the vector retrieval method (e.g., Salton & McGill, 1983) and the use of singular-value decomposition (SUP) applied to the TREC collection. The existing LSI/SVD software was used for analyzing the training and test collectioans, and for query processing and retrieval"
            },
            "slug": "LSI-meets-TREC:-A-Status-Report-Dumais",
            "title": {
                "fragments": [],
                "text": "LSI meets TREC: A Status Report"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Describes the Latent Semantic Indexing approach, an extension of the vector retrieval method and the use of singular-value decomposition applied to the TREC collection."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2532711"
                        ],
                        "name": "Jan Kristoferson",
                        "slug": "Jan-Kristoferson",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Kristoferson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Kristoferson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078819344"
                        ],
                        "name": "Anders Holst",
                        "slug": "Anders-Holst",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Holst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders Holst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 83
                            }
                        ],
                        "text": "Yet another inherently different word-space implementation is Random Indexing (RI) (Kanerva et al., 2000; Karlgren & Sahlgren, 2001; Sahlgren, 2005), which was developed at the Swedish Institute of Computer Science (SICS) based on Pentti Kanerva\u2019s work on sparse distributed memory (Kanerva, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60571601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932c8b99ef910bedd0f49d889230aba308004e0a",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Random Indexing of Text Samples for Latent Semantic Analysis Pentti Kanerva Jan Kristoferson Anders Holst kanerva@sics.se, janke@sics.se, aho@sics.se RWCP Theoretical Foundation SICS Laboratory Swedish Institute of Computer Science, Box 1263, SE-16429 Kista, Sweden Latent Semantic Analysis is a method of computing vectors|and it has several randomly placed ; 1s and high-dimensional semantic vectors, or context vectors, 1s, with the rest 0s (e.g., four each of ; 1 and 1, or for words from their co-occurrence statistics. An exper- eight non-0s in 1,800, instead of one non-0 in 30,000 iment by Landauer & Dumais (1997) covers a vocabu- as above). Thus, we would accumulate the same data lary of 60,000 words (unique letter strings delimited by into a 60,000 1,800 words-by-contexts matrix instead word-space characters) in 30,000 contexts (text samples of 60,000 30,000. or \\documents of about 150 words each). The data are Our method has been veried with dierent data, a rst collected into a 60,000 30,000 words-by-contexts ten-million-word \\TASA corpus consisting of a 79,000- co-occurrence matrix, with each row representing a word word vocabulary (when words are truncated after the 8th and each column representing a text sample so that each character) in 37,600 text samples. The data were accu- entry gives the frequency of a given word in a given mulated into a 79,000 1,800 words-by-contexts matrix, text sample. The frequencies are normalized, and the which was normalized by thresholding into a matrix of normalized matrix is transformed with Singular-Value ; 1s, 0s, and 1s. The unnormalized 1,800-dimensional Decomposition (SVD) reducing its original 30,000 doc- context vectors gave 35{44% correct in the TOEFL test ument dimensions into a much smaller number of latent and the normalized ones gave 48{51% correct, which cor- dimensions, 300 proving to be optimal. Thus words are respond to Landauer & Dumais' 36% for their normal- represented by 300-dimensional semantic vectors. ized 30,000-dimensional vectors before SVD, for a dier- The point in all of this is that the vectors capture ent corpus (see above). Our words-by-contexts matrix meaning. Landauer and Dumais demonstrate it with a can be transformed further, for example with SVD as in synonym test called TOEFL (for \\Test Of English as a LSA, except that the matrix is much smaller. Mathematically, the 30,000- or 37,600-dimensional in- Foreign Language ). For each test word, four alterna- dex vectors are orthogonal, whereas the 1,800-dimen- tives are given, and the \\contestant is asked to nd the one that's the most synonymous. Choosing at random sional ones are only nearly orthogonal. They seem to would yield 25% correct. However, when the seman- work just as well, in addition to which they are more tic vector for the test word is compared to the seman- \\brainlike and less aected by the number of text sam- tic vectors for the four alternatives, it correlates most ples (1,800-dimensional index vectors can cover a wide- highly with the correct alternative in 64% of the cases. ranging number of text samples). We have used such However, when the same test is based on the 30,000- vectors also to index words in narrow context windows, dimensional vectors before SVD, the result is not nearly getting 62{70% correct, and conclude that random in- as good: only 36% correct. The authors conclude that dexing deserves to be studied and understood more fully. Acknowledgments. This research is supported by the reorganization of information by SVD somehow cor- Japan's Ministry of International Trade and Industry responds to human psychology. under the Real World Computing Partnership We have studied high-dimensional random distributed (MITI) (RWCP) The TASA corpus and 80 TOEFL representations, as models of brainlike representation of test items program. were made available to us by courtesy of Pro- information (Kanerva, 1994; Kanerva & Sj\u007fodin, 1999). fessor Thomas Landauer, University of Colorado. In this poster we report on the use of such a repre- sentation to reduce the dimensionality of the original words-by-contexts matrix. The method can be explained Kanerva, P. (1994). References The Spatter Code for encoding by looking at the 60,000 30,000 matrix of frequencies concepts at many levels. In M. Marinaro and P. G. above. Assume that each text sample is represented by a Morasso (eds.), ICANN '94, Proc. Int'l Conference 30,000-bit vector with a single 1 marking the place of the on Articial Neural Networks (Sorrento, Italy), vol. 1, sample in a list of all samples, and call it the sample's pp. 226{229. London: Springer-Verlag. index vector (i.e., the n th bit of the index vector for the Kanerva, P., and Sj\u007fodin, G. (1999). Stochastic Pattern n th text sample is 1|the representation is unitary or lo- Computing. Proc. 2000 Real World Computing Sym- cal). Then the words-by-contexts matrix of frequencies bosium (Report TR-99-002, pp. 271{276). Tsukuba- can be gotten by the following procedure: every time city, Japan: Real World Computing Partnership. that the word w occurs in the n th text sample, the n th index vector is added to the row for the word w . Landauer, T. K., and Dumais, S. T. (1997). A solution We use the same procedure for accumulating a words- to Plato's problem: The Latent Semantic Analysis by-contexts matrix, except that the index vectors are theory of the acquisition, induction, and representa- not unitary. A text-sample's index vector is \\small tion of knowledge. Psychological Review 104 (2):211{ by comparison|we have used 1,800-dimensional index"
            },
            "slug": "Random-indexing-of-text-samples-for-latent-semantic-Kanerva-Kristoferson",
            "title": {
                "fragments": [],
                "text": "Random indexing of text samples for latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Random Indexing of Text Samples for Latent Semantic Analysis Pentti Kanerva Jan Kristoferson Anders Holst kanerva@sics.se, aho@sic.se RWCP Theoretical Foundation SICS Laboratory Swedish Institute of Computer Science, Box 1263, SE-16429 Kista, Sweden LatentSemantic Analysis is a method of computing vectors that captures ent corpus and the vectors capture words-by-contexts matrix meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1144461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68dd4b89ce1407372a29d05ca9e4e1a2e0513617",
            "isKey": false,
            "numCitedBy": 5789,
            "numCiting": 210,
            "paperAbstract": {
                "fragments": [],
                "text": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."
            },
            "slug": "A-Solution-to-Plato's-Problem:-The-Latent-Semantic-Landauer-Dumais",
            "title": {
                "fragments": [],
                "text": "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 47
                            }
                        ],
                        "text": "\u201d Similarity between words was then defined as (Lin, 1998b):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5659557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc0c3033ea7d4e19e1f5ac71934759507e126162",
            "isKey": false,
            "numCitedBy": 4466,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains."
            },
            "slug": "An-Information-Theoretic-Definition-of-Similarity-Lin",
            "title": {
                "fragments": [],
                "text": "An Information-Theoretic Definition of Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work presents an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model and demonstrates how this definition can be used to measure the similarity in a number of different domains."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46973696"
                        ],
                        "name": "D. Nelson",
                        "slug": "D.-Nelson",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Nelson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2327242"
                        ],
                        "name": "C. McEvoy",
                        "slug": "C.-McEvoy",
                        "structuredName": {
                            "firstName": "Cathy",
                            "lastName": "McEvoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. McEvoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2853950"
                        ],
                        "name": "T. A. Schreiber",
                        "slug": "T.-A.-Schreiber",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Schreiber",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. A. Schreiber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8890546,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "78466285abd953288db4a9832e4616e1249dba19",
            "isKey": false,
            "numCitedBy": 1983,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Preexisting word knowledge is accessed in many cognitive tasks, and this article offers a means for indexing this knowledge so that it can be manipulated or controlled. We offer free association data for 72,000 word pairs, along with over a million entries of related data, such as forward and backward strength, number of competing associates, and printed frequency. A separate file contains the 5,019 normed words, their statistics, and thousands of independently normed rhyme, stem, and fragment cues. Other files providen \u00d7 n associative networks for more than 4,000 words and a list of idiosyncratic responses for each normed word. The database will be useful for investigators interested in cuing, priming, recognition, network theory, linguistics, and implicit testing applications. They also will be useful for evaluating the predictive value of free association probabilities as compared with other measures, such as similarity ratings and co-occurrence norms. Of several procedures for measuring preexisting strength between two words, the best remains to be determined. The norms may be downloaded fromwww.psychonomic.org/archive/."
            },
            "slug": "The-University-of-South-Florida-free-association,-Nelson-McEvoy",
            "title": {
                "fragments": [],
                "text": "The University of South Florida free association, rhyme, and word fragment norms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The database will be useful for investigators interested in cuing, priming, recognition, network theory, linguistics, and implicit testing applications, and for evaluating the predictive value of free association probabilities as compared with other measures, such as similarity ratings and co-occurrence norms."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods, instruments, & computers : a journal of the Psychonomic Society, Inc"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756977"
                        ],
                        "name": "S. Small",
                        "slug": "S.-Small",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Small",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Small"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524582"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144171250"
                        ],
                        "name": "M. Tanenhaus",
                        "slug": "M.-Tanenhaus",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tanenhaus",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tanenhaus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59683898,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "e34d37fbd055ecc3f61c680bf172be9de76da8c0",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Lexical-ambiguity-resolution-Small-Cottrell",
            "title": {
                "fragments": [],
                "text": "Lexical ambiguity resolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 70
                            }
                        ],
                        "text": "The applicability of LSA for information retrieval is well documented (Deerwester et al., 1990; Dumais, 1993; Dumais et al., 1997; Jiang & Littman, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 64
                            }
                        ],
                        "text": "LSA was developed under the name Latent Semantic Indexing (LSI) (Dumais et al., 1988; Deerwester et al., 1990) in the late 1980s as an extension to the traditional vector-space model in information retrieval."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7019,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39952106"
                        ],
                        "name": "Michael B. W. Wolfe",
                        "slug": "Michael-B.-W.-Wolfe",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wolfe",
                            "middleNames": [
                                "B.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael B. W. Wolfe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144029655"
                        ],
                        "name": "M. E. Schreiner",
                        "slug": "M.-E.-Schreiner",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Schreiner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Schreiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2197285"
                        ],
                        "name": "B. Rehder",
                        "slug": "B.-Rehder",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Rehder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rehder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444937"
                        ],
                        "name": "Darrell Laham",
                        "slug": "Darrell-Laham",
                        "structuredName": {
                            "firstName": "Darrell",
                            "lastName": "Laham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darrell Laham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036336"
                        ],
                        "name": "P. Foltz",
                        "slug": "P.-Foltz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Foltz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Foltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62526953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0cb6eb8776d372f76be62c95252162086b46eff",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This study examines the hypothesis that the ability of a reader to learn from text depends on the match between the background knowledge of the reader and the difficulty of the text information. Latent Semantic Analysis (LSA), a statistical technique that represents the content of a document as a vector in high\u2010dimensional semantic space based on a large text corpus, is used to predict how much readers will learn from texts based on the estimated conceptual match between their topic knowledge and the text information. Participants completed tests to assess their knowledge of the human heart and circulatory system, then read one of four texts that ranged in difficulty from elementary to medical school level, then completed the tests again. Results show a nonmonotonic relation in which learning was greatest for texts that were neither too easy nor too difficult. LSA proved as effective at predicting learning from these texts as traditional knowledge assessment measures. For these texts, optimal assignment o..."
            },
            "slug": "Learning-from-text:-Matching-readers-and-texts-by-Wolfe-Schreiner",
            "title": {
                "fragments": [],
                "text": "Learning from text: Matching readers and texts by latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results show a nonmonotonic relation in which learning was greatest for texts that were neither too easy nor too difficult, and LSA proved as effective at predicting learning from these texts as traditional knowledge assessment measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799021"
                        ],
                        "name": "Michael Ramscar",
                        "slug": "Michael-Ramscar",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Ramscar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Ramscar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 17
                            }
                        ],
                        "text": "Examples include Nakov et al. (2001), who studied the effects of using different weighting schemes in LSA; Bingham and Mannila (2001), who investigated the effects of using different dimensionality-reduction techniques; and Weeds et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13161362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "878e4a4bcbd9938269cc44e28888e7492bd1df93",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Testing the Distributional Hypothesis: The Influence of Context on Judgements of Semantic Similarity Scott McDonald (scottm@cogsci.ed.ac.uk) Michael Ramscar (michael@cogsci.ed.ac.uk) Institute for Communicating and Collaborative Systems, University of Edinburgh 2 Buccleuch Place, Edinburgh EH8 9LW Scotland Abstract Distributional information has recently been implicated as playing an important role in several aspects of lan- guage ability. Learning the meaning of a word is thought to be dependent, at least in part, on exposure to the word in its linguistic contexts of use. In two experiments, we manipulated subjects\u2019 contextual experience with mar- ginally familiar and nonce words. Results showed that similarity judgements involving these words were af- fected by the distributional properties of the contexts i n which they were read. The accrual of contextual experi- ence was simulated in a semantic space model, by succes- sively adding larger amounts of experience in the form of item-in-context exemplars sampled from the British National Corpus. The experiments and the simulation provide support for the role of distributional information in developing representations of word meaning. The Distributional Hypothesis The basic human ability of language understanding \u2013 mak- ing sense of another person\u2019s utterances \u2013 does not develop in isolation from the environment. There is a growing body of research suggesting that distributional information plays a more powerful role than previously thought in a number of aspects of language processing. The exploita- tion of statistical regularities in the linguistic environment has been put forward to explain how language learners accomplish tasks from segmenting speech to bootstrap- ping word meaning. For example, Saffran, Aslin and Newport (1996) have demonstrated that infants are highly sensitive to simple conditional probability statistics, indicating how the ability to segment the speech stream into words may be realised. Adults, when faced with the task of identifying the word boundaries in an artificial language, also appear able to readily exploit such statistics (Saffran, Newport & Aslin, 1996). Redington, Chater and Finch (1998) have proposed that distributional information may contribute to the acquisition of syntactic knowledge by children. Useful information about the similarities and differences in the meaning of words has also been shown to be present in simple distributional statistics (e.g., Landauer & Dumais, 1997; McDonald, 2000). Based on the convergence of these recent studies into a cognitive role for distributional information in explaining language ability, we call the general principle under exploration the Distributional Hypothesis. The purpose of the present paper is to further test the distributional hypothesis, by examining the influence of context on similarity judgements involving marginally familiar and novel words. Our investigations are framed under the \u2018semantic space\u2019 approach to representing word meaning, to which we turn next. Distributional Models of Word Meaning The distributional hypothesis has provided the motivation for a class of objective statistical methods for representing meaning. Although the surge of interest in the approach arose in the fields of computational linguistics and infor- mation retrieval (e.g., Schutze, 1998; Grefenstette, 1994), where large-scale models of lexical semantics are crucial for tasks such as word sense disambiguation, high- dimensional \u2018semantic space\u2019 models are also useful tools for investigating how the brain represents the meaning of words. Word meaning can be considered to vary along many dimensions; semantic space models attempt to capture this variation in a coherent way, by positioning words in a geometric space. How to determine what the crucial dimensions are has been a long-standing problem; a recent and fruitful approach to this issue has been to label the dimensions of semantic space with words. A word is located in the space according to the degree to which it co- occurs with each of the words labelling the dimensions of the space. Co-occurrence frequency information is extracted from a record of language experience \u2013 a large corpus of natural language. Using this approach, two words that tend to occur in similar linguistic contexts \u2013 that is, they are distributionally similar \u2013 will be positioned closer together in semantic space than two words which are not as distributionally similar. Such simple distributional knowledge has been implicated in a variety of language processing behaviours, such as lexical priming (e.g., Lowe & McDonald, 2000; Lund, Burgess & Atchley, 1995; McDonald & Lowe, 1998), synonym selection (Landauer & Dumais, 1997), retrieval in analogical reason- ing (Ramscar & Yarlett, 2000) and judgements of semantic similarity (McDonald, 2000). Contextual co-occurrence, the fundamental relationship underlying the success of the semantic space approach to representing word meaning, can be defined in a number of ways. Perhaps the simplest (and the approach taken in the majority of the studies cited above) is to define co- occurrence in terms of a \u2018context window\u2019: the co-occur-"
            },
            "slug": "Testing-the-Distributioanl-Hypothesis:-The-of-on-of-McDonald-Ramscar",
            "title": {
                "fragments": [],
                "text": "Testing the Distributioanl Hypothesis: The influence of Context on Judgements of Semantic Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The general principle under exploration the Distributional Hypothesis, which combines the convergence of these recent studies into a cognitive role for distributional information in explaining language ability, is called."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30882434"
                        ],
                        "name": "Justin Picard",
                        "slug": "Justin-Picard",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Picard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 63
                            }
                        ],
                        "text": "occur in the same document) better than a syntagmatic use, and Picard (1999) observes that a syntagmatic use of context can only be used for very frequent words, while a paradigmatic use may be applied for all words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10678570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "691d23a1c059d9ad9eab906bc76772308502fe85",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful. The hypothesis under examination is that useful terms tend to be more similar to each other than to other query terms. Preliminary experiments with similarities computed using first-order and second-order co-occurrence seem to confirm the hypothesis. Term similarities could then be used for determining which query terms are useful and best reflect the user's information need. A possible application would be to use this source of evidence for tuning the weights of the query terms."
            },
            "slug": "Finding-content-bearing-terms-using-term-Picard",
            "title": {
                "fragments": [],
                "text": "Finding content-bearing terms using term similarities"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Preliminary experiments with similarities computed using first-order and second-order co-occurrence seem to confirm the hypothesis that useful terms tend to be more similar to each other than to other query terms."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15696874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00d4ca77732163c5ce6e78e6e3af0704bc3ed980",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Study of semantics has the general goal of modeling human linguistic competence as a theory, probing the constraints and limitations of language as a system of expression and representation, and of providing language engineering applications with a model of meaning, appropriate to its tasks. In general, there is no need to design a semantic model intended for practical processing to be neurologically or psychologically plausible but since human performance is impressive in certain respects there certainly is reason to investigate it to find if it can provide inspiration, examples, or constraints for implementations. Human information processing is efficient and effortless. The human information processor is flexible, dynamic, ever learning, does not stumble at inconsistencies, and does not require formal or explicit instruction. What sort of demands would we want to pose on a model of meaning, from the standpoint of language engineering for information access? Some specific requirements are at the forefront for information access analysis. Information access involves matching brief or even incomplete expressions of information need to relatively more verbose documents and items of information. The documents are not necessarily formulated for ease of retrieval in mind. For this class of tasks, models that are based on dynamically observed data"
            },
            "slug": "Meaningful-models-for-information-access-systems-Karlgren",
            "title": {
                "fragments": [],
                "text": "Meaningful models for information access systems"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Study of semantics has the general goal of modeling human linguistic competence as a theory, probing the constraints and limitations of language as a system of expression and representation, and of providing language engineering applications with a model of meaning, appropriate to its tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2197285"
                        ],
                        "name": "B. Rehder",
                        "slug": "B.-Rehder",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Rehder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rehder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144029655"
                        ],
                        "name": "M. E. Schreiner",
                        "slug": "M.-E.-Schreiner",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Schreiner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Schreiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39952106"
                        ],
                        "name": "Michael B. W. Wolfe",
                        "slug": "Michael-B.-W.-Wolfe",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wolfe",
                            "middleNames": [
                                "B.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael B. W. Wolfe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444937"
                        ],
                        "name": "Darrell Laham",
                        "slug": "Darrell-Laham",
                        "structuredName": {
                            "firstName": "Darrell",
                            "lastName": "Laham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darrell Laham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 3
                            }
                        ],
                        "text": "In Sahlgren (2004) and Sahlgren & Karlgren (2005a), we used RI to acquire bilingual lexica from aligned parallel data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1935697,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "a41a040d683c66d2e63ed9d5015e7dd8e9ddbb4b",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In another article (Wolfe et al., 1998/this issue) we showed how Latent Semantic Analysis (LSA) can be used to assess student knowledge\u2014how essays can be graded by LSA and how LSA can match students with appropriate instructional texts. We did this by comparing an essay written by a student with one or more target instructional texts in terms of the cosine between the vector representation of the student's essay and the instructional text in question. This simple method was effective for the purpose, but questions remain about how LSA achieves its results and how the results might be improved. Here, we address four such questions: (a) What role does the use of technical vocabulary play? (b) how long should the student essays be? (c) is the cosine the optimal measure of semantic relatedness? and (d) how does one deal with the directionality of knowledge in the high\u2010dimensional space?"
            },
            "slug": "Using-latent-semantic-analysis-to-assess-knowledge:-Rehder-Schreiner",
            "title": {
                "fragments": [],
                "text": "Using latent semantic analysis to assess knowledge: Some technical considerations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086250482"
                        ],
                        "name": "J. Deese",
                        "slug": "J.-Deese",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Deese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deese"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 24
                            }
                        ],
                        "text": "taken from work done by Deese (1964). Deese\u2019s original list included 39 antonym pairs (see Table 13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 145539036,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "8c7aee4d436ff52a85a8c9982d28c6ebda3f2b8c",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-associative-structure-of-some-common-english-Deese",
            "title": {
                "fragments": [],
                "text": "The associative structure of some common english adjectives"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 3
                            }
                        ],
                        "text": "In Sahlgren & C\u00f6ster (2004), we used word-space representations \u2014 what we called bag-of-concepts (BoC) \u2014 to improve the performance of a Support Vector Machine (SVM) (Vapnik, 1995) classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 83
                            }
                        ],
                        "text": "Yet another inherently different word-space implementation is Random Indexing (RI) (Kanerva et al., 2000; Karlgren & Sahlgren, 2001; Sahlgren, 2005), which was developed at the Swedish Institute of Computer Science (SICS) based on Pentti Kanerva\u2019s work on sparse distributed memory (Kanerva, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17228581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ef1b0469b43acc8ede2e56d8f001ad090b04826",
            "isKey": false,
            "numCitedBy": 500,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Word space models enjoy considerable attention in current research on semantic indexing. Most notably, Latent Semantic Analysis/Indexing (LSA/LSI; Deerwester et al., 1990, Landauer & Dumais, 1997) has become a household name in information access research, and deservedly so; LSA has proven its mettle in numerous applications, and has more or less spawned an entire research field since its introduction around 1990. Today, there is a rich flora of word space models available, and there are numerous publications that report exceptional results in many different applications, including information retrieval (Dumais et al., 1988), word sense disambiguation (Schutze, 1993), various semantic knowledge tests (Lund et al., 1995, Karlgren & Sahlgren, 2001), and text categorization (Sahlgren & Karlgren, 2004). This paper introduces the Random Indexing word space approach, which presents an efficient, scalable and incremental alternative to standard word space methods. The paper is organized as follows: in the next section, we review the basic word space methodology. We then look at some of the problems that are inherent in the basic methodology, and also review some of the solutions that have been proposed in the literature. In the final section, we introduce the Random Indexing word space approach, and briefly review some of the experimental results that have been achieved with Random Indexing."
            },
            "slug": "An-Introduction-to-Random-Indexing-Sahlgren",
            "title": {
                "fragments": [],
                "text": "An Introduction to Random Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Random Indexing word space approach is introduced, which presents an efficient, scalable and incremental alternative to standard word space methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144841068"
                        ],
                        "name": "C. S. Yang",
                        "slug": "C.-S.-Yang",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Yang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Yang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62592845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7fd9037b571862246e366189f1fc1c58fe0dd9c",
            "isKey": false,
            "numCitedBy": 635,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The existing practice in automatic indexing is reviewed, and it is shown that the standard theories for the specification of term values (or weights) are not adequate. New techniques are introduced for the assignment of weights to index terms, based on the characteristics of individual document collections. The effectiveness of some of the proposed methods is evaluated."
            },
            "slug": "On-the-Specification-of-Term-Values-in-Automatic-Salton-Yang",
            "title": {
                "fragments": [],
                "text": "On the Specification of Term Values in Automatic Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "It is shown that the standard theories for the specification of term values (or weights) are not adequate, and new techniques are introduced for the assignment of weights to index terms, based on the characteristics of individual document collections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2971978"
                        ],
                        "name": "Y. Wilks",
                        "slug": "Y.-Wilks",
                        "structuredName": {
                            "firstName": "Yorick",
                            "lastName": "Wilks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wilks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14064211"
                        ],
                        "name": "D. Fass",
                        "slug": "D.-Fass",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Fass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13074891"
                        ],
                        "name": "Cheng-ming Guo",
                        "slug": "Cheng-ming-Guo",
                        "structuredName": {
                            "firstName": "Cheng-ming",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-ming Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145506724"
                        ],
                        "name": "J. E. McDonald",
                        "slug": "J.-E.-McDonald",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McDonald",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. E. McDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859277"
                        ],
                        "name": "T. Plate",
                        "slug": "T.-Plate",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Plate",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759748"
                        ],
                        "name": "B. Slator",
                        "slug": "B.-Slator",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Slator",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Slator"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23516201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3550ccd0b7b78b15f9d0883e348cab435f158e51",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine readable dictionaries (Mrds) contain knowledge about language and the world essential for tasks in natural language processing (Nlp). However, this knowledge, collected and recorded by lexicographers for human readers, is not presented in a manner for Mrds to be used directly for Nlp tasks. What is badly needed are machine tractable dictionaries (Mtds): Mrds transformed into a format usable for Nlp. This paper discusses three different but related large-scale computational methods to transform Mrds into Mtds. The Mrd used is The Longman Dictionary of Contemporary English (Ldoce). The three methods differ in the amount of knowledge they start with and the kinds of knowledge they provide. All require some handcoding of initial information but are largely automatic. Method I, a statistical approach, uses the least handcoding. It generates \u201crelatedness\u201d networks for words in Ldoce and presents a method for doing partial word sense disambiguation. Method II employs the most handcoding because it develops and builds lexical entries for a very carefully controlled defining vocabulary of 2,000 word senses (1,000 words). The payoff is that the method will provide an Mtd containing highly structured semantic information. Method III requires the handcoding of a grammar and the semantic patterns used by its parser, but not the handcoding of any lexical material. This is because the method builds up lexical material from sources wholly within Ldoce. The information extracted is a set of sources of information, individually weak, but which can be combined to give a strong and determinate linguistic data base."
            },
            "slug": "Providing-machine-tractable-dictionary-tools-Wilks-Fass",
            "title": {
                "fragments": [],
                "text": "Providing machine tractable dictionary tools"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper discusses three different but related large-scale computational methods to transform Mrds into Mtds, the Longman Dictionary of Contemporary English (Ldoce), which is a statistical approach and requires some handcoding of initial information but is largely automatic."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Translation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069986425"
                        ],
                        "name": "F. Jiang",
                        "slug": "F.-Jiang",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5078338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e7b3d51817c97cde35dddc7f2d3a057c79ad332",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector-based information retrieval methods such as the vector space model (VSM), latent semantic indexing (LSI), and the generalized vector space model (GVSM) represent both queries and documents by high-dimensional vectors learned from analyzing a training corpus of text. VSM scales well to large collections, but cannot represent term\u2013term correlations, which prevents it from being used in translingual retrieval. GVSM and LSI can represent term\u2013term correlations, but do not scale well to very large retrieval collections. We present a novel method we call approximate dimension equalization (ADE) that combines ideas from VSM, LSI, and GVSM to produce a method that performs well on large collections, scales well computationally, and can represent term\u2013term correlations. We compare the performance of ADE to the other methods on both large and small collections of both monolingual and bilingual text. ADE outperforms all other methods on large bilingual collections, and performs close to the best in all other cases."
            },
            "slug": "Approximate-Dimension-Equalization-in-Vector-based-Jiang-Littman",
            "title": {
                "fragments": [],
                "text": "Approximate Dimension Equalization in Vector-based Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a novel method they call approximate dimension equalization (ADE) that combines ideas from VSM, LSI, and GVSM to produce a method that performs well on large collections, scales well computationally, and can represent term\u2013term correlations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 37537305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e30675b08daa4cfbe7995cb2666b52235a4042a4",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "A discussion on various experiments to utilize stylistic variation among texts for information retrieval purposes."
            },
            "slug": "Stylistic-Experiments-for-Information-Retrieval-Karlgren",
            "title": {
                "fragments": [],
                "text": "Stylistic Experiments for Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A discussion on various experiments to utilize stylistic variation among texts for information retrieval purposes and how this affects decision-making in the field of retrieval."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828241"
                        ],
                        "name": "Rickard C\u00f6ster",
                        "slug": "Rickard-C\u00f6ster",
                        "structuredName": {
                            "firstName": "Rickard",
                            "lastName": "C\u00f6ster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rickard C\u00f6ster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15070970,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "2692f0ca3c05f09347642b88348ee5137bc67253",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": "Personalised information access systems use historical feedback data, such as implicit and explicit ratings for textual documents and other items, to better locate the right or relevant information ..."
            },
            "slug": "Algorithms-and-Representations-for-Personalised-C\u00f6ster",
            "title": {
                "fragments": [],
                "text": "Algorithms and Representations for Personalised Information Access"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Personalised information access systems use historical feedback data, such as implicit and explicit ratings for textual documents and other items, to better locate the right or relevant information."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069255025"
                        ],
                        "name": "A. Popova",
                        "slug": "A.-Popova",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Popova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Popova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3121597"
                        ],
                        "name": "P. Mateev",
                        "slug": "P.-Mateev",
                        "structuredName": {
                            "firstName": "Plamen",
                            "lastName": "Mateev",
                            "middleNames": [
                                "S"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mateev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70904187"
                        ],
                        "name": "J. Bourchier",
                        "slug": "J.-Bourchier",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bourchier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bourchier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14202849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92c7f1fb9da7d183db7f2844a07249cd55dd6011",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents experimental results of usage of LSA for analysis of English literature texts. Several preliminary transformations of the frequency text-document matrix with different weight functions are tested on the basis of control subsets. Additional clustering based on correlation matrix is applied in order to reveal the latent structure. The algorithm creates a shaded form matrix via singular values and vectors. The results are interpreted as a quality of the transformations and compared to the control set tests."
            },
            "slug": "Weight-functions-impact-on-LSA-performance-Nakov-Popova",
            "title": {
                "fragments": [],
                "text": "Weight functions impact on LSA performance"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Experimental results of usage of LSA for analysis of English literature texts and preliminary transformations of the frequency text-document matrix with different weight functions are tested on the basis of control subsets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47211604"
                        ],
                        "name": "H. E. Stiles",
                        "slug": "H.-E.-Stiles",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Stiles",
                            "middleNames": [
                                "Edmund"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. E. Stiles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14320013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38880ee628ea803bb390dafaccae270513b1b8be",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an all computer document retrieval system which can find documents related to a request even though they may not be indexed by the exact terms of the request, and can present these documents in the order of their relevance to the request. The key to this ability lies in the application of a statistical formula by which the computer calculates the degree of association between pairs of index terms. With proper manipulation of these associations (entirely within the machine) a vocabulary of synonyms, near synonyms and other words closely related to any given term or group of terms is derived. Such a vocabulary related to a group of request terms is believed to be a much more powerful tool for selecting documents from a collection than has been available heretofore. By noting the number of matching terms between this extended list of request terms and the terms used to index a document, and with due regard for their degree of association, documents are selected by the computer and arranged in the order of their relevance to the request. Like all other documentalists who are operating large coordinate indexes, we are searching for better ways to exploit this type of information system. In our library we have already eliminated the time-consuming job of posting document numbers manually by enlisting the aid of a 705 computer. (The computer periodically prepares revised posting cards to replace the outdated ones.) Now we are searching for better solutions to our retrieval problems. One obvious retrieval problem in any large system is the time required to \"coordinate\" heavily posted terms. We are convinced we must mechanize if we are to allow our collection to grow indefinitely. A second problem is the retrieval of so many documents related to a single request that the customer finds it difficult to decide which to examine first. Since he has no precise means of determining which document is most closely related to a request, we have been forced into assisting him to use somewhat arbitrary or subjective means. The date of the document is sometimes used as a relevance criterion with the hope that the most recent document will be the most pertinent, or the name of the author is used with the hope that a known author will answer the request better than an unknown one. The pitfalls of such criteria are apparent. The third, and \u2026"
            },
            "slug": "The-Association-Factor-in-Information-Retrieval-Stiles",
            "title": {
                "fragments": [],
                "text": "The Association Factor in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An all computer document retrieval system which can find documents related to a request even though they may not be indexed by the exact terms of the request, and can present these documents in the order of their relevance to the request is described."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8102474"
                        ],
                        "name": "Bruce E. Nevin",
                        "slug": "Bruce-E.-Nevin",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Nevin",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bruce E. Nevin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 177
                            }
                        ],
                        "text": "So where does meaning fit into the distributional paradigm? Reviewers of Harris\u2019 work are not entirely unanimous regarding the role of meaning in the distributional methodology (Nevin, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 170936590,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "14ec44285dce62d6b2985a3f453afe2cb9fa8b52",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARYZellig S. Harris (1909-1992) is a familiar icon of American structuralism. According to received views of the history of linguistics in the 20th century, he is an exemplar of 'taxonomic linguistics' seeking practical discovery procedures whereby one could mechanically derive a grammar from distributional analysis of a corpus of utterances without reference to meaning, and a proponent of empiricist and behaviorist views that have been overthrown by the revolution of Generative linguistics. An examination of what he actually wrote, however, shows a lifelong concern with the analysis and representation of meaning. Harris' approach to the evaluation of alternative tools of analysis, alternative grammars, and alternative theories of language arises from a crucial but little acknowledged dilemma of linguistics grounded in a fundamental property of language, namely, that it contains within itself virtually unrestricted metalinguistic capacities, upon which any description of language whatever either directly or indirectly depends.RESUMEZellig S. Harris (1909-1992) est un icone familier du structuralisme americain. En plus d'etre l'auteur d'opinions empiristes et behavioristes qui ont ete renversees par la revolution de la linguistique generative, Harris serait, selon les idees recues sur l'histoire de la linguistique, un produit de la 'linguistique taxonomique', recherchant des procedures de decouverte pratiques par lesquelles l'on peut deriver mecaniquement une grammaire a partir d'une analyse distributicnelle d'un corpus d'enonces, sans reference au sens. Un examen attentif de ses ecrits revele, cependant, un veritable souci de l'analyse et de la representation du sens, et cela sa vie durant. L'approche de Harris relative a l'evaluation d'outils alternatifs pour l'analyse, de grammaires alternatives et de theories du language alternatives est motivee par un dilemme linguistique crucial mais peu reconnu, encre dans une propriete fondamentale du language, a savoir que le language contient des capacites metalinguistiques quasi illimitees desquelles depend, soit directement ou indirectement, toute description de celui-ci.ZUSAMMENFASSUNGZellig S. Harris (1909-1992) ist ein gelaufiges Bild des Amerikanischen Structuralism. Den ublichen Darstellungen in der Wissenschaftsgeschichte der Linguistik des 20. Jahrhunderts zufolge stellt er ein Muster der 'taxonomischen Sprachwissenschaft' dar, stets auf der Suche nach praktischen Entdeckungs-prozeduren, durch die es moglich sein solle, auf mechanischem Wege eine Grammatik aufgrund einer distributionellen Analyse aus Korpus von Sprach-auserungen ohne Bezug auf die Bedeutung zu erstellen. Gleichzeitig gilt er als ein Vertreter von empiristischen und behavioristischen Ansichten, die durch durch die generative Revolution in der Linguistik uber den Haufen geworfen worden seien. Dagegen ergibt eine sorgfaltige Untersuchung dessen, was Harris tatsachlich schrieb, daB er ein lebenslanges Engagement fur die Analyse und Darstellung der Bedeutung hatte. Harris' Herangehen an die Bewertung alternativer Werkzeuge fur die Analyse, alternative Grammatiken, und alternative Sprachtheorien nimmt seinen Ausgang im kruzialen, jedoch wenig erkann-ten Dilemma der Linguistik, die auf einer fundamentalen Eigenschaft der Spra-che basiert, namlich die, daB sie in sich unbeschrankte metalinguistische Mog-lichkeiten enthalt, von der eine jede Sprachbeschreibung entweder direkt oder indirekt abhangt."
            },
            "slug": "A-minimalist-program-for-linguistics:-the-work-of-Nevin",
            "title": {
                "fragments": [],
                "text": "A minimalist program for linguistics: the work of Zellig Harris on meaning and information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16902271"
                        ],
                        "name": "H. P. Luhn",
                        "slug": "H.-P.-Luhn",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Luhn",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. P. Luhn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "The observation that frequency is a viable indicator of the quality of index terms originates in the work of Hans Peter Luhn in the late 1950\u2019s (Luhn, 1958)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15475171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6dcc17c6f3dbc2d203ade9ff671a895a9dead7c",
            "isKey": false,
            "numCitedBy": 3148,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Excerpts of technical papers and magazine articles that serve the purposes of conventional abstracts have been created entirely by automatic means. In the exploratory research described, the complete text of an article in machine-readable form is scanned by an IBM 704 data-processing machine and analyzed in accordance with a standard program. Statistical information derived from word frequency and distribution is used by the machine to compute a relative measure of significance, first for individual words and then for sentences. Sentences scoring highest in significance are extracted and printed out to become the \"auto-abstract.\""
            },
            "slug": "The-Automatic-Creation-of-Literature-Abstracts-Luhn",
            "title": {
                "fragments": [],
                "text": "The Automatic Creation of Literature Abstracts"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "In the exploratory research described, the complete text of an article in machine-readable form is scanned by an IBM 704 data-processing machine and analyzed in accordance with a standard program."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742115"
                        ],
                        "name": "Y. Choueka",
                        "slug": "Y.-Choueka",
                        "structuredName": {
                            "firstName": "Yaacov",
                            "lastName": "Choueka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Choueka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26898124"
                        ],
                        "name": "Serge Lusignan",
                        "slug": "Serge-Lusignan",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Lusignan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge Lusignan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5930633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1f7769dc8bde42baa056ce7da37e825a747c603",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a technique that we believe can be of great help in many text-processing situations, and reports on an experiment recently conducted to test its validity and scope. As a background we shall present in the following sections some fundamental clarifications and remarks on our specific view of lemmatization and disambiguation. Our starting point is the double assertion that we believe would be shared by many workers in applied computational linguistics and large text-processing projects, to wit: that on the one hand lemmatization is one of the most important and crucial steps in many non-trivial text-processing cycles, but on the other hand, no operational, reasonably general, fully automatic and high-quality context-sensitive text-lemmatization system nowadays is easily accessible for any natural language. Given these two premises, the problem is how to introduce a partial element (at least) of machineaided work in the process of text-lemmatization, so as to avoid the extremely laborious and frustrating task of a word-per-word manual lemmatization of large corpora as was done in the early days of automatic text-processing projects. (For a thorough report on mechanical lemmatization programs, see ref. 4.) In this paper we focus on the analysis and experimental testing of one idea that fits naturally into this framework, namely that of disambiguation by short contexts. (The somewhat unexpected shift from \"lemmatization\" to \"disambiguation\" will be justified in the sections to come.) Based on"
            },
            "slug": "Disambiguation-by-short-contexts-Choueka-Lusignan",
            "title": {
                "fragments": [],
                "text": "Disambiguation by short contexts"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This paper describes a technique that is of great help in many text-processing situations, and reports on an experiment recently conducted to test its validity and scope, namely that of disambiguation by short contexts."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Humanit."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14977016,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25fcfd7c56248cba82d93978e1aefebff94aa840",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Discrimination decisions arise in many natural language processing tasks. Three classical tasks are discriminating texts by their authors (author identification), discriminating documents by their relevance to some query (information retrieval), and discriminating multi-meaning words by their meanings (sense discrimination). Many other discrimination tasks arise regularly, such as determining whether a particular proper noun represents a person or a place, or whether a given work from some teletype text would be capitalized if both cases had been used.We [9] introduced a method designed for the sense discrimination problems mentioned.We also discuss areas for research based on observed shortcomings of the method. In particular, an example in the author identification task shows the need for a robust version of the method. Also, the method makes an assumption of independence which is demonstrably false, yet there has been no careful study of the results of this assumption."
            },
            "slug": "Discrimination-decisions-for-100,000-dimensional-Gale-Church",
            "title": {
                "fragments": [],
                "text": "Discrimination decisions for 100,000-dimensional spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method designed for the sense discrimination problems mentioned is introduced and areas for research based on observed shortcomings of the method are discussed, including the need for a robust version of this method."
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Oper. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255019"
                        ],
                        "name": "F. J. Damerau",
                        "slug": "F.-J.-Damerau",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Damerau",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. J. Damerau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 124
                            }
                        ],
                        "text": "More sophisticated statistical criteria includes the tfidf, and different variants and mixtures of the Poisson distribution (Damerau, 1965; Harter, 1975; Katz, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62586725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "919d288044fca38e727903d71c191f33f5308469",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes a method of indexing documents which is based on the assumptions, (1) that a subset of the words in a document can be an effective index to that document and, (2) that this subject can be approximated by selecting those words from the document whose frequencies are statistically unexpectedly high. The results of the experiment are encouraging, although not definitive because any index set chosen must be tested by using it for retrieval from a large collection."
            },
            "slug": "An-experiment-in-automatic-indexing-Damerau",
            "title": {
                "fragments": [],
                "text": "An experiment in automatic indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results of the experiment are encouraging, although not definitive because any index set chosen must be tested by using it for retrieval from a large collection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34986152"
                        ],
                        "name": "S. P. Harter",
                        "slug": "S.-P.-Harter",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Harter",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. P. Harter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20821537,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "27c77fe6cfe62204e4134eb7a4c447879d8e3a14",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In Part I of this study,* a mixture of two Poisson distributions was examined as a model of specialty word distribution. Formulas expressing the three parameters of the model in terms of empirical frequency statistics were derived, and a statistical measure intended to identify specialty words, consistent with the model, was proposed. \n \n \n \nIn the present paper, Part II of the study, a probabilistic model of keyword indexing is outlined, and some of the consequences of the model are examined. An algorithm defining a measure of indexability is developed-a measure intended to reflect the relative significance of words in documents. The measure is evaluated and is found to consistently produce indexes superior to those produced by another measure which had previously been identified in the literature as producing the best results."
            },
            "slug": "A-probabilistic-approach-to-automatic-keyword-Part-Harter",
            "title": {
                "fragments": [],
                "text": "A probabilistic approach to automatic keyword indexing. Part II. An algorithm for probabilistic indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm defining a measure of indexability is developed-a measure intended to reflect the relative significance of words in documents that is found to consistently produce indexes superior to those produced by another measure which had previously been identified in the literature as producing the best results."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "An experimental result that supports this claim is the success of Eric Brill\u2019s (1994) simple rule-based technique for recognizing syntactic categories without looking further"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12309040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "733234e097dceb9011baa8914930861996eb0b5e",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In (Brill 1992), a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty."
            },
            "slug": "Some-Advances-in-Transformation-Based-Part-of-Brill",
            "title": {
                "fragments": [],
                "text": "Some Advances in Transformation-Based Part of Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method for expressing lexical relations in tagging that stochastic taggers are currently unable to express is described and how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828241"
                        ],
                        "name": "Rickard C\u00f6ster",
                        "slug": "Rickard-C\u00f6ster",
                        "structuredName": {
                            "firstName": "Rickard",
                            "lastName": "C\u00f6ster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rickard C\u00f6ster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15626977,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63468fc01b9626b8df76bac09c3a07a37271e2ee",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This year, the SICS team decided to concentrate on query processing and on the internal topical structure of the query: we have identified this as one of the major bottlenecks for cross-lingual access systems. Previous years, the SICS team has investigated, among other issues, how to translate compounds. Compound translation is non-trivial due to dependencies between compound elements and has been treated in various ways in the treatment of compounding languages such as Swedish. We decided this year to investigate the topical dependencies between query terms, under the hypothesis that the complexity of translating compounds is a special case of the more general case of understanding the respective topicality of query terms. The question under investigation is how much each query term contributes in terms of topicality in the documents of the collection under consideration. If a query term happens to be non-topical or noise, it should be discarded or given a low weight when ranking retrieved documents; if a query term shows high topicality its weight should be boosted. Our base system is used with two different enhancements to test the hypothesis that boosting topically active terms is beneficial for retreival results. Both schemes are based on the analysis of the distributional character of query terms: one using similarity of occurrence context between query terms; the other using the likelihood of individual terms to appear topically in text. These are two different avenues of analysis and will most likely provide different results if pursued further than these initial experiments. The results of the boosting schemes delivered uncontroversially improved results. These results will provide impetus for the further study of translation of complex terms \u2014 the question which first prompted this set of experiments in the first place."
            },
            "slug": "Principled-Query-Processing-Karlgren-Sahlgren",
            "title": {
                "fragments": [],
                "text": "Principled Query Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This year, the SICS team decided to investigate the topical dependencies between query terms, under the hypothesis that the complexity of translating compounds is a special case of the more general case of understanding the respective topicality of query terms."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38602469"
                        ],
                        "name": "P. Husbands",
                        "slug": "P.-Husbands",
                        "structuredName": {
                            "firstName": "Parry",
                            "lastName": "Husbands",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Husbands"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35124559"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Simon",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737469"
                        ],
                        "name": "C. Ding",
                        "slug": "C.-Ding",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1072120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46960902e23cc011c6b892ffd6a60bd387ba7029",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of the Singular Value Decomposition (SVD) has been proposed for text retrieval in several recent works. This technique uses the SVD to project very high dimensional document and query vectors into a low dimensional space. In this new space it is hoped that the underlying structure of the collection is revealed thus enhancing retrieval performance. Theoretical results have provided some evidence for this claim and to some extent experiments have confirmed this. However, these studies have mostly used small test collections and simplified document models. In this work we investigate the use of the SVD on large document collections. We show that, if interpreted as a mechanism for representing the terms of the collection, this technique alone is insufficient for dealing with the variability in term occurrence. Section 2 introduces the text retrieval concepts necessary for our work. A short description of our experimental architecture is presented in Section 3. Section 4 describes how term occurrence variability affects the SVD and then shows how the decomposition influences retrieval performance. A possible way of improving SVD-based techniques is presented in Section 5 and concluded in Section 6."
            },
            "slug": "On-the-use-of-the-singular-value-decomposition-for-Husbands-Simon",
            "title": {
                "fragments": [],
                "text": "On the use of the singular value decomposition for text retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that, if interpreted as a mechanism for representing the terms of the collection, this technique alone is insufficient for dealing with the variability in term occurrence, and how the decomposition influences retrieval performance is shown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72549687"
                        ],
                        "name": "F. Saussure",
                        "slug": "F.-Saussure",
                        "structuredName": {
                            "firstName": "Ferdinand",
                            "lastName": "Saussure",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Saussure"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12367651,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1dbcdb06369d365dba0e909811825daf7dca6061",
            "isKey": false,
            "numCitedBy": 2996,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to the Bloomsbury Revelations Edition Preface to the First Edition Preface to the Second Edition Preface to the Third Edition Editor's Introduction, Roy Harris Introduction 1. A Brief Survey of the History of Linguistics 2. Data and Aims of Linguistics: Connexions with Related Sciences 3. The Object of Study 4. Linguistics of Language Structure and Linguistics of Speech 5. Internal and External Elements of a Language 6. Representation of a Language by Writing 7. Physiological Phonetics Appendix: Principles of Physiological Phonetics 1. Sound Types 2. Sounds in Spoken Sequences Part One: General Principles 1. Nature of the Linguistic Sign 2. Invariability and Variability of the Sign 3. Static Linguistics and Evolutionary Linguistics Part Two: Synchronic Linguistics 1. General Observations 2. Concrete Entities of a Language 3. Identities, Realities, Values 4. Linguistic Value 5. Syntagmatic Relations and Associative Relations 6. The Language Mechanism 7. Grammar and Its Subdivisions 8. Abstract Entities in Grammar Part Three: Diachronic Linguistics 1. General Observations 2. Sound Changes 3. Grammatical Consequences of Phonetic Evolution 4. Analogy 5. Analogy and Evolution 6. Popular Etymology 7. Agglutination 8. Diachronic Units,Identities and Realities Appendices Part Four: Geographical Linguistics 1. On the Diversity of Languages 2. Geographical Diversity: Its Complexity 3. Causes of Geographical Diversity 4. Propagation of Linguistic Waves Part Five: Questions of Retrospective Linguistics Conclusion 1. The Two Perspectives of Diachronic Linguistics 2. Earliest Languages and Prototypes 3. Reconstructions 4. Linguistic Evidence in Anthropology and Prehistory 5. Language Families and Linguistic Types Index"
            },
            "slug": "Course-in-General-Linguistics-Saussure",
            "title": {
                "fragments": [],
                "text": "Course in General Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49948286"
                        ],
                        "name": "N. Henley",
                        "slug": "N.-Henley",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Henley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Henley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 143374968,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f2fbf00976331cb2d09936851c48c775f07a49b1",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-psychological-study-of-the-semantics-of-animal-Henley",
            "title": {
                "fragments": [],
                "text": "A psychological study of the semantics of animal terms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 200
                            }
                        ],
                        "text": "Other attempts at using linguistic information for computing distributional similarity between words include Hindle (1990), who used predicate\u2013argument structure to determine the similarity of nouns; Hearst (1992), who extracted hyponyms by using lexical\u2013syntactic templates; Ruge (1992), who used head\u2013modifier relations for extracting similar words; and Grefenstette (1992a, 1992b, 1993), who also used syntactic context to measure similarity between words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 32164517,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "99eadd5e29a85f30cafef7f2c915f384715e3b89",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We may not be able to make you love reading, but mathematical structures of language will lead you to love reading starting from now. Book is the window to open the new world. The world that you want is in the better stage and level. World will always guide you to even the prestige stage of the life. You know, this is some of how reading will give you the kindness. In this case, more books you read more knowledge you know, but it can mean also the bore is full."
            },
            "slug": "Mathematical-structures-of-language-Harris",
            "title": {
                "fragments": [],
                "text": "Mathematical structures of language"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors may not be able to make you love reading, but mathematical structures of language will lead you to love reading starting from now."
            },
            "venue": {
                "fragments": [],
                "text": "Interscience tracts in pure and applied mathematics"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711144"
                        ],
                        "name": "Samuel Kaski",
                        "slug": "Samuel-Kaski",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Kaski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Kaski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 218
                            }
                        ],
                        "text": "For RI, the choice of dimensionality is a trade-off between efficiency and performance (random projection techniques perform better the closer the dimensionality of the vectors is to the number of contexts in the data (Kaski, 1999; Bingham & Mannila, 2001))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 105
                            }
                        ],
                        "text": "Since there are many more nearly orthogonal than truly orthogonal directions in a high-dimensional space (Kaski, 1999), choosing random directions \u2014 as we do when generating the index vectors \u2014 can get us very close to orthogonality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13803457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e2987872ddc1ba9516aed3f85f8b4b9cf0125fb",
            "isKey": false,
            "numCitedBy": 435,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "When the data vectors are high-dimensional it is computationally infeasible to use data analysis or pattern recognition algorithms which repeatedly compute similarities or distances in the original data space. It is therefore necessary to reduce the dimensionality before, for example, clustering the data. If the dimensionality is very high, like in the WEBSOM method which organizes textual document collections on a self-organizing map, then even the commonly used dimensionality reduction methods like the principal component analysis may be too costly. It is demonstrated that the document classification accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the final dimensionality is sufficiently large (about 100 out of 6000). In fact, it can be shown that the inner product (similarity) between the mapped vectors follows closely the inner product of the original vectors."
            },
            "slug": "Dimensionality-reduction-by-random-mapping:-fast-Kaski",
            "title": {
                "fragments": [],
                "text": "Dimensionality reduction by random mapping: fast similarity computation for clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that the document classification accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the final dimensionality is sufficiently large."
            },
            "venue": {
                "fragments": [],
                "text": "1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144514944"
                        ],
                        "name": "W. Lowe",
                        "slug": "W.-Lowe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17785635,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9b456ad931c1dfc318f951308cf1171cfee1292c",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Using an auditory semantic priming paradigm, Moss, Ostrin, Tyler and Marslen-Wilson (1995, Experiment 2) demonstrated facilitation for category coordinates and functionally-related stimuli both with and without the additive effect of normative association strength. In this paper we replicate these results computationally using a corpus-derived Contextual Similarity measure. In Experiment 1 we consider the adequacy of the Contextual Similarity measure in accounting for Moss et al.\u2019s results, and discuss how functional and categorical semantic relations are represented in corpus-based approaches to lexical semantics. We also offer an explanation for how the Contextual Similarity measure succeeds in replicating the additive effect of association strength on semantic priming without postulating a qualitatively different mechanism for associative priming. We then investigate why previous corpus-based approaches (Lund, Burgess & Atchley, 1995) have failed to produce similar results. We argue that this is because vector representations partly encode temporal co-occurrence information. This explanation is tested in Experiment 2."
            },
            "slug": "Modelling-functional-priming-and-the-associative-McDonald-Lowe",
            "title": {
                "fragments": [],
                "text": "Modelling functional priming and the associative boost"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865606"
                        ],
                        "name": "Henri Avancini",
                        "slug": "Henri-Avancini",
                        "structuredName": {
                            "firstName": "Henri",
                            "lastName": "Avancini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henri Avancini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192424"
                        ],
                        "name": "A. Lavelli",
                        "slug": "A.-Lavelli",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Lavelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lavelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145077269"
                        ],
                        "name": "F. Sebastiani",
                        "slug": "F.-Sebastiani",
                        "structuredName": {
                            "firstName": "Fabrizio",
                            "lastName": "Sebastiani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Sebastiani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36024018"
                        ],
                        "name": "Roberto Zanoli",
                        "slug": "Roberto-Zanoli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Zanoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Zanoli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 47
                            }
                        ],
                        "text": "6) according to 42 general semantic categories (Avancini et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18134913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5746b33abac1f1834b326b5feba65294fa74871",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss an approach to the automatic expansion of domain-specific lexicons by means of <i>term categorization</i>, a novel task employing techniques from information retrieval (IR) and machine learning (ML). Specifically, we view the expansion of such lexicons as a process of learning previously unknown associations between terms and <i>domains</i>. The process generates, for each <i>c<inf>i</inf></i> in a set <i>C</i> = {<i>c</i><inf>1</inf>,..., <i>c<inf>m</inf></i>} of domains, a lexicon <i>L<sup>i</sup></i><inf>1</inf>, boostrapping from an initial lexicon <i>L<sup>i</sup></i><inf>0</inf> and a set of documents \u03b8 given as input. The method is inspired by <i>text categorization</i> (TC), the discipline concerned with labelling natural language texts with labels from a predefined set of domains, or categories. However, while TC deals with documents represented as vectors in a space of terms, we formulate the task of term categorization as one in which terms are (dually) represented as vectors in a space of documents, and in which terms (instead of documents) are labelled with domains."
            },
            "slug": "Expanding-domain-specific-lexicons-by-term-Avancini-Lavelli",
            "title": {
                "fragments": [],
                "text": "Expanding domain-specific lexicons by term categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An approach to the automatic expansion of domain-specific lexicons by means of term categorization, a novel task employing techniques from information retrieval (IR) and machine learning (ML), is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "SAC '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Sch\u00fctze & Pedersen (1993) compute syntagmatic and paradigmatic relations between words by using a directional wordsby-words matrix similar to that used in HAL, which they transform using SVD."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51933055"
                        ],
                        "name": "M. Mitra",
                        "slug": "M.-Mitra",
                        "structuredName": {
                            "firstName": "Manclar",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mitra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 247
                            }
                        ],
                        "text": "There are many variations of document-length normalization, ranging from very simple measures that just count the number of tokens in a document (Robertson & Sp\u00e4rck Jones, 1997), to more complex ones, such as pivoted document-length normalization (Singhal et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13184498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "173443511c450bd8f61e3d1122982f74c94147ae",
            "isKey": false,
            "numCitedBy": 587,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic information retrieval systems have to deal with documents of varying lengths in a text collection. Document length normalization is used to fairly retrieve documents of all lengths. In this study, we ohserve that a normalization scheme that retrieves documents of all lengths with similar chances as their likelihood of relevance will outperform another scheme which retrieves documents with chances very different from their likelihood of relevance. We show that the retrievaf probabilities for a particular normalization method deviate systematically from the relevance probabilities across different collections. We present pivoted normalization, a technique that can be used to modify any normalization function thereby reducing the gap between the relevance and the retrieval probabilities. Training pivoted normalization on one collection, we can successfully use it on other (new) text collections, yielding a robust, collectzorz independent normalization technique. We use the idea of pivoting with the well known cosine normalization function. We point out some shortcomings of the cosine function andpresent two new normalization functions\u2013-pivoted unique normalization and piuotert byte size nornaahzation."
            },
            "slug": "Pivoted-document-length-normalization-Singhal-Buckley",
            "title": {
                "fragments": [],
                "text": "Pivoted document length normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Pivoted normalization is presented, a technique that can be used to modify any normalization function thereby reducing the gap between the relevance and the retrieval probabilities, and two new normalization functions are presented\u2013-pivoted unique normalization and piuotert byte size nornaahzation."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48911597"
                        ],
                        "name": "J. Lawler",
                        "slug": "J.-Lawler",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lawler",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lawler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143716922"
                        ],
                        "name": "G. Lakoff",
                        "slug": "G.-Lakoff",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Lakoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lakoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34262157"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1123,
                                "start": 0
                            }
                        ],
                        "text": "Lavelli et al. compare these two representations using term categorization and term clustering with WordNetDomains(42) as evaluation resource.(3) The results are very clear: the paradigmatic (TCOR) representations outperform the syntagmatic (DOR) ones in both tests. Lavelli et al. hypothesize that the supremacy of the paradigmatic representation can be explained by its ability to \u201ccapture some phenomena related to semantic similarity better\u201d than the syntagmatic ones. They also claim that the paradigmatic representations are more discriminative (as measured using a function based on mutual information) than the syntagmatic ones, and therefore constitute \u201cinherently better features.\u201d Lavelli et al. are not the only ones who argue for the supremacy of paradigmatic uses of context. Stiles (1961) argues that syntagmatic uses of context only generate what he calls statistical relationships between terms, and that only a paradigmatic use of context can \u201cproject us beyond the purely statistical relationships and into the realm of meaningful associations.\u201d Similar claims are made by Rubenstein & Goodenough (1965), Grefenstette (1992b), and Charles (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 804,
                                "start": 0
                            }
                        ],
                        "text": "Lavelli et al. compare these two representations using term categorization and term clustering with WordNetDomains(42) as evaluation resource.(3) The results are very clear: the paradigmatic (TCOR) representations outperform the syntagmatic (DOR) ones in both tests. Lavelli et al. hypothesize that the supremacy of the paradigmatic representation can be explained by its ability to \u201ccapture some phenomena related to semantic similarity better\u201d than the syntagmatic ones. They also claim that the paradigmatic representations are more discriminative (as measured using a function based on mutual information) than the syntagmatic ones, and therefore constitute \u201cinherently better features.\u201d Lavelli et al. are not the only ones who argue for the supremacy of paradigmatic uses of context. Stiles (1961) argues that syntagmatic uses of context only generate what he calls statistical relationships between terms, and that only a paradigmatic use of context can \u201cproject us beyond the purely statistical relationships and into the realm of meaningful associations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1898149,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1456fba651c261326d8cbec451aca4925092141f",
            "isKey": false,
            "numCitedBy": 11078,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "People use metaphors every time they speak. Some of those metaphors are literary - devices for making thoughts more vivid or entertaining. But most are much more basic than that - they're \"metaphors we live by\", metaphors we use without even realizing we're using them. In this book, George Lakoff and Mark Johnson suggest that these basic metaphors not only affect the way we communicate ideas, but actually structure our perceptions and understandings from the beginning. Bringing together the perspectives of linguistics and philosophy, Lakoff and Johnson offer an intriguing and surprising guide to some of the most common metaphors and what they can tell us about the human mind. And for this new edition, they supply an afterword both extending their arguments and offering a fascinating overview of the current state of thinking on the subject of the metaphor."
            },
            "slug": "Metaphors-We-Live-by-Lawler-Lakoff",
            "title": {
                "fragments": [],
                "text": "Metaphors We Live by"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144430625"
                        ],
                        "name": "S. Robertson",
                        "slug": "S.-Robertson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Robertson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Robertson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107421195"
                        ],
                        "name": "K. S. Jones",
                        "slug": "K.-S.-Jones",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Jones",
                            "middleNames": [
                                "Sp\u0308arck"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11914954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afeb1632d0d0eb421ebda898b226de1cf482965d",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This technical note describes straightforward techniques for document indexing and retrieval that have been solidly established through extensive testing and are easy to apply. They are useful for many different types of text material, are viable for very large files, and have the advantage that they do not require special skills or training for searching, but are easy for end users. The document and text retrieval methods described here have a sound theoretical basis, are well established by extensive testing, and the ideas involved are now implemented in some commercial retrieval systems. Testing in the last few years has, in particular, shown that the methods presented here work very well with full texts, not only title and abstracts, and with large files of texts containing three quarters of a million documents. These tests, the TREC Tests (see Harman 1993\u20131997; IPM on term weighting exploiting statistical information about term occurrences; on scoring for request-document matching, using these weights, to obtain a ranked search output; and on relevance feedback to modify request weights or term sets in iterative searching. The normal implementation is via an inverted file organisation using a term list with linked document identifiers, plus counting data, and pointers to the actual texts. The user\u2019s request can be a word list, phrases, sentences or extended text. 1 Terms and matching Index terms are normally content words (but see section 6). In request processing, stop words (e.g. prepositions and conjunctions) are eliminated via a stop word list, and they are usually removed, for economy reasons, in inverted file construction. Terms are also generally stems (or roots) rather than full words, since this means that matches are not missed through trivial word variation, as with singular/plural forms. Stemming can be achieved most simply by the user truncating his request words, to match any inverted index words that include them; but it is a better strategy to truncate using a standard stemming algorithm and suffix list (see Porter 1980), which is nicer for the user and reduces the inverted term list. The request is taken as an unstructured list of terms. If the terms are unweighted, output could be ranked by the number of matching terms \u2013 i.e. for a request with 5 terms first by documents with all 5, then by documents with any 4, etc. However, performance may be improved considerably by giving a weight to each term (or each term-document combination). In this case, output is ranked by sum of weights (see below)."
            },
            "slug": "Simple,-proven-approaches-to-text-retrieval-Robertson-Jones",
            "title": {
                "fragments": [],
                "text": "Simple, proven approaches to text retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This technical note describes straightforward techniques for document indexing and retrieval that have been solidly established through extensive testing and are easy to apply and have the advantage that they do not require special skills or training for searching, but are easy for end users."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17581,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "36eff99a7f23cec395e4efc80ff7f937934c7be6",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "From Pythagoras's harmonic sequence to Einstein's theory of relativity, geometric models of position, proximity, ratio, and the underlying properties of physical space have provided us with powerful ideas and accurate scientific tools. Currently, similar geometric models are being applied to another type of space the conceptual space of information and meaning, where the contributions of Pythagoras and Einstein are a part of the landscape itself. The rich geometry of conceptual space can be glimpsed, for instance, in internet documents: while the documents themselves define a structure of visual layouts and point-to-point links, search engines create an additional structure by matching keywords to nearby documents in a spatial arrangement of content. What the \"Geometry of Meaning\" provides is a much-needed exploration of computational techniques to represent meaning and of the conceptual spaces on which these representations are founded.\""
            },
            "slug": "Geometry-and-Meaning-Widdows",
            "title": {
                "fragments": [],
                "text": "Geometry and Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "What the \"Geometry of Meaning\" provides is a much-needed exploration of computational techniques to represent meaning and of the conceptual spaces on which these representations are founded."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35031308"
                        ],
                        "name": "C. Osgood",
                        "slug": "C.-Osgood",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Osgood",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Osgood"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 158
                            }
                        ],
                        "text": "The pioneer in this field is Charles Osgood and his colleagues, who in the early 1950s developed the semantic differential approach to meaning representation (Osgood, 1952; Osgood et al., 1957)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17077596,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ca12a908e86a87db152c0991ae9c5a40f1a5d2a3",
            "isKey": false,
            "numCitedBy": 1185,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "s, 1934, 8, No. 153. 50. KARWOSKI, T. F., & BERTHOLD, F., JR. Psychological studies in semantics: II. Reliability of free association tests. /. soc. Psychol, 1945,22,87-102. 51. KARWOSKI, T. F., GRAMLICH, F. W., & ARNOTT, P. Psychological studiesin semantics: I. Free association reactions to words, drawings, and NATURE AND MEASUREMENT OF MEANING 235 objects. J, soc. PsychoL, 1944, 20, 233-247. 52. KARWOSKI, T. F., & ODBERT, H. S. Color-music. PsychoL Monogr., 1938, 50, No. 2 (Whole No. 222). 53. KARWOSKI, T. F., ODBERT, H. S., & OSGOOD, C. E. Studies in synesthetic thinking: II. The roles of form in visual responses to music. J. gen. PsychoL, 1942, 26, 199-222. 54. KARWOSKI, T. F., & SCHACHTER, J. Psychological studies in semantics: III. Reaction times for similarity and difference. J. soc, PsychoL, 1948, 28, 103-120. 55. KELLER, MARGARET. Mediated generalization: the generalization of a conditioned galvanic skin response established to a pictured object. Amer. J. PsychoL, 1943, 56, 438448. 56. KENT, GRACE H., & ROSANOFF, A, J. A study of association in insanity. Amer. J. Insanity, 1910, 67, 37-96, 317-390. 57. KOFFKA, K. Principles of gestalt psychology. New York: Harcourt Brace, 1935. 58. KOHLER, W. Tlie mentality of apes. (Trans, by E. Winter.) New York: Harcourt Brace, 1925. 59. KOTLIAREVSKY, L. I. Cardio-vascular conditioned reflexes to direct and to verbal stimuli (trans, from Russian title), 1936. PsychoL Abstracts, 1939, 13, No. 4046. 60. Langfeld, H. S. Note on a case of chromaesthesia. PsychoL Bull., 1914, 11, 113-114. 61. Lazarus, R. S., & McCLEARY, R. A. Autonomic discrimination without awareness: A study of subception. PsychoL Rev., 1951, 58, 113-122, 62. LEVINE, R., CHEIN, I., & MURPHY, G. The relation of the intensity of a need to the amount of perceptual distortion: a preliminary report. /. PsychoL, 1942, 13, 283-293. 63. LYNCH, C. A. The memory values of certain alleged emotionally toned words. /. exper. Psycho!., 1932, 15, 298-315. 64. MCCLELLAND, D. C., & ATKINSON, J. W. The projective expression of needs: I. The effect of different intensities of the hunger drive on perception. /. PsychoL, 1948, 25, 205-222. 65. McGiNNiES, E. Emotionality and perceptual defense. PsychoL Rev., 1949, 56, 244-251. 66. McGiNNiES, E. Discussion of Howes' and Solomon's note on \"Emotionality and perceptual defense,\" PsychoL Rev., 1950, 57, 235-240. 67. MAIER, N. R. F. Reasoning in humans. III. The mechanisms of equivalent stimuli and of reasoning. /. exp. PsychoL, 1945, 35, 349-360. 68. MALINOWSKI, B. The problem of learning in primitive languages. Supplement in C. K. Ogden, & I. A. Richards. The meaning of meaning. New York: Harcourt Brace, 1938. 69. MARBE, K. Experimentell-psychologische Untersuchungen fiber das Urteil. Leipzig: Engelmann, 1901. 70. MASON, M. Changes in the galvanic skin response accompanying reports of changes in meaning during oral repetition. /. gen. PsychoL, 1941, 25, 353-401. 71. MAUSNER, B., & SIEGEL, A. The effect of variation in \"value\" on perceptual thresholds. J. abnorm, soc. PsychoL, 1950, 45, 760-763. 72. MAX, L. W. An experimental study of the motor theory of consciousness. III. Action-current responses in deaf-mutes during sleep, sensory stimulation, and dreams. /. comp. PsychoL, 1935, 19, 469486, 73. MAX, L. W. An experimental study of the motor theory of consciousness. IV. Action-current responses in the deaf during awakening, 236 CHARLES E. OSGOOD kinaesthetic imagery, and abstract thinking. J. conip. Psycho!,., 1937, 24, 301-344. 74. MELTON, A. W., & IRWIN, J. McQ. The influence of degree of interpolated learning on retroactive inhibition and the overt transfer of specific response. Amer. J, Psycho!., 1940, 53, 173-203. 75. METZNER, C. A. The influence of preliminary stimulation upon human eyelid responses during conditioning and during subsequent heteromodalgeneralization. Sitmm. Doct. Diss. Univ. Wis., 1942, 7,"
            },
            "slug": "The-nature-and-measurement-of-meaning.-Osgood",
            "title": {
                "fragments": [],
                "text": "The nature and measurement of meaning."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological bulletin"
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749391"
                        ],
                        "name": "N. Foo",
                        "slug": "N.-Foo",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Foo",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Foo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 61
                            }
                        ],
                        "text": "\u201d Similar claims are made by Rubenstein & Goodenough (1965), Grefenstette (1992b), and Charles (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15426942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30d82b86f2377f772ad91badbdbc19e638419243",
            "isKey": false,
            "numCitedBy": 852,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "nightmare\u2014small changes in the intended semantics of predicates or procedures cause large and usually catastrophic changes in performance. There have been, I think, two responses to this. One is owed to my colleague Paul Compton and his coworkers, using ripple-down rules (RDRs), which at the risk of oversimplification is the online adaptation of an expert sys-tem\u2014users provide feedback as they apply the system in real life. The beau-Conceptual Spaces\u2014The Geometry of Thought is a book by Peter G\u00e4rdenfors, professor of cognitive science at Lund University, Sweden. G\u00e4rdenfors has authored another book in this series (based on work with Carlos Alchour-ron and David Makinson), Knowledge in Flux, a definitive account of the widely examined AGM (after Alchour-ron, G\u00e4rdenfors, and Makinson) theory of belief revision. The AGM theory is firmly based on classical logic and its model theory, and by his founding participation in developing it, G\u00e4rden-fors has earned the right to critique knowledge representation. His new book is not primarily about logic, but it is certainly not an apostasy either. If I may be permitted a minor irrever-ence, I would say that this book came not to destroy logic but to fulfill. Knowledge representation as we know it has reached an impasse. There seems to be two problems, or perhaps, they are merely different perspectives of an underlying problem. The first is knowledge acquisition, and the second is machine learning. Conceptual Spaces has important messages for both. For machines to use knowledge it has to be somehow acquired. In principle , this knowledge can be hand coded into machines by experts or even elicited from them by an automated process. The practice is disappointing. The systems so constructed suffer from the aptly named brittle-ness problem, which is AI's version of the control engineer's sensitivity to be organized as collections of classes that reside in trees with their implicit hierarchy. Much AI a decade or more ago was devoted to the algebras and logics of property and method inheritance among classes so ordered. Today, a lot of effort is going into the automation, or at least semi-automation, of the construction of such ontologies. There is as yet no convincing evidence that these two responses will succeed in large-scale practice. Machine learning is commercially hot. There is no enterprise with access to large customer databases (social class, behavior patterns, credit history, and so on) that has not attempted to data mine them. My \u2026"
            },
            "slug": "Conceptual-Spaces-\u2014-The-Geometry-of-Thought-Foo",
            "title": {
                "fragments": [],
                "text": "Conceptual Spaces \u2014 The Geometry of Thought"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Conceptual Spaces\u2014The Geometry of Thought is a book by Peter G\u00e4rdenfors, professor of cognitive science at Lund University, Sweden, a definitive account of the widely examined AGM theory of belief revision."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13490933"
                        ],
                        "name": "E. Bingham",
                        "slug": "E.-Bingham",
                        "structuredName": {
                            "firstName": "Ella",
                            "lastName": "Bingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bingham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 547,
                                "start": 77
                            }
                        ],
                        "text": "(2001), who studied the effects of using different weighting schemes in LSA; Bingham and Mannila (2001), who investigated the effects of using different dimensionality-reduction techniques; and Weeds et al. (2004), who studied the effects of using different similarity measures for computing distributional similarity. By contrast, the impact of using different types of context has been severely neglected. The only previous investigation of the impact of different contexts on the word-space model that I am aware of is by Lavelli et al. (2004), to which I will return in Chapter 15."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 77
                            }
                        ],
                        "text": "(2001), who studied the effects of using different weighting schemes in LSA; Bingham and Mannila (2001), who investigated the effects of using different dimensionality-reduction techniques; and Weeds et al. (2004), who studied the effects of using different similarity measures for computing distributional similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 77
                            }
                        ],
                        "text": "(2001), who studied the effects of using different weighting schemes in LSA; Bingham and Mannila (2001), who investigated the effects of using different dimensionality-reduction techniques; and Weeds et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 194
                            }
                        ],
                        "text": "Proponents of factor-analytic dimensionality-reduction techniques tend to argue that these techniques uncover the topics in the form of latent dimensions, or independent or principal components (Bingham, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 297427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5a74dde7186bcdfd5a9dbee4392acad15e708c5",
            "isKey": true,
            "numCitedBy": 32,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis considers the problem of finding latent structure in high dimensional data. It is assumed that the observed data are generated by unknown latent variables and their interactions. The task is to find these latent variables and the way they interact, given the observed data only. It is assumed that the latent variables do not depend on each other but act independently. A popular method for solving the above problem is independent component analysis (ICA). It is a statistical method for expressing a set of multidimensional observations as a combination of unknown latent variables that are statistically independent of each other. Starting from ICA, several methods of estimating the latent structure in different problem settings are derived and presented in this thesis. An ICA algorithm for analyzing complex valued signals is given; a way of using ICA in the context of regression is discussed; and an ICA-type algorithm is used for analyzing the topics in dynamically changing text data. In addition to ICA-type methods, two algorithms are given for estimating the latent structure in binary valued data. Experimental results are given on all of the presented methods. Another, partially overlapping problem considered in this thesis is dimensionality reduction. Empirical validation is given on a computationally simple method called random projection: it does not introduce severe distortions in the data. It is also proposed that random projection could be used as a preprocessing method prior to ICA, and experimental results are shown to support this claim. This thesis also contains several literature surveys on various aspects of finding the latent structure in high dimensional data."
            },
            "slug": "Advances-in-independent-component-analysis-with-to-Bingham",
            "title": {
                "fragments": [],
                "text": "Advances in independent component analysis with applications to data mining"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Several methods of estimating the latent structure in different problem settings are derived and presented and it is proposed that random projection could be used as a preprocessing method prior to ICA, and experimental results are shown to support this claim."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144252965"
                        ],
                        "name": "Edgar Ch\u00e1vez",
                        "slug": "Edgar-Ch\u00e1vez",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Ch\u00e1vez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edgar Ch\u00e1vez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143678972"
                        ],
                        "name": "G. Navarro",
                        "slug": "G.-Navarro",
                        "structuredName": {
                            "firstName": "Gonzalo",
                            "lastName": "Navarro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Navarro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 49994396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41ec81b8043a19a62c6e7a28500f4d9696fb809c",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Searching in metric spaces is the problem of, given a set of elements and a distance function deened among them, nd all the elements close enough to a given query element. For eeciency, they try to minimize the number of evaluations of the distance function. This problem has a large number of applications, and a well-known particular case is that of vector spaces, where the objects ar\u00e8-dimensional coordinates. The search problem is known to be diicult as the dimension`grows. However, in practice the \\intrinsic\" dimension of a real-world vector space is less than`because the elements are not uniformly distributed. The behavior of all the search algorithms is known to be related to the intrinsic dimension, but deening it is diicult. Moreover, a similar phenomenon is observed in general metric spaces, where there are no coordinates. In this paper we introduce a new deenition of intrinsic dimensionality that is simple and eecient to estimate and which is shown analytically and experimentally to capture the essential feature of metric spaces that determine the behavior of all the search algorithms. We prove analytically that our deenition extends naturally that of vector spaces and that lower bounds on the behavior of a large class of proximity search algorithms can be stated as a function of our intrinsic dimensionality measure."
            },
            "slug": "Measuring-the-Dimensionality-of-General-Metric-Ch\u00e1vez-Navarro",
            "title": {
                "fragments": [],
                "text": "Measuring the Dimensionality of General Metric Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper introduces a new deenition of intrinsic dimensionality that is simple and eecient to estimate and which is shown analytically and experimentally to capture the essential feature of metric spaces that determine the behavior of all the search algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103834654"
                        ],
                        "name": "Roy Harris",
                        "slug": "Roy-Harris",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Harris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roy Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 109
                            }
                        ],
                        "text": "Other attempts at using linguistic information for computing distributional similarity between words include Hindle (1990), who used predicate\u2013argument structure to determine the similarity of nouns; Hearst (1992), who extracted hyponyms by using lexical\u2013syntactic templates; Ruge (1992), who used head\u2013modifier relations for extracting similar words; and Grefenstette (1992a, 1992b, 1993), who also used syntactic context to measure similarity between words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 143658585,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "2975a81b406539927974083144ad1cf5470106d1",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Reviews of the First Edition 'In a very readable study, Roy Harris examines the mis/representation of Saussurean ideas by certain linguists and by French thinkers of a Structuralist and post-structuralist persuasion. This is therefore essentially a study in intellectual history dealing with the thorny question of \"influence\" of one thinker's ideas on another ! Harris has done us a favour by bringing his critical eye to bear on a range of (mis)interpreters of the Cours who had hitherto largely been considered in isolation.' - Modern Language Review This book is the first major reassessment of the reception of Saussure's ideas in the academic world of the twentieth century. It is well known that Saussure's work profoundly influenced developments in such diverse fields as linguistics, anthropology, psychology and literary studies. But what exactly were Saussure's views taken to be by his interpreters? How well were Saussure's ideas understood by those who took them up? Or how badly misunderstood? And why? The answers to these questions address central issues in the history of Western culture. Each chapter focuses on one particular interpreter of Saussure's work, but many others are mentioned in context for purposes of comparison, and attention is drawn to connections and disparities between their interpretations. Those whose interpretations are examined in detail include Bloomfield, Hjelmslev, Jakobson, Levi-Strauss, Chomsky, Barthes and Derrida. In an important supplement to the new edition, account is taken of recently found notes in Saussure's hand. The discovery re-opens the whole question of the extent to which the Cours de linguistique generale, posthumously published in 1916, accurately reflects the stage that Saussure's thinking about language and communication had reached by the time of his death. It suggests a new interpretation of Saussure that differs significantly from any of those previously advanced. Features: *The only comprehensive survey of this field *Up-to-date coverage of recent developments in Saussurean studies *Analysis by one of today's leading authorities on Saussure."
            },
            "slug": "Saussure-and-his-Interpreters-Harris",
            "title": {
                "fragments": [],
                "text": "Saussure and His Interpreters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143716922"
                        ],
                        "name": "G. Lakoff",
                        "slug": "G.-Lakoff",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Lakoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lakoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49900438"
                        ],
                        "name": "Mark L. Johnson",
                        "slug": "Mark-L.-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark L. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16103621,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology",
                "Art"
            ],
            "id": "1745ec3f918cd551a8579261d3cfb0403de6a7be",
            "isKey": false,
            "numCitedBy": 5903,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "* Introduction: Who Are We? How The Embodied Mind Challenges The Western Philosophical Tradition * The Cognitive Unconscious * The Embodied Mind * Primary Metaphor and Subjective Experience * The Anatomy of Complex Metaphor * Embodied Realism: Cognitive Science Versus A Priori Philosophy * Realism and Truth * Metaphor and Truth The Cognitive Science Of Basic Philosophical Ideas * The Cognitive Science of Philosophical Ideas * Time * Events and Causes * The Mind * The Self * Morality The Cognitive Science Of Philosophy * The Cognitive Science of Philosophy * The Pre-Socratics: The Cognitive Science of Early Greek Metaphysics * Plato * Aristotle * Descartes and the Enlightenment Mind * Kantian Morality * Analytic Philosophy * Chomskys Philosophy and Cognitive Linguistics * The Theory of Rational Action * How Philosophical Theories Work Embodied Philosophy * Philosophy in the Flesh"
            },
            "slug": "Philosophy-in-the-flesh-:-the-embodied-mind-and-its-Lakoff-Johnson",
            "title": {
                "fragments": [],
                "text": "Philosophy in the flesh : the embodied mind and its challenge to Western thought"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57931704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcdb9bd64e3d7885c10938291153257b94f3df91",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nMotivated by the remarkable fluidity of memory the way in which items are pulled spontaneously and effortlessly from our memory by vague similarities to what is currently occupying our attention Sparse Distributed Memory presents a mathematically elegant theory of human long term memory. \nThe book, which is self contained, begins with background material from mathematics, computers, and neurophysiology; this is followed by a step by step development of the memory model. The concluding chapter describes an autonomous system that builds from experience an internal model of the world and bases its operation on that internal model. Close attention is paid to the engineering of the memory, including comparisons to ordinary computer memories. \nSparse Distributed Memory provides an overall perspective on neural systems. The model it describes can aid in understanding human memory and learning, and a system based on it sheds light on outstanding problems in philosophy and artificial intelligence. Applications of the memory are expected to be found in the creation of adaptive systems for signal processing, speech, vision, motor control, and (in general) robots. Perhaps the most exciting aspect of the memory, in its implications for research in neural networks, is that its realization with neuronlike components resembles the cortex of the cerebellum. \nPentti Kanerva is a scientist at the Research Institute for Advanced Computer Science at the NASA Ames Research Center and a visiting scholar at the Stanford Center for the Study of Language and Information. A Bradford Book."
            },
            "slug": "Sparse-Distributed-Memory-Kanerva",
            "title": {
                "fragments": [],
                "text": "Sparse Distributed Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Pentti Kanerva's Sparse Distributed Memory presents a mathematically elegant theory of human long term memory that resembles the cortex of the cerebellum, and provides an overall perspective on neural systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13490933"
                        ],
                        "name": "E. Bingham",
                        "slug": "E.-Bingham",
                        "structuredName": {
                            "firstName": "Ella",
                            "lastName": "Bingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712654"
                        ],
                        "name": "H. Mannila",
                        "slug": "H.-Mannila",
                        "structuredName": {
                            "firstName": "Heikki",
                            "lastName": "Mannila",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mannila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 547,
                                "start": 77
                            }
                        ],
                        "text": "(2001), who studied the effects of using different weighting schemes in LSA; Bingham and Mannila (2001), who investigated the effects of using different dimensionality-reduction techniques; and Weeds et al. (2004), who studied the effects of using different similarity measures for computing distributional similarity. By contrast, the impact of using different types of context has been severely neglected. The only previous investigation of the impact of different contexts on the word-space model that I am aware of is by Lavelli et al. (2004), to which I will return in Chapter 15."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 77
                            }
                        ],
                        "text": "(2001), who studied the effects of using different weighting schemes in LSA; Bingham and Mannila (2001), who investigated the effects of using different dimensionality-reduction techniques; and Weeds et al. (2004), who studied the effects of using different similarity measures for computing distributional similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 77
                            }
                        ],
                        "text": "(2001), who studied the effects of using different weighting schemes in LSA; Bingham and Mannila (2001), who investigated the effects of using different dimensionality-reduction techniques; and Weeds et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1854295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c58166098c07f2efe30651446a0f4f19b9b7ce9",
            "isKey": false,
            "numCitedBy": 1362,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection."
            },
            "slug": "Random-projection-in-dimensionality-reduction:-to-Bingham-Mannila",
            "title": {
                "fragments": [],
                "text": "Random projection in dimensionality reduction: applications to image and text data"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38757,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3185778"
                        ],
                        "name": "D. Sahlin",
                        "slug": "D.-Sahlin",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Sahlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sahlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14352431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2573845ba7f1fa9746d7e66ece5be1202e672059",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 119,
            "paperAbstract": {
                "fragments": [],
                "text": "A partial evaluator for Prolog takes a program and a query and returns a program specialized for all instances of that query. The intention is that the generated program will execute more efficiently than the original one for those instances. This thesis presents \u201cMixtus\u201d, an automatic partial evaluator for full Prolog, i.e. including non-logical features such as cut, assert, var, write etc. Mixtus does not need any annotations to guide it in its process, but will automatically generate a program given only an input program and a query. A definition of partial evaluation based on the procedural semantics rather than declarative semantics is presented. A program transformed by Mixtus will behave identically as the original one, including side-effects and the order of the solutions returned. The three basic program transformations \u201cunfolding\u201d, \u201cfolding\u201d and \u201cdefinition\u201d are adapted to full Prolog. The most important transformation is \u201cunfolding\u201d which is used by all partial evaluators to eliminate run-time procedure calls. The \u201cfolding\u201d and \u201cdefinition\u201d transformations generate new recursive predicates specialized for the query. A loop prevention mechanism specifically tailored for partial evaluation together with a mechanism for \u201cgeneralized restart\u201d, will guarantee the termination of the partial evaluation for all programs. Special attention is given to built-in predicates, the cut in particular. To facilitate the execution and removal of cuts during partial evaluation a module for determinacy analysis is used. As negation in Prolog is based on the cut, it will also be treated correctly. Some of the other predicates that are given extra care are dif/2, findall/3 and bagof/3. Many program transformations possible in a logic programming framework will not work correctly for full Prolog. Built-in and generated predicates are therefore classified according to their declarative \u201cpurity\u201d so that the \u201cimpure\u201d predicates will not hinder the optimization process unnecessarily. It is a non-trivial task to implement a partial evaluator, so the basic features of the implementation of Mixtus are also presented. A number of sample programs are partially evaluated, and the effect is discussed."
            },
            "slug": "An-Automatic-Partial-Evaluator-for-Full-Prolog-Sahlin",
            "title": {
                "fragments": [],
                "text": "An Automatic Partial Evaluator for Full Prolog"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This thesis presents \u201cMixtus\u201d, an automatic partial evaluator for full Prolog, i.e. Mixtus does not need any annotations to guide it in its process, but will automatically generate a program given only an input program and a query."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11126643"
                        ],
                        "name": "A. Waern",
                        "slug": "A.-Waern",
                        "structuredName": {
                            "firstName": "Annika",
                            "lastName": "Waern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waern"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59702858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab682207d8c1a90b25712e0ff1208ae7f1c4e8fe",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "To My Children Curae mihi non est quod alii parentes faciant. Abstract Plan recognition is the task of ascribing intentions about plans to an actor, based on observing the agent's actions or utterances. The plan recognition problem appears in three diierent forms: Plan recognition when the actor is aware and actively cooperating to the recognition, for example by choosing actions that make the task easier (intended plan recognition), plan recognition when the actor is unaware of or indiierent to the plan recognition process (keyhole plan recognition), or plan recognition when the actor is aware of and actively obstructs the plan recognition process (obstructed plan recognition). I consider a speciic application of plan recognition: that when a computer system ascribes intended plans to human users interacting with the system. In computer interfaces , intended plan recognition becomes an almost trivial task, similar to the task of interpreting a command language, whereas keyhole plan recognition can be hard, or even impossible, to achieve. In this thesis, I formulate ways to combine intended and keyhole plan recognition in human-computer interaction applications. I deene a principle for designing interfaces, \\co-operative task enrichment\" that employ plan recognition, in which users can inspect and control the results of keyhole plan recognition. This design principle achieves a way to use the results from keyhole plan recognition to encourage intended plan recognition in the interface design. The design principle has been utilised in an information seeking application, where the answers to user queries are adapted to the user's task. The main reason that keyhole plan recognition may be hard or impossible to achieve in interfaces, is that the user may not be following a pre-planned behaviour, and for this reason seem to frequently change his or her plan. For some applications, this problem can be circumvented by restricting the system's \\memory\", so that plan recognition only takes into account a number of the most recent actions instead of all observed user actions. I describe a very simple way to implement such a plan recognition mechanism, the \\intention guessing\" approach. I describe a reasoning framework for plan recognition, in which keyhole and intended plan recognition can be integrated. The main feature of the integrated reasoning framework is that several components that usually are kept implicit in frameworks for plan recognition are here represented explicitly. The integration of keyhole and intended plan recognition requires that multiple deenitions of intentions can \u2026"
            },
            "slug": "Recognising-Human-Plans:-Issues-for-Plan-in-Human-Waern",
            "title": {
                "fragments": [],
                "text": "Recognising Human Plans: Issues for Plan Recognition in Human - Computer Interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This thesis formulate ways to combine intended and keyhole plan recognition in human-computer interaction applications, and describes a reasoning framework for plan recognition, in which keyhole and intended plan recognition can be integrated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54122395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27e4a7373e6940a038f927a01e6d51958ad7c417",
            "isKey": false,
            "numCitedBy": 10143,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Self-Organising Map (SOM) algorithm was introduced by the author in 1981. Its theory and many applications form one of the major approaches to the contemporary artificial neural networks field, and new technologies have already been based on it. The most important practical applications are in exploratory data analysis, pattern recognition, speech analysis, robotics, industrial and medical diagnostics, instrumentation, and control, and literally hundreds of other tasks. In this monograph the mathematical preliminaries, background, basic ideas, and implications are expounded in a manner which is accessible without prior expert knowledge."
            },
            "slug": "Self-Organizing-Maps-Kohonen",
            "title": {
                "fragments": [],
                "text": "Self-Organizing Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The mathematical preliminaries, background, basic ideas, and implications of the Self-Organising Map algorithm are expounded in a manner which is accessible without prior expert knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "Springer Series in Information Sciences"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46607442"
                        ],
                        "name": "Yoshinori Uesaka",
                        "slug": "Yoshinori-Uesaka",
                        "structuredName": {
                            "firstName": "Yoshinori",
                            "lastName": "Uesaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshinori Uesaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7142317"
                        ],
                        "name": "H. Asoh",
                        "slug": "H.-Asoh",
                        "structuredName": {
                            "firstName": "Hideki",
                            "lastName": "Asoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Asoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61134577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10540457c44daa9424d27eebb02f71a0dffb64d6",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In 1992 Japan's Ministry of International Trade and Industry (MITI) began a research program in Real World Computing, as a successor to the Fifth Generation Computing program of the previous decade, complementing the fifth-generation approach. Its objective is to lay a foundation and to pursue the technical realisation of humanlike flexible and intelligent information processing. This book collects results of ten years of original research by five research laboratories in Japan and Europe, whose research focus has been the theoretical and algorithmic foundations of intelligence as manifested in the real world an in our dealing with it. \nReal-world intelligent systems handle complex, uncertain, dynamic, multimodal information in real time. Both explicit and implicit information are important. Hence we need to develop a novel integrated framework of representing knowledge and making inferences based in it. It is impossible to pre-program all the knowledge needed for coping with the variety and complexity of real environments, and therefore learning and adaptation are keys to intelligence. Learning is a kind of meta-programming strategy. Instead of writing programs for specific tasks, we must write programs that modify themselves based on a system's interaction with its environment. \nThe book includes chapters on inference and learning with graphical models, approximate reasoning, evolutionary computation and beyond, methodology of distributed and active learning, and computing with large random patterns. The treatment is mathematically rigorous, and the discussion of issues is of general interest to an educated reader at large. The book provides excellent reading for graduate courses in Computer Science, Cognitive Science, Artificial Intelligence, and Applied Statistics. \nYoshinori Uesaka is a professor of information sciences at the Science University of Tokyo. Pentti Kanerva is a senior researcher at the Swedish Institute of Computer Science. Hideki Asoh is a senior researcher at the Electrotechnical Laboratory in Tsukuba City, Japan"
            },
            "slug": "Foundations-of-real-world-intelligence-Uesaka-Kanerva",
            "title": {
                "fragments": [],
                "text": "Foundations of real-world intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book collects results of ten years of original research by five research laboratories in Japan and Europe, whose research focus has been the theoretical and algorithmic foundations of intelligence as manifested in the real world an in the authors' dealing with it."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82220451"
                        ],
                        "name": "Sameh El-Ansary",
                        "slug": "Sameh-El-Ansary",
                        "structuredName": {
                            "firstName": "Sameh",
                            "lastName": "El-Ansary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameh El-Ansary"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57056902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05d7ac051fa497bc87ef8fc0ca50ec5efd25ef85",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Peer-to-Peer (P2P) computing is a recent hot topic in the areas of networking and distributed systems. Work on P2P computing was triggered by a number of ad-hoc systems that made the concept popular. Later, academic research efforts started to investigate P2P computing issues based on scientific principles. Some of that research produced a number of structured P2P systems that were collectively referred to by the term \"Distributed Hash Tables\" (DHTs). However, the research occurred in a diversified way leading to the appearance of similar concepts yet lacking a common perspective and not heavily analyzed. In this thesis we present a number of papers representing our research results in the area of structured P2P systems grouped as two sets labeled respectively \"Designs\" and \"Analyses\". \n \nThe contribution of the first set of papers is as follows. First, we present the princi- ple of distributed k-ary search and argue that it serves as a framework for most of the recent P2P systems known as DHTs. That is, given this framework, understanding existing DHT systems is done simply by seeing how they are instances of that frame- work. We argue that by perceiving systems as instances of that framework, one can optimize some of them. We illustrate that by applying the framework to the Chord system, one of the most established DHT systems. Second, we show how the frame- work helps in the design of P2P algorithms by two examples: (a) The DKS(n; k; f) system which is a system designed from the beginning on the principles of distributed k-ary search. (b) Two broadcast algorithms that take advantage of the distributed k-ary search tree. \n \nThe contribution of the second set of papers is as follows. We account for two approaches that we used to evaluate the performance of a particular class of DHTs, namely the one adopting periodic stabilization for topology maintenance. The first approach was of an intrinsic empirical nature. In this approach, we tried to perceive a DHT as a physical system and account for its properties in a size-independent manner. The second approach was of a more analytical nature. In this approach, we applied the technique of Master Equations, which is a widely used technique in the analysis of natural systems. The application of the technique lead to a highly accurate description of the behavior of structured overlays. Additionally, the thesis contains a primer on structured P2P systems that tries to capture the main ideas prevailing in the field."
            },
            "slug": "Designs-and-Analyses-in-Structured-Peer-To-Peer-El-Ansary",
            "title": {
                "fragments": [],
                "text": "Designs and Analyses in Structured Peer-To-Peer Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis presents the princi- ple of distributed k-ary search and argue that it serves as a framework for most of the recent P2P systems known as DHTs and argues that by perceiving systems as instances of that framework, one can optimize some of them."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340833"
                        ],
                        "name": "B. Ahlgren",
                        "slug": "B.-Ahlgren",
                        "structuredName": {
                            "firstName": "Bengt",
                            "lastName": "Ahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ahlgren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15069462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e35e895d9aaab05ea4d99f77cf8e2d3923dd0b7",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "New computer applications that handle large quantities of data are made feasible by data communication networks with high communication throughput. Desktop computers , however, have problems delivering the full network throughput to applications. The bottleneck is the rate at which data can be transferred in and out of the computer's memory. In this thesis, composed of six papers, I study techniques for reducing the memory bandwidth used by the computer's communication software and hardware. The topics of the papers include network adapter design, application programming interfaces and the implementation of data manipulation functions in communication protocols. In the area of network adapters I study new communication software architectures that use less memory bandwidth by not copying data and the resulting consequences on the adapter hardware. I present and experimentally evaluate an application program interface that does not copy data. I show that the interface has a signiicantly improved data throughput compared to a conventional interface. Data manipulation functions, such as checksum calculation and presentation encoding , access message data in the computer's memory. Integrated Layer Processing (ILP) is an implementation technique which purpose is to reduce the function's memory accesses and thus increase communication throughput. I demonstrate that ILP in some circumstances instead increases the number of accesses, and therefore reduces throughput. It occurs when the aggregated function state can not be t in the processor registers. I present a performance model that predicts the performance of ILP and conventional implementations of data manipulation functions. The relationship between memory accesses and use of registers is central in the model. The model can be used to choose implementation technique before a possibly expensive implementation project is started. To my family Acknowledgments First of all I would like to thank my advisor and dear friend, Per Gunningberg. He has without sellshness given me the opportunity to grow as a researcher, treating me almost more like a research partner than his student. He has not only advised me, he has also made sure I had suucient funding for my thesis work at SICS. And thank you, Per, for bringing out th\u00e8blowtorch' now and then when I lost speed. I will always be grateful for all the help that Mats Bjj orkman has provided in the course of preparing this thesis, especially for the excellent collaboration as a co-author of three of the papers. I also very much appreciate his travel company on the \u2026"
            },
            "slug": "Improving-computer-communication-performance-by-Ahlgren",
            "title": {
                "fragments": [],
                "text": "Improving computer communication performance by reducing memory bandwidth consumption"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis studies techniques for reducing the memory bandwidth used by the computer's communication software and hardware, and presents a performance model that predicts the performance of ILP and conventional implementations of data manipulation functions."
            },
            "venue": {
                "fragments": [],
                "text": "SICS dissertation series"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50537578"
                        ],
                        "name": "W. Johnson",
                        "slug": "W.-Johnson",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Johnson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117819162,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1d0635cda34b8af995313848a0c42bac6efe79ec",
            "isKey": false,
            "numCitedBy": 2547,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "(Here ll&lltip is the Lipschitz constant of the function g.) A classical result of Kirszbraun's [14, p. 48] states that L(t2, n) = 1 for all n, but it is easy to see that L(X, n) ~ ~ as n ~ ~ for many metric spaces X. Marcus and Pisier [10] initiated the study of L(X, n) for X = Lp. (For brevity, we will use hereafter the notation L(p, n) for L(Lp(O,l), n).) They prove that for each 1 < p < 2 there is a constant C(p) so that for n = 2, 3, 4, , , ,"
            },
            "slug": "Extensions-of-Lipschitz-mappings-into-Hilbert-space-Johnson",
            "title": {
                "fragments": [],
                "text": "Extensions of Lipschitz mappings into Hilbert space"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103849073"
                        ],
                        "name": "L. Rasmusson",
                        "slug": "L.-Rasmusson",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Rasmusson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rasmusson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 166528343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4ade501b2fa180bdbd12a1be947d5dc3fe75f0a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "A design of an automatic network capacity markets, often referred to as a bandwidth market, is presented. Three topics are investigated. First, a network model is proposed. The proposed model is based upon a trisection of the participant roles into network users, network owners, and market middlemen. The network capacity is defined in a way that allows it to be traded, and to have a well defined price. The network devices are modeled as core nodes, access nodes, and border nodes. Requirements on these are given. It is shown how their functionalities can be implemented in a network. Second, a simulated capacity market is presented, and a statistical method for estimating the price dynamics in the market is proposed. A method for pricing network services based on shared capacity is proposed, in which the price of a service is equivalent to that of a financial derivative contract on a number of simple capacity shares.Third, protocols for the interaction between the participants are proposed. The market participants need to commit to contracts with an auditable protocol with a small overhead. The proposed protocol is based on a public key infrastructure and on known protocols for multi party contract signing. The proposed model allows network capacity to be traded in a manner that utilizes the network efficiently. A new feature of this market model, compared to other network capacity markets, is that the prices are not controlled by the network owners. It is the end-users who, by middlemen, trade capacity among each other. Therefore, financial, rather than control theoretic, methods are used for the pricing of capacity."
            },
            "slug": "Network-capacity-sharing-with-QoS-as-a-financial-Rasmusson",
            "title": {
                "fragments": [],
                "text": "Network capacity sharing with QoS as a financial derivative pricing problem: algorithms and network"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A design of an automatic network capacity markets, often referred to as a bandwidth market, is presented, and a statistical method for estimating the price dynamics in the market is proposed and protocols for the interaction between the participants are proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50666631"
                        ],
                        "name": "J. Armstrong",
                        "slug": "J.-Armstrong",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Armstrong",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Armstrong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28795665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2048676806baee4c27934153ae7aa22f17094cec",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "The work described in this thesis is the result of a research program started in 1981 to find better ways of programming Telecom applications. These applications are large programs which despite careful testing will probably contain many errors when the program is put into service. We assume that such programs do contain errors, and investigate methods for building reliable systems despite such errors. \nThe research has resulted in the development of a new programming language (called Erlang), together with a design methodology, and set of libraries for building robust systems (called OTP). At the time of writing the technology described here is used in a number of major Ericsson, and Nortel products. A number of small companies have also been formed which exploit the technology. \nThe central problem addressed by this thesis is the problem of constructing reliable systems from programs which may themselves contain errors. Constructing such systems imposes a number of requirements on any programming language that is to be used for the construction. I discuss these language requirements, and show how they are satisfied by Erlang. \nProblems can be solved in a programming language, or in the standard libraries which accompany the language. I argue how certain of the requirements necessary to build a fault-tolerant system are solved in the language, and others are solved in the standard libraries. Together these form a basis for building fault-tolerant software systems. \nNo theory is complete without proof that the ideas work in practice. To demonstrate that these ideas work in practice I present a number of case studies of large commercially successful products which use this technology. At the time of writing the largest of these projects is a major Ericsson product, having over a million lines of Erlang code. This product (the AXD301) is thought to be one of the most reliable products ever made by Ericsson. \nFinally, I ask if the goal of finding better ways to program Telecom applications was fulfilled --- I also point to areas where I think the system could be improved."
            },
            "slug": "Making-reliable-distributed-systems-in-the-presence-Armstrong",
            "title": {
                "fragments": [],
                "text": "Making reliable distributed systems in the presence of software errors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued how certain of the requirements necessary to build a fault-tolerant system are solved in the language, and others are solve in the standard libraries, which form a basis for building fault-Tolerant software systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221630"
                        ],
                        "name": "M. Sj\u00f6linder",
                        "slug": "M.-Sj\u00f6linder",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Sj\u00f6linder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sj\u00f6linder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 110108197,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "855cd594ab4de66e19ad65d23f405950693bd774",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 261,
            "paperAbstract": {
                "fragments": [],
                "text": "The older population is increasing, as is life expectancy. Technical devices are becoming more widespread and used for many everyday tasks. Knowledge about new technology is important to remain as an active and independent part of the society. However, if an old user group should have equal access to this technology, new demands will be placed on the design of interfaces and devices. With respect to old users it is and will be important to develop technical devices and interfaces that take the age-related decline in physical and cognitive abilities into account. The aim of this work was to investigate to what extent the age-related cognitive decline affects performance on different computer-related tasks and the use of different interfaces. With respect to the use of computer interfaces, two studies were conducted. In the first study, the information was presented with a hierarchical structure. In the second study the information was presented as a 3D-environment, and it was also investigated how an overview map could support navigation. The third study examined the age-related cognitive decline in the use of a small mobile phone display with a hierarchical information structure. The results from the studies showed that the most pronounced age-related difference was found in the use of the 3D-environment. Within this environment, prior experience was found to have the largest impact on performance. Regarding the hierarchical information structures, prior experience seemed to have a larger impact on performance of easy tasks, while age and cognitive abilities had a larger impact on performance of more complex tasks. With respect to navigation aids, the overview map in the 3D-environment did not reduce the age-differences; however, it contributed to a better perceived orientation and reduced the feeling of being lost."
            },
            "slug": "Age-related-cognitive-decline-and-navigation-in-Sj\u00f6linder",
            "title": {
                "fragments": [],
                "text": "Age-related cognitive decline and navigation in electronic environments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144440"
                        ],
                        "name": "Markus Bylund",
                        "slug": "Markus-Bylund",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Bylund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Bylund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8823631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b467c569500d24690ce215ab4a98eff0b86f53b",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 258,
            "paperAbstract": {
                "fragments": [],
                "text": "The vision of pervasive computing promises a shift from information \ntechnology per se to what can be accomplished by using it, thereby \nfundamentally changing the relationship between people and information \ntechnology. In order to realize this vision, a large number of issues \nconcerning user experience, contextual change, and technical \nrequirements should be addressed. We provide a design rationale for \npervasive computing that encompasses these issues, in which we argue \nthat a prominent aspect of user experience is to provide user control, \nprimarily founded in human values. As one of the more significant \naspects of the user experience, we provide an extended discussion about \nprivacy. With contextual change, we address the fundamental change in \npreviously established relationships between the practices of \nindividuals, social institutions, and physical environments that \npervasive computing entails. Finally, issues of technical requirements \nrefer to technology neutrality and openness--factors that we argue are \nfundamental for realizing pervasive computing. \nWe describe a number of empirical and technical studies, the results of \nwhich have helped to verify aspects of the design rationale as well as \nshaping new aspects of it. The empirical studies include an \nethnographic-inspired study focusing on information technology support \nfor everyday activities, a study based on structured interviews \nconcerning relationships between contexts of use and everyday planning \nactivities, and a focus group study of laypeople\u2019s interpretations of \nthe concept of privacy in relation to information technology. The first \ntechnical study concerns the model of personal service environments as a \nmeans for addressing a number of challenges concerning user experience, \ncontextual change, and technical requirements. Two other technical \nstudies relate to a model for device-independent service development and \nthe wearable server as a means to address issues of continuous usage \nexperience and technology neutrality respectively."
            },
            "slug": "A-Design-Rationale-for-Pervasive-Computing-User-and-Bylund",
            "title": {
                "fragments": [],
                "text": "A Design Rationale for Pervasive Computing - User Experience, Contextual Change, and Technical Requirements"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A design rationale for pervasive computing is provided that a prominent aspect of user experience is to provide user control, primarily founded in human values, and an extended discussion about privacy is provided."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 24
                            }
                        ],
                        "text": "(1990), Sch\u00fctze (1992), Pereira et al. (1993), and Niwa & Nitta (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 678,
                                "start": 24
                            }
                        ],
                        "text": "(1990), Sch\u00fctze (1992), Pereira et al. (1993), and Niwa & Nitta (1994). The arguably most influential work from this period comes from Hinrich Sch\u00fctze (1992, 1993), who builds context vectors (which he calls \u201cterm vectors\u201d or \u201cword vectors\u201d) in precisely the manner described in Section 3.1 above: co-occurrence counts are collected in a words-by-words matrix, in which the elements record the number of times two words co-occur within a set window of word tokens. Context vectors are then defined as the rows or the columns of the matrix (the matrix is symmetric, so it does not matter if the rows or the columns are used). A similar approach is described by Qiu & Frei (1993), with the difference that they use a words-by-documents matrix to collect the cooccurrence counts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 24
                            }
                        ],
                        "text": "(1990), Sch\u00fctze (1992), Pereira et al. (1993), and Niwa & Nitta (1994). The arguably most influential work from this period comes from Hinrich Sch\u00fctze (1992, 1993), who builds context vectors (which he calls \u201cterm vectors\u201d or \u201cword vectors\u201d) in precisely the manner described in Section 3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributional clustering of english"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 56
                            }
                        ],
                        "text": "One of the earliest examples of such methods comes from Gallant (1991b), who, in addition to the (traditional) feature vectors, used what he called dynamic context vectors computed from the contexts in which the words occur."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 588,
                                "start": 104
                            }
                        ],
                        "text": "Waltz & Pollack\u2019s version of the feature-space approach was in its turn a major inspiration for Stephen Gallant, who introduced the term \u201ccontext vector\u201d to describe the feature-space representations (Gallant, 1991a, 1991b). In Gallant\u2019s algorithm, context vectors were defined by a set of manually derived features, such as \u201chuman,\u201d \u201cman,\u201d \u201cmachine,\u201d etc. A simplified example of a manually defined context vector, such as those used in Gallant\u2019s algorithm is displayed in Table 3.4. Remnants of the feature space approach is still used in cognitive science, by, e.g., G\u00e4rdenfors (2000) under the term conceptual spaces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Context vector representations for document retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "AAAI Natural Language Text Retrieval Workshop."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145267461"
                        ],
                        "name": "M. Bj\u00f6rkman",
                        "slug": "M.-Bj\u00f6rkman",
                        "structuredName": {
                            "firstName": "Mats",
                            "lastName": "Bj\u00f6rkman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bj\u00f6rkman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195837887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f511f4b6e6485c84ff7646f1559a29944d80c89b",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Architectures-for-high-performance-communication-Bj\u00f6rkman",
            "title": {
                "fragments": [],
                "text": "Architectures for high performance communication"
            },
            "venue": {
                "fragments": [],
                "text": "SICS dissertation series"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144541931"
                        ],
                        "name": "Edward E. Smith",
                        "slug": "Edward-E.-Smith",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Smith",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward E. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023259"
                        ],
                        "name": "D. Medin",
                        "slug": "D.-Medin",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Medin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Medin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 144952337,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "f9f79457703ec65a8c21b2017365bd9e176be800",
            "isKey": false,
            "numCitedBy": 1740,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Categories-and-concepts-Smith-Medin",
            "title": {
                "fragments": [],
                "text": "Categories and concepts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122118215,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b1649d21217a439d885e929e08d4ec38d83d33a1",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-Words-to-Understanding-Karlgren-Sahlgren",
            "title": {
                "fragments": [],
                "text": "From Words to Understanding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226331"
                        ],
                        "name": "C. Leacock",
                        "slug": "C.-Leacock",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Leacock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leacock"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117435097,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6c2a780a6db16d429e0ade90bb4a453725a9c725",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Lexical-representations-for-sentence-processing-Miller-Leacock",
            "title": {
                "fragments": [],
                "text": "Lexical representations for sentence processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4632093"
                        ],
                        "name": "G. Zipf",
                        "slug": "G.-Zipf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zipf",
                            "middleNames": [
                                "Kingsley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zipf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141120597,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac",
            "isKey": false,
            "numCitedBy": 7038,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-behavior-and-the-principle-of-least-effort-Zipf",
            "title": {
                "fragments": [],
                "text": "Human behavior and the principle of least effort"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69334380"
                        ],
                        "name": "J. M. Kittross",
                        "slug": "J.-M.-Kittross",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kittross",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Kittross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 158
                            }
                        ],
                        "text": "The pioneer in this field is Charles Osgood and his colleagues, who in the early 1950s developed the semantic differential approach to meaning representation (Osgood, 1952; Osgood et al., 1957)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": "(6)It is interesting to note that Osgood and colleagues already in 1957 (Osgood et al., 1957) \u2014 roughly 30 years before the advent of LSA \u2014 mentioned the use of factor analysis to uncover orthogonal dimensions in the semantic space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59962008,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "30a3177646528a00f19d716a1a03946e3502fc9d",
            "isKey": false,
            "numCitedBy": 4075,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-measurement-of-meaning-Kittross",
            "title": {
                "fragments": [],
                "text": "The measurement of meaning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51955156"
                        ],
                        "name": "G. W. Fumas",
                        "slug": "G.-W.-Fumas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Fumas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. W. Fumas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071282465"
                        ],
                        "name": "L. L. Beck",
                        "slug": "L.-L.-Beck",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Beck",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. L. Beck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59739393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70380dfd9b5e3ea1148471bf7449c1380d62c6d9",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-information-retrieval-using-latent-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Improving information retrieval using latent semantic indexing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57299021,
            "fieldsOfStudy": [],
            "id": "f1c8dc879137f7283a2af467a29f67eae8f1a8ec",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discrimination Decisions for 100,000-Dimensional Spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80727966"
                        ],
                        "name": "A. Kaplan",
                        "slug": "A.-Kaplan",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Kaplan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kaplan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 352,
                                "start": 0
                            }
                        ],
                        "text": "Kaplan (1955) asked people to identify the sense of a polysemous word when they were shown only the words in its immediate vicinity. They were almost always able to determine the sense of the word when shown a string of five words \u2014 i.e. a 2+2-sized context window. This experiment has been replicated with the same result by Choueka & Lusignan (1985). Our previous experiments (Karlgren & Sahlgren, 2001) also indicate that a narrow context window is preferable for acquiring semantic information."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Kaplan (1955) asked people to identify the sense of a polysemous word when they were shown only the words in its immediate vicinity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28060239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b1011458a3a662ad06201efc47d8ce4d36059bc",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-experimental-study-of-ambiguity-and-context-Kaplan",
            "title": {
                "fragments": [],
                "text": "An experimental study of ambiguity and context"
            },
            "venue": {
                "fragments": [],
                "text": "Mech. Transl. Comput. Linguistics"
            },
            "year": 1955
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic lexica for"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 70
                            }
                        ],
                        "text": "The applicability of LSA for information retrieval is well documented (Deerwester et al., 1990; Dumais, 1993; Dumais et al., 1997; Jiang & Littman, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic crosslanguage retrieval using latent semantic indexing"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the AAAI Symposium on Cross-Language Text and Speech Retrieval"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Course in general Linguistics Dimensions of meaning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1992 ACM/IEEE Conference on Supercomputing, Supercomputing'92"
            },
            "year": 1916
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 83
                            }
                        ],
                        "text": "Yet another inherently different word-space implementation is Random Indexing (RI) (Kanerva et al., 2000; Karlgren & Sahlgren, 2001; Sahlgren, 2005), which was developed at the Swedish Institute of Computer Science (SICS) based on Pentti Kanerva\u2019s work on sparse distributed memory (Kanerva, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random indexing of text samples"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Word association norms, mutual information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extensions of lipshitz mapping"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 13
                            }
                        ],
                        "text": "For example, Sch\u00fctze (1992) uses a window size of 1 000 characters, with the argument that a few long words are possibly better than many short words, which tend to be high-frequency function words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 13
                            }
                        ],
                        "text": "For example, Sch\u00fctze (1992) uses a window size of 1 000 characters, with the argument that a few long words are possibly better than many short words, which tend to be high-frequency function words. Yarowsky (1992) uses 100 words, while Gale et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to modern information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exhaustivity and specificity"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Documentation, 28, 11\u201321."
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 110
                            }
                        ],
                        "text": "Other early attempts at deriving context vectors automatically from the contexts in which words occur include Wilks et al. (1990), Sch\u00fctze (1992), Pereira et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometry and meaning. Stanford, USA: CSLI Publications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Restructuring sparse high dimensional data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic bilingual lexicon acquisition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large neural networks for the resolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Producing high-dimensional semantic spaces Chapter 16. Bibliography from lexical co-occurrence"
            },
            "venue": {
                "fragments": [],
                "text": "Behavior Research Methods, Instrumentation, and Computers"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards building contextual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A probabilistic approach to automated keyword indexing"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Society for Information Science,"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A connectionist scheme for modeling word sense"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Producing high-dimensional semantic spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximate dimension equalization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 697,
                                "start": 221
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999). Although these probabilistic approaches do rely on the distributional methodology as discovery procedure, they do not utilize the geometric metaphor of meaning as representational basis, and thus fall outside the scope of this venture. A good explanation of the difference between the geometric and the probabilistic approaches is the distinction made by Ruge (1992):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 158
                            }
                        ],
                        "text": "Instead, this paradigm uses a probabilistic framework, where similarities between words are expressed in terms of functions over distributional probabilities (Church & Hanks, 1989; Hindle, 1990; Hearst, 1992; Ruge, 1992; Dagan et al., 1993; Pereira et al., 1993; Grefenstette, 1994; Lin, 1997; Baker & McCallum, 1998; Lee, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contextual word similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A connectionist scheme for modeling word sense disambiguation"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition and Brain Theory,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weight functions impact on lsa"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Incremental construction of an associative"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "From words to understanding Foundations of real-world intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "From words to understanding Foundations of real-world intelligence"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 95
                            }
                        ],
                        "text": "An inherently different word-space implementation is the Hyperspace Analogue to Language (HAL) (Lund et al., 1995), which, in contrast to LSA, was developed specifically for word-space research, and was explicitly influenced by Sch\u00fctze\u2019s paper from 1992."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic and associative priming in high-dimensional semantic space"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the 17th Annual Conference of the Cognitive Science Society,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Testing the distributional hypothesis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discrimination decisions for 100,000"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 209
                            }
                        ],
                        "text": "Thus, the argument goes, when using truncated SVD to restructure the data, the word space will not only group together words that properly co-occur in a document, but also words that occur in similar contexts (Landauer et al., 1998)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to latent semantic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic lexica for query translation Multilingual information access for text, speech and images, 5th workshop of the Cross-Language Evaluation Forum, CLEF'04"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamic lexica for query translation Multilingual information access for text, speech and images, 5th workshop of the Cross-Language Evaluation Forum, CLEF'04"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 121
                            }
                        ],
                        "text": "The only other experimental investigation of how different contexts influence the word-space model that I am aware of is Lavelli et al. (2004), who compare what they call document occurrence representation (DOR) and term co-occurrence representation (TCOR)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 133
                            }
                        ],
                        "text": "However, word-space algorithms that use a words-by-words co-occurrence matrix normally do not use tfidf-weights (the exception being Lavelli et al. (2004), who I will return to in Chapter 15)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributional term representations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 95
                            }
                        ],
                        "text": "An inherently different word-space implementation is the Hyperspace Analogue to Language (HAL) (Lund et al., 1995), which, in contrast to LSA, was developed specifically for word-space research, and was explicitly influenced by Sch\u00fctze\u2019s paper from 1992."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic and associative priming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large neural networks for the resolution of lexi- Chapter 16. Bibliography cal ambiguity"
            },
            "venue": {
                "fragments": [],
                "text": "Computational lexical semantics"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organizing maps. Berlin, Heidelberg: Springer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 33,
            "methodology": 38,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 167,
        "totalPages": 17
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Word-Space-Model-:-Using-distributional-to-and-Sahlgren/1521ddb27860cc8834f8a82e62665bf983c8ad2c?sort=total-citations"
}