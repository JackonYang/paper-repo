{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 69
                            }
                        ],
                        "text": "The spectrograms are processed by a deep bidirectional LSTM network (Graves et al., 2013) with a Connectionist Temporal Classification (CTC) output layer (Graves et al., 2006; Graves, 2012, Chapter 7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 68
                            }
                        ],
                        "text": "The spectrograms are processed by a deep bidirectional LSTM network (Graves et al., 2013) with a Connectionist Temporal Classification (CTC) output layer (Graves et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 107
                            }
                        ],
                        "text": "If LSTM is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM (Graves et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206741496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
            "isKey": false,
            "numCitedBy": 6899,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
            },
            "slug": "Speech-recognition-with-deep-recurrent-neural-Graves-Mohamed",
            "title": {
                "fragments": [],
                "text": "Speech recognition with deep recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751126"
                        ],
                        "name": "F. Eyben",
                        "slug": "F.-Eyben",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Eyben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Eyben"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103575"
                        ],
                        "name": "M. W\u00f6llmer",
                        "slug": "M.-W\u00f6llmer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "W\u00f6llmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. W\u00f6llmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145411696"
                        ],
                        "name": "Bj\u00f6rn Schuller",
                        "slug": "Bj\u00f6rn-Schuller",
                        "structuredName": {
                            "firstName": "Bj\u00f6rn",
                            "lastName": "Schuller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bj\u00f6rn Schuller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 108
                            }
                        ],
                        "text": "The combination of bidirectional LSTM and CTC has been applied to characterlevel speech recognition before (Eyben et al., 2009), however the relatively shallow architecture used in that work did not deliver compelling results (the best character error rate was almost 20%)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2687308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39d37eb3ef1f5c9273ed3b197c0e2548547b6741",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Main-stream automatic speech recognition systems are based on modelling acoustic sub-word units such as phonemes. Phonemisation dictionaries and language model based decoding techniques are applied to transform the phoneme hypothesis into orthographic transcriptions. Direct modelling of graphemes as sub-word units using HMM has not been successful. We investigate a novel ASR approach using Bidirectional Long Short-Term Memory Recurrent Neural Networks and Connectionist Temporal Classification, which is capable of transcribing graphemes directly and yields results highly competitive with phoneme transcription. In design of such a grapheme based speech recognition system phonemisation dictionaries are no longer required. All that is needed is text transcribed on the sentence level, which greatly simplifies the training procedure. The novel approach is evaluated extensively on the Wall Street Journal 1 corpus."
            },
            "slug": "From-speech-to-letters-using-a-novel-neural-network-Eyben-W\u00f6llmer",
            "title": {
                "fragments": [],
                "text": "From speech to letters - using a novel neural network architecture for grapheme based ASR"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work investigates a novel ASR approach using Bidirectional Long Short-Term Memory Recurrent Neural Networks and Connectionist Temporal Classification, which is capable of transcribing graphemes directly and yields results highly competitive with phoneme transcription."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111912"
                        ],
                        "name": "Navdeep Jaitly",
                        "slug": "Navdeep-Jaitly",
                        "structuredName": {
                            "firstName": "Navdeep",
                            "lastName": "Jaitly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navdeep Jaitly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14902530"
                        ],
                        "name": "P. Nguyen",
                        "slug": "P.-Nguyen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 124
                            }
                        ],
                        "text": "While automatic speech recognition has greatly benefited from the introduction of neural networks (Bourlard & Morgan, 1993; Hinton et al., 2012), the networks are at present only a single component in a complex pipeline."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7230302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e33cbb25a8c7390aec6a398e36381f4f7770c283",
            "isKey": false,
            "numCitedBy": 2219,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current speech recognition systems use hidden Markov models ( HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternati ve way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients a s input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech rec ognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and repres nts the shared views of four research groups who have had recent successes in using deep neural networks for a coustic modeling in speech recognition."
            },
            "slug": "Deep-Neural-Networks-for-Acoustic-Modeling-in-Hinton-Deng",
            "title": {
                "fragments": [],
                "text": "Deep Neural Networks for Acoustic Modeling in Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper provides an overview of this progress and repres nts the shared views of four research groups who have had recent successes in using deep neural networks for a coustic modeling in speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111912"
                        ],
                        "name": "Navdeep Jaitly",
                        "slug": "Navdeep-Jaitly",
                        "structuredName": {
                            "firstName": "Navdeep",
                            "lastName": "Jaitly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navdeep Jaitly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14902530"
                        ],
                        "name": "P. Nguyen",
                        "slug": "P.-Nguyen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 203
                            }
                        ],
                        "text": "Full-sequence training methods such as Maximum Mutual Information have been used to directly train HMM-neural network hybrids to maximise the probability of the correct transcription (Bahl et al., 1986; Jaitly et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13521651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artificial Neural Network Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems 5870 hours of Voice Search and 1400 hours of YouTube data. On the first dataset, the pretrained ANN/HMM system outperforms the best Gaussian Mixture Model Hidden Markov Model (GMM/HMM) baseline, built with a much larger dataset by 3.7% absolute WER, while on the second dataset, it outperforms the GMM/HMM baseline by 4.7% absolute. Maximum Mutual Information (MMI) fine tuning and model combination using Segmental Conditional Random Fields (SCARF) give additional gains of 0.1% and 0.4% on the first dataset and 0.5% and 0.9% absolute on the second dataset."
            },
            "slug": "Application-of-Pretrained-Deep-Neural-Networks-to-Jaitly-Nguyen",
            "title": {
                "fragments": [],
                "text": "Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reports results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously, and outperforms the best Gaussian Mixture Model Hidden Markov Model baseline."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 92
                            }
                        ],
                        "text": "(15) can be efficiently evaluated and differentiated using a dynamic programming algorithm (Graves et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 155
                            }
                        ],
                        "text": "The spectrograms are processed by a deep bidirectional LSTM network (Graves et al., 2013) with a Connectionist Temporal Classification (CTC) output layer (Graves et al., 2006; Graves, 2012, Chapter 7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9901844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96494e722f58705fa20302fe6179d483f52705b4",
            "isKey": false,
            "numCitedBy": 3475,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN."
            },
            "slug": "Connectionist-temporal-classification:-labelling-Graves-Fern\u00e1ndez",
            "title": {
                "fragments": [],
                "text": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems of sequence learning and post-processing."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099761"
                        ],
                        "name": "K. Paliwal",
                        "slug": "K.-Paliwal",
                        "structuredName": {
                            "firstName": "Kuldip",
                            "lastName": "Paliwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Paliwal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 28
                            }
                        ],
                        "text": "Bidirectional RNNs (BRNNs) (Schuster & Paliwal, 1997)\ndo this by processing the data in both directions with two separate hidden layers, which are then fed forwards to the same output layer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18375389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85",
            "isKey": false,
            "numCitedBy": 5376,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported."
            },
            "slug": "Bidirectional-recurrent-neural-networks-Schuster-Paliwal",
            "title": {
                "fragments": [],
                "text": "Bidirectional recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61058350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d82e058a5c40954b8f5db170a298a889a254c37",
            "isKey": false,
            "numCitedBy": 1409,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nConnectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state-of-the-art continuous speech recognition systems based on Hidden Markov Models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e., HMM emission probability estimation and feature extraction. The book describes a successful five year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical system. Using standard databases and comparing with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition: A Hybrid Approach is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. This book is also suitable as a text for advanced courses on neural networks or speech processing."
            },
            "slug": "Connectionist-Speech-Recognition:-A-Hybrid-Approach-Bourlard-Morgan",
            "title": {
                "fragments": [],
                "text": "Connectionist Speech Recognition: A Hybrid Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2750400"
                        ],
                        "name": "M. Bisani",
                        "slug": "M.-Bisani",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Bisani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bisani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 230
                            }
                        ],
                        "text": "Lexical states such as graphemes and characters have been considered for HMMbased recognisers as a way of dealing with out of vocabulary (OOV) words (Galescu, 2003; Bisani & Ney, 2005), however they were used to augment rather than replace phonetic models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7156723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "325e90957eb2797e6a0e9f85331e19bc59b077a4",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Today\u2019s speech recognition systems are able to recognize arbitrary sentences over a large but finite vocabulary. However, many important speech recognition tasks feature an open, constantly changing vocabulary. (E.g. broadcast news transcription, translation of political debates, etc. Ideally, a system designed for such open vocabulary tasks would be able to recognize arbitrary, even previously unseen words. To some extent this can be achieved by using sub-lexical language models. We demonstrate that, by using a simple flat hybrid model, we can significantly improve a well-optimized state-ofthe-art speech recognition system over a wide range of out-of-vocabulary rates."
            },
            "slug": "Open-vocabulary-speech-recognition-with-flat-hybrid-Bisani-Ney",
            "title": {
                "fragments": [],
                "text": "Open vocabulary speech recognition with flat hybrid models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that, by using a simple flat hybrid model, a well-optimized state-ofthe-art speech recognition system over a wide range of out-of-vocabulary rates can be significantly improved."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 344,
                                "start": 341
                            }
                        ],
                        "text": "2, a BRNN computes the forward hidden sequence \u2212\u2192 h , the backward hidden sequence \u2190\u2212 h and the output sequence y by iterating the backward layer from t = T to 1, the forward layer from t = 1 to T and then updating the output layer:\n\u2212\u2192 h t = H ( W x \u2212\u2192 h xt +W\u2212\u2192h\u2212\u2192h \u2212\u2192 h t\u22121 + b\u2212\u2192h ) (8) \u2190\u2212 h t = H ( W x \u2190\u2212 h xt +W\u2190\u2212h\u2190\u2212h \u2190\u2212 h t+1 + b\u2190\u2212h ) (9)\nyt =W\u2212\u2192h y \u2212\u2192 h t +W\u2190\u2212h y \u2190\u2212 h t + bo (10)\nCombing BRNNs with LSTM gives bidirectional LSTM (Graves & Schmidhuber, 2005), which can access long-range context in both input directions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 156
                            }
                        ],
                        "text": "\u2026h t = H ( W x \u2212\u2192 h xt +W\u2212\u2192h\u2212\u2192h \u2212\u2192 h t\u22121 + b\u2212\u2192h ) (8) \u2190\u2212 h t = H ( W x \u2190\u2212 h xt +W\u2190\u2212h\u2190\u2212h \u2190\u2212 h t+1 + b\u2190\u2212h ) (9)\nyt =W\u2212\u2192h y \u2212\u2192 h t +W\u2190\u2212h y \u2190\u2212 h t + bo (10)\nCombing BRNNs with LSTM gives bidirectional LSTM (Graves & Schmidhuber, 2005), which can access long-range context in both input directions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1856462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f83f6e1afadf0963153974968af6b8342775d82",
            "isKey": false,
            "numCitedBy": 3295,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Framewise-phoneme-classification-with-bidirectional-Graves-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 354,
                                "start": 350
                            }
                        ],
                        "text": "2, a BRNN computes the forward hidden sequence \u2212\u2192 h , the backward hidden sequence \u2190\u2212 h and the output sequence y by iterating the backward layer from t = T to 1, the forward layer from t = 1 to T and then updating the output layer:\n\u2212\u2192 h t = H ( W x \u2212\u2192 h xt +W\u2212\u2192h\u2212\u2192h \u2212\u2192 h t\u22121 + b\u2212\u2192h ) (8) \u2190\u2212 h t = H ( W x \u2190\u2212 h xt +W\u2190\u2212h\u2190\u2212h \u2190\u2212 h t+1 + b\u2190\u2212h ) (9)\nyt =W\u2212\u2192h y \u2212\u2192 h t +W\u2190\u2212h y \u2190\u2212 h t + bo (10)\nCombing BRNNs with LSTM gives bidirectional LSTM (Graves & Schmidhuber, 2005), which can access long-range context in both input directions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "The spectrograms are processed by a deep bidirectional LSTM network (Graves et al., 2013) with a Connectionist Temporal Classification (CTC) output layer (Graves et al., 2006; Graves, 2012, Chapter 7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "However the baseline system overtook the RNN as the LM was strengthened: in this case the RNN\u2019s implicit LM may work against it by inter-\nfering with the explicit model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "One shortcoming of conventional RNNs is that they are only able to make use of previous context."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Deep RNNs can be created by stacking multiple RNN hidden layers on top of each other, with the output sequence of one layer forming the input sequence for the next, as shown in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "The network outputs yt are\nyt =WhNyh N t + bo (12)\nDeep bidirectional RNNs can be implemented by replacing each hidden sequence hn with the forward and backward sequences \u2212\u2192 h n and \u2190\u2212 h n, and ensuring that every hidden layer receives input from both the forward and backward layers at the level below."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 79
                            }
                        ],
                        "text": "Although it is possible to directly transcribe raw speech waveforms with RNNs (Graves, 2012, Chapter 9) or features learned with restricted Boltzmann machines (Jaitly & Hinton, 2011), the computational cost is high and performance tends to be worse than conventional preprocessing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Bidirectional RNNs (BRNNs) (Schuster & Paliwal, 1997)\ndo this by processing the data in both directions with two separate hidden layers, which are then fed forwards to the same output layer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 45
                            }
                        ],
                        "text": "Connectionist Temporal Classification (CTC) (Graves, 2012, Chapter 7) is an objective function that allows an RNN to be trained for sequence transcription tasks without requiring any prior alignment between the input and target sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2118350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a97b5db17acc731ef67321832dbbaf5766153135",
            "isKey": false,
            "numCitedBy": 1748,
            "numCiting": 220,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of the long short-term memory network architecture to multidimensional data, such as images and video sequences."
            },
            "slug": "Supervised-Sequence-Labelling-with-Recurrent-Neural-Graves",
            "title": {
                "fragments": [],
                "text": "Supervised Sequence Labelling with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and an extension of the long short-term memory network architecture to multidimensional data, such as images and video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Studies in Computational Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111912"
                        ],
                        "name": "Navdeep Jaitly",
                        "slug": "Navdeep-Jaitly",
                        "structuredName": {
                            "firstName": "Navdeep",
                            "lastName": "Jaitly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navdeep Jaitly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 228
                            }
                        ],
                        "text": "Although it is possible to directly transcribe raw speech waveforms with RNNs (Graves, 2012, Chapter 9) or features learned with restricted Boltzmann machines (Jaitly & Hinton, 2011), the computational cost is high and performance tends to be worse than conventional preprocessing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1020152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46af78834358337447001241cd2e18828ed926f0",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "State of the art speech recognition systems rely on preprocessed speech features such as Mel cepstrum or linear predictive coding coefficients that collapse high dimensional speech sound waves into low dimensional encodings. While these have been successfully applied in speech recognition systems, such low dimensional encodings may lose some relevant information and express other information in a way that makes it difficult to use for discrimination. Higher dimensional encodings could both improve performance in recognition tasks, and also be applied to speech synthesis by better modeling the statistical structure of the sound waves. In this paper we present a novel approach for modeling speech sound waves using a Restricted Boltzmann machine (RBM) with a novel type of hidden variable and we report initial results demonstrating phoneme recognition performance better than the current state-of-the-art for methods based on Mel cepstrum coefficients."
            },
            "slug": "Learning-a-better-representation-of-speech-using-Jaitly-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning a better representation of speech soundwaves using restricted boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel approach for modeling speech sound waves using a Restricted Boltzmann machine (RBM) with a novel type of hidden variable is presented and initial results demonstrate phoneme recognition performance better than the current state-of-the-art for methods based on Mel cepstrum coefficients."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2268620"
                        ],
                        "name": "Arnab Ghoshal",
                        "slug": "Arnab-Ghoshal",
                        "structuredName": {
                            "firstName": "Arnab",
                            "lastName": "Ghoshal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arnab Ghoshal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2541218"
                        ],
                        "name": "Gilles Boulianne",
                        "slug": "Gilles-Boulianne",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Boulianne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gilles Boulianne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816892"
                        ],
                        "name": "L. Burget",
                        "slug": "L.-Burget",
                        "structuredName": {
                            "firstName": "Luk\u00e1\u0161",
                            "lastName": "Burget",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Burget"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3075141"
                        ],
                        "name": "O. Glembek",
                        "slug": "O.-Glembek",
                        "structuredName": {
                            "firstName": "Ondrej",
                            "lastName": "Glembek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Glembek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46356878"
                        ],
                        "name": "N. Goel",
                        "slug": "N.-Goel",
                        "structuredName": {
                            "firstName": "Nagendra",
                            "lastName": "Goel",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Goel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2592983"
                        ],
                        "name": "M. Hannemann",
                        "slug": "M.-Hannemann",
                        "structuredName": {
                            "firstName": "Mirko",
                            "lastName": "Hannemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hannemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2745667"
                        ],
                        "name": "P. Motl\u00edcek",
                        "slug": "P.-Motl\u00edcek",
                        "structuredName": {
                            "firstName": "Petr",
                            "lastName": "Motl\u00edcek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Motl\u00edcek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2480051"
                        ],
                        "name": "Y. Qian",
                        "slug": "Y.-Qian",
                        "structuredName": {
                            "firstName": "Yanmin",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35455336"
                        ],
                        "name": "Petr Schwarz",
                        "slug": "Petr-Schwarz",
                        "structuredName": {
                            "firstName": "Petr",
                            "lastName": "Schwarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petr Schwarz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3330139"
                        ],
                        "name": "J. Silovsk\u00fd",
                        "slug": "J.-Silovsk\u00fd",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Silovsk\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Silovsk\u00fd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708033"
                        ],
                        "name": "G. Stemmer",
                        "slug": "G.-Stemmer",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Stemmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Stemmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459598"
                        ],
                        "name": "Karel Vesel\u00fd",
                        "slug": "Karel-Vesel\u00fd",
                        "structuredName": {
                            "firstName": "Karel",
                            "lastName": "Vesel\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karel Vesel\u00fd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 108
                            }
                        ],
                        "text": "The DNN-HMM was created using alignments from an SGMM-HMM system trained using Kaldi recipe \u2018s5\u2019, model \u2018tri4b\u2019(Povey et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1774023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a1a2cff2b70fb84a7ca7d97f8adcc5855851795",
            "isKey": false,
            "numCitedBy": 5126,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state automata (using the freely available OpenFst), together with detailed documentation and a comprehensive set of scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users."
            },
            "slug": "The-Kaldi-Speech-Recognition-Toolkit-Povey-Ghoshal",
            "title": {
                "fragments": [],
                "text": "The Kaldi Speech Recognition Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The design of Kaldi is described, a free, open-source toolkit for speech recognition research that provides a speech recognition system based on finite-state automata together with detailed documentation and a comprehensive set of scripts for building complete recognition systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71262932"
                        ],
                        "name": "L. Lee",
                        "slug": "L.-Lee",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Lee",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786323"
                        ],
                        "name": "R. Rose",
                        "slug": "R.-Rose",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Rose",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rose"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 280
                            }
                        ],
                        "text": "\u2026first stage of the pipeline is input feature extraction: standard techniques include mel-scale filterbanks (Davis & Mermelstein, 1980) (with or without a further transform into Cepstral coefficients) and speaker normalisation techniques such as vocal tract length normalisation (Lee & Rose, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13285745,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "2f5ca3bfcf00a688d65ffde89c6580979aa9c970",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In an effort to reduce the degradation in speech recognition performance caused by variation in vocal tract shape among speakers, a frequency warping approach to speaker normalization is investigated. A set of low complexity, maximum likelihood based frequency warping procedures have been applied to speaker normalization for a telephone based connected digit recognition task. This paper presents an efficient means for estimating a linear frequency warping factor and a simple mechanism for implementing frequency warping by modifying the filterbank in mel-frequency cepstrum feature analysis. An experimental study comparing these techniques to other well-known techniques for reducing variability is described. The results have shown that frequency warping is consistently able to reduce word error rate by 20% even for very short utterances."
            },
            "slug": "A-frequency-warping-approach-to-speaker-Lee-Rose",
            "title": {
                "fragments": [],
                "text": "A frequency warping approach to speaker normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An efficient means for estimating a linear frequency Warping factor and a simple mechanism for implementing frequency warping by modifying the filterbank in mel-frequency cepstrum feature analysis are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 75
                            }
                        ],
                        "text": "However we have found that the Long Short-Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997), which uses purpose-built memory cells to store information, is better at finding and exploiting long range context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51682,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 44
                            }
                        ],
                        "text": "For the version of LSTM used in this paper (Gers et al., 2002) H is implemented by the following composite function:\nit = \u03c3 (Wxixt +Whiht\u22121 +Wcict\u22121 + bi) (3) ft = \u03c3 (Wxfxt +Whfht\u22121 +Wcfct\u22121 + bf ) (4) ct = ftct\u22121 + it tanh (Wxcxt +Whcht\u22121 + bc) (5) ot = \u03c3 (Wxoxt +Whoht\u22121 +Wcoct + bo) (6) ht = ot\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 43
                            }
                        ],
                        "text": "For the version of LSTM used in this paper (Gers et al., 2002) H is implemented by the following composite function:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 474078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "047655e733a9eed9a500afd916efa566915b9110",
            "isKey": false,
            "numCitedBy": 1270,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals."
            },
            "slug": "Learning-Precise-Timing-with-LSTM-Recurrent-Gers-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Learning Precise Timing with LSTM Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work finds that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686309"
                        ],
                        "name": "Lucian Galescu",
                        "slug": "Lucian-Galescu",
                        "structuredName": {
                            "firstName": "Lucian",
                            "lastName": "Galescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucian Galescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 215
                            }
                        ],
                        "text": "Lexical states such as graphemes and characters have been considered for HMMbased recognisers as a way of dealing with out of vocabulary (OOV) words (Galescu, 2003; Bisani & Ney, 2005), however they were used to augment rather than replace phonetic models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12338470,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "6fe0ed86150f7fc94d271507463d995187f82c46",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A major source of recognition errors, out-of-vocabulary (OOV) words are also semantically important; recognizing them is, therefore, crucial for understanding. Success, so far, has been modest, even on very constrained tasks. In this paper we present a new approach to unlimited vocabulary speech recognition based on using graphemeto-phoneme correspondences for sub-lexical modeling of OOV words, and also some very encouraging results we obtained with our approach on a large vocabulary speech recognition task."
            },
            "slug": "Recognition-of-out-of-vocabulary-words-with-models-Galescu",
            "title": {
                "fragments": [],
                "text": "Recognition of out-of-vocabulary words with sub-lexical language models"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A new approach to unlimited vocabulary speech recognition based on using graphemeto-phoneme correspondences for sub-lexical modeling of OOV words is presented, and some very encouraging results are obtained with the approach on a large vocabularyspeech recognition task."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 139
                            }
                        ],
                        "text": "For example, convolutional neural networks are now able to directly classify raw pixels into high-level concepts such as object categories (Krizhevsky et al., 2012) and messages on traffic signs (Ciresan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 140
                            }
                        ],
                        "text": "For example, convolutional neural networks are now able to directly classify raw pixels into high-level concepts such as object categories (Krizhevsky et al., 2012) and messages on traffic signs (Ciresan et al., 2011), without using hand-designed feature extraction algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80938,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426718"
                        ],
                        "name": "Jonathan Masci",
                        "slug": "Jonathan-Masci",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Masci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Masci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 196
                            }
                        ],
                        "text": "For example, convolutional neural networks are now able to directly classify raw pixels into high-level concepts such as object categories (Krizhevsky et al., 2012) and messages on traffic signs (Ciresan et al., 2011), without using hand-designed feature extraction algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 38
                            }
                        ],
                        "text": ", 2012) and messages on traffic signs (Ciresan et al., 2011), without using hand-designed feature extraction algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1983697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd7f8b53e6802787179a961e766760cbbe2d5011",
            "isKey": false,
            "numCitedBy": 358,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the approach that won the preliminary phase of the German traffic sign recognition benchmark with a better-than-human recognition rate of 98.98%.We obtain an even better recognition rate of 99.15% by further training the nets. Our fast, fully parameterizable GPU implementation of a Convolutional Neural Network does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. A CNN/MLP committee further boosts recognition performance."
            },
            "slug": "A-committee-of-neural-networks-for-traffic-sign-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "A committee of neural networks for traffic sign classification"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This work describes the approach that won the preliminary phase of the German traffic sign recognition benchmark with a better-than-human recognition rate, and obtains an even better recognition rate by further training the nets."
            },
            "venue": {
                "fragments": [],
                "text": "The 2011 International Joint Conference on Neural Networks"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The DBN was trained layerwise then used to initialise a DNN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "The 14 hour subset was first used to train a Deep Belief Network (DBN) (Hinton & Salakhutdinov, 2006) with six hidden layers of 2000 units each."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14640,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 184
                            }
                        ],
                        "text": "Full-sequence training methods such as Maximum Mutual Information have been used to directly train HMM-neural network hybrids to maximise the probability of the correct transcription (Bahl et al., 1986; Jaitly et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56128297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f09ce0dd760857e0d0e4879f6e2543f04c5d33",
            "isKey": false,
            "numCitedBy": 926,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented comparing this method with maximum likelihood estimation."
            },
            "slug": "Maximum-mutual-information-estimation-of-hidden-for-Bahl-Brown",
            "title": {
                "fragments": [],
                "text": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method for estimating the parameters of hidden Markov models of speech is described and recognition results are presented comparing this method with maximum likelihood estimation."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145197867"
                        ],
                        "name": "Jan Peters",
                        "slug": "Jan-Peters",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Peters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 79
                            }
                        ],
                        "text": "As has been widely discussed in the policy gradients literature and elsewhere (Peters & Schaal, 2008), noise minimisation is crucial when optimising with stochastic gradient estimates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15454626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb5b459c8a3e56064158fb3514eeab763486e437",
            "isKey": false,
            "numCitedBy": 840,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reinforcement-learning-of-motor-skills-with-policy-Peters-Schaal",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning of motor skills with policy gradients"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 124
                            }
                        ],
                        "text": "While automatic speech recognition has greatly benefited from the introduction of neural networks (Bourlard & Morgan, 1993; Hinton et al., 2012), the networks are at present only a single component in a complex pipeline."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine"
            },
            "venue": {
                "fragments": [],
                "text": "Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long Short - Term Mem"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Towards-End-To-End-Speech-Recognition-with-Neural-Graves-Jaitly/0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f?sort=total-citations"
}