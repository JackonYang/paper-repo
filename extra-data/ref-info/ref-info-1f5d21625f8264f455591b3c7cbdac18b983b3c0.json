{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17097754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50e2733d2a8a9b3929bda278d382c37711f3fa8e",
            "isKey": false,
            "numCitedBy": 1275,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model's statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source."
            },
            "slug": "Data-Compression-Using-Adaptive-Coding-and-Partial-Cleary-Witten",
            "title": {
                "fragments": [],
                "text": "Data Compression Using Adaptive Coding and Partial String Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28270470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f54bc3a24a1f01808e6e8479a11e5b0244f5523",
            "isKey": false,
            "numCitedBy": 465,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The problems arising in the modeling and coding of strings for compression purposes are discussed. The notion of an information source that simplifies and sharpens the traditional one is axiomatized, and adaptive and nonadaptive models are defined. With a measure of complexity assigned to the models, a fundamental theorem is proved which states that models that use any kind of alphabet extension are inferior to the best models using no alphabet extensions at all. A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models. Because the coding parameters are the probabilities that define the model, their design is easy, and the application of the code is straightforward even with adaptively changing source models."
            },
            "slug": "Universal-modeling-and-coding-Rissanen-Langdon",
            "title": {
                "fragments": [],
                "text": "Universal modeling and coding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231060"
                        ],
                        "name": "M. Guazzo",
                        "slug": "M.-Guazzo",
                        "structuredName": {
                            "firstName": "Mauro",
                            "lastName": "Guazzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guazzo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45795043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbfdbec08d67349e364898f6c6ab289c06e00460",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for the minimum-redundancy encoding of a discrete information source is proposed. In the case of memoryless sources it is shown that the theoretical compression can be appmached within any desired threshold without the burden of alphabet extensions (i.e., the encodhg of blocks of L primary symbols) and also irrespective of 1) the primary and secondary alphabet sizes 2) the numerical values of primary symbol probabillties, and 3) the order and structure of the encoding tree. The same algorithm is then extended to sources with memory and to cases in which there is a constraint on the statistical description of the secondary sequence (e.g., secondary symbol probabilities are given). The technique can thus be used to transform any given discrete source into any other given discrete source while minimizing the ratio of average secondary sequence length to average primary sequence length."
            },
            "slug": "A-general-minimum-redundancy-source-coding-Guazzo",
            "title": {
                "fragments": [],
                "text": "A general minimum-redundancy source-coding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "An algorithm for the minimum-redundancy encoding of a discrete information source is proposed and it is shown that the theoretical compression can be appmached within any desired threshold without the burden of alphabet extensions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696575"
                        ],
                        "name": "J. Bentley",
                        "slug": "J.-Bentley",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Bentley",
                            "middleNames": [
                                "Louis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bentley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721040"
                        ],
                        "name": "D. Sleator",
                        "slug": "D.-Sleator",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sleator",
                            "middleNames": [
                                "Dominic"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sleator"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721050"
                        ],
                        "name": "R. Tarjan",
                        "slug": "R.-Tarjan",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tarjan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tarjan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066245870"
                        ],
                        "name": "V. K. Wei",
                        "slug": "V.-K.-Wei",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wei",
                            "middleNames": [
                                "K.-W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. K. Wei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Imagine a coding scheme in which the encoder reads the next Word of text, looks it up in a list, and transmits the index in the list in place of the word itself [ 16 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5854590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d90e9ec4f34e877d58782ab5cfb6b2ddf665050",
            "isKey": false,
            "numCitedBy": 559,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A data compression scheme that exploits locality of reference, such as occurs when words are used frequently over short intervals and then fall into long periods of disuse, is described. The scheme is based on a simple heuristic for self-organizing sequential search and on variable-length encodings of integers. We prove that it never performs much worse than Huffman coding and can perform substantially better; experiments on real files show that its performance is usually quite close to that of Huffman coding. Our scheme has many implementation advantages: it is simple, allows fast encoding and decoding, and requires only one pass over the data to be compressed (static Huffman coding takes two passes)."
            },
            "slug": "A-locally-adaptive-data-compression-scheme-Bentley-Sleator",
            "title": {
                "fragments": [],
                "text": "A locally adaptive data compression scheme"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A data compression scheme that exploits locality of reference, such as occurs when words are used frequently over short intervals and then fall into long periods of disuse, is described and proves that it never performs much worse than Huffman coding and can perform substantially better."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145720405"
                        ],
                        "name": "J. Ziv",
                        "slug": "J.-Ziv",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Ziv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ziv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50154247"
                        ],
                        "name": "A. Lempel",
                        "slug": "A.-Lempel",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Lempel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lempel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20900807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5be4e0eccca2892d31406a03b0c485f7a395fe5a",
            "isKey": false,
            "numCitedBy": 3487,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Compressibility of individual sequences by the class of generalized finite-state information-lossless encoders is investigated. These encoders can operate in a variable-rate mode as well as a fixed-rate one, and they allow for any finite-state scheme of variable-length-to-variable-length coding. For every individual infinite sequence x a quantity \\rho(x) is defined, called the compressibility of x , which is shown to be the asymptotically attainable lower bound on the compression ratio that can be achieved for x by any finite-state encoder. This is demonstrated by means of a constructive coding theorem and its converse that, apart from their asymptotic significance, also provide useful performance criteria for finite and practical data-compression tasks. The proposed concept of compressibility is also shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences. While the definition of \\rho(x) allows a different machine for each different sequence to be compressed, the constructive coding theorem leads to a universal algorithm that is asymptotically optimal for all sequences."
            },
            "slug": "Compression-of-individual-sequences-via-coding-Ziv-Lempel",
            "title": {
                "fragments": [],
                "text": "Compression of individual sequences via variable-rate coding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed concept of compressibility is shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 97814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58135a17632cb70c29883b3100f21dd017f7aeef",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of noiselessly encoding a message when prior statistics are known is considered. The close relationship of arithmetic and enumerative coding for this problem is shown by computing explicit arithmetic coding probabilities for various enumerative coding examples. This enables a comparison to be made of the coding efficiency of Markov models and enumerative codes as well as a new coding scheme intermediate between the two. These codes are then extended to messages whose statistics are not known {\\em a priori} Two adaptive codes are described for this problem whose coding efficiency is upper-bounded by the extended enumerative codes. On some practical examples the adaptive codes perform significantly better than the nonadaptive ones."
            },
            "slug": "A-comparison-of-enumerative-and-adaptive-codes-Cleary-Witten",
            "title": {
                "fragments": [],
                "text": "A comparison of enumerative and adaptive codes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two adaptive codes are described for this problem whose coding efficiency is upper-bounded by the extended enumerative codes and on some practical examples the adaptive codes perform significantly better than the nonadaptive ones."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46538929"
                        ],
                        "name": "G. H. Toulmin",
                        "slug": "G.-H.-Toulmin",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Toulmin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. H. Toulmin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We then present a Poisson process formulation for the appearance of new tokens, which was originally introduced to estimate the number of unseen biological species [ 14 ], and later applied to estimate the rate of appearance of new words in natural language text [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This remarkable result first appeared in [ 14 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123300067,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7c40c5ae2f40c225d19caa4978233c272c1ccf5f",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A sample of size N is drawn at random from a population of animals of various species. Methods are given for estimating, knowing only the contents of this sample, the number of species which will be represented r times in a second sample of size AN; these also enable us to estimate the number of different species and the proportion of the whole population represented in the second sample. A formula is found for the variance of the estimate; when A > 2, this variance becomes in general very large, so that the estimate is useless without some modification. This difficulty can be partly overcome, at least for A < 5, by using Euler's method with a suitable parameter or the methods described by Shanks (1955) to hasten the convergence of the series by which the estimate is expressed. The methods are applied to samples of words from Our Mutual Friend, to an entomological sample, and to a sample of nouns from Macaulay's essay on Bacon."
            },
            "slug": "THE-NUMBER-OF-NEW-SPECIES,-AND-THE-INCREASE-IN-WHEN-Good-Toulmin",
            "title": {
                "fragments": [],
                "text": "THE NUMBER OF NEW SPECIES, AND THE INCREASE IN POPULATION COVERAGE, WHEN A SAMPLE IS INCREASED"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145720405"
                        ],
                        "name": "J. Ziv",
                        "slug": "J.-Ziv",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Ziv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ziv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50154247"
                        ],
                        "name": "A. Lempel",
                        "slug": "A.-Lempel",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Lempel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lempel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9267632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59c9f2036e673d8bc9713eed851d12c6c9fe53cb",
            "isKey": false,
            "numCitedBy": 5369,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source."
            },
            "slug": "A-universal-algorithm-for-sequential-data-Ziv-Lempel",
            "title": {
                "fragments": [],
                "text": "A universal algorithm for sequential data compression"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable- to-block codes designed to match a completely specified source."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5591864"
                        ],
                        "name": "R. Thisted",
                        "slug": "R.-Thisted",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Thisted",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Thisted"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We then present a Poisson process formulation for the appearance of new tokens, which was originally introduced to estimate the number of unseen biological species [14], and later applied to estimate the rate of appearance of new words in natural language text [ 15 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Given the number of species, and the number of individual butterflies in each species, captured in one day on a desert island, how many different species might one expect there to be all told? Or, given that Shakespeare\u2019s complete works of 885 000 words include 31500 different ones, of which 14400 appear only once, 4300 twice, etc., how many words did he know? The latter question was studied by Efron and Thisted [ 15 ], and the analysis ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67807844,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "45bd1a97bde5ffe34638b37ad54d1b929568426d",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Shakespeare wrote 31 534 different words, of which 14376 appear only once, 4343 twice, etc. The question considered is how many words he knew but did not use. A parametric empirical Bayes model due to Fisher and a nonparametric model due to Good & Toulmin are examined. The latter theory is augmented using linear programming methods. We conclude that the models are equivalent to supposing that Shakespeare knew at least 35 000 more words."
            },
            "slug": "Estimating-the-number-of-unseen-species:-How-many-Efron-Thisted",
            "title": {
                "fragments": [],
                "text": "Estimating the number of unseen species: How many words did Shakespeare know? Biometrika 63"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353544"
                        ],
                        "name": "R. Fisher",
                        "slug": "R.-Fisher",
                        "structuredName": {
                            "firstName": "Rory",
                            "lastName": "Fisher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46949456"
                        ],
                        "name": "A. Corbet",
                        "slug": "A.-Corbet",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Corbet",
                            "middleNames": [
                                "Steven"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corbet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152154405"
                        ],
                        "name": "C. B. Williams",
                        "slug": "C.-B.-Williams",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Williams",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. B. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 55246210,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "63d68b7d95601f9c7406e2ed8eba49cfc50a8e58",
            "isKey": false,
            "numCitedBy": 2960,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1. It is shown that in a large collection of Lepidoptera captured in Malaya the frequency of the number of species represented by different numbers of individuals fitted somewhat closely to a hyperbola type of curve, so long as only the rarer species were considered. The data for the commoner species was not so strictly `randomized', but the whole series could be closely fitted by a series of the logarithmic type as described by Fisher in Part 3. Other data for random collections of insects in the field were also shown to fit fairly well to this series. Part 2. Extensive data on the capture of about 1500 Macrolepidoptera of about 240 species in a light-trap at Harpenden is analysed in relation to Fisher's mathematical theory and is shown to fit extremely closely to the calculations. The calculations are applied first to the frequency of occurrence of species represented by different numbers of individuals--and secondly to the number of species in samples of different sizes from the same population. The parameter ` alpha ', which it is suggested should be called the `index of diversity', is shown to have a regular seasonal change in the case of the Macrolepidoptera in the trap. In addition, samples from two traps which overlooked somewhat different vegetation are shown to have ` alpha ' values which are significantly different. It is shown that, provided the samples are not small, ` alpha ' is the increase in the number of species obtained by increasing the size of a sample by e (2.718). A diagram is given (Fig. 8) from which any one of the values, total number of species, total number of individuals and index of diversity (alpha), can be obtained approximately if the other two are known. The standard error of alpha is also indicated on the same diagram. Part 3. A theoretical distribution is developed which appears to be suitable for the frequencies with which different species occur in a random collection, in the common case in which many species are so rare that their chance of inclusion is small. The relationships of the new distribution with the negative binomial and the Poisson series are established. Numerical processes are exhibited for fitting the series to observations containing given numbers of species and individuals, and for estimating the parameter alpha representing the richness in species of the material sampled; secondly, for calculating the standard error of alpha, and thirdly, for testing whether the series exhibits a significant deviation from the limiting form used. Special tables are presented for facilitating these calculations."
            },
            "slug": "The-Relation-Between-the-Number-of-Species-and-the-Fisher-Corbet",
            "title": {
                "fragments": [],
                "text": "The Relation Between the Number of Species and the Number of Individuals in a Random Sample of an Animal Population"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is shown that in a large collection of Lepidoptera captured in Malaya the frequency of the number of species represented by different numbers of individuals fitted somewhat closely to a hyperbola type of curve, so long as only the rarer species were considered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1943
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 39909636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20d673dc3200ac1742ee0827535a291eb6e051f8",
            "isKey": false,
            "numCitedBy": 759,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases. An outstanding feature of this technique is that alphabet extensions are not required. A complete decodability analysis is given. The relationship of arithmetic coding to other known nonblock codes is illuminated."
            },
            "slug": "Arithmetic-Coding-Rissanen-Langdon",
            "title": {
                "fragments": [],
                "text": "Arithmetic Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16011297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fcb8d85e3d429f3816861fc7999e1bb68eefd39",
            "isKey": false,
            "numCitedBy": 569,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for encoding and decoding finite strings over a finite alphabet are described. The coding operations are arithmetic involving rational numbers li as parameters such that \u03a3i2-l i\u22642-\u2208. This coding technique requires no blocking, and the per-symbol length of the encoded string approaches the associated entropy within \u2208. The coding speed is comparable to that of conventional coding methods."
            },
            "slug": "Generalized-Kraft-Inequality-and-Arithmetic-Coding-Rissanen",
            "title": {
                "fragments": [],
                "text": "Generalized Kraft Inequality and Arithmetic Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This coding technique requires no blocking, and the per-symbol length of the encoded string approaches the associated entropy within \u2208, which is comparable to that of conventional coding methods."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Arithmetic coding provides a way of coding tokens with respect to a probability distribution in an optimal way; an accessible implementation of the technique can be found in [ 7 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3343393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd23c9168418324e81881365f297fb6a1caa3a07",
            "isKey": false,
            "numCitedBy": 3142,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method. Arithmetic coding gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding."
            },
            "slug": "Arithmetic-coding-for-data-compression-Witten-Neal",
            "title": {
                "fragments": [],
                "text": "Arithmetic coding for data compression"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method, which gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040420"
                        ],
                        "name": "F. Rubin",
                        "slug": "F.-Rubin",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rubin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rubin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35964389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47d1350e9d5b637a7fc888b52c5b9291985b9ed8",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms are presented for encoding and decoding strings of characters as real binary fractions, using registers of fixed precision. The encoding is left to right and does not require blocking. The algorithms have storage requirements O(N) and computation time O(n \\log_{2}N) for string length n and alphabet size N ."
            },
            "slug": "Arithmetic-stream-coding-using-fixed-precision-Rubin",
            "title": {
                "fragments": [],
                "text": "Arithmetic stream coding using fixed precision registers"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Algorithms are presented for encoding and decoding strings of characters as real binary fractions, using registers of fixed precision, and have storage requirements and computation time O(n \\log_{2}N) for string length n and alphabet size N."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222411842,
            "fieldsOfStudy": [],
            "id": "acb94e335c64a4bbda6a493d6386e1138bcd109c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1896
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49014513"
                        ],
                        "name": "T. Bell",
                        "slug": "T.-Bell",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64795149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eca032da22009c471c1901c2527b70977be7b1a0",
            "isKey": false,
            "numCitedBy": 767,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-Compression-Bell-Cleary",
            "title": {
                "fragments": [],
                "text": "Text Compression"
            },
            "venue": {
                "fragments": [],
                "text": "125 Problems in Text Algorithms"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2126645"
                        ],
                        "name": "Richard C. Pasco",
                        "slug": "Richard-C.-Pasco",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Pasco",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard C. Pasco"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The coding problem is solved by the method of \u201carithmetic coding,\u201d discovered in 1976 by Pasco [ 2 ] and Rissanen [31 and developed further in [4]-[6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60531818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e9a6af4bc23b4cd139e671f4d1b2ca18228f660",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Source-coding-algorithms-for-fast-data-compression-Pasco",
            "title": {
                "fragments": [],
                "text": "Source coding algorithms for fast data compression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111886011"
                        ],
                        "name": "Michael G Roberts",
                        "slug": "Michael-G-Roberts",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Roberts",
                            "middleNames": [
                                "G"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael G Roberts"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59657448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ca7765fec63e961aa23eb242d8364e0be15a09b",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Local-order-estimating-Markovian-analysis-for-and-Roberts",
            "title": {
                "fragments": [],
                "text": "Local order estimating Markovian analysis for noiseless source coding and authorship identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2172770680"
                        ],
                        "name": "N. L. Johnson",
                        "slug": "N.-L.-Johnson",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Johnson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. L. Johnson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59765807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63f07cfb5ac5a32fa15c9319d5494c1ea41946d4",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Urn-models-and-their-application-Johnson",
            "title": {
                "fragments": [],
                "text": "Urn models and their application"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4525095"
                        ],
                        "name": "G. Kolata",
                        "slug": "G.-Kolata",
                        "structuredName": {
                            "firstName": "Gina",
                            "lastName": "Kolata",
                            "middleNames": [
                                "Bari"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kolata"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21170484,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b62b471d89e17c7820dd23644b2ee2368e422fe6",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Shakespeare's-New-Poem:-An-Ode-to-Statistics:-Two-a-Kolata",
            "title": {
                "fragments": [],
                "text": "Shakespeare's New Poem: An Ode to Statistics: Two statisticians are using a powerful method to determine whether Shakespeare could have written the newly discovered poem that has been attributed to him."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1986
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/The-zero-frequency-problem:-Estimating-the-of-novel-Witten-Bell/1f5d21625f8264f455591b3c7cbdac18b983b3c0?sort=total-citations"
}