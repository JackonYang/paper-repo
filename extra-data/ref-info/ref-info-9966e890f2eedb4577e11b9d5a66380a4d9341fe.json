{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 53
                            }
                        ],
                        "text": "The method is generally biased, even asymptotically (Carreira-Perpin\u0303a\u0301n and Hinton, 2005b), except in some special cases such as the multivariate Gaussian distribution (Carreira-Perpi\u0303na\u0301n and Hinton, 2005a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17861266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e270bfa5b662c531a61a5b274da636603c23a734",
            "isKey": false,
            "numCitedBy": 705,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called \u201ccontrastive divergence\u201d (CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution. Consider a probability distribution over a vector x (assumed discrete w.l.o.g.) and with parameters W p(x;W) = 1 Z(W) e (1) where Z(W) = \u2211 x e \u2212E(x;W) is a normalisation constant and E(x;W) is an energy function. This class of random-field distributions has found many practical applications (Li, 2001; Winkler, 2002; Teh et al., 2003; He et al., 2004). Maximum-likelihood (ML) learning of the parameters W given an iid sample X = {xn}n=1 can be done by gradient ascent: W = W + \u03b7 \u2202L(W;X ) \u2202W \u2223"
            },
            "slug": "On-Contrastive-Divergence-Learning-Carreira-Perpi\u00f1\u00e1n-Hinton",
            "title": {
                "fragments": [],
                "text": "On Contrastive Divergence Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The properties of CD learning are studied and it is shown that it provides biased estimates in general, but that the bias is typically very small."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745655"
                        ],
                        "name": "C. Bouman",
                        "slug": "C.-Bouman",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bouman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bouman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145701804"
                        ],
                        "name": "K. Sauer",
                        "slug": "K.-Sauer",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Sauer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 138
                            }
                        ],
                        "text": "Non-normalized models are often encountered in continous-valued Markov random fields, which are widely used in image modelling, see e.g. (Bouman and Sauer, 1993; Li, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1673003,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "5f9ddaa86fb2bfcdc28f5add2e758db4335b8b80",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a Markov random field model which allows realistic edge modeling while providing stable maximum a posterior (MAP) solutions. The model, referred to as a generalized Gaussian Markov random field (GGMRF), is named for its similarity to the generalized Gaussian distribution used in robust detection and estimation. The model satisfies several desirable analytical and computational properties for map estimation, including continuous dependence of the estimate on the data, invariance of the character of solutions to scaling of data, and a solution which lies at the unique global minimum of the a posteriori log-likelihood function. The GGMRF is demonstrated to be useful for image reconstruction in low-dosage transmission tomography."
            },
            "slug": "A-generalized-Gaussian-image-model-for-MAP-Bouman-Sauer",
            "title": {
                "fragments": [],
                "text": "A generalized Gaussian image model for edge-preserving MAP estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The authors present a Markov random field model which allows realistic edge modeling while providing stable maximum a posterior (MAP) solutions and is demonstrated to be useful for image reconstruction in low-dosage transmission tomography."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "An interesting approximative MCMC method called contrastive divergence was recently proposed by Hinton (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4571,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52865368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b95799a25def71b100bd12e7ebb32cbcee6590bf",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces."
            },
            "slug": "Energy-Based-Models-for-Sparse-Overcomplete-Teh-Welling",
            "title": {
                "fragments": [],
                "text": "Energy-Based Models for Sparse Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A new way of extending independent components analysis (ICA) to overcomplete representations that defines features as deterministic (linear) functions of the inputs and assigns energies to the features through the Boltzmann distribution."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144616256"
                        ],
                        "name": "D. Pham",
                        "slug": "D.-Pham",
                        "structuredName": {
                            "firstName": "Dinh",
                            "lastName": "Pham",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46355559"
                        ],
                        "name": "Philippe Garat",
                        "slug": "Philippe-Garat",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Garat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippe Garat"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 149
                            }
                        ],
                        "text": "\u2026can use a simple trick of partial integration to compute the objective function very easily, as shown by the following theorem:\nTheorem 1 Assume that the model score function\u03c8(\u03be;\u03b8) is differentiable, as well as some weak regularity conditions.1\nThen, the objective function J in (2) can be\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Interestingly, it now performs slightly better than maximum likelihood estimation (which would more properly be called quasi-maximum likelihoodestimation due to the misspecification (Pham and Garrat, 1997))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32042337,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f93fed0bac99e37acf30ea0c1356725f74ec2b78",
            "isKey": false,
            "numCitedBy": 494,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two methods for separating mixture of independent sources without any precise knowledge of their probability distribution. They are obtained by considering a maximum likelihood (ML) solution corresponding to some given distributions of the sources and relaxing this assumption afterward. The first method is specially adapted to temporally independent non-Gaussian sources and is based on the use of nonlinear separating functions. The second method is specially adapted to correlated sources with distinct spectra and is based on the use of linear separating filters. A theoretical analysis of the performance of the methods has been made. A simple procedure for optimally choosing the separating functions is proposed. Further, in the second method, a simple implementation based on the simultaneous diagonalization of two symmetric matrices is provided. Finally, some numerical and simulation results are given, illustrating the performance of the method and the good agreement between the experiments and the theory."
            },
            "slug": "Blind-separation-of-mixture-of-independent-sources-Pham-Garat",
            "title": {
                "fragments": [],
                "text": "Blind separation of mixture of independent sources through a quasi-maximum likelihood approach"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Two methods for separating mixture of independent sources without any precise knowledge of their probability distribution are proposed by considering a maximum likelihood (ML) solution corresponding to some given distributions of the sources and relaxing this assumption afterward."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34679741"
                        ],
                        "name": "S. Li",
                        "slug": "S.-Li",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 162
                            }
                        ],
                        "text": "Non-normalized models are often encountered in continous-valued Markov random fields, which are widely used in image modelling, see e.g. (Bouman and Sauer, 1993; Li, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12779752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9701ad65e256bd8841c4f80ced09b4ca1d5e331",
            "isKey": false,
            "numCitedBy": 1427,
            "numCiting": 417,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov random field (MRF) theory provides a basis for modeling contextual constraints in visual processing and interpretation. It enables systematic development of optimal vision algorithms when used with optimization principles. This detailed and thoroughly enhanced third edition presents a comprehensive study / reference to theories, methodologies and recent developments in solving computer vision problems based on MRFs, statistics and optimisation. It treats various problems in low- and high-level computational vision in a systematic and unified way within the MAP-MRF framework. Among the main issues covered are: how to use MRFs to encode contextual constraints that are indispensable to image understanding; how to derive the objective function for the optimal solution to a problem; and how to design computational algorithms for finding an optimal solution. Easy-to-follow and coherent, the revised edition is accessible, includes the most recent advances, and has new and expanded sections on such topics as: Discriminative Random Fields (DRF) Strong Random Fields (SRF) Spatial-Temporal Models Total Variation Models Learning MRF for Classification (motivation + DRF) Relation to Graphic Models Graph Cuts Belief Propagation Features: Focuses on the application of Markov random fields to computer vision problems, such as image restoration and edge detection in the low-level domain, and object matching and recognition in the high-level domain Presents various vision models in a unified framework, including image restoration and reconstruction, edge and region segmentation, texture, stereo and motion, object matching and recognition, and pose estimation Uses a variety of examples to illustrate how to convert a specific vision problem involving uncertainties and constraints into essentially an optimization problem under the MRF setting Introduces readers to the basic concepts, important models and various special classes of MRFs on the regular image lattice and MRFs on relational graphs derived from images Examines the problems of parameter estimation and function optimization Includes an extensive list of references This broad-ranging and comprehensive volume is an excellent reference for researchers working in computer vision, image processing, statistical pattern recognition and applications of MRFs. It has been class-tested and is suitable as a textbook for advanced courses relating to these areas."
            },
            "slug": "Markov-Random-Field-Modeling-in-Image-Analysis-Li",
            "title": {
                "fragments": [],
                "text": "Markov Random Field Modeling in Image Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This detailed and thoroughly enhanced third edition presents a comprehensive study / reference to theories, methodologies and recent developments in solving computer vision problems based on MRFs, statistics and optimisation."
            },
            "venue": {
                "fragments": [],
                "text": "Computer Science Workbench"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 103
                            }
                        ],
                        "text": "The model is related to ICA with overcomplete bases (Hyva\u0308rinen et al., 2001; Hyv\u0308arinen and Inki, 2002; Olshausen and Field, 1997), i.e. the case where there are more independent components and basis vectors than observed variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The package can be downloaded at http://www.cs.helsinki.fi/patrik.hoyer/.\nwhy the results are much less smooth and \u201cbeautiful\u201d than some published ICA results (Hyva\u0308rinen et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": false,
            "numCitedBy": 3574,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1845853"
                        ],
                        "name": "M. Inki",
                        "slug": "M.-Inki",
                        "structuredName": {
                            "firstName": "Mika",
                            "lastName": "Inki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Inki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 77
                            }
                        ],
                        "text": "The model is related to ICA with overcomplete bases (Hyva\u0308rinen et al., 2001; Hyv\u0308arinen and Inki, 2002; Olshausen and Field, 1997), i.e. the case where there are more independent components and basis vectors than observed variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The package can be downloaded at http://www.cs.helsinki.fi/patrik.hoyer/.\nwhy the results are much less smooth and \u201cbeautiful\u201d than some published ICA results (Hyva\u0308rinen et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 216
                            }
                        ],
                        "text": "However, the normalization constant Z is not known whenG is non-quadratic, i.e. when the model is non-Gaussian, which is why previous research had to resort to MCMC methods (Teh et al., 2003) or some approximations (Hyva\u0308rinen and Inki, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8121779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94565856cc7f2fb3a1e046eec3c805a1c84b85ef",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimating overcomplete ICA bases for image windows is a difficult problem. Most algorithms require the estimation of values of the independent components which leads to computationally heavy procedures. Here we first review the existing methods, and then introduce two new algorithms that estimate an approximate overcomplete basis quite fast in a high-dimensional space. The first algorithm is based on the prior assumption that the basis vectors are randomly distributed in the space, and therefore close to orthogonal. The second replaces the conventional orthogonalization procedure by a transformation of the marginal density to gaussian."
            },
            "slug": "Estimating-Overcomplete-Independent-Component-Bases-Hyv\u00e4rinen-Inki",
            "title": {
                "fragments": [],
                "text": "Estimating Overcomplete Independent Component Bases for Image Windows"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two new algorithms are introduced that estimate an approximate overcomplete basis quite fast in a high-dimensional space and replace the conventional orthogonalization procedure by a transformation of the marginal density to gaussian."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Mathematical Imaging and Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 74
                            }
                        ],
                        "text": "Other recent work in image modelling also includes non-normalized models (Hyva\u0308rinen and Hoyer, 2001; Teh et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 161
                            }
                        ],
                        "text": "The basic principle is to use an MCMC method for computing the derivative of the logarithm of the normalization factorZ, but the MCMC is allowed to run for only a single iteration (or a few iterations) before doing the gradient step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 202
                            }
                        ],
                        "text": "(12)\nWe estimated the model for image patches of 8\u00d7 pixels taken from natural images, see P.O. Hoyer\u2019simageicapackage.3 As preprocessing, the DC component (i.e. the mean gray-scale value) was removed from each image patch, reducing the effective dimensionality of the data to n = 63."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 29
                            }
                        ],
                        "text": "Constraining\u03b1k = 1 andm= n and allowing thewk to have any norm, this becomes the basic ICA model of the preceding subsection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Since the vectorswi were normalized to unit norm, this shows that no twowi were close to equal, and we did findmdifferent vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6302770,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7c032d555a9d7096f7bb88441f10e33d3302d5be",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 130,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-two-layer-sparse-coding-model-learns-simple-and-Hyv\u00e4rinen-Hoyer",
            "title": {
                "fragments": [],
                "text": "A two-layer sparse coding model learns simple and complex cell receptive fields and topography from natural images"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103290585"
                        ],
                        "name": "F. Y. Edgeworth",
                        "slug": "F.-Y.-Edgeworth",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Edgeworth",
                            "middleNames": [
                                "Ysidro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Y. Edgeworth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20448516"
                        ],
                        "name": "C. Mccann",
                        "slug": "C.-Mccann",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Mccann",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mccann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 11
                            }
                        ],
                        "text": "Assume we observe a random vectorx \u2208 Rn which has a probability density function (pdf) denoted bypx(.)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 182
                            }
                        ],
                        "text": "For simplicity, we call this the score function, although according the conventional definition, it is actually the score function with respect to a hypothetical location parameter (Schervish, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60105901,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "2a18da050602d0dbe40e437e04b3f8414c54cda0",
            "isKey": false,
            "numCitedBy": 480,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Volume 1: probability the law of error. Volume 2. Volume 3: applications of probability and statistical theory to economics and the social sciences applications of probability and statistical theory to physics, chemistry and biology applications of probability and statistical theory to education applications of probability and statistical theory to psychial research."
            },
            "slug": "The-theory-of-statistics-Edgeworth-Mccann",
            "title": {
                "fragments": [],
                "text": "The theory of statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This book discusses applications of probability and statistical theory to economics and the social sciences, and the law of error, and its applications to education and psychial research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 182
                            }
                        ],
                        "text": "Usually, estimation of non-normalized models is approached by Markov ChainMonte Carlo (MCMC) methods, which are very slow, or by making some approximations, which may be quite poor (Mackay, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5436619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7f15848cd0fbb3d08f351595da833b1627de9c3",
            "isKey": false,
            "numCitedBy": 8764,
            "numCiting": 249,
            "paperAbstract": {
                "fragments": [],
                "text": "Fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering."
            },
            "slug": "Information-Theory,-Inference,-and-Learning-Mackay",
            "title": {
                "fragments": [],
                "text": "Information Theory, Inference, and Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 41
                            }
                        ],
                        "text": "Some consistency proofs were provided by Besag (1974, 1977), but these only apply to special cases such as Gaussian or binary random fields."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120503853,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ed91ce023b6500c586802de7d23d8f8f01e5aa1b",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficiency-of-pseudolikelihood-estimation-for-Besag",
            "title": {
                "fragments": [],
                "text": "Efficiency of pseudolikelihood estimation for simple Gaussian fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 Comparison with Pseudo-Likelihood Estimation A related method for estimating non-normalized models is maximization of pseudo-likelihood (Besag, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 92
                            }
                        ],
                        "text": "A related method for estimating non-normalized models is maximization of pseudo-likelihood (Besag, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 41
                            }
                        ],
                        "text": "Some consistency proofs were provided by Besag (1974, 1977), but these only apply to special cases such as Gaussian or binary random fields."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42087677,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8bf730243ed967afd5349bef053641a6043517a0",
            "isKey": false,
            "numCitedBy": 6165,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spatial-Interaction-and-the-Statistical-Analysis-of-Besag",
            "title": {
                "fragments": [],
                "text": "Spatial Interaction and the Statistical Analysis of Lattice Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 53
                            }
                        ],
                        "text": "The method is generally biased, even asymptotically (Carreira-Perpin\u0303a\u0301n and Hinton, 2005b), except in some special cases such as the multivariate Gaussian distribution (Carreira-Perpi\u0303na\u0301n and Hinton, 2005a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On contrastive divergence (CD) learning"
            },
            "venue": {
                "fragments": [],
                "text": "On contrastive divergence (CD) learning"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 147
                            }
                        ],
                        "text": "Score matching estimation consisted of minimizingJ\u0303 in (10) by a simple gradient descent; likelihood was maximized using a natural gradient method (Amari et al., 1996; Hyva\u0308rinen et al., 2001), using the true value ofZ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 148
                            }
                        ],
                        "text": "Score matching estimation consisted of minimizing J\u0303 in (10) by a simple gradient descent; likelihood was maximized using a natural gradient method (Amari et al., 1996; Hyv\u00e4rinen et al., 2001), using the true value of Z."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new learning algorithm for blind source separation"
            },
            "venue": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new learning algorithm for blindsource separation"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 11,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Estimation-of-Non-Normalized-Statistical-Models-by-Hyv\u00e4rinen/9966e890f2eedb4577e11b9d5a66380a4d9341fe?sort=total-citations"
}