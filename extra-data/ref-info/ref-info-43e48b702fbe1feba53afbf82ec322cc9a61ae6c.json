{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144430305"
                        ],
                        "name": "Jean-Fran\u00e7ois Lalonde",
                        "slug": "Jean-Fran\u00e7ois-Lalonde",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Lalonde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Fran\u00e7ois Lalonde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 112
                            }
                        ],
                        "text": "Just as massive databases of images and objects have led to new advances in image editing [Hays and Efros 2007; Lalonde et al. 2007] and object recognition [Russell et al. 2008], we believe that a large and comprehensive catalog of contextual surface appearance properties is critical for everyday\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12312803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddf7f6917a3b81f0ba45dba96e60d96a452b5c38",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system for inserting new objects into existing photographs by querying a vast image-based object library, pre-computed using a publicly available Internet object database. The central goal is to shield the user from all of the arduous tasks typically involved in image compositing. The user is only asked to do two simple things: 1) pick a 3D location in the scene to place a new object; 2) select an object to insert using a hierarchical menu. We pose the problem of object insertion as a data-driven, 3D-based, context-sensitive object retrieval task. Instead of trying to manipulate the object to change its orientation, color distribution, etc. to fit the new image, we simply retrieve an object of a specified class that has all the required properties (camera pose, lighting, resolution, etc) from our large object library. We present new automatic algorithms for improving object segmentation and blending, estimating true 3D object size and orientation, and estimating scene lighting conditions. We also present an intuitive user interface that makes object insertion fast and simple even for the artistically challenged."
            },
            "slug": "Photo-clip-art-Lalonde-Hoiem",
            "title": {
                "fragments": [],
                "text": "Photo clip art"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A system for inserting new objects into existing photographs by querying a vast image-based object library, pre-computed using a publicly available Internet object database, to shield the user from all of the arduous tasks typically involved in image compositing."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710772"
                        ],
                        "name": "Kristin J. Dana",
                        "slug": "Kristin-J.-Dana",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Dana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristin J. Dana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8038506"
                        ],
                        "name": "B. Ginneken",
                        "slug": "B.-Ginneken",
                        "structuredName": {
                            "firstName": "Bram",
                            "lastName": "Ginneken",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ginneken"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750470"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Shree",
                            "lastName": "Nayar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716904"
                        ],
                        "name": "J. Koenderink",
                        "slug": "J.-Koenderink",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koenderink",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koenderink"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 154
                            }
                        ],
                        "text": "\u2026including the MERL database [Matusik et al. 2003] with 4D BRDF measurements for 105 materials, fit to various BRDF models [Ngan et al. 2005], and CUReT [Dana et al. 1999], with 6D measured BTFs (bidirectional texture functions) for 61 samples with various lighting and illumination conditions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 622815,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "16ba88cb3c3a0438bd9e5ace9096f9655ddc63df",
            "isKey": false,
            "numCitedBy": 1074,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we investigate the visual appearance of real-world surfaces and the dependence of appearance on imaging conditions. We present a BRDF (bidirectional reflectance distribution function) database with reflectance measurements for over 60 different samples, each observed with over 200 different combinations of viewing and source directions. We fit the BRDF measurements to two recent models to obtain a BRDF parameter database. These BRDF parameters can be directly used for both image analysis and image synthesis. Finally, we present a BTF (bidirectional texture function) database with image textures from over 60 different samples, each observed with over 200 different combinations of viewing and source directions. Each of these unique databases has important implications for a variety of vision algorithms and each is made publicly available."
            },
            "slug": "Reflectance-and-texture-of-real-world-surfaces-Dana-Ginneken",
            "title": {
                "fragments": [],
                "text": "Reflectance and texture of real-world surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The visual appearance of real-world surfaces and the dependence of appearance on imaging conditions is investigated and a BRDF (bidirectional reflectance distribution function) database with reflectance measurements for over 60 different samples, each observed with over 200 different combinations of viewing and source directions is presented."
            },
            "venue": {
                "fragments": [],
                "text": "TOGS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561280"
                        ],
                        "name": "R. Dror",
                        "slug": "R.-Dror",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Dror",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dror"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 73
                            }
                        ],
                        "text": "Optimization [Romeiro and Zickler 2010] and machine learning approaches [Dror et al. 2001; Liu et al. 2010] to inferring materials have been studied recently, but do not yet demonstrate the performance necessary to annotate materials in noisy, real-world images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17723721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ca9abf859eb9561edd2ed919cdd2b6f25aee51fd",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Physical surfaces such as metal, plastic, and paper possess different optical qualities that lead to different characteristics in images. We have found that humans can effectively estimate certain surface reflectance properties from a single image without knowledge of illumination. We develop a machine vision system to perform similar reflectance estimation tasks automatically. The problem of estimating reflectance form single images under unknown, complex illumination proves highly under-constrained due to the variety of potential reflectances and illuminations. Our solution relies on statistical regularities in the spatial structure of real-world illumination. These regularities translate into predictable relationships between surface reflectance and certain statistical features of the image. We determine these relationships using machine learning techniques. Our algorithms do not depend on color or polarization; they apply even to monochromatic imagery. An ability to estimate reflectance under uncontrolled illumination will further efforts to recognize materials and surface properties, tp capture computer graphics models from photographs, and to generalize classical motion and stereo algorithms such that they can handle non-Lambertian surfaces."
            },
            "slug": "Estimating-surface-reflectance-properties-from-Dror-Adelson",
            "title": {
                "fragments": [],
                "text": "Estimating surface reflectance properties from images under unknown illumination"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A machine vision system is developed to perform similar reflectance estimation tasks automatically and relies on statistical regularities in the spatial structure of real-world illumination to determine predictable relationships between surface reflectance and certain statistical features of the image."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35813794"
                        ],
                        "name": "B. Walter",
                        "slug": "B.-Walter",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Walter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Walter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934153"
                        ],
                        "name": "Pramook Khungurn",
                        "slug": "Pramook-Khungurn",
                        "structuredName": {
                            "firstName": "Pramook",
                            "lastName": "Khungurn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pramook Khungurn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144374926"
                        ],
                        "name": "K. Bala",
                        "slug": "K.-Bala",
                        "structuredName": {
                            "firstName": "Kavita",
                            "lastName": "Bala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 187
                            }
                        ],
                        "text": "To further explore the effect of d, and further validate the quality of this pipeline, we added a synthetically rendered image rendered with a state-of-art global illumination algorithm [Walter et al. 2012]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8363740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "607485af425077f3f19f0cee2bdd0c40e410efc6",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Scenes modeling the real-world combine a wide variety of phenomena including glossy materials, detailed heterogeneous anisotropic media, subsurface scattering, and complex illumination. Predictive rendering of such scenes is difficult; unbiased algorithms are typically too slow or too noisy. Virtual point light (VPL) based algorithms produce low noise results across a wide range of performance/accuracy tradeoffs, from interactive rendering to high quality offline rendering, but their bias means that locally important illumination features may be missing. We introduce a bidirectional formulation and a set of weighting strategies to significantly reduce the bias in VPL-based rendering algorithms. Our approach, bidirectional lightcuts, maintains the scalability and low noise global illumination advantages of prior VPL-based work, while significantly extending their generality to support a wider range of important materials and visual cues. We demonstrate scalable, efficient, and low noise rendering of scenes with highly complex materials including gloss, BSSRDFs, and anisotropic volumetric models."
            },
            "slug": "Bidirectional-lightcuts-Walter-Khungurn",
            "title": {
                "fragments": [],
                "text": "Bidirectional lightcuts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces a bidirectional formulation and a set of weighting strategies to significantly reduce the bias in VPL-based rendering algorithms, and demonstrates scalable, efficient, and low noise rendering of scenes with highly complex materials including gloss, BSSRDFs, and anisotropic volumetric models."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37615584"
                        ],
                        "name": "Kevin Karsch",
                        "slug": "Kevin-Karsch",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Karsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Karsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3236027"
                        ],
                        "name": "Varsha Hedau",
                        "slug": "Varsha-Hedau",
                        "structuredName": {
                            "firstName": "Varsha",
                            "lastName": "Hedau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varsha Hedau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 69
                            }
                        ],
                        "text": "Other techniques for recreating lighting with user annotation (e.g. [Karsch et al. 2011]) would be worth considering in the future."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12447228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66bb1604482741a16f3abb1bfb1c531790c66a57",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to realistically insert synthetic objects into existing photographs without requiring access to the scene or any additional scene measurements. With a single image and a small amount of annotation, our method creates a physical model of the scene that is suitable for realistically rendering synthetic objects with diffuse, specular, and even glowing materials while accounting for lighting interactions between the objects and the scene. We demonstrate in a user study that synthetic images produced by our method are confusable with real scenes, even for people who believe they are good at telling the difference. Further, our study shows that our method is competitive with other insertion methods while requiring less scene information. We also collected new illumination and reflectance datasets; renderings produced by our system compare well to ground truth. Our system has applications in the movie and gaming industry, as well as home decorating and user content creation, among others."
            },
            "slug": "Rendering-synthetic-objects-into-legacy-photographs-Karsch-Hedau",
            "title": {
                "fragments": [],
                "text": "Rendering synthetic objects into legacy photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work proposes a method to realistically insert synthetic objects into existing photographs without requiring access to the scene or any additional scene measurements, and shows that the method is competitive with other insertion methods while requiring less scene information."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405709509"
                        ],
                        "name": "A. Ben-Artzi",
                        "slug": "A.-Ben-Artzi",
                        "structuredName": {
                            "firstName": "Aner",
                            "lastName": "Ben-Artzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ben-Artzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681231"
                        ],
                        "name": "R. Overbeck",
                        "slug": "R.-Overbeck",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Overbeck",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Overbeck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752236"
                        ],
                        "name": "R. Ramamoorthi",
                        "slug": "R.-Ramamoorthi",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Ramamoorthi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ramamoorthi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 70
                            }
                        ],
                        "text": "This precluded the use of sophisticated precomputation-based methods [Ben-Artzi et al. 2006] with high compute and memory requirements."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1885621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "468026cbf1844b6a2e73b129ecbbcc8331440984",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Current systems for editing BRDFs typically allow users to adjust analytic parameters while visualizing the results in a simplified setting (e.g. unshadowed point light). This paper describes a real-time rendering system that enables interactive edits of BRDFs, as rendered in their final placement on objects in a static scene, lit by direct, complex illumination. All-frequency effects (ranging from near-mirror reflections and hard shadows to diffuse shading and soft shadows) are rendered using a precomputation-based approach. Inspired by real-time relighting methods, we create a linear system that fixes lighting and view to allow real-time BRDF manipulation. In order to linearize the image's response to BRDF parameters, we develop an intermediate curve-based representation, which also reduces the rendering and precomputation operations to 1D while maintaining accuracy for a very general class of BRDFs. Our system can be used to edit complex analytic BRDFs (including anisotropic models), as well as measured reflectance data. We improve on the standard precomputed radiance transfer (PRT) rendering computation by introducing an incremental rendering algorithm that takes advantage of frame-to-frame coherence. We show that it is possible to render reference-quality images while only updating 10% of the data at each frame, sustaining frame-rates of 25-30fps."
            },
            "slug": "Real-time-BRDF-editing-in-complex-lighting-Ben-Artzi-Overbeck",
            "title": {
                "fragments": [],
                "text": "Real-time BRDF editing in complex lighting"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A real-time rendering system that enables interactive edits of BRDFs, as rendered in their final placement on objects in a static scene, lit by direct, complex illumination is described."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "Just as massive databases of images and objects have led to new advances in image editing [Hays and Efros 2007; Lalonde et al. 2007] and object recognition [Russell et al. 2008], we believe that a large and comprehensive catalog of contextual surface appearance properties is critical for everyday\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 940100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "714d48570351de99eb755735b543c7b84bd9fb46",
            "isKey": false,
            "numCitedBy": 707,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "What can you do with a million images? In this paper, we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless, but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks, we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data driven, requiring no annotations or labeling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of image completions and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image completion approaches."
            },
            "slug": "Scene-completion-using-millions-of-photographs-Hays-Efros",
            "title": {
                "fragments": [],
                "text": "Scene completion using millions of photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new image completion algorithm powered by a huge database of photographs gathered from the Web that can generate a diverse set of image completions and allow users to select among them."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2007"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 48
                            }
                        ],
                        "text": "These databases include 80 Million Tiny Images [Torralba et al. 2008], ImageNet [Deng et al. 2009], the SUN scene database [Xiao et al. 2010], and the LabelMe dataset [Russell et al. 2008]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7487588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d2b5c64a67f65c5dd812b89e07973f97699552",
            "isKey": false,
            "numCitedBy": 1868,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "slug": "80-Million-Tiny-Images:-A-Large-Data-Set-for-Object-Torralba-Fergus",
            "title": {
                "fragments": [],
                "text": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "For certain classes that are particularly prevalent in the dataset, such as people, this work is able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436224"
                        ],
                        "name": "R. Fleming",
                        "slug": "R.-Fleming",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Fleming",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fleming"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20371345,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "88815b76a0635fa79e26af361ac180f9d2d8da89",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Many materials, including leaves, water, plastic, and chrome exhibit specular reflections. It seems reasonable that the visual system can somehow exploit specular reflections to recover three-dimensional (3D) shape. Previous studies (e.g., J. T. Todd & E. Mingolla, 1983; J. F. Norman, J. T. Todd, & G. A. Orban, 2004) have shown that specular reflections aid shape estimation, but the relevant image information has not yet been isolated. Here we explain how specular reflections can provide reliable and accurate constraints on 3D shape. We argue that the visual system can treat specularities somewhat like textures, by using the systematic patterns of distortion across the image of a specular surface to recover 3D shape. However, there is a crucial difference between textures and specularities: In the case of textures, the image compressions depend on the first derivative of the surface depth (i.e., surface orientation), whereas in the case of specularities, the image compressions depend on the second derivative (i.e., surfaces curvatures). We suggest that this difference provides a cue that can help the visual system distinguish between textures and specularities, even when present simultaneously. More importantly, we show that the dependency of specular distortions on the second derivative of the surface leads to distinctive fields of image orientation as the reflected world is warped across the surface. We find that these \"orientation fields\" are (i) diagnostic of 3D shape, (ii) remain surprisingly stable when the world reflected in the surface is changed, and (iii) can be extracted from the image by populations of simple oriented filters. Thus the use of specular reflections for 3D shape perception is both easier and more reliable than previous computational work would suggest."
            },
            "slug": "Specular-reflections-and-the-perception-of-shape.-Fleming-Torralba",
            "title": {
                "fragments": [],
                "text": "Specular reflections and the perception of shape."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the dependency of specular distortions on the second derivative of the surface leads to distinctive fields of image orientation as the reflected world is warped across the surface, which are diagnostic of 3D shape and can be extracted from the image by populations of simple oriented filters."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2656633"
                        ],
                        "name": "Ganesh Ramanarayanan",
                        "slug": "Ganesh-Ramanarayanan",
                        "structuredName": {
                            "firstName": "Ganesh",
                            "lastName": "Ramanarayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ganesh Ramanarayanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256646"
                        ],
                        "name": "J. Ferwerda",
                        "slug": "J.-Ferwerda",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Ferwerda",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ferwerda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35813794"
                        ],
                        "name": "B. Walter",
                        "slug": "B.-Walter",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Walter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Walter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144374926"
                        ],
                        "name": "K. Bala",
                        "slug": "K.-Bala",
                        "structuredName": {
                            "firstName": "Kavita",
                            "lastName": "Bala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 70
                            }
                        ],
                        "text": "However, we found that users were not skilled at recreating lighting [Ramanarayanan et al. 2007], especially in the low-dynamic range input images we provide."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1514900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89d5e446cc40da2b3ae347b99f3d1205e58cfd81",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient, realistic rendering of complex scenes is one of the grand challenges in computer graphics. Perceptually based rendering addresses this challenge by taking advantage of the limits of human vision. However, existing methods, based on predicting visible image differences, are too conservative because some kinds of image differences do not matter to human observers. In this paper, we introduce the concept of visual equivalence, a new standard for image fidelity in graphics. Images are visually equivalent if they convey the same impressions of scene appearance, even if they are visibly different. To understand this phenomenon, we conduct a series of experiments that explore how object geometry, material, and illumination interact to provide information about appearance, and we characterize how two kinds of transformations on illumination maps (blurring and warping) affect these appearance attributes. We then derive visual equivalence predictors (VEPs): metrics for predicting when images rendered with transformed illumination maps will be visually equivalent to images rendered with reference maps. We also run a confirmatory study to validate the effectiveness of these VEPs for general scenes. Finally, we show how VEPs can be used to improve the efficiency of two rendering algorithms: Light-cuts and precomputed radiance transfer. This work represents some promising first steps towards developing perceptual metrics based on higher order aspects of visual coding."
            },
            "slug": "Visual-equivalence:-towards-a-new-standard-for-Ramanarayanan-Ferwerda",
            "title": {
                "fragments": [],
                "text": "Visual equivalence: towards a new standard for image fidelity"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The concept of visual equivalence is introduced, a new standard for image fidelity in graphics that ensures that images are visually equivalent if they convey the same impressions of scene appearance, even if they are visibly different."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436224"
                        ],
                        "name": "R. Fleming",
                        "slug": "R.-Fleming",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Fleming",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fleming"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561280"
                        ],
                        "name": "R. Dror",
                        "slug": "R.-Dror",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Dror",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dror"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "Humans are reasonably good at identifying materials and their properties over a range of lighting conditions [Fleming et al. 2003], and the availability of crowdsourcing lets us collect annotations at scale for large image collections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8688474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9915582867ce80444038321ab08482068a427acb",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Under typical viewing conditions, we find it easy to distinguish between different materials, such as metal, plastic, and paper. Recognizing materials from their surface reflectance properties (such as lightness and gloss) is a nontrivial accomplishment because of confounding effects of illumination. However, if subjects have tacit knowledge of the statistics of illumination encountered in the real world, then it is possible to reject unlikely image interpretations, and thus to estimate surface reflectance even when the precise illumination is unknown. A surface reflectance matching task was used to measure the accuracy of human surface reflectance estimation. The results of the matching task demonstrate that subjects can match surface reflectance properties reliably and accurately in the absence of context, as long as the illumination is realistic. Matching performance declines when the illumination statistics are not representative of the real world. Together these findings suggest that subjects do use stored assumptions about the statistics of real-world illumination to estimate surface reflectance. Systematic manipulations of pixel and wavelet properties of illuminations reveal that the visual system's assumptions about illumination are of intermediate complexity (e.g., presence of edges and bright light sources), rather than of high complexity (e.g., presence of recognizable objects in the environment)."
            },
            "slug": "Real-world-illumination-and-the-perception-of-Fleming-Dror",
            "title": {
                "fragments": [],
                "text": "Real-world illumination and the perception of surface reflectance properties."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The findings suggest that subjects do use stored assumptions about the statistics of real-world illumination to estimate surface reflectance, and that the visual system's assumptions about illumination are of intermediate complexity, rather than of high complexity."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34494023"
                        ],
                        "name": "A. Ngan",
                        "slug": "A.-Ngan",
                        "structuredName": {
                            "firstName": "Addy",
                            "lastName": "Ngan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ngan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145403226"
                        ],
                        "name": "F. Durand",
                        "slug": "F.-Durand",
                        "structuredName": {
                            "firstName": "Fr\u00e9do",
                            "lastName": "Durand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Durand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752521"
                        ],
                        "name": "W. Matusik",
                        "slug": "W.-Matusik",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Matusik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Matusik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "\u2026calibrated measurements, including the MERL database [Matusik et al. 2003] with 4D BRDF measurements for 105 materials, fit to various BRDF models [Ngan et al. 2005], and CUReT [Dana et al. 1999], with 6D measured BTFs (bidirectional texture functions) for 61 samples with various lighting and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6613882,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "bf977ca18b4b04e05be41d86fe6ab10573993217",
            "isKey": false,
            "numCitedBy": 497,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bidirectional Reflectance Distribution Function (BRDF) describes the appearance of a material by its interaction with light at a surface point. A variety of analytical models have been proposed to represent BRDFs. However, analysis of these models has been scarce due to the lack of high-resolution measured data. In this work we evaluate several well-known analytical models in terms of their ability to fit measured BRDFs. We use an existing high-resolution data set of a hundred isotropic materials and compute the best approximation for each analytical model. Furthermore, we have built a new setup for efficient acquisition of anisotropic BRDFs, which allows us to acquire anisotropic materials at high resolution. We have measured four samples of anisotropic materials (brushed aluminum, velvet, and two satins). Based on the numerical errors, function plots, and rendered images we provide insights into the performance of the various models. We conclude that for most isotropic materials physically-based analytic reflectance models can represent their appearance quite well. We illustrate the important difference between the two common ways of defining the specular lobe: around the mirror direction and with respect to the half-vector. Our evaluation shows that the latter gives a more accurate shape for the reflection lobe. Our analysis of anisotropic materials indicates current parametric reflectance models cannot represent their appearances faithfully in many cases. We show that using a sampled microfacet distribution computed from measurements improves the fit and qualitatively reproduces the measurements."
            },
            "slug": "Experimental-analysis-of-BRDF-models-Ngan-Durand",
            "title": {
                "fragments": [],
                "text": "Experimental analysis of BRDF models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work evaluates several well-known analytical models in terms of their ability to fit measured BRDFs, and shows that using a sampled microfacet distribution computed from measurements improves the fit and qualitatively reproduces the measurements."
            },
            "venue": {
                "fragments": [],
                "text": "EGSR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 65
                            }
                        ],
                        "text": "In a pilot study, we designed an interface, inspired by LabelMe [Russell et al. 2008], where a user is presented with a material segment and asked to enter a freeform text label, with an \u201cauto complete\u201d feature to suggest material names from a database."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 147
                            }
                        ],
                        "text": "However, our system can also be run as a stand-alone interface hosted on our servers, so that new photos can continue to be annotated (similar to [Russell et al. 2008] for object labeling)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 168
                            }
                        ],
                        "text": "These databases include 80 Million Tiny Images [Torralba et al. 2008], ImageNet [Deng et al. 2009], the SUN scene database [Xiao et al. 2010], and the LabelMe dataset [Russell et al. 2008]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026massive databases of images and objects have led to new advances in image editing [Hays and Efros 2007; Lalonde et al. 2007] and object recognition [Russell et al. 2008], we believe that a large and comprehensive catalog of contextual surface appearance properties is critical for everyday\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": true,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681442"
                        ],
                        "name": "Ce Liu",
                        "slug": "Ce-Liu",
                        "structuredName": {
                            "firstName": "Ce",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ce Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240195"
                        ],
                        "name": "Lavanya Sharan",
                        "slug": "Lavanya-Sharan",
                        "structuredName": {
                            "firstName": "Lavanya",
                            "lastName": "Sharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lavanya Sharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680975"
                        ],
                        "name": "R. Rosenholtz",
                        "slug": "R.-Rosenholtz",
                        "structuredName": {
                            "firstName": "Ruth",
                            "lastName": "Rosenholtz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenholtz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 60
                            }
                        ],
                        "text": "This category includes the Flickr Materials Database (FMD) [Liu et al. 2010] and the datasets of Hu, et al. [2011]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 91
                            }
                        ],
                        "text": "Optimization [Romeiro and Zickler 2010] and machine learning approaches [Dror et al. 2001; Liu et al. 2010] to inferring materials have been studied recently, but do not yet demonstrate the performance necessary to annotate materials in noisy, real-world images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1965245,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba6e446a9c0aeb181e420dfb42c4c0d0a2742b28",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We are interested in identifying the material category, e.g. glass, metal, fabric, plastic or wood, from a single image of a surface. Unlike other visual recognition tasks in computer vision, it is difficult to find good, reliable features that can tell material categories apart. Our strategy is to use a rich set of low and mid-level features that capture various aspects of material appearance. We propose an augmented Latent Dirichlet Allocation (aLDA) model to combine these features under a Bayesian generative framework and learn an optimal combination of features. Experimental results show that our system performs material recognition reasonably well on a challenging material database, outperforming state-of-the-art material/texture recognition systems."
            },
            "slug": "Exploring-features-in-a-Bayesian-framework-for-Liu-Sharan",
            "title": {
                "fragments": [],
                "text": "Exploring features in a Bayesian framework for material recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An augmented Latent Dirichlet Allocation model is proposed to combine a rich set of low and mid-level features that capture various aspects of material appearance under a Bayesian generative framework and learn an optimal combination of features."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716904"
                        ],
                        "name": "J. Koenderink",
                        "slug": "J.-Koenderink",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koenderink",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koenderink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7977369"
                        ],
                        "name": "A. Doorn",
                        "slug": "A.-Doorn",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Doorn",
                            "middleNames": [
                                "J.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doorn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699586"
                        ],
                        "name": "A. Kappers",
                        "slug": "A.-Kappers",
                        "structuredName": {
                            "firstName": "Astrid",
                            "lastName": "Kappers",
                            "middleNames": [
                                "M.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kappers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 85
                            }
                        ],
                        "text": "While prior work typically uses a gauge figure to represent a surface normal figure [Koenderink et al. 1992; Cole et al. 2009], we found a 3D perspective grid to be much more effective."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7349914,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "0c6b682dc1597fb9621c29fcd605c1cfb0757256",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Subjects adjusted a local gauge figure such as to perceptually \u201cfit\u201d the apparent surfaces of objects depicted in photographs. We obtained a few hundred data points per session, covering the picture according to a uniform lattice. Settings were repeated 3 times for each of 3 subjects. Almost all of the variability resided in the slant; the relative spread in the slant was about 25% (Weber fraction). The tilt was reproduced with a typical spread of about 10\u2020. The rank correlation of the slant settings of different observers was high, thus the slant settings of different subjects were monotonically related. The variability could be predicted from the scatter in repeated settings by the individual observers. Although repeated settings by a single observer agreed within 5%, observers did not agree on the value of the slant, even on the average. Scaling factors of a doubling in the depth dimension were encountered between different subjects. The data conformed quite well to some hypothetical fiducial global surface, the orientation of which was \u201cprobed\u201d by the subject\u2019s local settings. The variability was completely accounted for by singleobserver scatter. These conclusions are based upon an analysis of the internal structure of the local settings. We did not address the problem of veridicality, that is, conformity to some \u201creal object.\u201d"
            },
            "slug": "Surface-perception-in-pictures-Koenderink-Doorn",
            "title": {
                "fragments": [],
                "text": "Surface perception in pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The data conformed quite well to some hypothetical fiducial global surface, the orientation of which was \u201cprobed\u201d by the subject\u2019s local settings, based upon an analysis of the internal structure of the local settings."
            },
            "venue": {
                "fragments": [],
                "text": "Perception & psychophysics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689241"
                        ],
                        "name": "Yanxi Liu",
                        "slug": "Yanxi-Liu",
                        "structuredName": {
                            "firstName": "Yanxi",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanxi Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143804028"
                        ],
                        "name": "Wen-Chieh Lin",
                        "slug": "Wen-Chieh-Lin",
                        "structuredName": {
                            "firstName": "Wen-Chieh",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-Chieh Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 110
                            }
                        ],
                        "text": "The PSU Near-Regular Texture Database consists of closeups of textured patterns, including material textures [Liu et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6286692,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "062a2de4a7a129026076be867b20785e4ffe4da3",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A near-regular texture deviates geometrically and photometrically from a regular congruent tiling. Although near-regular textures are ubiquitous in the man-made and natural world, they present computational challenges for state of the art texture analysis and synthesis algorithms. Using regular tiling as our anchor point, and with user-assisted lattice extraction, we can explicitly model the deformation of a near-regular texture with respect to geometry, lighting and color. We treat a deformation field both as a function that acts on a texture and as a texture that is acted upon, and develop a multi-modal framework where each deformation field is subject to analysis, synthesis and manipulation. Using this formalization, we are able to construct simple parametric models to faithfully synthesize the appearance of a near-regular texture and purposefully control its regularity."
            },
            "slug": "Near-regular-texture-analysis-and-manipulation-Liu-Lin",
            "title": {
                "fragments": [],
                "text": "Near-regular texture analysis and manipulation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work develops a multi-modal framework where each deformation field is subject to analysis, synthesis and manipulation and is able to construct simple parametric models to faithfully synthesize the appearance of a near-regular texture and purposefully control its regularity."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39578349"
                        ],
                        "name": "Forrester Cole",
                        "slug": "Forrester-Cole",
                        "structuredName": {
                            "firstName": "Forrester",
                            "lastName": "Cole",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Forrester Cole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808416"
                        ],
                        "name": "Kevin Sanik",
                        "slug": "Kevin-Sanik",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Sanik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Sanik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716722"
                        ],
                        "name": "D. DeCarlo",
                        "slug": "D.-DeCarlo",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "DeCarlo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCarlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37737599"
                        ],
                        "name": "A. Finkelstein",
                        "slug": "A.-Finkelstein",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Finkelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807080"
                        ],
                        "name": "T. Funkhouser",
                        "slug": "T.-Funkhouser",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Funkhouser",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Funkhouser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7723706"
                        ],
                        "name": "S. Rusinkiewicz",
                        "slug": "S.-Rusinkiewicz",
                        "structuredName": {
                            "firstName": "Szymon",
                            "lastName": "Rusinkiewicz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rusinkiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152964847"
                        ],
                        "name": "Manish Singh",
                        "slug": "Manish-Singh",
                        "structuredName": {
                            "firstName": "Manish",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manish Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 143
                            }
                        ],
                        "text": "\u2026data is gaining adoption, and has been used in recent approaches to a range of problems, including understanding shape through gauge figures [Cole et al. 2009], creating a mesh segmentation database [Chen et al. 2009], devising a retargeting evaluation framework [Rubinstein et al. 2010],\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 109
                            }
                        ],
                        "text": "While prior work typically uses a gauge figure to represent a surface normal figure [Koenderink et al. 1992; Cole et al. 2009], we found a 3D perspective grid to be much more effective."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13925565,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "f18dff37545729f68b3477a5e4bb674b96a55d16",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the ability of sparse line drawings to depict 3D shape. We perform a study in which people are shown an image of one of twelve 3D objects depicted with one of six styles and asked to orient a gauge to coincide with the surface normal at many positions on the object's surface. The normal estimates are compared with each other and with ground truth data provided by a registered 3D surface model to analyze accuracy and precision. The paper describes the design decisions made in collecting a large data set (275,000 gauge measurements) and provides analysis to answer questions about how well people interpret shapes from drawings. Our findings suggest that people interpret certain shapes almost as well from a line drawing as from a shaded image, that current computer graphics line drawing techniques can effectively depict shape and even match the effectiveness of artist's drawings, and that errors in depiction are often localized and can be traced to particular properties of the lines used. The data collected for this study will become a publicly available resource for further studies of this type."
            },
            "slug": "How-well-do-line-drawings-depict-shape-Cole-Sanik",
            "title": {
                "fragments": [],
                "text": "How well do line drawings depict shape?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The findings suggest that people interpret certain shapes almost as well from a line drawing as from a shaded image, that current computer graphics line drawing techniques can effectively depict shape and even match the effectiveness of artist's drawings, and that errors in depiction are often localized and can be traced to particular properties of the lines used."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3246404"
                        ],
                        "name": "Peiran Ren",
                        "slug": "Peiran-Ren",
                        "structuredName": {
                            "firstName": "Peiran",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peiran Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124913497"
                        ],
                        "name": "Jiaping Wang",
                        "slug": "Jiaping-Wang",
                        "structuredName": {
                            "firstName": "Jiaping",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaping Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144399473"
                        ],
                        "name": "John M. Snyder",
                        "slug": "John-M.-Snyder",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Snyder",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John M. Snyder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145786191"
                        ],
                        "name": "Xin Tong",
                        "slug": "Xin-Tong",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Tong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Tong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143632999"
                        ],
                        "name": "B. Guo",
                        "slug": "B.-Guo",
                        "structuredName": {
                            "firstName": "Baining",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Guo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 97
                            }
                        ],
                        "text": "To help address these issues, appearance acquisition research has focused on hardware solutions [Ren et al. 2011], but large databases of measured materials are still difficult to acquire."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4375887,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9a2c2e7df1fb6a67da6afa97f7009e0cd42972c9",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple, fast solution for reflectance acquisition using tools that fit into a pocket. Our method captures video of a flat target surface from a fixed video camera lit by a hand-held, moving, linear light source. After processing, we obtain an SVBRDF. We introduce a BRDF chart, analogous to a color \"checker\" chart, which arranges a set of known-BRDF reference tiles over a small card. A sequence of light responses from the chart tiles as well as from points on the target is captured and matched to reconstruct the target's appearance. We develop a new algorithm for BRDF reconstruction which works directly on these LDR responses, without knowing the light or camera position, or acquiring HDR lighting. It compensates for spatial variation caused by the local (finite distance) camera and light position by warping responses over time to align them to a specular reference. After alignment, we find an optimal linear combination of the Lambertian and purely specular reference responses to match each target point's response. The same weights are then applied to the corresponding (known) reference BRDFs to reconstruct the target point's BRDF. We extend the basic algorithm to also recover varying surface normals by adding two spherical caps for diffuse and specular references to the BRDF chart. We demonstrate convincing results obtained after less than 30 seconds of data capture, using commercial mobile phone cameras in a casual environment."
            },
            "slug": "Pocket-reflectometry-Ren-Wang",
            "title": {
                "fragments": [],
                "text": "Pocket reflectometry"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new algorithm for BRDF reconstruction is developed which works directly on these LDR responses, without knowing the light or camera position, or acquiring HDR lighting, and extends the basic algorithm to also recover varying surface normals."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778676"
                        ],
                        "name": "P. Debevec",
                        "slug": "P.-Debevec",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Debevec",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Debevec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 77
                            }
                        ],
                        "text": "We selected the high dynamic range environment map of the Ennis-Brown House [Debevec 1998] (a high-quality environment map of an indoor scene)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11112830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edb61be35363db146cb8e0d915d05ed93d020bbb",
            "isKey": false,
            "numCitedBy": 759,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method that uses measured scene radiance and global illumination in order to add new objects to light-based models with correct lighting. The method uses a high dynamic range image-based model of the scene, rather than synthetic light sources, to illuminate the new objects. To compute the illumination, the scene is considered as three components: the distant scene, the local scene, and the synthetic objects. The distant scene is assumed to be photometrically unaffected by the objects, obviating the need for reflectance model information. The local scene is endowed with estimated reflectance model information so that it can catch shadows and receive reflected light from the new objects. Renderings are created with a standard global illumination method by simulating the interaction of light amongst the three components. A differential rendering technique allows for good results to be obtained when only an estimate of the local scene reflectance properties is known.\n We apply the general method to the problem of rendering synthetic objects into real scenes. The light-based model is constructed from an approximate geometric model of the scene and by using a light probe to measure the incident illumination at the location of the synthetic objects. The global illumination solution is then composited into a photograph of the scene using the differential rendering technique. We conclude by discussing the relevance of the technique to recovering surface reflectance properties in uncontrolled lighting situations. Applications of the method include visual effects, interior design, and architectural visualization."
            },
            "slug": "Rendering-synthetic-objects-into-real-scenes:-and-Debevec",
            "title": {
                "fragments": [],
                "text": "Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A method that uses measured scene radiance and global illumination in order to add new objects to light-based models with correct lighting and the relevance of the technique to recovering surface reflectance properties in uncontrolled lighting situations is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '08"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031580"
                        ],
                        "name": "W. B. Kerr",
                        "slug": "W.-B.-Kerr",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Kerr",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. B. Kerr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757883"
                        ],
                        "name": "F. Pellacini",
                        "slug": "F.-Pellacini",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Pellacini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pellacini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 136
                            }
                        ],
                        "text": "In another preliminary study we found that matching the color to the real object was the hardest part of the task (as also reported in [Kerr and Pellacini 2010])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 71
                            }
                        ],
                        "text": "To specify color, we use a HSV (hue, saturation, value) widget, as in [Kerr and Pellacini 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14563203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "396e0af67dc81469af0345f99330217407306109",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Material design is the process by which artists specify the reflectance properties of a surface, such as its diffuse color and specular roughness. We present a user study to evaluate the relative benefits of different material design interfaces, focusing on novice users since they stand to gain the most from intuitive interfaces. Specifically, we investigate the editing of the parameters of analytic bidirectional distribution functions (BRDFs) using three interface paradigms: physical sliders by which users set the parameters of analytic BRDF models, such as diffuse albedo and specular roughness; perceptual sliders by which users set perceptually-inspired parameters, such as diffuse luminance and gloss contrast; and image navigation by which material variations are displayed in arrays of image thumbnails and users make edits by selecting them. We investigate two design tasks: precise adjustment and artistic exploration. We collect objective and subjective data, finding that subjects can perform equally well with physical and perceptual sliders as long as the interface responds interactively. Image navigation performs worse than the other interfaces on precise adjustment tasks, but excels at aiding in artistic exploration. We find that given enough time, novices can perform relatively complex material editing tasks with little training, and most novices work similarly to one another."
            },
            "slug": "Toward-evaluating-material-design-interface-for-Kerr-Pellacini",
            "title": {
                "fragments": [],
                "text": "Toward evaluating material design interface paradigms for novice users"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A user study to evaluate the relative benefits of different material design interfaces, focusing on novice users since they stand to gain the most from intuitive interfaces, finds that given enough time, novices can perform relatively complex material editing tasks with little training."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117344661"
                        ],
                        "name": "Chen Feng",
                        "slug": "Chen-Feng",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145535036"
                        ],
                        "name": "Fei Deng",
                        "slug": "Fei-Deng",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828552"
                        ],
                        "name": "V. Kamat",
                        "slug": "V.-Kamat",
                        "structuredName": {
                            "firstName": "Vineet",
                            "lastName": "Kamat",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kamat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 205
                            }
                        ],
                        "text": "We obtain VPs by finding line segments with LSD [von Gioi et al. 2010], then clustering the segments with J-Linkage [Toldo and Fusiello 2008], and solving for the optimal VP for each cluster [Tardif 2009; Feng et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14239004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c2fc9caea8f7c68ab239770edf073672f8c6210",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel algorithm that enables the semi-automatic reconstruction of man-made structures (e.g. buildings) into piecewise planar 3D models from a single image, allowing the models to be readily used for data acquisition in 3D GIS or in other virtual or augmented reality applications. Contrary to traditional labor intensive but accurate Single View Reconstruction (SVR) solutions that are based purely on geometric constraints, and recent fully automatic albeit low-accuracy SVR algorithms that are based on statistical inference, the presented method achieves a compromise between speed and accuracy, leading to less user input and acceptable visual effects compared to prior approaches. Most of the user input required in the presented approach is a line drawing that represents an outline of the building to be reconstructed. Using this input, the developed method takes advantage of a newly proposed Vanishing Point (VP) detection algorithm that can simultaneously estimate multiple VPs in an image. With those VPs, the normal direction of planes which are projected onto the image plane as polygons in the line drawing can be automatically calculated. Following this step, a linear system similar to traditional SVR solutions can be used to achieve 3D reconstruction. Experiments that demonstrate the efficacy and visual effects of the developed method are also described."
            },
            "slug": "SEMI-AUTOMATIC-3D-RECONSTRUCTION-OF-PIECEWISE-FROM-Feng-Deng",
            "title": {
                "fragments": [],
                "text": "SEMI-AUTOMATIC 3D RECONSTRUCTION OF PIECEWISE PLANAR BUILDING MODELS FROM SINGLE IMAGE"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel algorithm is presented that enables the semi-automatic reconstruction of man-made structures into piecewise planar 3D models from a single image, allowing the models to be readily used for data acquisition in 3D GIS or in other virtual or augmented reality applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2533751"
                        ],
                        "name": "P. Vangorp",
                        "slug": "P.-Vangorp",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Vangorp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Vangorp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097529"
                        ],
                        "name": "J. Laurijssen",
                        "slug": "J.-Laurijssen",
                        "structuredName": {
                            "firstName": "Jurgen",
                            "lastName": "Laurijssen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Laurijssen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51338913"
                        ],
                        "name": "P. Dutr\u00e9",
                        "slug": "P.-Dutr\u00e9",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Dutr\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dutr\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6275783,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "aa231701d0c1a3692690f1216093014eb96fd816",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual observation is our principal source of information in determining the nature of objects, including shape, material or roughness. The physiological and cognitive processes that resolve visual input into an estimate of the material of an object are influenced by the illumination and the shape of the object. This affects our ability to select materials by observing them on a point-lit sphere, as is common in current 3D modeling applications. In this paper we present an exploratory psychophysical experiment to study various influences on material discrimination in a realistic setting. The resulting data set is analyzed using a wide range of statistical techniques. Analysis of variance is used to estimate the magnitude of the influence of geometry, and fitted psychometric functions produce significantly diverse material discrimination thresholds across different shapes and materials. Suggested improvements to traditional material pickers include direct visualization on the target object, environment illumination, and the use of discrimination thresholds as a step size for parameter adjustments."
            },
            "slug": "The-influence-of-shape-on-the-perception-of-Vangorp-Laurijssen",
            "title": {
                "fragments": [],
                "text": "The influence of shape on the perception of material reflectance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An exploratory psychophysical experiment is presented to study various influences on material discrimination in a realistic setting and suggested improvements to traditional material pickers include direct visualization on the target object, environment illumination, and the use of discrimination thresholds as a step size for parameter adjustments."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 50
                            }
                        ],
                        "text": "The material name is meant to indicate the \u201cstuff\u201d [Adelson 2001] that gives the surface its appearance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2336823,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "761d0e9f4510bd314a7efd10b9baaf2720b33165",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The perception of objects is a well-developed field, but the perception of materials has been studied rather little. This is surprising given how important materials are for humans, and how important they must become for intelligent robots. We may learn something by looking at other fields in which material appearance is recognized as important. Classical artists were highly skilled at generating convincing materials. The simulation of material appearance is a topic of great importance in 3D computer graphics. Some fields, such as mineralogy, use the concept of a 'habit' which is a combination of shape and texture, and which may be used for characterizing certain objects or materials. We have recently taken steps toward material recognition by machines, using techniques derived from the domain of texture analysis."
            },
            "slug": "On-seeing-stuff:-the-perception-of-materials-by-and-Adelson",
            "title": {
                "fragments": [],
                "text": "On seeing stuff: the perception of materials by humans and machines"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The perception of objects is a well-developed field, but the perception of materials has been studied rather little, which is surprising given how important materials are for humans and how important they must become for intelligent robots."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784306"
                        ],
                        "name": "T. Weyrich",
                        "slug": "T.-Weyrich",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Weyrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Weyrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937540"
                        ],
                        "name": "Jason Lawrence",
                        "slug": "Jason-Lawrence",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Lawrence",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809190"
                        ],
                        "name": "H. Lensch",
                        "slug": "H.-Lensch",
                        "structuredName": {
                            "firstName": "Hendrik",
                            "lastName": "Lensch",
                            "middleNames": [
                                "P.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lensch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7723706"
                        ],
                        "name": "S. Rusinkiewicz",
                        "slug": "S.-Rusinkiewicz",
                        "structuredName": {
                            "firstName": "Szymon",
                            "lastName": "Rusinkiewicz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rusinkiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713451"
                        ],
                        "name": "Todd E. Zickler",
                        "slug": "Todd-E.-Zickler",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Zickler",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Todd E. Zickler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 78
                            }
                        ],
                        "text": "Material acquisition is an active area of research (for a recent survey, see [Weyrich et al. 2009])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 190
                            }
                        ],
                        "text": "A central challenge in creating such a catalog is that automatically recovering material properties from images is a notoriously difficult inverse problem that requires careful calibration [Weyrich et al. 2009] or strong assumptions about the image formation model [Romeiro and Zickler 2010]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18136202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f3819f0fc2820b71c17fb80b26add64abdc8887",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 319,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for scene understanding and realistic image synthesis require accurate models of the way real-world materials scatter light. This class describes recent work in the graphics community to measure the spatially- and directionally-varying reflectance and subsurface scattering of complex materials, and to develop efficient representations and analysis tools for these datasets. We describe the design of acquisition devices and capture strategies for BRDFs and BSSRDFs, efficient factored representations, and a case study of capturing the appearance of human faces."
            },
            "slug": "Principles-of-appearance-acquisition-and-Weyrich-Lawrence",
            "title": {
                "fragments": [],
                "text": "Principles of appearance acquisition and representation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Recent work in the graphics community to measure the spatially- and directionally-varying reflectance and subsurface scattering of complex materials, and to develop efficient representations and analysis tools for these datasets is described."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070035319"
                        ],
                        "name": "Diane Hu",
                        "slug": "Diane-Hu",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diane Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144651486"
                        ],
                        "name": "Liefeng Bo",
                        "slug": "Liefeng-Bo",
                        "structuredName": {
                            "firstName": "Liefeng",
                            "lastName": "Bo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liefeng Bo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1395805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "866942b26cd59dfad2dbef030d4f76d6b6660fea",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Material recognition is a fundamental problem in perception that is receiving increasing attention. Following the recent work using Flickr [16, 23], we empirically study material recognition of real-world objects using a rich set of local features. We use the Kernel Descriptor framework [5] and extend the set of descriptors to include materialmotivated attributes using variances of gradient orientation and magnitude. Large-Margin Nearest Neighbor learning is used for a 30-fold dimension reduction. We improve the state-of-the-art accuracy on the Flickr dataset [16] from 45% to 54%. We also introduce two new datasets using ImageNet and macro photos, extensively evaluating our set of features and showing promising connections between material and object recognition."
            },
            "slug": "Toward-Robust-Material-Recognition-for-Everyday-Hu-Bo",
            "title": {
                "fragments": [],
                "text": "Toward Robust Material Recognition for Everyday Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work empirically study material recognition of real-world objects using a rich set of local features using the Kernel Descriptor framework and extends the set of descriptors to include materialmotivated attributes using variances of gradient orientation and magnitude."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757883"
                        ],
                        "name": "F. Pellacini",
                        "slug": "F.-Pellacini",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Pellacini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pellacini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256646"
                        ],
                        "name": "J. Ferwerda",
                        "slug": "J.-Ferwerda",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Ferwerda",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ferwerda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748054"
                        ],
                        "name": "D. Greenberg",
                        "slug": "D.-Greenberg",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Greenberg",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Greenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 204
                            }
                        ],
                        "text": "We made our choice based on pilot studies as well as the material design study of Kerr and Pellacini [2010], which found no preference between three broad classes of models: Ward [1992], perceptual Ward [Pellacini et al. 2000], and a microfacet-based model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10382829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1093f06b53c32d7af40e43c4fdba7b0365116f94",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a new light reflection model for image synthesis based on experimental studies of surface gloss perception. To develop the model, we've conducted two experiments that explore the relationships between the physical parameters used to describe the reflectance properties of glossy surfaces and the perceptual dimensions of glossy appearance. In the first experiment we use multidimensional scaling techniques to reveal the dimensionality of gloss perception for simulated painted surfaces. In the second experiment we use magnitude estimation methods to place metrics on these dimensions that relate changes in apparent gloss to variations in surface reflectance properties. We use the results of these experiments to rewrite the parameters of a physically-based light reflection model in perceptual terms. The result is a new psychophysically-based light reflection model where the dimensions of the model are perceptually meaningful, and variations along the dimensions are perceptually uniform. We demonstrate that the model can facilitate describing surface gloss in graphics rendering applications. This work represents a new methodology for developing light reflection models for image synthesis."
            },
            "slug": "Toward-a-psychophysically-based-light-reflection-Pellacini-Ferwerda",
            "title": {
                "fragments": [],
                "text": "Toward a psychophysically-based light reflection model for image synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new psychophysically-based light reflection model where the dimensions of the model are perceptually meaningful, and variations along the dimensions are perceptially uniform is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752521"
                        ],
                        "name": "W. Matusik",
                        "slug": "W.-Matusik",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Matusik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Matusik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 98
                            }
                        ],
                        "text": "A few public databases exist with carefully calibrated measurements, including the MERL database [Matusik et al. 2003] with 4D BRDF measurements for 105 materials, fit to various BRDF models [Ngan et al. 2005], and CUReT [Dana et al. 1999], with 6D measured BTFs (bidirectional texture functions)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 935718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50ba65f0dc271c4af6a8423dc6a9c4d91eaf37da",
            "isKey": false,
            "numCitedBy": 781,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data. Instead of using analytical reflectance models, we represent each BRDF as a dense set of measurements. This allows us to interpolate and extrapolate in the space of acquired BRDFs to create new BRDFs. We treat each acquired BRDF as a single high-dimensional vector taken from a space of all possible BRDFs. We apply both linear (subspace) and non-linear (manifold) dimensionality reduction tools in an effort to discover a lower-dimensional representation that characterizes our measurements. We let users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space. On the low-dimensional manifold, movement along these directions produces novel but valid BRDFs."
            },
            "slug": "A-data-driven-reflectance-model-Matusik",
            "title": {
                "fragments": [],
                "text": "A data-driven reflectance model"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work presents a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data that lets users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40151622"
                        ],
                        "name": "F. Romeiro",
                        "slug": "F.-Romeiro",
                        "structuredName": {
                            "firstName": "Fabiano",
                            "lastName": "Romeiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Romeiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713451"
                        ],
                        "name": "Todd E. Zickler",
                        "slug": "Todd-E.-Zickler",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Zickler",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Todd E. Zickler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 14
                            }
                        ],
                        "text": "Optimization [Romeiro and Zickler 2010] and machine learning approaches [Dror et al. 2001; Liu et al. 2010] to inferring materials have been studied recently, but do not yet demonstrate the performance necessary to annotate materials in noisy, real-world images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 266
                            }
                        ],
                        "text": "A central challenge in creating such a catalog is that automatically recovering material properties from images is a notoriously difficult inverse problem that requires careful calibration [Weyrich et al. 2009] or strong assumptions about the image formation model [Romeiro and Zickler 2010]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15699360,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d346bf9379dec8ee78088a76ec5d4abeae811a92",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of inferring homogeneous reflectance (BRDF) from a single image of a known shape in an unknown real-world lighting environment. With appropriate representations of lighting and reflectance, the image provides bilinear constraints on the two signals, and our task is to blindly isolate the latter. We achieve this by leveraging the statistics of real-world illumination and estimating the reflectance that is most likely under a distribution of probable illumination environments. Experimental results with a variety of real and synthetic images suggest that useable reflectance information can be inferred in many cases, and that these estimates are stable under changes in lighting."
            },
            "slug": "Blind-Reflectometry-Romeiro-Zickler",
            "title": {
                "fragments": [],
                "text": "Blind Reflectometry"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results with a variety of real and synthetic images suggest that useable reflectance information can be inferred in many cases and that these estimates are stable under changes in lighting."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865091"
                        ],
                        "name": "Krista A. Ehinger",
                        "slug": "Krista-A.-Ehinger",
                        "structuredName": {
                            "firstName": "Krista",
                            "lastName": "Ehinger",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krista A. Ehinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 124
                            }
                        ],
                        "text": "These databases include 80 Million Tiny Images [Torralba et al. 2008], ImageNet [Deng et al. 2009], the SUN scene database [Xiao et al. 2010], and the LabelMe dataset [Russell et al. 2008]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 30
                            }
                        ],
                        "text": "Images from the SUN database [Xiao et al. 2010] were not usable for our purposes because they are typically not of high enough resolution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1309931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "908091b4a8757c3b2f7d9cfa2c4f616ee12c5157",
            "isKey": false,
            "numCitedBy": 2352,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes."
            },
            "slug": "SUN-database:-Large-scale-scene-recognition-from-to-Xiao-Hays",
            "title": {
                "fragments": [],
                "text": "SUN database: Large-scale scene recognition from abbey to zoo"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images and uses 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2930640"
                        ],
                        "name": "P. Welinder",
                        "slug": "P.-Welinder",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Welinder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Welinder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3251767"
                        ],
                        "name": "Steve Branson",
                        "slug": "Steve-Branson",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Branson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Branson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16484321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2e250b4b49a9aa04b68dfd40dc69b022b1f8b3d",
            "isKey": false,
            "numCitedBy": 782,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different \"schools of thought\" amongst the annotators, and can group together images belonging to separate categories."
            },
            "slug": "The-Multidimensional-Wisdom-of-Crowds-Welinder-Branson",
            "title": {
                "fragments": [],
                "text": "The Multidimensional Wisdom of Crowds"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A method for estimating the underlying value of each image from (noisy) annotations provided by multiple annotators, based on a model of the image formation and annotation process, which predicts ground truth labels on both synthetic and real data more accurately than state of the art methods."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2948968"
                        ],
                        "name": "D. Brainard",
                        "slug": "D.-Brainard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Brainard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Brainard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005339984"
                        ],
                        "name": "W. A. Brunt",
                        "slug": "W.-A.-Brunt",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Brunt",
                            "middleNames": [
                                "A.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. A. Brunt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1826332"
                        ],
                        "name": "J. Speigle",
                        "slug": "J.-Speigle",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Speigle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Speigle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "While human beings are able to judge material properties in a range of lighting conditions due to color constancy, there are limits to this ability [Brainard et al. 1997], and the lighting conditions in online consumer photographs can be poor and highly variable from photo to photo."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11314381,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9795b2631105a9887a98c49c27a59b39be0f98f5",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "Most empirical work on color constancy is based on simple laboratory models of natural viewing conditions. These typically consist of spots seen against uniform backgrounds or computer simulations of flat surfaces seen under spatially uniform illumination. We report measurements made under more natural viewing conditions. The experiments were conducted in a room where the illumination was under computer control. Observers used a projection colorimeter to set asymmetric color matches across a spatial illumination gradient. Observers' matches can be described by either of two simple models. One model posits gain control in one-specific pathways. This diagonal model may be linked to ideas about the action of early visual mechanisms. The other model posits that the observer estimates and corrects for changes in illumination but does so imperfectly. This equivalent illuminant model provides a link between human performance and computational models of color constancy."
            },
            "slug": "Color-constancy-in-the-nearly-natural-image.-I.-Brainard-Brunt",
            "title": {
                "fragments": [],
                "text": "Color constancy in the nearly natural image. I. Asymmetric matches."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Measurements made under more natural viewing conditions provide a link between human performance and computational models of color constancy."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics, image science, and vision"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109485114"
                        ],
                        "name": "Xiaobai Chen",
                        "slug": "Xiaobai-Chen",
                        "structuredName": {
                            "firstName": "Xiaobai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139470"
                        ],
                        "name": "Aleksey Golovinskiy",
                        "slug": "Aleksey-Golovinskiy",
                        "structuredName": {
                            "firstName": "Aleksey",
                            "lastName": "Golovinskiy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleksey Golovinskiy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807080"
                        ],
                        "name": "T. Funkhouser",
                        "slug": "T.-Funkhouser",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Funkhouser",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Funkhouser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 147
                            }
                        ],
                        "text": "\u2026approaches to a range of problems, including understanding shape through gauge figures [Cole et al. 2009], creating a mesh segmentation database [Chen et al. 2009], devising a retargeting evaluation framework [Rubinstein et al. 2010], and for integrating humans into the loop for microtasks\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 13327116,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cf2fa36135344d1be1549a2e1502b005d44c920",
            "isKey": false,
            "numCitedBy": 523,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a benchmark for evaluation of 3D mesh segmentation salgorithms. The benchmark comprises a data set with 4,300 manually generated segmentations for 380 surface meshes of 19 different object categories, and it includes software for analyzing 11 geometric properties of segmentations and producing 4 quantitative metrics for comparison of segmentations. The paper investigates the design decisions made in building the benchmark, analyzes properties of human-generated and computer-generated segmentations, and provides quantitative comparisons of 7 recently published mesh segmentation algorithms. Our results suggest that people are remarkably consistent in the way that they segment most 3D surface meshes, that no one automatic segmentation algorithm is better than the others for all types of objects, and that algorithms based on non-local shape features seem to produce segmentations that most closely resemble ones made by humans."
            },
            "slug": "A-benchmark-for-3D-mesh-segmentation-Chen-Golovinskiy",
            "title": {
                "fragments": [],
                "text": "A benchmark for 3D mesh segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results suggest that people are remarkably consistent in the way that they segment most 3D surface meshes, that no one automatic segmentation algorithm is better than the others for all types of objects, and that algorithms based on non-local shape features seem to produce segmentations that most closely resemble ones made by humans."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 55
                            }
                        ],
                        "text": "As in previous work on obtaining images of categories [Deng et al. 2009], we must curate the images of each scene category to find the relevant ones."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 81
                            }
                        ],
                        "text": "These databases include 80 Million Tiny Images [Torralba et al. 2008], ImageNet [Deng et al. 2009], the SUN scene database [Xiao et al. 2010], and the LabelMe dataset [Russell et al. 2008]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27407,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216839"
                        ],
                        "name": "J. Tardif",
                        "slug": "J.-Tardif",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Tardif",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tardif"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da5d100f8bf07d4902c41f6490a9de60cfb24c8a",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm that quickly and accurately estimates vanishing points in images of man-made environments. Contrary to previously proposed solutions, ours is neither iterative nor relies on voting in the space of vanishing points. Our formulation is based on a recently proposed algorithm for the simultaneous estimation of multiple models called J-Linkage. Our method avoids representing edges on the Gaussian sphere and the computations and error measures are done in the image. We show that a consistency measure between a vanishing point and an edge of the image can be computed in closed-form while being geometrically meaningful. Finally, given a set of estimated vanishing points, we show how this consistency measure can be used to identify the three vanishing points corresponding to the Manhattan directions. We compare our algorithm with other approaches on the York Urban Database and show significant performance improvements."
            },
            "slug": "Non-iterative-approach-for-fast-and-accurate-point-Tardif",
            "title": {
                "fragments": [],
                "text": "Non-iterative approach for fast and accurate vanishing point detection"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "An algorithm that quickly and accurately estimates vanishing points in images of man-made environments is presented and it is shown that a consistency measure between a vanishing point and an edge of the image can be computed in closed-form while being geometrically meaningful."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402950816"
                        ],
                        "name": "D. Geisler-Moroder",
                        "slug": "D.-Geisler-Moroder",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Geisler-Moroder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geisler-Moroder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688040"
                        ],
                        "name": "A. D\u00fcr",
                        "slug": "A.-D\u00fcr",
                        "structuredName": {
                            "firstName": "Arne",
                            "lastName": "D\u00fcr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D\u00fcr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 244
                            }
                        ],
                        "text": "\u2026the intuitive explanation of the perceptual Ward parameters (contrast gloss (c), and distinctness of image (d) respectively) worked well in the MTurk setting, so we selected the perceptual Ward with a balanced optimization for grazing angles [Geisler-Moroder and Du\u0308r 2010] as our model of choice."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39073105,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "6632346e5334013110e5c2c68647dc67f08fbf88",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to its realistic appearance, computational convenience, and efficient Monte Carlo sampling, Ward's anisotropic BRDF is widely used in computer graphics for modeling specular reflection. Incorporating the criticism that the Ward and the Ward\u2010D\u00fcr model do not meet energy balance at grazing angles, we propose a modified BRDF that is energy conserving and preserves Helmholtz reciprocity. The new BRDF is computationally cheap to evaluate, admits efficient importance sampling, and thus sustains the main benefits of the Ward model. We show that the proposed BRDF is better suited for fitting measured reflectance data of a linoleum floor used in a real\u2010world building than the Ward and the Ward\u2010D\u00fcr model."
            },
            "slug": "A-New-Ward-BRDF-Model-with-Bounded-Albedo-Geisler-Moroder-D\u00fcr",
            "title": {
                "fragments": [],
                "text": "A New Ward BRDF Model with Bounded Albedo"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that the proposed BRDF is better suited for fitting measured reflectance data of a linoleum floor used in a real\u2010world building than the Ward and the Ward\u2010D\u00fcr model."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Graph. Forum"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144544291"
                        ],
                        "name": "Michael Rubinstein",
                        "slug": "Michael-Rubinstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Rubinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Rubinstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143876232"
                        ],
                        "name": "D. Gutierrez",
                        "slug": "D.-Gutierrez",
                        "structuredName": {
                            "firstName": "Diego",
                            "lastName": "Gutierrez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gutierrez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398656181"
                        ],
                        "name": "O. Sorkine-Hornung",
                        "slug": "O.-Sorkine-Hornung",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Sorkine-Hornung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Sorkine-Hornung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2947946"
                        ],
                        "name": "Ariel Shamir",
                        "slug": "Ariel-Shamir",
                        "structuredName": {
                            "firstName": "Ariel",
                            "lastName": "Shamir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ariel Shamir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 197
                            }
                        ],
                        "text": "\u2026a range of problems, including understanding shape through gauge figures [Cole et al. 2009], creating a mesh segmentation database [Chen et al. 2009], devising a retargeting evaluation framework [Rubinstein et al. 2010], and for integrating humans into the loop for microtasks [Gingold et al. 2012]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3332468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bc923def51156de0757dc6594f87fd60a85b880",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The numerous works on media retargeting call for a methodological approach for evaluating retargeting results. We present the first comprehensive perceptual study and analysis of image retargeting. First, we create a benchmark of images and conduct a large scale user study to compare a representative number of state-of-the-art retargeting methods. Second, we present analysis of the users' responses, where we find that humans in general agree on the evaluation of the results and show that some retargeting methods are consistently more favorable than others. Third, we examine whether computational image distance metrics can predict human retargeting perception. We show that current measures used in this context are not necessarily consistent with human rankings, and demonstrate that better results can be achieved using image features that were not previously considered for this task. We also reveal specific qualities in retargeted media that are more important for viewers. The importance of our work lies in promoting better measures to assess and guide retargeting algorithms in the future. The full benchmark we collected, including all images, retargeted results, and the collected user data, are available to the research community for further investigation at http://people.csail.mit.edu/mrub/retargetme."
            },
            "slug": "A-comparative-study-of-image-retargeting-Rubinstein-Gutierrez",
            "title": {
                "fragments": [],
                "text": "A comparative study of image retargeting"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work creates a benchmark of images and conducts a large scale user study to compare a representative number of state-of-the-art retargeting methods, and presents analysis of the users' responses, where it is found that humans in general agree on the evaluation of the results and show that some retargeted methods are consistently more favorable than others."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145020548"
                        ],
                        "name": "E. Reinhard",
                        "slug": "E.-Reinhard",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Reinhard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Reinhard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30227660"
                        ],
                        "name": "Michael M. Stark",
                        "slug": "Michael-M.-Stark",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stark",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael M. Stark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144033462"
                        ],
                        "name": "P. Shirley",
                        "slug": "P.-Shirley",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Shirley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shirley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256646"
                        ],
                        "name": "J. Ferwerda",
                        "slug": "J.-Ferwerda",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Ferwerda",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ferwerda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 55
                            }
                        ],
                        "text": "We assume that the log-average luminance (required by [Reinhard et al. 2002]) is approximately constant, so it can be stored offline."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2234474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "333625793b8da01b5fbe171b779f97480f5f71a9",
            "isKey": false,
            "numCitedBy": 1081,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "A classic photographic task is the mapping of the potentially high dynamic range of real world luminances to the low dynamic range of the photographic print. This tone reproduction problem is also faced by computer graphics practitioners who map digital images to a low dynamic range print or screen. The work presented in this paper leverages the time-tested techniques of photographic practice to develop a new tone reproduction operator. In particular, we use and extend the techniques developed by Ansel Adams to deal with digital images. The resulting algorithm is simple and produces good results for a wide variety of images."
            },
            "slug": "Photographic-tone-reproduction-for-digital-images-Reinhard-Stark",
            "title": {
                "fragments": [],
                "text": "Photographic tone reproduction for digital images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The work presented in this paper leverages the time-tested techniques of photographic practice to develop a new tone reproduction operator and uses and extends the techniques developed by Ansel Adams to deal with digital images."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876626"
                        ],
                        "name": "G. Ward",
                        "slug": "G.-Ward",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Ward",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ward"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3051069,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "804b9b7a7bece07b4cf885b3764bd4412325a55b",
            "isKey": false,
            "numCitedBy": 1310,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A new device for measuring the spatial reflectancedistributionsof surfaces is introduced, along with a new mathematical model of sniaorropic reflectance. The reflectance model presented is both simple and accurate, permitting efficient reflectance data reduction rasdreproduction. Tire validity of the model is substantiated with comparisons to complete meaarsremems of surface reflectance functions gathered with the novel retlectometry device. This new device uses imaging technology to capture the entire hemisphem of reflected directions simttkarreously, which greatly accelerates the reflectance data gathering process, making it pssible to measure dozens of surfaces in the time that it used to take to do one. Example measurements and simulations are shown. and a table of fitted parameters for several surfaces is presented. General Terms: algorithms, measurement, theory, verification. CR Categories and Descriptors: 1.3.7 Three-dimensionalgraphics and rw#ism, 1.6.4 Model validation and analysis. Additional"
            },
            "slug": "Measuring-and-modeling-anisotropic-reflection-Ward",
            "title": {
                "fragments": [],
                "text": "Measuring and modeling anisotropic reflection"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new device for measuring the spatial reflectancedistributions of surfaces is introduced, along with a new mathematical model of sniaorropic reflectance, which is both simple and accurate, permitting efficient reflectance data reduction in production."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697606"
                        ],
                        "name": "Yotam I. Gingold",
                        "slug": "Yotam-I.-Gingold",
                        "structuredName": {
                            "firstName": "Yotam",
                            "lastName": "Gingold",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yotam I. Gingold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2947946"
                        ],
                        "name": "Ariel Shamir",
                        "slug": "Ariel-Shamir",
                        "structuredName": {
                            "firstName": "Ariel",
                            "lastName": "Shamir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ariel Shamir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388323541"
                        ],
                        "name": "D. Cohen-Or",
                        "slug": "D.-Cohen-Or",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cohen-Or",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cohen-Or"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 279
                            }
                        ],
                        "text": "\u2026a range of problems, including understanding shape through gauge figures [Cole et al. 2009], creating a mesh segmentation database [Chen et al. 2009], devising a retargeting evaluation framework [Rubinstein et al. 2010], and for integrating humans into the loop for microtasks [Gingold et al. 2012]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10208195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3865310f2009baefceb8c2f6fd05c075b22c7e7b",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Human Computation (HC) utilizes humans to solve problems or carry out tasks that are hard for pure computational algorithms. Many graphics and vision problems have such tasks. Previous HC approaches mainly focus on generating data in batch, to gather benchmarks, or perform surveys demanding nontrivial interactions. We advocate a tighter integration of human computation into online, interactive algorithms. We aim to distill the differences between humans and computers and maximize the advantages of both in one algorithm. Our key idea is to decompose such a problem into a massive number of very simple, carefully designed, human micro-tasks that are based on perception, and whose answers can be combined algorithmically to solve the original problem. Our approach is inspired by previous work on micro-tasks and perception experiments. We present three specific examples for the design of micro perceptual human computation algorithms to extract depth layers and image normals from a single photograph, and to augment an image with high-level semantic information such as symmetry."
            },
            "slug": "Micro-perceptual-human-computation-for-visual-tasks-Gingold-Shamir",
            "title": {
                "fragments": [],
                "text": "Micro perceptual human computation for visual tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work advocates a tighter integration of human computation into online, interactive algorithms and presents three specific examples for the design of micro perceptual human computation algorithms to extract depth layers and image normals from a single photograph, and to augment an image with high-level semantic information such as symmetry."
            },
            "venue": {
                "fragments": [],
                "text": "TOGS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782393"
                        ],
                        "name": "P. Alliez",
                        "slug": "P.-Alliez",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Alliez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Alliez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699311"
                        ],
                        "name": "Andreas Fabri",
                        "slug": "Andreas-Fabri",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Fabri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Fabri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3014955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a8701feb22b5eb8a6a3386546fcaeb6d125fc46",
            "isKey": false,
            "numCitedBy": 564,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The CGAL Open Source Project provides easy access to efficient and reliable geometric algorithms in the form of a C++ library, offering geometric data structures and algorithms, which are efficient, robust, easy to use, and easy to integrate in existing software. The usage of de facto standard libraries increases productivity, as it allows software developers to focus on the application layer. This course is targeted at software developers with geometric needs, and course graduates will be able to select and use the appropriate algorithms and data structures provided by CGAL in their current or upcoming projects."
            },
            "slug": "Computational-geometry-algorithms-library-Alliez-Fabri",
            "title": {
                "fragments": [],
                "text": "Computational geometry algorithms library"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This course is targeted at software developers with geometric needs, and course graduates will be able to select and use the appropriate algorithms and data structures provided by CGAL in their current or upcoming projects."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831988"
                        ],
                        "name": "Ian Endres",
                        "slug": "Ian-Endres",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Endres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Endres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "Other work has extended systems such as LabelMe to material name annotations [Endres et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6240529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31a4e3d0dcb2d3190cf8a9bb493cdd3a624f61e4",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Several recent works have explored the benefits of providing more detailed annotations for object recognition. These annotations provide information beyond object names, and allow a detector to reason and describe individual instances in plain English. However, by demanding more specific details from annotators, new difficulties arise, such as stronger language dependencies and limited annotator attention. In this work, we present the challenges of constructing such a detailed dataset, and discuss why the benefits of using this data outweigh the difficulties of collecting it."
            },
            "slug": "The-benefits-and-challenges-of-collecting-richer-Endres-Farhadi",
            "title": {
                "fragments": [],
                "text": "The benefits and challenges of collecting richer object annotations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The challenges of constructing such a detailed dataset are presented, and why the benefits of using this data outweigh the difficulties of collecting it are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24628516"
                        ],
                        "name": "A. Rider",
                        "slug": "A.-Rider",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rider",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803283"
                        ],
                        "name": "P. McOwan",
                        "slug": "P.-McOwan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "McOwan",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. McOwan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41211372"
                        ],
                        "name": "A. Johnston",
                        "slug": "A.-Johnston",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Johnston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Johnston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44301217,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a8f02ae935989cba8a9f2b9312cc2014b9016e30",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Objects in motion appear shifted in space. For global motion stimuli we can ask whether the shift depends on the local or global motion. We constructed arrays of randomly oriented Gaussian enveloped drifting sine gratings (dynamic Gabors) whose speed was set such that the normal component of motion was consistent with a single global velocity. The array appears shifted in space in the direction of the global motion. The size of the shift is the same as for arrays of uniformly oriented dynamic Gabors that are moving in the same direction at the same global speed. Arrays made up of vertically oriented gratings whose speeds were set to the horizontal component of the random array elements were shifted less far. This shows that motion-induced position shifts of coherently moving surface patches are generated after the completion of the global motion computation."
            },
            "slug": "Motion-induced-position-shifts-in-global-dynamic-Rider-McOwan",
            "title": {
                "fragments": [],
                "text": "Motion-induced position shifts in global dynamic Gabor arrays."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work constructed arrays of randomly oriented Gaussian enveloped drifting sine gratings (dynamic Gabors) whose speed was set such that the normal component of motion was consistent with a single global velocity."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2293870"
                        ],
                        "name": "R. G. V. Gioi",
                        "slug": "R.-G.-V.-Gioi",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Gioi",
                            "middleNames": [
                                "Grompone",
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. G. V. Gioi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34913672"
                        ],
                        "name": "J. Jakubowicz",
                        "slug": "J.-Jakubowicz",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00e9mie",
                            "lastName": "Jakubowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jakubowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27053481"
                        ],
                        "name": "J. Morel",
                        "slug": "J.-Morel",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Morel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Morel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145176057"
                        ],
                        "name": "G. Randall",
                        "slug": "G.-Randall",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Randall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Randall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 53
                            }
                        ],
                        "text": "We obtain VPs by finding line segments with LSD [von Gioi et al. 2010], then clustering the segments with J-Linkage [Toldo and Fusiello 2008], and solving for the optimal VP for each cluster [Tardif 2009; Feng et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1793896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89a4618d9af12b98af656f9d620256e7cfcd4a1c",
            "isKey": false,
            "numCitedBy": 1347,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a linear-time line segment detector that gives accurate results, a controlled number of false detections, and requires no parameter tuning. This algorithm is tested and compared to state-of-the-art algorithms on a wide set of natural images."
            },
            "slug": "LSD:-A-Fast-Line-Segment-Detector-with-a-False-Gioi-Jakubowicz",
            "title": {
                "fragments": [],
                "text": "LSD: A Fast Line Segment Detector with a False Detection Control"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A linear-time line segment detector that gives accurate results, a controlled number of false detections, and requires no parameter tuning is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830169"
                        ],
                        "name": "R. Toldo",
                        "slug": "R.-Toldo",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Toldo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Toldo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779107"
                        ],
                        "name": "Andrea Fusiello",
                        "slug": "Andrea-Fusiello",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Fusiello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Fusiello"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 117
                            }
                        ],
                        "text": "We obtain VPs by finding line segments with LSD [von Gioi et al. 2010], then clustering the segments with J-Linkage [Toldo and Fusiello 2008], and solving for the optimal VP for each cluster [Tardif 2009; Feng et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7183148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0455e5596d734e3dcf60c0179efb6404e62ceabb",
            "isKey": false,
            "numCitedBy": 407,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper tackles the problem of fitting multiple instances of a model to data corrupted by noise and outliers. The proposed solution is based on random sampling and conceptual data representation. Each point is represented with the characteristic function of the set of random models that fit the point. A tailored agglomerative clustering, called J-linkage, is used to group points belonging to the same model. The method does not require prior specification of the number of models, nor it necessitate parameters tuning. Experimental results demonstrate the superior performances of the algorithm."
            },
            "slug": "Robust-Multiple-Structures-Estimation-with-Toldo-Fusiello",
            "title": {
                "fragments": [],
                "text": "Robust Multiple Structures Estimation with J-Linkage"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed solution is based on random sampling and conceptual data representation, and a tailored agglomerative clustering, called J-linkage, is used to group points belonging to the same model."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372948"
                        ],
                        "name": "M. Marge",
                        "slug": "M.-Marge",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Marge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110878863"
                        ],
                        "name": "S. Banerjee",
                        "slug": "S.-Banerjee",
                        "structuredName": {
                            "firstName": "Satanjeev",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783635"
                        ],
                        "name": "Alexander I. Rudnicky",
                        "slug": "Alexander-I.-Rudnicky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rudnicky",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander I. Rudnicky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12602706,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "70b87f86a2d1de02721171f3b0b1a5ceb2763feb",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate whether Amazon's Mechanical Turk (MTurk) service can be used as a reliable method for transcription of spoken language data. Utterances with varying speaker demographics (native and non-native English, male and female) were posted on the MTurk marketplace together with standard transcription guidelines. Transcriptions were compared against transcriptions carefully prepared in-house through conventional (manual) means. We found that transcriptions from MTurk workers were generally quite accurate. Further, when transcripts for the same utterance produced by multiple workers were combined using the ROVER voting scheme, the accuracy of the combined transcript rivaled that observed for conventional transcription methods. We also found that accuracy is not particularly sensitive to payment amount, implying that high quality results can be obtained at a fraction of the cost and turnaround time of conventional methods."
            },
            "slug": "Using-the-Amazon-Mechanical-Turk-for-transcription-Marge-Banerjee",
            "title": {
                "fragments": [],
                "text": "Using the Amazon Mechanical Turk for transcription of spoken language"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It was found that transcriptions from MTurk workers were generally quite accurate, and when transcripts for the same utterance produced by multiple workers were combined using the ROVER voting scheme, the accuracy of the combined transcript rivaled that observed for conventional transcription methods."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240195"
                        ],
                        "name": "Lavanya Sharan",
                        "slug": "Lavanya-Sharan",
                        "structuredName": {
                            "firstName": "Lavanya",
                            "lastName": "Sharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lavanya Sharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680975"
                        ],
                        "name": "R. Rosenholtz",
                        "slug": "R.-Rosenholtz",
                        "structuredName": {
                            "firstName": "Ruth",
                            "lastName": "Rosenholtz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenholtz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 72
                            }
                        ],
                        "text": "Since material recognition is still possible in distorted color spaces [Sharan et al. 2009], we only use white balancing to filter inputs for appearance matching (Stage 8)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 144931877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cfec137b3a242fc5aa7e1b36ee913ea06a5445c",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Material-perception:-What-can-you-see-in-a-brief-Sharan-Rosenholtz",
            "title": {
                "fragments": [],
                "text": "Material perception: What can you see in a brief glance?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055460158"
                        ],
                        "name": "John Hart",
                        "slug": "John-Hart",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64813808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cae8a08979c34110a85428f06c973b38828eac6",
            "isKey": false,
            "numCitedBy": 1472,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ACM-Transactions-on-Graphics-Hart",
            "title": {
                "fragments": [],
                "text": "ACM Transactions on Graphics"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2004"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Micro perceptual human computation"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Transactions on Graphics"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OpenSurfaces: A Richly Annotated Catalog of Surface Appearance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 85
                            }
                        ],
                        "text": "While prior work typically uses a gauge figure to represent a surface normal figure [Koenderink et al. 1992; Cole et al. 2009], we found a 3D perspective grid to be much more effective."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Surface perception in pictures. Perception & Psychophysics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 279
                            }
                        ],
                        "text": "\u2026a range of problems, including understanding shape through gauge figures [Cole et al. 2009], creating a mesh segmentation database [Chen et al. 2009], devising a retargeting evaluation framework [Rubinstein et al. 2010], and for integrating humans into the loop for microtasks [Gingold et al. 2012]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Micro perceptual human computation"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Transactions on Graphics 31,"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Surfaces: Visual Research for Artists and Designers"
            },
            "venue": {
                "fragments": [],
                "text": "Surfaces: Visual Research for Artists and Designers"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 104
                            }
                        ],
                        "text": "We selected the potential names from the results of our pilot study, and taxonomies in interior design [Juracek 1996]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Surfaces: Visual Research for Artists and Designers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 25
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 53,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/OpenSurfaces:-a-richly-annotated-catalog-of-surface-Bell-Upchurch/43e48b702fbe1feba53afbf82ec322cc9a61ae6c?sort=total-citations"
}