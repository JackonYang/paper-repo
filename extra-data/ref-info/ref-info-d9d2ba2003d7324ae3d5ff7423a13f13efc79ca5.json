{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "This ultimately would require both a revamping of acoustical feature extraction and a fresh look at the incorporation of these features into statistical models representing speech."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10128153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2b439b063874df0a074449c7c8616ac0880c9c5",
            "isKey": false,
            "numCitedBy": 2112,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Performance of even the best current stochastic recognizers severely degrades in an unexpected communications environment. In some cases, the environmental effect can be modeled by a set of simple transformations and, in particular, by convolution with an environmental impulse response and the addition of some environmental noise. Often, the temporal properties of these environmental effects are quite different from the temporal properties of speech. We have been experimenting with filtering approaches that attempt to exploit these differences to produce robust representations for speech recognition and enhancement and have called this class of representations relative spectra (RASTA). In this paper, we review the theoretical and experimental foundations of the method, discuss the relationship with human auditory perception, and extend the original method to combinations of additive noise and convolutional noise. We discuss the relationship between RASTA features and the nature of the recognition models that are required and the relationship of these features to delta features and to cepstral mean subtraction. Finally, we show an application of the RASTA technique to speech enhancement. >"
            },
            "slug": "RASTA-processing-of-speech-Hermansky-Morgan",
            "title": {
                "fragments": [],
                "text": "RASTA processing of speech"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The theoretical and experimental foundations of the RASTA method are reviewed, the relationship with human auditory perception is discussed, the original method is extended to combinations of additive noise and convolutional noise, and an application is shown to speech enhancement."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "FINAL WORDS In [7] we whimsically referred to the increase in error rates as a goal in speech recognition research."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17828677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "891610da0e97caa3f5f2ce96156b5b7c2a4dc961",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-increasing-speech-recognition-error-rates-Bourlard-Hermansky",
            "title": {
                "fragments": [],
                "text": "Towards increasing speech recognition error rates"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34644387"
                        ],
                        "name": "M. Hunt",
                        "slug": "M.-Hunt",
                        "structuredName": {
                            "firstName": "Melvyn",
                            "lastName": "Hunt",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hunt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144092947"
                        ],
                        "name": "C. Lefebvre",
                        "slug": "C.-Lefebvre",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Lefebvre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lefebvre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026PROCESSING MAGAZINE [81] SEPTEMBER 2005\n\u00a9 ARTVILLE & COMSTOCK\n[Nelson Morgan, Qifeng Zhu, Andreas Stolcke, Kemal S\u00f6nmez, Sunil Sivadas, Takahiro Shinozaki,\nMari Ostendorf, Pratibha Jain, Hynek Hermansky,\nDan Ellis, George Doddington, Barry Chen, \u00d6zg\u00fcr \u00c7etin, Herv\u00e9 Bourlard, and Marios Athineos]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61294000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c058c0609c3f254465281ce3e3a6ea945ae6bdd9",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Several acoustic representations have been compared in speaker-dependent and independent connected and isolated-word recognition tests with undegraded speech and with speech degraded by adding white noise and by applying a 6-dB/octave spectral tilt. The representations comprised the output of an auditory model, cepstrum coefficients derived from an FFT-based mel-scale filter bank with various weighting schemes applied to the coefficients, cepstrum coefficients augmented with measures of their rates of change with time, and sets of linear discriminant functions derived from the filter-bank output and called IMELDA. The model outperformed the cepstrum representations except in noise-free connected-word tests, where it had a high insertion rate. The best cepstrum weighting scheme was derived from within-class variances. Its behavior may explain the empirical adjustments found necessary with other schemes. IMELDA outperformed all other representations in all conditions and is computationally simple.<<ETX>>"
            },
            "slug": "A-comparison-of-several-acoustic-representations-Hunt-Lefebvre",
            "title": {
                "fragments": [],
                "text": "A comparison of several acoustic representations for speech recognition with degraded and undegraded speech"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Several acoustic representations have been compared in speaker-dependent and independent connected and isolated-word recognition tests with undegraded speech and with speech degraded by adding white noise and by applying a 6-dB/octave spectral tilt."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705797"
                        ],
                        "name": "M. Athineos",
                        "slug": "M.-Athineos",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Athineos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Athineos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745455"
                        ],
                        "name": "D. Ellis",
                        "slug": "D.-Ellis",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ellis",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ellis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 251
                            }
                        ],
                        "text": "We have designed features based on linear prediction coefficients calculated on the spectrum, which form a mathematical dual to the more familiar time-domain linear predictive coding (LPC) models; we call this frequencydomain linear prediction (FDLP) [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 245
                            }
                        ],
                        "text": "We have designed features based on linear prediction coefficients calculated on the spectrum, which form a mathematical dual to the more familiar time-domain linear predictive coding (LPC) models; we call this frequency-\ndomain linear prediction (FDLP) [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 213
                            }
                        ],
                        "text": "The energy-based representations of temporal trajectories (or, more generally, of spectrotemporal patterns) could be replaced by autoregressive models for these components of the time-frequency plane, such as the FDLP or LP-TRAP approaches described earlier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "In subsequent work, we incorporated the FDLP model into the TRAP framework, yielding the LP-TRAP [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "First, the Hilbert envelopes in criticalband subbands are fit (so-called subband FDLP), and then all-pole models are fit to smooth the energy across subbands."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "As noted above for the newer FDLP and LP-TRAP approaches, we continue to do early experiments with small tasks to permit many experiments, but we take the best (and most developed) of these methods and validate their generality on large tasks."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1983108,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "34e9153565a2f8b2e6bd6bf606388ab1d18271e3",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Current speech recognition systems uniformly employ short-time spectral analysis, usually over windows of 10-30 ms, as the basis for their acoustic representations. Any detail below this timescale is lost, and even temporal structures above this level are usually only weakly represented in the form of deltas etc. We address this limitation by proposing a novel representation of the temporal envelope in different frequency bands by exploring the dual of conventional linear prediction (LPC) when applied in the transform domain. With this technique of frequency-domain linear prediction (FDLP), the 'poles' of the model describe temporal, rather than spectral, peaks. By using analysis windows on the order of hundreds of milliseconds, the procedure automatically decides how to distribute poles to model the temporal structure best within the window. While this approach offers many possibilities for novel speech features, we experiment with one particular form, an index describing the 'sharpness' of individual poles within a window, and show a relatively large word error rate improvement from 4.97% to 3.81% in a recognizer trained on general conversational telephone speech and tested on a small-vocabulary spontaneous numbers task. We analyze this improvement in terms of the confusion matrices and suggest how the newly-modeled fine temporal structure may be helping."
            },
            "slug": "Frequency-domain-linear-prediction-for-temporal-Athineos-Ellis",
            "title": {
                "fragments": [],
                "text": "Frequency-domain linear prediction for temporal features"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes a novel representation of the temporal envelope in different frequency bands by exploring the dual of conventional linear prediction (LPC) when applied in the transform domain and shows a relatively large word error rate improvement in a recognizer trained on general conversational telephone speech and tested on a small-vocabulary spontaneous numbers task."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143624561"
                        ],
                        "name": "\u00d6. \u00c7etin",
                        "slug": "\u00d6.-\u00c7etin",
                        "structuredName": {
                            "firstName": "\u00d6zg\u00fcr",
                            "lastName": "\u00c7etin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d6. \u00c7etin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The best performance, however, was obtained by a variable-rate extension in which the number of observations at one scale corresponding to an observation at the next coarsest scale can vary [ 8 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2670399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "289e8512c1463c67aad3ff659613d6f8617e5691",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a multi-rate extension of hidden Markov models (HMMs), for joint acoustic modeling of speech at multiple time scales. The approach complements the usual short-term, phone-based representation of speech with wide modeling units and long-term temporal features. We consider two alternatives for coarse scale, representing either phones, or syllable structure and lexical stress, and both fixed- and variable-rate dependencies between time scales. Experiments on conversational telephone speech (CTS) show that the proposed multi-rate approach significantly improves recognition accuracy over HMM- and other coupled HMM-based approaches (e.g. feature concatenation) for combining short- and long-term acoustic and linguistic information."
            },
            "slug": "Multi-rate-and-variable-rate-modeling-of-speech-at-\u00c7etin-Ostendorf",
            "title": {
                "fragments": [],
                "text": "Multi-rate and variable-rate modeling of speech at phone and syllable time scales [speech recognition applications]"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments on conversational telephone speech (CTS) show that the proposed multi-rate approach significantly improves recognition accuracy over HMM- and other coupled H MM-based approaches (e.g. feature concatenation) for combining short- and long-term acoustic and linguistic information."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145468185"
                        ],
                        "name": "M. S\u00f6nmez",
                        "slug": "M.-S\u00f6nmez",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "S\u00f6nmez",
                            "middleNames": [
                                "Kemal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. S\u00f6nmez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724747"
                        ],
                        "name": "M. Plauch\u00e9",
                        "slug": "M.-Plauch\u00e9",
                        "structuredName": {
                            "firstName": "Madelaine",
                            "lastName": "Plauch\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Plauch\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70422141"
                        ],
                        "name": "Elizabeth Shriberg",
                        "slug": "Elizabeth-Shriberg",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Shriberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Shriberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145660147"
                        ],
                        "name": "H. Franco",
                        "slug": "H.-Franco",
                        "structuredName": {
                            "firstName": "Horacio",
                            "lastName": "Franco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Franco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7179795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b4a2cfc8010c2b92c2d7aa74f4eda3958ee4fdd",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The constant frame length in typical ASR front ends is too long to capture transient phenomena in speech, such as stop bursts. However, current HMM systems have consistently outperformed systems based solely on non-uniform units. This work investigates an approach to \u201cadd back\u201d such transient information to a speech recognizer, without losing the robustness of the standard a coustic models. We demonstrate a set of phonetically-motivated acoustic features that discriminate a preliminary test set of highly ambiguous voiceless stops in CV contexts. The features are automatically computed from data that had been hand-marked for consonant burst location and voicing onset (extension to automatic marking is also proposed). Two corpora are processed using a parallel set of features: conversational speech over the telephone (Switchboard), and a corpus of carefully elicited speech. The latter provides a n upper bound on discrimination, and allows for comparison of feature usage across speaking style. We explore data-driven approaches to obtaining variable-length time-localized features compatible with an HMM statistical framework. We also suggest techniques for extension to automatic annotation of burst location, for computation of features at such points, and for augmentation of an HMM system with the added information."
            },
            "slug": "Consonant-discrimination-in-elicited-and-speech:-a-S\u00f6nmez-Plauch\u00e9",
            "title": {
                "fragments": [],
                "text": "Consonant discrimination in elicited and spontaneous speech: a case for signal-adaptive front ends in ASR"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A set of phonetically-motivated acoustic features are demonstrated that discriminate a preliminary test set of highly ambiguous voiceless stops in CV contexts and are suggested for extension to automatic annotation of burst location, for computation of features at such points, and for augmentation of an HMM system with the added information."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108322421"
                        ],
                        "name": "Steven C. Lee",
                        "slug": "Steven-C.-Lee",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Lee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026PROCESSING MAGAZINE [81] SEPTEMBER 2005\n\u00a9 ARTVILLE & COMSTOCK\n[Nelson Morgan, Qifeng Zhu, Andreas Stolcke, Kemal S\u00f6nmez, Sunil Sivadas, Takahiro Shinozaki,\nMari Ostendorf, Pratibha Jain, Hynek Hermansky,\nDan Ellis, George Doddington, Barry Chen, \u00d6zg\u00fcr \u00c7etin, Herv\u00e9 Bourlard, and Marios Athineos]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9302699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2835b39b724ade614756ab394b4fa43f66935c55",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we investigate modifications to a probabilistic segmentation algorithm to achieve a real-time, and pipelined capability for our segment-based speech recognizer [4]. The existing algorithm used a Viterbi and backwards A search to hypothesize phonetic segments [2]. We were able to reduce the computational requirements of this algorithm by reducing the effective search space to acoustic landmarks, and were able to achieve pipelined capability by executing theA search in blocks defined by reliably detected phonetic boundaries. The new algorithm produces 30% fewer segments, and improves TIMIT phonetic recognition performance by 2.4% over an acoustic segmentation baseline. We were also able to produce 30% fewer segments on a word recognition task in a weather information domain [11]."
            },
            "slug": "Real-time-probabilistic-segmentation-for-speech-Lee-Glass",
            "title": {
                "fragments": [],
                "text": "Real-time probabilistic segmentation for segment-based speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Modifications to a probabilistic segmentation algorithm are investigated to achieve a real-time, and pipelined capability for the authors' segment-based speech recognizer and produce 30% fewer segments on a word recognition task in a weather information domain."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38940652"
                        ],
                        "name": "H. Soltau",
                        "slug": "H.-Soltau",
                        "structuredName": {
                            "firstName": "Hagen",
                            "lastName": "Soltau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Soltau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, we note the related and impressive work done recently at IBM on incorporating low-dimensional projections of high-dimensional, Gaussian-derived posteriors, also incorporated in the feature vector used for recognition [ 25 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 75541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a70ac6680d319905d6bfca4cea0b4dc6c15f420",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "MPE (minimum phone error) is a previously introduced technique for discriminative training of HMM parameters. fMPE applies the same objective function to the features, transforming the data with a kernel-like method and training millions of parameters, comparable to the size of the acoustic model. Despite the large number of parameters, fMPE is robust to over-training. The method is to train a matrix projecting from posteriors of Gaussians to a normal size feature space, and then to add the projected features to normal features such as PLP. The matrix is trained from a zero start using a linear method. Sparsity of posteriors ensures speed in both training and test time. The technique gives similar improvements to MPE (around 10% relative). MPE on top of fMPE results in error rates up to 6.5% relative better than MPE alone, or more if multiple layers of transform are trained."
            },
            "slug": "fMPE:-discriminatively-trained-features-for-speech-Povey-Kingsbury",
            "title": {
                "fragments": [],
                "text": "fMPE: discriminatively trained features for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108424673"
                        ],
                        "name": "Barry Y. Chen",
                        "slug": "Barry-Y.-Chen",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Chen",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barry Y. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9024683"
                        ],
                        "name": "Q. Zhu",
                        "slug": "Q.-Zhu",
                        "structuredName": {
                            "firstName": "Qifeng",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "As reported in [10], a variant of this approach called hidden activation TRAPs (HATs) has led to relative reductions in error of around 8% when these features are combined with posterior estimates from perceptual linear prediction (PLP) and its first two derivatives and incorporated in the SRI decipher system (reducing the word error rate (WER) from 25."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15123425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22b496b7c63e89b51ca2f905195339e03c9deb46",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Incorporating long-term (500-1000 ms) temporal information using multi-layered perceptrons (MLPs) has improved performance on ASR tasks, especially when used to complement traditional short-term (25-100 ms) features. This paper further studies techniques for incorporating long-term temporal information in the acoustic model by presenting experiments showing: 1) that simply widening acoustic context by using more frames of full band speech energies as input to the MLP is suboptimal compared to a more constrained two-stage approach that first focuses on long-term temporal patterns in each critical band separately and then combines them, 2) that the best two-stage approach studied utilizes hidden activation values of MLPs trained on the log critical band energies (LCBEs) of 51 consecutive frames, and 3) that combining the best two-stage approach with conventional short-term features significantly reduces word error rates on the 2001 NIST Hub-5 conversational telephone speech (CTS) evaluation set with models trained using the Switchboard Corpus."
            },
            "slug": "Learning-long-term-temporal-features-in-LVCSR-using-Chen-Zhu",
            "title": {
                "fragments": [],
                "text": "Learning long-term temporal features in LVCSR using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experiments show that combining the best two-stage approach with conventional short-term features significantly reduces word error rates on the 2001 NIST Hub-5 conversational telephone speech (CTS) evaluation set with models trained using the Switchboard Corpus."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38093680"
                        ],
                        "name": "Jont B. Allen",
                        "slug": "Jont-B.-Allen",
                        "structuredName": {
                            "firstName": "Jont",
                            "lastName": "Allen",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jont B. Allen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026PROCESSING MAGAZINE [81] SEPTEMBER 2005\n\u00a9 ARTVILLE & COMSTOCK\n[Nelson Morgan, Qifeng Zhu, Andreas Stolcke, Kemal S\u00f6nmez, Sunil Sivadas, Takahiro Shinozaki,\nMari Ostendorf, Pratibha Jain, Hynek Hermansky,\nDan Ellis, George Doddington, Barry Chen, \u00d6zg\u00fcr \u00c7etin, Herv\u00e9 Bourlard, and Marios Athineos]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36444993,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "b96b4501f85678364e18b09bb0ffbb736ae6ee8a",
            "isKey": false,
            "numCitedBy": 379,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Until the performance of automatic speech recognition (ASR) hardware surpasses human performance in accuracy and robustness, we stand to gain by understanding the basic principles behind human speech recognition (HSR). This problem was studied exhaustively at Bell Labs between the years of 1918 and 1950 by Harvey Fletcher and his colleagues. The motivation for these studies was to quantify the quality of speech sounds in the telephone plant to both improve speech intelligibility and preference. To do this he and his group studied the effects of filtering and noise on speech recognition accuracy for nonsense consonant-vowel-consonant (CVC) syllables, words, and sentences. Fletcher used the term \"articulation\" as the probability of correct recognition for nonsense sounds, and \"intelligibility\" as the probability of correction recognition for words (sounds having meaning). In 1919, Fletcher found a way to transform articulation data for filtered speech into an additive density function D(f) and found a formula that accurately predicts the average articulation. The area under D(S) is called the \"articulation index.\" Fletcher then went on to find relationships between the recognition errors for the nonsense speech sounds, words, and sentences. This work has recently been reviewed and partially replicated by Boothroyd and by Bronkhorst, et al. (1980). Taken as a whole, these studies tell us a great deal about how humans process and recognize speech sounds. >"
            },
            "slug": "How-do-humans-process-and-recognize-speech-Allen",
            "title": {
                "fragments": [],
                "text": "How do humans process and recognize speech?"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Until the performance of automatic speech recognition (ASR) hardware surpasses human performance in accuracy and robustness, the authors stand to gain by understanding the basic principles behind human Speech recognition (HSR)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699205"
                        ],
                        "name": "S. Furui",
                        "slug": "S.-Furui",
                        "structuredName": {
                            "firstName": "Sadaoki",
                            "lastName": "Furui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Furui"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While many systems also incorporate time derivatives [ 16 ] and/or projections from five or more frames to a lower dimension [21], [17], the fundamental character of the acoustic features has remained quite similar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40519557,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f64038de5e388bce2f0575cfb4a291a41e3bab57",
            "isKey": false,
            "numCitedBy": 899,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new isolated word recognition technique based on a combination of instantaneous and dynamic features of the speech spectrum. This technique is shown to be highly effective in speaker-independent speech recognition. Spoken utterances are represented by time sequences of cepstrum coefficients and energy. Regression coefficients for these time functions are extracted for every frame over an approximately 50 ms period. Time functions of regression coefficients extracted for cepstrum and energy are combined with time functions of the original cepstrum coefficients, and used with a staggered array DP matching algorithm to compare multiple templates and input speech. Speaker-independent isolated word recognition experiments using a vocabulary of 100 Japanese city names indicate that a recognition error rate of 2.4 percent can be obtained with this method. Using only the original cepstrum coefficients the error rate is 6.2 percent."
            },
            "slug": "Speaker-independent-isolated-word-recognition-using-Furui",
            "title": {
                "fragments": [],
                "text": "Speaker-independent isolated word recognition using dynamic features of speech spectrum"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper proposes a new isolated word recognition technique based on a combination of instantaneous and dynamic features of the speech spectrum that is shown to be highly effective in speaker-independent speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24546467"
                        ],
                        "name": "H. Dudley",
                        "slug": "H.-Dudley",
                        "structuredName": {
                            "firstName": "Homer",
                            "lastName": "Dudley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Dudley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122064644,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f1233ff160d9a66bb83a12ef035b549d52aa59e2",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A program of active research on the vocoder was initiated in July 1934. The first public demonstration of the vocoder was at the Harvard Tercentenary in September 1936. Since then, there have been studies of the underlying nature of speech and the recognition of speech\u2010sound elements in transforming from the spoken to the written word or setting up other automatic voice operation. There has also been much research directed toward straightforward bandwidth compression and such compression as an aid to the attainment of other goals. The entire field of research in speech analysis and/or synthesis is reviewed, including a discussion of limitations and requirements. A demonstration tape is played from an improved 2400\u2010bit/sec digitized vocoder."
            },
            "slug": "Thirty-Years-of-Vocoder-Research-Dudley",
            "title": {
                "fragments": [],
                "text": "Thirty Years of Vocoder Research"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390103475"
                        ],
                        "name": "Reinhold H\u00e4b-Umbach",
                        "slug": "Reinhold-H\u00e4b-Umbach",
                        "structuredName": {
                            "firstName": "Reinhold",
                            "lastName": "H\u00e4b-Umbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhold H\u00e4b-Umbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50232515"
                        ],
                        "name": "Dieter Geller",
                        "slug": "Dieter-Geller",
                        "structuredName": {
                            "firstName": "Dieter",
                            "lastName": "Geller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dieter Geller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026PROCESSING MAGAZINE [81] SEPTEMBER 2005\n\u00a9 ARTVILLE & COMSTOCK\n[Nelson Morgan, Qifeng Zhu, Andreas Stolcke, Kemal S\u00f6nmez, Sunil Sivadas, Takahiro Shinozaki,\nMari Ostendorf, Pratibha Jain, Hynek Hermansky,\nDan Ellis, George Doddington, Barry Chen, \u00d6zg\u00fcr \u00c7etin, Herv\u00e9 Bourlard, and Marios Athineos]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57374514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d223675258f7eaa37b5db1288878250cf559bcbe",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Four methods were used to reduce the error rate of a continuous-density hidden Markov-model-based speech recognizer on the TI/NIST connected-digits recognition task. Energy thresholding sets a lower limit on the energy in each frequency channel to suppress spurious distortion accumulation caused by random noise. This led to an improvement in error rate by 15%. Spectrum normalization was used to compensate for across-speaker variations, resulting in an additional improvement by 20%. The acoustic resolution was increased up to 32 component densities per mixture. Each doubling of the number of component densities yielded a reduction in error rate by roughly 20%. Linear discriminant analysis was used for improved feature selection. A single class-independent transformation matrix was applied to a large input vector consisting of several adjacent frames, resulting in an improvement by 20% for high acoustic resolution. The final string error rate was 0.84%.<<ETX>>"
            },
            "slug": "Improvements-in-connected-digit-recognition-using-H\u00e4b-Umbach-Geller",
            "title": {
                "fragments": [],
                "text": "Improvements in connected digit recognition using linear discriminant analysis and mixture densities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Four methods were used to reduce the error rate of a continuous-density hidden Markov-model-based speech recognizer on the TI/NIST connected-digits recognition task."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145502114"
                        ],
                        "name": "R. Reddy",
                        "slug": "R.-Reddy",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Reddy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Reddy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "The core acoustic operation has essentially remained the same for decades: a single feature vector (derived from the power spectral envelope over a 20\u201330 ms window, stepped forward by \u223c10 ms per frame) is compared to a set of distributions derived from training data for an inventory of subword\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39233108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1a93cd7f9302e93462c7d694a84527ae23bab7f",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual and speech perception tasks, which can be performed with no apparent effort by people, have proved to be difficult for machines. This may be in part due to the absence of cognitive models of perception of the type proposed above by Jakobson. In this paper we attempt to give a unified view of the research in machine perception of speech and vision in the hope that a clear appreciation of similarities and differences may lead to better information processing models of perception. Being active in research in both computer vision and speech, we have found it useful to look at the problems that have arisen in one domain and anticipate corresponding problems in the other (Reddy, 1969). Thus, this paper represents a comparitive study of the issues, systems and unsolved problems that are, at present, of interest to visual and speech recognition research."
            },
            "slug": "Eyes-and-Ears-for-Computers-Reddy",
            "title": {
                "fragments": [],
                "text": "Eyes and Ears for Computers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper represents a comparitive study of the issues, systems and unsolved problems that are, at present, of interest to visual and speech recognition research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121786"
                        ],
                        "name": "V. Digalakis",
                        "slug": "V.-Digalakis",
                        "structuredName": {
                            "firstName": "Vassilios",
                            "lastName": "Digalakis",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Digalakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3353221"
                        ],
                        "name": "O. Kimball",
                        "slug": "O.-Kimball",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Kimball",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kimball"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One approach that has been proposed is to add feature dependencies or explicitly model the dynamics of frame-based features in various extensions of HMMs [5], [ 24 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15742861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8658d645b716d59c5edc80e33e1936e33574bc26",
            "isKey": false,
            "numCitedBy": 702,
            "numCiting": 147,
            "paperAbstract": {
                "fragments": [],
                "text": "Many alternative models have been proposed to address some of the shortcomings of the hidden Markov model (HMM), which is currently the most popular approach to speech recognition. In particular, a variety of models that could be broadly classified as segment models have been described for representing a variable-length sequence of observation vectors in speech recognition applications. Since there are many aspects in common between these approaches, including the general recognition and training problems, it is useful to consider them in a unified framework. The paper describes a general stochastic model that encompasses most of the models proposed in the literature, pointing out similarities of the models in terms of correlation and parameter tying assumptions, and drawing analogies between segment models and HMMs. In addition, we summarize experimental results assessing different modeling assumptions and point out remaining open questions."
            },
            "slug": "From-HMM's-to-segment-models:-a-unified-view-of-for-Ostendorf-Digalakis",
            "title": {
                "fragments": [],
                "text": "From HMM's to segment models: a unified view of stochastic modeling for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A general stochastic model is described that encompasses most of the models proposed in the literature for speech recognition, pointing out similarities in terms of correlation and parameter tying assumptions, and drawing analogies between segment models and HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748118"
                        ],
                        "name": "J. Bilmes",
                        "slug": "J.-Bilmes",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Bilmes",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bilmes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is supported by information theoretic analysis, which shows discriminative dependence conditional on underlying phones between features separated in time by up to several hundred milliseconds [ 6 ], [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12303329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "186133e5027fc8a32c003a8ef732d25472683836",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In maximum-likelihood based speech recognition systems, it is important to accurately estimate the joint distribution of feature vectors given a particular acoustic model. In previous work, we showed we can boost the accuracy in this task by modeling the joint distribution of time-localized feature vectors along with the statistics relating those feature vectors to their surrounding context. In this work, we evaluate information preserving reduction strategies for those statistics. We claim that those statistics corresponding to spectro-temporal loci in speech with relatively large mutual information are most useful in estimating the information contained in the feature-vector joint distribution. Furthermore, we claim that such statistics are most likely to generalize. Using an EM algorithm to compute the mutual information between pairs of points in the time-frequency grid, we verify these hypotheses using both overlap plots and speech recognition word error results."
            },
            "slug": "Maximum-mutual-information-based-reduction-for-Bilmes",
            "title": {
                "fragments": [],
                "text": "Maximum mutual information based reduction strategies for cross-correlation based joint distributional modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is claimed that statistics corresponding to spectro-temporal loci in speech with relatively large mutual information are most useful in estimating the information contained in the feature-vector joint distribution and are most likely to generalize."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145622870"
                        ],
                        "name": "Don X. Sun",
                        "slug": "Don-X.-Sun",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Sun",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don X. Sun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026PROCESSING MAGAZINE [81] SEPTEMBER 2005\n\u00a9 ARTVILLE & COMSTOCK\n[Nelson Morgan, Qifeng Zhu, Andreas Stolcke, Kemal S\u00f6nmez, Sunil Sivadas, Takahiro Shinozaki,\nMari Ostendorf, Pratibha Jain, Hynek Hermansky,\nDan Ellis, George Doddington, Barry Chen, \u00d6zg\u00fcr \u00c7etin, Herv\u00e9 Bourlard, and Marios Athineos]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43641529,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "2b4c543a6d532e67e0e534b4258aa88258d85b77",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Our efforts in developing a feature-based general statistical framework intended for unlimited-vocabulary speech recognition are reported. The design of the feature-based atomic units of speech is aimed at parsimonious scheme to share the inter-word and inter-phone speech data. Our basic design philosophy has been motivated by the theory of distinctive features and by a new form of phonology which argues for the use of multidimensional articulatory structures. The work reported is a significant extension of our earlier studies (see Deng and Erler (1992) and Deng, Lenning and Mermelstein (1990)) in three aspects. First, a comprehensive set of features is developed, enabling the recognisor to operate on all classes of English sound. Second, a more efficient strategy is derived for feature-based lexical representation. Third, more extensive evaluation results, including both the phonetic classification and phonetic recognition results, are reported.<<ETX>>"
            },
            "slug": "Phonetic-classification-and-recognition-using-HMM-Deng-Sun",
            "title": {
                "fragments": [],
                "text": "Phonetic classification and recognition using HMM representation of overlapping articulatory features for all classes of English sounds"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A comprehensive set of features is developed, enabling the recognisor to operate on all classes of English sound, and a more efficient strategy is derived for feature-based lexical representation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1394316685"
                        ],
                        "name": "Phil Clendeninn",
                        "slug": "Phil-Clendeninn",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Clendeninn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phil Clendeninn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "The core acoustic operation has essentially remained the same for decades: a single feature vector (derived from the power spectral envelope over a 20\u201330 ms window, stepped forward by \u223c10 ms per frame) is compared to a set of distributions derived from training data for an inventory of subword\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4080107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d4fc99e373f05baa040d0fc59e740303a9f5ddb",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "AT the World's Fairs in New York and San Francisco great interest was shown in the speech synthesizer shown in the Bell System exhibits. In the December number of the Bell Laboratories Record, H. Dudley, of their research department, describes this device, known as the 'Vocoder'. The 'voder' is an offshoot of a more extensive system first demonstrated in its experimental stage several years ago. It first analysed spoken sounds and then used the information to control the synthesizing circuit. As World's Fair displays were then under consideration it was seen that the synthesizer, manually controlled, could be made into a dramatic demonstration. Development was at first concentrated in this field, but when a successful Voder became assured, attention was shifted back to the broader and parent system and it was called the 'vocoder', since it operates on the principle of deriving voice codes to re-create the speech which it analyses. The analyser is at the left and the synthesizer at the right of the vocoder."
            },
            "slug": "The-Vocoder-Clendeninn",
            "title": {
                "fragments": [],
                "text": "The Vocoder"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The 'vocoder' is an offshoot of a more extensive system first demonstrated in its experimental stage several years ago, which first analysed spoken sounds and then used the information to control the synthesizing circuit."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1940
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380069"
                        ],
                        "name": "B. Atal",
                        "slug": "B.-Atal",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Atal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Atal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 270
                            }
                        ],
                        "text": "\u2026envelope away from its role as the sole source of acoustic information incorporated by the statistical models of modern speech\nIEEE SIGNAL PROCESSING MAGAZINE [82] SEPTEMBER 2005\nrecognition systems (SRSs), particularly in the context of the conversational telephone speech recognition task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41317064,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "14f4fc0bc3fae237641f10b9881e5706eda0cf7f",
            "isKey": false,
            "numCitedBy": 1030,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Several different parametric representations of speech derived from the linear prediction model are examined for their effectiveness for automatic recognition of speakers from their voices. Twelve predictor coefficients were determined approximately once every 50 msec from speech sampled at 10 kHz. The predictor coefficients and other speech parameters derived from them, such as the impulse response function, the autocorrelation function, the area function, and the cepstrum function were used as input to an automatic speaker\u2010recognition system. The speech data consisted of 60 utterances, consisting of six repetitions of the same sentence spoken by 10 speakers. The identification decision was based on the distance of the test sample vector from the reference vector for different speakers in the population; the speaker corresponding to the reference vector with the smallest distance was judged to be the unknown speaker. In verification, the speaker was verified if the distance between the test sample vector and the reference vector for the claimed speaker was less than a fixed threshold. Among all the parameters investigated, the cepstrum was found to be the most effective, providing an identification accuracy of 70% for speech 50 msec in duration, which increased to more than 98% for a duration of 0.5 sec. Using the same speech data, the verification accuracy was found to be approximately 83% for a duration of 50 msec, increasing to 98% for a duration of 1 sec. In a separate study to determine the feasibility of text\u2010independent speaker identification, an identification accuracy of 93% was achieved for speech 2 sec in duration even though the texts of the test and reference samples were different."
            },
            "slug": "Effectiveness-of-linear-prediction-characteristics-Atal",
            "title": {
                "fragments": [],
                "text": "Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The cepstrum was found to be the most effective, providing an identification accuracy of 70% for speech 50 msec in duration, which increased to more than 98% for a duration of 0.5 sec."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705797"
                        ],
                        "name": "M. Athineos",
                        "slug": "M.-Athineos",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Athineos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Athineos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745455"
                        ],
                        "name": "D. Ellis",
                        "slug": "D.-Ellis",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ellis",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ellis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "As noted above for the newer FDLP and LP-TRAP approaches, we continue to do early experiments with small tasks to permit many experiments, but we take the best (and most developed) of these methods and validate their generality on large tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "In subsequent work, we incorporated the FDLP model into the TRAP framework, yielding the LP-TRAP [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 221
                            }
                        ],
                        "text": "The energy-based representations of temporal trajectories (or, more generally, of spectrotemporal patterns) could be replaced by autoregressive models for these components of the time-frequency plane, such as the FDLP or LP-TRAP approaches described earlier."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7177124,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "44b07651b22996ca0df36826227420812becc2c1",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Autoregressive modeling is applied for approximating the temporal evolution of spectral density in critical-band-sized sub-bands of a segment of speech signal. The generalized autocorrelation linear predictive technique allows for a compromise between fitting the peaks and the troughs of the Hilbert envelope of the signal in the sub-band. The cosine transform coefficients of the approximated sub-band envelopes, computed recursively from the all-pole polynomials, are used as inputs to a TRAP-based speech recognition system and are shown to improve recognition accuracy."
            },
            "slug": "LP-TRAP:-linear-predictive-temporal-patterns-Athineos-Hermansky",
            "title": {
                "fragments": [],
                "text": "LP-TRAP: linear predictive temporal patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The cosine transform coefficients of the approximated sub-band envelopes, computed recursively from the all-pole polynomials, are used as inputs to a TRAP-based speech recognition system and are shown to improve recognition accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145037594"
                        ],
                        "name": "S. Vuuren",
                        "slug": "S.-Vuuren",
                        "structuredName": {
                            "firstName": "Sarel",
                            "lastName": "Vuuren",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vuuren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109947002"
                        ],
                        "name": "Sangita Sharma",
                        "slug": "Sangita-Sharma",
                        "structuredName": {
                            "firstName": "Sangita",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sangita Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026PROCESSING MAGAZINE [81] SEPTEMBER 2005\n\u00a9 ARTVILLE & COMSTOCK\n[Nelson Morgan, Qifeng Zhu, Andreas Stolcke, Kemal S\u00f6nmez, Sunil Sivadas, Takahiro Shinozaki,\nMari Ostendorf, Pratibha Jain, Hynek Hermansky,\nDan Ellis, George Doddington, Barry Chen, \u00d6zg\u00fcr \u00c7etin, Herv\u00e9 Bourlard, and Marios Athineos]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7574102,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "9d43f71cbfa84467293f6a33e14ea6840e83803d",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relevance-of-time-frequency-features-for-phonetic-Yang-Vuuren",
            "title": {
                "fragments": [],
                "text": "Relevance of time-frequency features for phonetic and speaker-channel classification"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711425"
                        ],
                        "name": "T. Robinson",
                        "slug": "T.-Robinson",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Robinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39213695"
                        ],
                        "name": "M. Hochberg",
                        "slug": "M.-Hochberg",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Hochberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hochberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Under good conditions, human phone error rate for nonsense syllables has been estimated to be as low as 1.5% [1], as compared with rates that are over an order of magnitude higher for the best machine phone recognizers [13], [23], [ 26 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16329790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a827beb108a769ee49f1ec88ff0546b11e5ac7c4",
            "isKey": true,
            "numCitedBy": 48,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes phone modelling improvements to the hybrid connectionist-hidden Markov model speech recognition system developed at Cambridge University. These improvements are applied to phone recognition from the TIMIT task and word recognition from the Wall Street Journal (WSJ) task. A recurrent net is used to map acoustic vectors to posterior probabilities of phone classes. The maximum likelihood phone or word string is then extracted using Markov models. The paper describes three improvements: connectionist model merging; explicit presentation of acoustic context; and improved duration modelling. The first is shown to provide a significant improvement in the TIMIT phone recognition rate and all three provide an improvement in the WSJ word recognition rate.<<ETX>>"
            },
            "slug": "IPA:-improved-phone-modelling-with-recurrent-neural-Robinson-Hochberg",
            "title": {
                "fragments": [],
                "text": "IPA: improved phone modelling with recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Three improvements to the hybrid connectionist-hidden Markov model speech recognition system are described: connectionist model merging; explicit presentation of acoustic context; and improved duration modelling, which provide a significant improvement in the TIMIT phone recognition rate."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1851916"
                        ],
                        "name": "M. Kleinschmidt",
                        "slug": "M.-Kleinschmidt",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kleinschmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kleinschmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739198"
                        ],
                        "name": "David Gelbart",
                        "slug": "David-Gelbart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gelbart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Gelbart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [22], a set of Gabor filters was applied to a log mel spectrogam, creating sensitivity to spectral, temporal, and spectrotemporal patterns at various scales."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16301257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "904dd068eb7dee4e045443ed53e4cbf38d438ed1",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel type of feature extraction for automatic speech recognition is investigated. Two-dimensional Gabor functions, with varying extents and tuned to different rates and directions of spectro-temporal modulation, are applied as filters to a spectro-temporal representation provided by mel spectra. The use of these functions is motivated by findings in neurophysiology and psychoacoustics. Data-driven parameter selection was used to obtain Gabor feature sets, the performance of which is evaluated on the Aurora 2 and 3 datasets both on their own and in combination with the Qualcomm-OGI-ICSI Aurora proposal. The Gabor features consistently provide performance improvements."
            },
            "slug": "Improving-word-accuracy-with-Gabor-feature-Kleinschmidt-Gelbart",
            "title": {
                "fragments": [],
                "text": "Improving word accuracy with Gabor feature extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A novel type of feature extraction for automatic speech recognition that uses two-dimensional Gabor functions applied as filters to a spectro-temporal representation provided by mel spectra is investigated."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31858096"
                        ],
                        "name": "J. David",
                        "slug": "J.-David",
                        "structuredName": {
                            "firstName": "Jr.",
                            "lastName": "David",
                            "middleNames": [
                                "E.E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026553"
                        ],
                        "name": "O. Selfridge",
                        "slug": "O.-Selfridge",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Selfridge",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Selfridge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This dependence on the spectral envelope for speech sound discrimination dates back to the 1950s, as described in [ 11 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51664397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e263a12ce240aff6d85377354ee12a4f60383d30",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Attempts to mechanize character reading and speech recognition have greatly accelerated in the past decade. This increased interest was prompted by the promise of computer inputs more flexible in format than punched cards or magnetic tape. Research has shown that automatic sensing can be done reliably if the task is suitably delimited. Cleverly designed marks on standard forms can be both machine and man readable. A single type font or a few fixed ones are tractable if the print quality is controlled. Handprinting can be handled for careful writers, as can meticulous handwriting. Isolated spoken words taken from a small number of talkers and a limited vocabulary can be automatically recognized. Typical error rates for these machine-sensings run between 0.5 and 25 per cent. These results imply that reading unrestricted typestyles, handwritten scrawl, or recognizing conversational speech is beyond the reach of present methods. From the engineering viewpoint, questions of values enter. Might it not be wiser to punch cards or tape while making copy rather than depend upon complex character recognition hardware? Is it useful to have voice input to a computer when a finger and typewriter are available? Answers to such questions will depend upon the specific application. Certainly, the utility of automatic sensing will depend upon what is to be done with the material after it enters the computer as well as the internal organization of the machine itself."
            },
            "slug": "Eyes-and-Ears-for-Computers-David-Selfridge",
            "title": {
                "fragments": [],
                "text": "Eyes and Ears for Computers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Research has shown that automatic sensing can be done reliably if the task is suitably delimited, and implies that reading unrestricted typestyles, handwritten scrawl, or recognizing conversational speech is beyond the reach of present methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IRE"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705797"
                        ],
                        "name": "M. Athineos",
                        "slug": "M.-Athineos",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Athineos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Athineos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745455"
                        ],
                        "name": "D. Ellis",
                        "slug": "D.-Ellis",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ellis",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ellis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We are also looking at combining the two dual forms of linear prediction, creating a new representation called perceptual linear prediction squared (PLP 2 ) [ 5 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One approach that has been proposed is to add feature dependencies or explicitly model the dynamics of frame-based features in various extensions of HMMs [ 5 ], [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60996254,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "4a3bea780b789e75a34312c22303b308f9692213",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The temporal trajectories of the spectral energy in auditory critical bands over 250\u00a0ms segments are approximated by an all-pole model, the time-domain dual of conventional linear prediction. This quarter-second auditory spectro-temporal pattern is further smoothed by iterative alternation of spectral and temporal all-pole modeling. Just as Perceptual Linear Prediction (PLP) uses an autoregressive model in the frequency domain to estimate peaks in an auditory-like short-term spectral slice, PLP$^2$ uses all-pole modeling in both time and frequency domains to estimate peaks of a two-dimensional spectro-temporal pattern, motivated by considerations of the auditory system."
            },
            "slug": "PLP2:-Autoregressive-modeling-of-auditory-like-2-D-Athineos-Hermansky",
            "title": {
                "fragments": [],
                "text": "PLP2: Autoregressive modeling of auditory-like 2-D spectro-temporal patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The temporal trajectories of the spectral energy in auditory critical bands over 250\u00a0ms segments are approximated by an all-pole model, the time-domain dual of conventional linear prediction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2104856341"
                        ],
                        "name": "PROCEssIng magazInE",
                        "slug": "PROCEssIng-magazInE",
                        "structuredName": {
                            "firstName": "PROCEssIng",
                            "lastName": "magazInE",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "PROCEssIng magazInE"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16557637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "510ae643010eb04fdd43e0d8ce34250b020005ca",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "and artificial intelligence may be viewed as instances of the summary-product algorithm (or belief/probability propagation algorithm), which operates by message passing in a graphical model. Specific instances of such algorithms include Kalman filtering and smoothing; the forward\u2013backward algorithm for hidden Markov models; probability propagation in Bayesian networks; and decoding algorithms for error-correcting codes such as the Viterbi algorithm, the BCJR algorithm, and the iterative decoding of turbo codes, low-density parity-check (LDPC) codes, and similar codes. New algorithms for complex detection and estimation problems can also be derived as instances of the summary-product algorithm. In this article, we give an introduction to this unified perspective in terms of (Forney-style) factor graphs. Hans-Andrea Loeliger"
            },
            "slug": "IEEE-Signal-Processing-Magazine-magazInE",
            "title": {
                "fragments": [],
                "text": "Ieee Signal Processing Magazine"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A unified perspective of probability propagation in Bayesian networks and artificial intelligence is given in terms of (Forney-style) factor graphs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109947002"
                        ],
                        "name": "Sangita Sharma",
                        "slug": "Sangita-Sharma",
                        "structuredName": {
                            "firstName": "Sangita",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sangita Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066975404"
                        ],
                        "name": "P. Jain",
                        "slug": "P.-Jain",
                        "structuredName": {
                            "firstName": "Pratibha",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "This yielded promising results for small tasks like numbers recognition, as reported in [20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 18782969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d2da776b4a1c40bb7aeb59b5fa29897b5c66ccc",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Rather long temporal trajectory of critical band logarithmic power spectrum energy at a given frequency is used as an input feature vector in a MLP-based phoneme classi er, trained on a task-independent hand-labeled development data. Class-speci c log likelihood vectors from the individual sub-classi ers form input to a merging MLP classi er trained on the training data. Output of this merging classi er forms a feature vector for subsequent HMM ASR."
            },
            "slug": "Data-Derived-Non-Linear-Mapping-for-Feature-in-HMM-Hermansky-Sharma",
            "title": {
                "fragments": [],
                "text": "Data-Derived Non-Linear Mapping for Feature Extraction in HMM"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Rather long temporal trajectory of critical band logarithmic power spectrum energy at a given frequency is used as an input feature vector in a MLP-based phoneme classi er, trained on a task-independent hand-labeled development data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "The overall structure provides a consistent mathematical framework that can incorporate powerful learning methods such as maximum likelihood training using expectation maximization [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48399,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109947002"
                        ],
                        "name": "Sangita Sharma",
                        "slug": "Sangita-Sharma",
                        "structuredName": {
                            "firstName": "Sangita",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sangita Sharma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "This structure is referred to as TempoRAl patterns (TRAPs) [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15276102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72f3f68664e27745e8f63fdc6cfd0eb5c37dc844",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The work proposes a radically di erent set of features for ASR where TempoRAl Patterns of spectral energies are used in place of the conventional spectral patterns. The approach has several inherent advantages, among them robustness to stationary or slowly varying disturbances."
            },
            "slug": "TRAPS-classifiers-of-temporal-patterns-Hermansky-Sharma",
            "title": {
                "fragments": [],
                "text": "TRAPS - classifiers of temporal patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The work proposes a radically different set of features for ASR where TempoRAl Patterns of spectral energies are used in place of the conventional spectral patterns."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398757367"
                        ],
                        "name": "Louise Spear-Swerling",
                        "slug": "Louise-Spear-Swerling",
                        "structuredName": {
                            "firstName": "Louise",
                            "lastName": "Spear-Swerling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Louise Spear-Swerling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46318686"
                        ],
                        "name": "R. Sternberg",
                        "slug": "R.-Sternberg",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Sternberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sternberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Of course, we did not mean this literally; rather, we intended to encourage intrepid exploration of the road \u201cless traveled\u201d [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 787488,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e00499bc53a19897a02cc68840e862b95005b8ba",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a theoretical model of reading disability that integrates a wide range of research findings in cognitive psychology, reading, and education across the age and grade span. The model shows how reading disability relates to normal reading acquisition, and includes four possible patterns of reading disability: nonalphabetic readers, compensatory readers, nonautomatic readers, and readers delayed in the acquisition of word-recognition skills. We compare our model to the models of other investigators and argue that our model is especially useful to practitioners. Finally, we discuss some of the educational implications of the model."
            },
            "slug": "The-Road-Not-Taken-Spear-Swerling-Sternberg",
            "title": {
                "fragments": [],
                "text": "The Road Not Taken"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "This article describes a theoretical model of reading disability that integrates a wide range of research findings in cognitive psychology, reading, and education across the age and grade span and argues that the model is especially useful to practitioners."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of learning disabilities"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The road not taken, \" in Mountain Interval"
            },
            "venue": {
                "fragments": [],
                "text": "The road not taken, \" in Mountain Interval"
            },
            "year": 1920
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi - rate and variable - rate modeling of speech at phone and syllable time scales , \u201d in"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . ICASSP"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acoustics, Speech, Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Acoustics, Speech, Signal Processing"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026PROCESSING MAGAZINE [81] SEPTEMBER 2005\n\u00a9 ARTVILLE & COMSTOCK\n[Nelson Morgan, Qifeng Zhu, Andreas Stolcke, Kemal S\u00f6nmez, Sunil Sivadas, Takahiro Shinozaki,\nMari Ostendorf, Pratibha Jain, Hynek Hermansky,\nDan Ellis, George Doddington, Barry Chen, \u00d6zg\u00fcr \u00c7etin, Herv\u00e9 Bourlard, and Marios Athineos]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IPA: Improved modelling with recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ICASSP-94"
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 9,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 34,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Pushing-the-envelope-aside-[speech-recognition]-Morgan-Zhu/d9d2ba2003d7324ae3d5ff7423a13f13efc79ca5?sort=total-citations"
}