{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610924"
                        ],
                        "name": "A. Bruckstein",
                        "slug": "A.-Bruckstein",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Bruckstein",
                            "middleNames": [
                                "Marcel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruckstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121924283"
                        ],
                        "name": "Y. Katz",
                        "slug": "Y.-Katz",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Katz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1912619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6e0fb4c77906bc23fe59a8f848ce62ba9687181",
            "isKey": false,
            "numCitedBy": 6193,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a pre-specified set of linear transforms, or by adapting the dictionary to a set of training signals. Both these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method \u2013 the K-SVD algorithm \u2013 generalizing the K-Means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary, and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results on both synthetic tests and in applications on real image data."
            },
            "slug": "K-SVD-:-An-Algorithm-for-Designing-of-Overcomplete-Aharon-Elad",
            "title": {
                "fragments": [],
                "text": "K-SVD : An Algorithm for Designing of Overcomplete Dictionaries for Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel algorithm for adapting dictionaries in order to achieve sparse signal representations, K-SVD, an iterative method that alternates between sparse coding of the examples based on the current dictionary, and a process of updating the dictionary atoms to better fit the data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610924"
                        ],
                        "name": "A. Bruckstein",
                        "slug": "A.-Bruckstein",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Bruckstein",
                            "middleNames": [
                                "Marcel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruckstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 85
                            }
                        ],
                        "text": "Problem Statement Classical dictionary \nlearning techniques (Olshausen &#38; Field, 1997; Aharon et al., 2006; Lee et al., 2007) consider a .nite \ntraining set of signals X =[x1,..., xn]in Rm\u00d7n and optimize the empirical cost function n n ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "\u2026\nproblem is to alter\u00adnate between the two variables, minimizing over one while keeping the other one .xed, \nas proposed by Lee et al. (2007) (see also Aharon et al. (2006), who use l0 rather than l1 penalties, \nfor related approaches).3 Since the computation of a dominates the cost of each\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "Most recent algorithms for dictionary learning (Olshausen &#38; Field, \n1997; Aharon et al., 2006; Lee et al., 2007) are second-order iterative batch procedures, accessing the \nwhole training set at each iteration in order to minimize a cost function under some constraints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Classical dictionary learning techniques (Olshausen & Field, 1997;  Aharon et al., 2006;  Lee et al., 2007) consider a finite training set of signals X = [x1, . . . , xn] in R m\u00d7n"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Most recent algorithms for dictionary learning (Olshausen & Field, 1997;  Aharon et al., 2006;  Lee et al., 2007) are second-order iterative batch procedures, accessing the whole training set at each iteration in order to minimize a cost function under some constraints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A natural approach to solving this problem is to alternate between the two variables, minimizing over one while keeping the other one fixed, as proposed by Lee et al. (2007) (see also  Aharon et al. (2006) , who use l0 rather than l1 penalties, for related approaches)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7477309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83b522f4bfa5db7f7d34f839475af7d078107634",
            "isKey": true,
            "numCitedBy": 7336,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data"
            },
            "slug": "$rm-K$-SVD:-An-Algorithm-for-Designing-Overcomplete-Aharon-Elad",
            "title": {
                "fragments": [],
                "text": "$rm K$-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A novel algorithm for adapting dictionaries in order to achieve sparse signal representations, the K-SVD algorithm, an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e64a9960734215e2b1866ea3cb723ffa5585ac14",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "slug": "Efficient-sparse-coding-algorithms-Lee-Battle",
            "title": {
                "fragments": [],
                "text": "Efficient sparse coding algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These algorithms are applied to natural images and it is demonstrated that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2599292"
                        ],
                        "name": "J. Mairal",
                        "slug": "J.-Mairal",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Mairal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mairal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699339"
                        ],
                        "name": "G. Sapiro",
                        "slug": "G.-Sapiro",
                        "structuredName": {
                            "firstName": "Guillermo",
                            "lastName": "Sapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 58
                            }
                        ],
                        "text": "Mairal, J., Bach, F., Ponce, \nJ., Sapiro, G., &#38; Zisserman, A. (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1166,
                                "start": 5
                            }
                        ],
                        "text": "Our last experiment demonstrates that our algorithm can be used for a difficult large-scale image processing task, namely, removing the text (inpainting) from the damaged 12-Megapixel image of Figure 2. Using a multi-threaded version of our implementation, we have learned a dictionary with 256 elements from the roughly 7 \u00d7 10(6) undamaged 12\u00d712 color patches in the image with two epochs in about 500 seconds on a 2.4GHz machine with eight cores. Once the dictionary has been learned, the text is removed using the sparse coding technique for inpainting of Mairal et al. (2008). Our intent here is of course not to evaluate our learning procedure in inpainting tasks, which would require a thorough comparison with state-the-art techniques on standard datasets. Instead, we just wish to demonstrate that the proposed method can indeed be applied to a realistic, non-trivial image processing task on a large image. Indeed, to the best of our knowledge, this is the first time that dictionary learning is used for image restoration on such large-scale data. For comparison, the dictionaries used for inpainting in the state-of-the-art method of Mairal et al. (2008) are learned (in batch mode) on only 200,000 patches."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 533,
                                "start": 7
                            }
                        ],
                        "text": "which, although it does not ensure the convergence of Dt, ensures the convergence of the series \u2211\u221e t=1 ||Dt \u2212Dt\u22121||(2)F , a classical condition in gradient descent convergence proofs (Bertsekas, 1999). In turn, this reduces to showing that Dt minimizes a parametrized quadratic function over C with parameters 1 t At and 1 t Bt, then showing that the solution is uniformly Lipschitz with respect to these parameters, borrowing some ideas from perturbation theory (Bonnans & Shapiro, 1998). At this point, and following Bottou (1998), proving the convergence of the sequence f\u0302t(Dt) amounts to showing that the stochastic positive process"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 3
                            }
                        ],
                        "text": "is a quasi-martingale. To do so, denoting by Ft the filtration of the past information, a theorem by Fisk (1965) states that if the positive sum \u2211\u221e t=1 E[max(E[ut+1 \u2212 ut|Ft], 0)] converges, then ut is a quasi-martingale which converges with probability one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 580,
                                "start": 5
                            }
                        ],
                        "text": "Our last experiment demonstrates that our algorithm can be used for a difficult large-scale image processing task, namely, removing the text (inpainting) from the damaged 12-Megapixel image of Figure 2. Using a multi-threaded version of our implementation, we have learned a dictionary with 256 elements from the roughly 7 \u00d7 10(6) undamaged 12\u00d712 color patches in the image with two epochs in about 500 seconds on a 2.4GHz machine with eight cores. Once the dictionary has been learned, the text is removed using the sparse coding technique for inpainting of Mairal et al. (2008). Our intent here is of course not to evaluate our learning procedure in inpainting tasks, which would require a thorough comparison with state-the-art techniques on standard datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 785,
                                "start": 25
                            }
                        ],
                        "text": "Since this convex optimization problem admits separable constraints in the updated blocks (columns), convergence to a global optimum is guaranteed (Bertsekas, 1999). In practice, since the vectors\u03b1i are sparse, the coefficients of the matrix A are in general concentrated on the diagonal, which makes the block-coordinate descent more efficient.(5) Since our algorithm uses the value ofDt\u22121 as a (5)Note that this assumption does not exactly hold: To be more exact, if a group of columns in D are highly correlated, the coefficients of the matrix A can concentrate on the corresponding principal submatrices ofA. warm restart for computing Dt, a single iteration has empirically been found to be enough. Other approaches have been proposed to update D, for instance, Lee et al. (2007) suggest using a Newton method on the dual of Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 682971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d65ba8bb20ae6dd001b9833c525c279dfe18916",
            "isKey": false,
            "numCitedBy": 1126,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "It is now well established that sparse signal models are well suited for restoration tasks and can be effectively learned from audio, image, and video data. Recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones. This paper proposes a new step in that direction, with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and discriminative class models. The linear version of the proposed model admits a simple probabilistic interpretation, while its most general variant admits an interpretation in terms of kernels. An optimization framework for learning all the components of the proposed model is presented, along with experimental results on standard handwritten digit and texture classification tasks."
            },
            "slug": "Supervised-Dictionary-Learning-Mairal-Bach",
            "title": {
                "fragments": [],
                "text": "Supervised Dictionary Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel sparse representation for signals belonging to different classes in terms of a shared dictionary and discriminative class models is proposed, with results on standard handwritten digit and texture classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2599292"
                        ],
                        "name": "J. Mairal",
                        "slug": "J.-Mairal",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Mairal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mairal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699339"
                        ],
                        "name": "G. Sapiro",
                        "slug": "G.-Sapiro",
                        "structuredName": {
                            "firstName": "Guillermo",
                            "lastName": "Sapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sapiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Once the dictionary has been learned, the text is removed using the sparse coding technique for inpainting of  Mairal et al. (2008) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 110
                            }
                        ],
                        "text": "Once the dictionary \nhas been learned, the text is removed using the sparse coding technique for inpainting of Mairal et al. \n(2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For comparison, the dictionaries used for inpainting in the state-of-the-art method of  Mairal et al. (2008)  are learned (in batch mode) on only 200,000 patches."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2627705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92281d5002178003bd7060fc66677a3471cdaa4b",
            "isKey": false,
            "numCitedBy": 1691,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse representations of signals have drawn considerable interest in recent years. The assumption that natural signals, such as images, admit a sparse decomposition over a redundant dictionary leads to efficient algorithms for handling such sources of data. In particular, the design of well adapted dictionaries for images has been a major challenge. The K-SVD has been recently proposed for this task and shown to perform very well for various grayscale image processing tasks. In this paper, we address the problem of learning dictionaries for color images and extend the K-SVD-based grayscale image denoising algorithm that appears in . This work puts forward ways for handling nonhomogeneous noise and missing information, paving the way to state-of-the-art results in applications such as color image denoising, demosaicing, and inpainting, as demonstrated in this paper."
            },
            "slug": "Sparse-Representation-for-Color-Image-Restoration-Mairal-Elad",
            "title": {
                "fragments": [],
                "text": "Sparse Representation for Color Image Restoration"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work puts forward ways for handling nonhomogeneous noise and missing information, paving the way to state-of-the-art results in applications such as color image denoising, demosaicing, and inpainting, as demonstrated in this paper."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, learning the dictionary instead of using off-the-shelf bases has been shown to dramatically improve signal reconstruction ( Elad & Aharon, 2006 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 132
                            }
                        ],
                        "text": "However, learning the dictionary instead of using off-the-shelf bases has been shown to dra\u00admatically \nimprove signal reconstruction (Elad &#38; Aharon, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The linear decomposition of a signal using a few atoms of a learned dictionary instead of a predefined one\u2014based on wavelets (Mallat, 1999) for example\u2014has recently led to state-of-the-art results for numerous low-level image processing tasks such as denoising ( Elad & Aharon, 2006 ) as well as higher-level tasks such as classification (Raina et al., 2007; Mairal et al., 2009), showing that sparse learned models are well adapted to ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026on wavelets (Mallat, 1999) for example has recently led to state-of-the-art \nresults for numerous low-level image pro\u00adcessing tasks such as denoising (Elad &#38; Aharon, 2006) as \nwell as higher-level tasks such as classi.cation (Raina et al., 2007; Mairal et al., 2009), showing that \nsparse\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6888534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e07416eabd4ba6c69fa473756bb04ae7161177be",
            "isKey": true,
            "numCitedBy": 4981,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods"
            },
            "slug": "Image-Denoising-Via-Sparse-and-Redundant-Over-Elad-Aharon",
            "title": {
                "fragments": [],
                "text": "Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This work addresses the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image, and uses the K-SVD algorithm to obtain a dictionary that describes the image content effectively."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941687"
                        ],
                        "name": "M. Protter",
                        "slug": "M.-Protter",
                        "structuredName": {
                            "firstName": "Matan",
                            "lastName": "Protter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Protter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15211762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08253ca281c3603e0eaf3a4955fa468d42d165b6",
            "isKey": false,
            "numCitedBy": 345,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider denoising of image sequences that are corrupted by zero-mean additive white Gaussian noise. Relative to single image denoising techniques, denoising of sequences aims to also utilize the temporal dimension. This assists in getting both faster algorithms and better output quality. This paper focuses on utilizing sparse and redundant representations for image sequence denoising. In the single image setting, the K-SVD algorithm is used to train a sparsifying dictionary for the corrupted image. This paper generalizes the above algorithm by offering several extensions: i) the atoms used are 3-D; ii) the dictionary is propagated from one frame to the next, reducing the number of required iterations; and iii) averaging is done on patches in both spatial and temporal neighboring locations. These modifications lead to substantial benefits in complexity and denoising performance, compared to simply running the single image algorithm sequentially. The algorithm's performance is experimentally compared to several state-of-the-art algorithms, demonstrating comparable or favorable results."
            },
            "slug": "Image-Sequence-Denoising-via-Sparse-and-Redundant-Protter-Elad",
            "title": {
                "fragments": [],
                "text": "Image Sequence Denoising via Sparse and Redundant Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper generalizes the above algorithm by offering several extensions: i) the atoms used are 3-D; ii) the dictionary is propagated from one frame to the next, reducing the number of required iterations; and iii) averaging is done on patches in both spatial and temporal neighboring locations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 138
                            }
                        ],
                        "text": "For example, \n.rst-order stochastic gradient descent with projections on the constraint set is sometimes used for dictionary \nlearn\u00ading (see Aharon and Elad (2008) for instance)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "In the case of dictionary learning, classical projected .rst\u00adorder stochastic gradient descent (as used \nby Aharon and Elad (2008) for instance) consists of a sequence of updates of D: [] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9573430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19de9e4850a208800db50615afec2b08b25d4f99",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling signals by sparse and redundant representations has been drawing considerable attention in recent years. Coupled with the ability to train the dictionary using signal examples, these techniques have been shown to lead to state-of-the-art results in a series of recent applications. In this paper we propose a novel structure of such a model for representing image content. The new dictionary is itself a small image, such that every patch in it (in varying location and size) is a possible atom in the representation. We refer to this as the image-signature-dictionary (ISD) and show how it can be trained from image examples. This structure extends the well-known image and video epitomes, as introduced by Jojic, Frey, and Kannan [in Proceedings of the IEEE International Conference on Computer Vision, 2003, pp. 34-41] and Cheung, Frey, and Jojic [in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005, pp. 42-49], by replacing a probabilistic averaging of patches with their sparse representations. The ISD enjoys several important features, such as shift and scale flexibilities, and smaller memory and computational requirements, compared to the classical dictionary approach. As a demonstration of these benefits, we present high-quality image denoising results based on this new model."
            },
            "slug": "Sparse-and-Redundant-Modeling-of-Image-Content-an-Aharon-Elad",
            "title": {
                "fragments": [],
                "text": "Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a novel structure of a model for representing image content by replacing a probabilistic averaging of patches with their sparse representations, and presents high-quality image denoising results based on this new model."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Imaging Sci."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9740,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145720668"
                        ],
                        "name": "J. Fuchs",
                        "slug": "J.-Fuchs",
                        "structuredName": {
                            "firstName": "Jean-Jacques",
                            "lastName": "Fuchs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fuchs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "This matrix is thus invertible and classical results (Fuchs, 2005) ensure the uniqueness of the sparse coding solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 56
                            }
                        ],
                        "text": "This matrix . is thus invertible and classical results (Fuchs, 2005) \nensure the uniqueness of the sparse coding solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 989399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2942c20f111d6fd38ef6dd53bb6eeb3b880e3d5f",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this contribution is to extend some recent results on sparse representations of signals in redundant bases developed in the noise-free case to the case of noisy observations. The type of question addressed so far is as follows: given an (n,m)-matrix A with m>n and a vector b=Axo, i.e., admitting a sparse representation xo, find a sufficient condition for b to have a unique sparsest representation. The answer is a bound on the number of nonzero entries in xo. We consider the case b=Axo+e where xo satisfies the sparsity conditions requested in the noise-free case and e is a vector of additive noise or modeling errors, and seek conditions under which xo can be recovered from b in a sense to be defined. The conditions we obtain relate the noise energy to the signal level as well as to a parameter of the quadratic program we use to recover the unknown sparsest representation. When the signal-to-noise ratio is large enough, all the components of the signal are still present when the noise is deleted; otherwise, the smallest components of the signal are themselves erased in a quite rational and predictable way"
            },
            "slug": "Recovery-of-exact-sparse-representations-in-the-of-Fuchs",
            "title": {
                "fragments": [],
                "text": "Recovery of exact sparse representations in the presence of bounded noise"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The purpose of this contribution is to extend some recent results on sparse representations of signals in redundant bases developed in the noise-free case to the case of noisy observations, finding a bound on the number of nonzero entries in xo."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": false,
            "numCitedBy": 3574,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Comparison with Stochastic \nGradientDescent."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 333,
                                "start": 314
                            }
                        ],
                        "text": "OnlineDictionaryLearningfor Sparse Coding Julien Mairal Francis Bach INRIA,1 45 rue d Ulm 75005 Paris, \nFrance JULIEN.MAIRAL@INRIA.FR FRANCIS.BACH@INRIA.FR Jean Ponce Ecole Normale Sup\u00b4erieure,1 45 rue d Ulm \n75005 Paris, France JEAN.PONCE@ENS.FR Guillermo Sapiro GUILLE@UMN.EDU University ofMinnesota -Department \nof Electrical and Computer Engineering, 200 Union Street SE,Minneapolis, USA Abstract Sparse coding that \nis, modelling data vectors as sparse linear combinations of basis elements is widely used in machine \nlearning, neuroscience, signal processing, and statistics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Top: Comparison between online and batch learning for various training set sizes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 143
                            }
                        ],
                        "text": "\u202645 rue d Ulm \n75005 Paris, France JEAN.PONCE@ENS.FR Guillermo Sapiro GUILLE@UMN.EDU University ofMinnesota -Department \nof Electrical and Computer Engineering, 200 Union Street SE,Minneapolis, USA Abstract Sparse coding that \nis, modelling data vectors as sparse linear combinations of basis\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "SIAMJournal on Scienti.c Computing, 20, 33 61."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Bottom: Comparison between our method and stochastic gradient (SG) descent with different learning rates \n.."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 171
                            }
                        ],
                        "text": "\u2026led to state-of-the-art \nresults for numerous low-level image pro\u00adcessing tasks such as denoising (Elad &#38; Aharon, 2006) as \nwell as higher-level tasks such as classi.cation (Raina et al., 2007; Mairal et al., 2009), showing that \nsparse learned models are well adapted to natural signals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "7: \nCompute Dt using Algorithm 2, with Dt-1 as warm restart, so that t n ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6692382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "isKey": true,
            "numCitedBy": 1611,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."
            },
            "slug": "Self-taught-learning:-transfer-learning-from-data-Raina-Battle",
            "title": {
                "fragments": [],
                "text": "Self-taught learning: transfer learning from unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data to form a succinct input representation and significantly improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691592"
                        ],
                        "name": "K. Engan",
                        "slug": "K.-Engan",
                        "structuredName": {
                            "firstName": "Kjersti",
                            "lastName": "Engan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Engan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3046011"
                        ],
                        "name": "S. O. Aase",
                        "slug": "S.-O.-Aase",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Aase",
                            "middleNames": [
                                "Ole"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. O. Aase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31557075"
                        ],
                        "name": "J. H. Hus\u00f8y",
                        "slug": "J.-H.-Hus\u00f8y",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hus\u00f8y",
                            "middleNames": [
                                "H\u00e5kon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hus\u00f8y"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10445312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0db2995460bf3369e8aaab3676afdf06b26f447",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The method of optimal directions (MOD) is an iterative method for designing frames for sparse representation purposes using a training set. In this paper we use frames designed by MOD in a multiframe compression (MFC) scheme. Both the MOD and the MFC need a vector selection algorithm, and orthogonal matching pursuit (OMP) is used in this paper. In the MFC scheme several different frames are used, each optimized for a fixed number of selected frame vectors in each approximation. We apply the MOD and the MFC scheme to ECG signals, and do experiments with both fixed size and variable size on the different frames used in the MFC scheme. Compared to traditional transform based compression, the experiments demonstrate improved rate-distortion performance by 1-4 dB, and that variable sized frames perform better than fixed sized frames."
            },
            "slug": "Frame-based-signal-compression-using-method-of-Engan-Aase",
            "title": {
                "fragments": [],
                "text": "Frame based signal compression using method of optimal directions (MOD)"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper uses frames designed by MOD in a multiframe compression (MFC) scheme to apply to ECG signals, and demonstrates improved rate-distortion performance by 1-4 dB, and that variable sized frames perform better than fixed sized frames."
            },
            "venue": {
                "fragments": [],
                "text": "ISCAS'99. Proceedings of the 1999 IEEE International Symposium on Circuits and Systems VLSI (Cat. No.99CH36349)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66739595"
                        ],
                        "name": "Holger Hofling",
                        "slug": "Holger-Hofling",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hofling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Hofling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 225
                            }
                        ],
                        "text": "Ajj (10) 1 dj . uj. max(||uj||2,1) 4: endfor 5: \nuntil convergence 6: Return D (updated dictionary). number of recent methods for solving this type of \nprob\u00adlems are based on coordinate descent with soft threshold\u00ading (Fu, 1998; Friedman et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 114
                            }
                        ],
                        "text": "number of recent methods for solving this type of problems are based on coordinate descent with soft thresholding (Fu, 1998; Friedman et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15413966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d50b597c475e87e03e8630e381011cb46e460ad8",
            "isKey": false,
            "numCitedBy": 1889,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider \u201cone-at-a-time\u201d coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the \u201cfused lasso,\u201d however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems. 1. Introduction. In this paper we consider statistical models that lead to convex optimization problems with inequality constraints. Typically, the optimization for these problems is carried out using a standard quadratic programming algorithm. The purpose of this paper is to explore \u201cone-at-a-time\u201d coordinate-wise descent algorithms for these problems. The equivalent of a coordinate descent algorithm has been proposed for the L1-penalized regression (lasso) in the literature, but it is not commonly used. Moreover, coordinate-wise algorithms seem too simple, and they are not often used in convex optimization, perhaps because they only work in specialized problems. We ourselves never appreciated the value of coordinate descent methods for convex statistical problems before working on this paper. In this paper we show that coordinate descent is very competitive with the wellknown LARS (or homotopy) procedure in large lasso problems, can deliver a path of solutions efficiently, and can be applied to many other convex statistical problems such as the garotte and elastic net. We then go on to explore a nonseparable problem in which coordinate-wise descent does not work\u2014the \u201cfused lasso.\u201d We derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to"
            },
            "slug": "PATHWISE-COORDINATE-OPTIMIZATION-Friedman-Hastie",
            "title": {
                "fragments": [],
                "text": "PATHWISE COORDINATE OPTIMIZATION"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that coordinate descent is very competitive with the well-known LARS procedure in large lasso problems, can deliver a path of solutions efficiently, and can be applied to many other convex statistical problems such as the garotte and elastic net."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48861660"
                        ],
                        "name": "H. Zou",
                        "slug": "H.-Zou",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Zou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is also possible to enforce this condition using an elastic net penalization ( Zou & Hastie, 2005 ), replacing ||\ufffd||1 by ||\ufffd||1 + \u03ba"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "It is also possible \nto enforce this condition using an elastic net penalization (Zou &#38; Hastie, 2005), replacing ||a||1 \nby .2 ||a||1 + ||a||2 and thus improving the numerical stabil\u00ad 22 ity of homotopyalgorithms such as LARS."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122419596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9f5723859a665c3e54b8d9a1a7eecb8b5084af6",
            "isKey": false,
            "numCitedBy": 13915,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary.\u2002 We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p\u226bn case. An algorithm called LARS\u2010EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso."
            },
            "slug": "Regularization-and-variable-selection-via-the-net-Zou-Hastie",
            "title": {
                "fragments": [],
                "text": "Regularization and variable selection via the elastic net"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is shown that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation, and an algorithm called LARS\u2010EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lamba."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7431525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5936754b5762260bf102ac95d7b26cfc9d31956a",
            "isKey": false,
            "numCitedBy": 1485,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation-estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways."
            },
            "slug": "The-Tradeoffs-of-Large-Scale-Learning-Bottou-Bousquet",
            "title": {
                "fragments": [],
                "text": "The Tradeoffs of Large Scale Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms and shows distinct tradeoffs for the case of small-scale and large-scale learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770186"
                        ],
                        "name": "A. Benveniste",
                        "slug": "A.-Benveniste",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Benveniste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Benveniste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47085520"
                        ],
                        "name": "M. M\u00e9tivier",
                        "slug": "M.-M\u00e9tivier",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "M\u00e9tivier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00e9tivier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2174182"
                        ],
                        "name": "P. Priouret",
                        "slug": "P.-Priouret",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Priouret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Priouret"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60753831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "990b10ce4ef643e148b6c719e99dbf2430671a74",
            "isKey": false,
            "numCitedBy": 1942,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive systems are widely encountered in many applications ranging through adaptive filtering and more generally adaptive signal processing, systems identification and adaptive control, to pattern recognition and machine intelligence: adaptation is now recognised as keystone of \"intelligence\" within computerised systems. These diverse areas echo the classes of models which conveniently describe each corresponding system. Thus although there can hardly be a \"general theory of adaptive systems\" encompassing both the modelling task and the design of the adaptation procedure, nevertheless, these diverse issues have a major common component: namely the use of adaptive algorithms, also known as stochastic approximations in the mathematical statistics literature, that is to say the adaptation procedure (once all modelling problems have been resolved). The juxtaposition of these two expressions in the title reflects the ambition of the authors to produce a reference work, both for engineers who use these adaptive algorithms and for probabilists or statisticians who would like to study stochastic approximations in terms of problems arising from real applications. Hence the book is organised in two parts, the first one user-oriented, and the second providing the mathematical foundations to support the practice described in the first part. The book covers the topcis of convergence, convergence rate, permanent adaptation and tracking, change detection, and is illustrated by various realistic applications originating from these areas of applications."
            },
            "slug": "Adaptive-Algorithms-and-Stochastic-Approximations-Benveniste-M\u00e9tivier",
            "title": {
                "fragments": [],
                "text": "Adaptive Algorithms and Stochastic Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The juxtaposition of these two expressions in the title reflects the ambition of the authors to produce a reference work, both for engineers who use adaptive algorithms and for probabilists or statisticians who would like to study stochastic approximations in terms of problems arising from real applications."
            },
            "venue": {
                "fragments": [],
                "text": "Applications of Mathematics"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34913208"
                        ],
                        "name": "M. R. Osborne",
                        "slug": "M.-R.-Osborne",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Osborne",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Osborne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3447771"
                        ],
                        "name": "B. Presnell",
                        "slug": "B.-Presnell",
                        "structuredName": {
                            "firstName": "Brett",
                            "lastName": "Presnell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Presnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2638770"
                        ],
                        "name": "B. Turlach",
                        "slug": "B.-Turlach",
                        "structuredName": {
                            "firstName": "Berwin",
                            "lastName": "Turlach",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Turlach"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 150
                            }
                        ],
                        "text": "\u2026in general highly correlated, and we have empirically \nobserved that a Cholesky-based im\u00adplementation of the LARS-Lasso algorithm, an homotopy method (Osborne \net al., 2000; Efron et al., 2004) that pro\u00advides the whole regularization path that is, the solutions \nfor all possible values of ., can\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 197
                            }
                        ],
                        "text": "However, the columns of learned dictionaries are in general highly correlated, and we have empirically observed that a Cholesky-based implementation of the LARS-Lasso algorithm, an homotopy method (Osborne et al., 2000; Efron et al., 2004) that provides the whole regularization path\u2014that is, the solutions for all possible values of \u03bb, can be as fast as approaches based on soft thresholding, while providing the solution with a higher accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14553495,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "db31973871f5648cffb02e7590582376e16ad0bb",
            "isKey": false,
            "numCitedBy": 915,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The title Lasso has been suggested by Tibshirani (1996) as a colourful name for a technique of variable selection which requires the minimization of a sum of squares subject to an l\n 1 bound \u03ba on the solution. This forces zero components in the minimizing solution for small values of \u03ba. Thus this bound can function as a selection parameter. This paper makes two contributions to computational problems associated with implementing the Lasso: (1) a compact descent method for solving the constrained problem for a particular value of \u03ba is formulated, and (2) a homotopy method, in which the constraint bound \u03ba becomes the homotopy parameter, is developed to completely describe the possible selection regimes. Both algorithms have a finite termination property. It is suggested that modified Gram-Schmidt orthogonalization applied to an augmented design matrix provides an effective basis for implementing the algorithms."
            },
            "slug": "A-new-approach-to-variable-selection-in-least-Osborne-Presnell",
            "title": {
                "fragments": [],
                "text": "A new approach to variable selection in least squares problems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A compact descent method for solving the constrained problem for a particular value of \u03ba is formulated, and a homotopy method, in which the constraint bound \u03ba becomes the Homotopy parameter, is developed to completely describe the possible selection regimes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16162039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "isKey": false,
            "numCitedBy": 36494,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described."
            },
            "slug": "Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Regression Shrinkage and Selection via the Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678771"
                        ],
                        "name": "P. Bickel",
                        "slug": "P.-Bickel",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bickel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100532061"
                        ],
                        "name": "Y. Ritov",
                        "slug": "Y.-Ritov",
                        "structuredName": {
                            "firstName": "Yaacov",
                            "lastName": "Ritov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ritov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2667773"
                        ],
                        "name": "A. Tsybakov",
                        "slug": "A.-Tsybakov",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Tsybakov",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsybakov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7048603,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb92178d5133f9165b704500cbe2e5a1b2dab01d",
            "isKey": false,
            "numCitedBy": 2211,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. For both methods, we derive, in parallel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the l p estimation loss for 1 \u2264 p \u2264 2 in the linear model when the number of variables can be much larger than the sample size."
            },
            "slug": "SIMULTANEOUS-ANALYSIS-OF-LASSO-AND-DANTZIG-SELECTOR-Bickel-Ritov",
            "title": {
                "fragments": [],
                "text": "SIMULTANEOUS ANALYSIS OF LASSO AND DANTZIG SELECTOR"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "It is shown that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior and derive, in parallel, oracle inequalities for the prediction risk in the general nonparametric regression model as well as bounds on the l p estimation loss in the linear model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726721"
                        ],
                        "name": "J. F. Bonnans",
                        "slug": "J.-F.-Bonnans",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bonnans",
                            "middleNames": [
                                "Fr\u00e9d\u00e9ric"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. F. Bonnans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31474779"
                        ],
                        "name": "A. Shapiro",
                        "slug": "A.-Shapiro",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Shapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shapiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Then, a classical theorem from perturbation theory ( Bonnans & Shapiro, 1998,  Theorem 4.1) shows that l(x, D) is C 1 . This, allows us to use a last result on empirical processes ensuring that f(Dt) \u2212 \u02c6 ft(Dt) converges almost surely to 0. Therefore f(Dt) converges as well with probability one."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "then showing that the solution is uniformly Lipschitz with respect to these parameters, borrowing some ideas from perturbation theory ( Bonnans & Shapiro, 1998 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 259
                            }
                        ],
                        "text": "In turn, this reduces to showing that Dt minimizes a parametrized quadratic function over C with parameters \n1At and 1Bt, tt then showing that the solution is uniformly Lipschitz with respect to these parameters, \nborrowing some ideas from perturbation theory (Bonnans &#38; Shapiro, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 52
                            }
                        ],
                        "text": "Then, a classical theorem from perturbation theory (Bon\u00adnans &#38; Shapiro, \n1998, Theorem 4.1) shows that l(x,D) is C1 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15254794,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6b5daecb4858cecb6203ad8a5f7cd39f275d4c45",
            "isKey": true,
            "numCitedBy": 297,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an overview of some recent, and significant, progress in the theory of optimization problems with perturbations. We put the emphasis on methods based on upper and lower estimates of the objective function of the perturbed problems. These methods allow one to compute expansions of the optimal value function and approximate optimal solutions in situations where the set of Lagrange multipliers is not a singleton, may be unbounded, or is even empty. We give rather complete results for nonlinear programming problems and describe some extensions of the method to more general problems. We illustrate the results by computing the equilibrium position of a chain that is almost vertical or horizontal."
            },
            "slug": "Optimization-Problems-with-Perturbations:-A-Guided-Bonnans-Shapiro",
            "title": {
                "fragments": [],
                "text": "Optimization Problems with Perturbations: A Guided Tour"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The emphasis on methods based on upper and lower estimates of the objective function of the perturbed problems allow one to compute expansions of the optimal value function and approximate optimal solutions in situations where the set of Lagrange multipliers is not a singleton, may be unbounded, or is even empty."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082299938"
                        ],
                        "name": "D. Tal",
                        "slug": "D.-Tal",
                        "structuredName": {
                            "firstName": "Doron",
                            "lastName": "Tal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 68
                            }
                        ],
                        "text": "25 \u00d7 10(6) patches from images in the Berkeley segmentation dataset (Martin et al., 2001), which is a standard image database; 10(6) of these are kept for training, and the rest for testing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1ed876196ec9733acb1daa6d65e35ff0414291",
            "isKey": false,
            "numCitedBy": 6039,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties."
            },
            "slug": "A-database-of-human-segmented-natural-images-and-to-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes is presented and an error measure is defined which quantifies the consistency between segmentations of differing granularities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "OnlineDictionaryLearningfor Sparse Coding Julien Mairal Francis Bach INRIA,1 45 rue d Ulm 75005 Paris, \nFrance JULIEN.MAIRAL@INRIA.FR FRANCIS.BACH@INRIA.FR Jean Ponce Ecole Normale Sup\u00b4erieure,1 45 rue d Ulm \n75005 Paris, France JEAN.PONCE@ENS.FR Guillermo Sapiro GUILLE@UMN.EDU University\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 137
                            }
                        ],
                        "text": "Introduction The linear decomposition of a signal using a few atoms of a learned dictionaryinstead \nof a prede.ned one based on wavelets (Mallat, 1999) for example has recently led to state-of-the-art \nresults for numerous low-level image pro\u00adcessing tasks such as denoising (Elad &#38; Aharon,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 147
                            }
                        ],
                        "text": "\u2026recently led to state-of-the-art \nresults for numerous low-level image pro\u00adcessing tasks such as denoising (Elad &#38; Aharon, 2006) as \nwell as higher-level tasks such as classi.cation (Raina et al., 2007; Mairal et al., 2009), showing that \nsparse learned models are well adapted to natural\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 96
                            }
                        ],
                        "text": "OnlineDictionaryLearningfor Sparse Coding Julien Mairal Francis Bach INRIA,1 45 rue d Ulm 75005 Paris, \nFrance JULIEN.MAIRAL@INRIA.FR FRANCIS.BACH@INRIA.FR Jean Ponce Ecole Normale Sup\u00b4erieure,1 45 rue d Ulm \n75005 Paris, France JEAN.PONCE@ENS.FR Guillermo Sapiro GUILLE@UMN.EDU University ofMinnesota -Department \nof Electrical and Computer Engineering, 200 Union Street SE,Minneapolis, USA Abstract Sparse coding that \nis, modelling data vectors as sparse linear combinations of basis elements is widely used in machine \nlearning, neuroscience, signal processing, and statistics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 79
                            }
                        ],
                        "text": "For natural images, \nprede.ned dictionaries based on various types of wavelets (Mallat, 1999) have been used for this task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122523543,
            "fieldsOfStudy": [
                "Geology",
                "Computer Science"
            ],
            "id": "d30e8f3e565d4a9df831875c383687507606d4f0",
            "isKey": true,
            "numCitedBy": 17949,
            "numCiting": 381,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-wavelet-tour-of-signal-processing-Mallat",
            "title": {
                "fragments": [],
                "text": "A wavelet tour of signal processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35237585"
                        ],
                        "name": "Wenjiang J. Fu",
                        "slug": "Wenjiang-J.-Fu",
                        "structuredName": {
                            "firstName": "Wenjiang",
                            "lastName": "Fu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenjiang J. Fu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 215
                            }
                        ],
                        "text": "Ajj (10) 1 dj . uj. max(||uj||2,1) 4: endfor 5: \nuntil convergence 6: Return D (updated dictionary). number of recent methods for solving this type of \nprob\u00adlems are based on coordinate descent with soft threshold\u00ading (Fu, 1998; Friedman et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 114
                            }
                        ],
                        "text": "number of recent methods for solving this type of problems are based on coordinate descent with soft thresholding (Fu, 1998; Friedman et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123095463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "619518eddc515e3cf306cd66fe3866cbf9843fc7",
            "isKey": false,
            "numCitedBy": 1074,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Bridge regression, a special family of penalized regressions of a penalty function \u03a3|\u03b2j|\u03b3 with \u03b3 \u2264 1, considered. A general approach to solve for the bridge estimator is developed. A new algorithm for the lasso (\u03b3 = 1) is obtained by studying the structure of the bridge estimators. The shrinkage parameter \u03b3 and the tuning parameter \u03bb are selected via generalized cross-validation (GCV). Comparison between the bridge model (\u03b3 \u2264 1) and several other shrinkage models, namely the ordinary least squares regression (\u03bb = 0), the lasso (\u03b3 = 1) and ridge regression (\u03b3 = 2), is made through a simulation study. It is shown that the bridge regression performs well compared to the lasso and ridge regression. These methods are demonstrated through an analysis of a prostate cancer data. Some computational advantages and limitations are discussed."
            },
            "slug": "Penalized-Regressions:-The-Bridge-versus-the-Lasso-Fu",
            "title": {
                "fragments": [],
                "text": "Penalized Regressions: The Bridge versus the Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that the bridge regression performs well compared to the lasso and ridge regression, and is demonstrated through an analysis of a prostate cancer data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145050324"
                        ],
                        "name": "G. Yin",
                        "slug": "G.-Yin",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Yin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117627119,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f9ba4779af6563ad71c06db968a59a8aca00ef45",
            "isKey": false,
            "numCitedBy": 2104,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction 1 Review of Continuous Time Models 1.1 Martingales and Martingale Inequalities 1.2 Stochastic Integration 1.3 Stochastic Differential Equations: Diffusions 1.4 Reflected Diffusions 1.5 Processes with Jumps 2 Controlled Markov Chains 2.1 Recursive Equations for the Cost 2.2 Optimal Stopping Problems 2.3 Discounted Cost 2.4 Control to a Target Set and Contraction Mappings 2.5 Finite Time Control Problems 3 Dynamic Programming Equations 3.1 Functionals of Uncontrolled Processes 3.2 The Optimal Stopping Problem 3.3 Control Until a Target Set Is Reached 3.4 A Discounted Problem with a Target Set and Reflection 3.5 Average Cost Per Unit Time 4 Markov Chain Approximation Method: Introduction 4.1 Markov Chain Approximation 4.2 Continuous Time Interpolation 4.3 A Markov Chain Interpolation 4.4 A Random Walk Approximation 4.5 A Deterministic Discounted Problem 4.6 Deterministic Relaxed Controls 5 Construction of the Approximating Markov Chains 5.1 One Dimensional Examples 5.2 Numerical Simplifications 5.3 The General Finite Difference Method 5.4 A Direct Construction 5.5 Variable Grids 5.6 Jump Diffusion Processes 5.7 Reflecting Boundaries 5.8 Dynamic Programming Equations 5.9 Controlled and State Dependent Variance 6 Computational Methods for Controlled Markov Chains 6.1 The Problem Formulation 6.2 Classical Iterative Methods 6.3 Error Bounds 6.4 Accelerated Jacobi and Gauss-Seidel Methods 6.5 Domain Decomposition 6.6 Coarse Grid-Fine Grid Solutions 6.7 A Multigrid Method 6.8 Linear Programming 7 The Ergodic Cost Problem: Formulation and Algorithms 7.1 Formulation of the Control Problem 7.2 A Jacobi Type Iteration 7.3 Approximation in Policy Space 7.4 Numerical Methods 7.5 The Control Problem 7.6 The Interpolated Process 7.7 Computations 7.8 Boundary Costs and Controls 8 Heavy Traffic and Singular Control 8.1 Motivating Examples &nb"
            },
            "slug": "Stochastic-Approximation-and-Recursive-Algorithms-Kushner-Yin",
            "title": {
                "fragments": [],
                "text": "Stochastic Approximation and Recursive Algorithms and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705817"
                        ],
                        "name": "J. Borwein",
                        "slug": "J.-Borwein",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Borwein",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borwein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32238160"
                        ],
                        "name": "A. Lewis",
                        "slug": "A.-Lewis",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Lewis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117765424,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "efe5519409c823737c75e0826ae00efa325f7721",
            "isKey": false,
            "numCitedBy": 979,
            "numCiting": 180,
            "paperAbstract": {
                "fragments": [],
                "text": "Background * Inequality constraints * Fenchel duality * Convex analysis * Special cases * Nonsmooth optimization * The Karush-Kuhn-Tucker Theorem * Fixed points * Postscript: infinite versus finite dimensions * List of results and notation."
            },
            "slug": "Convex-analysis-and-nonlinear-optimization-:-theory-Borwein-Lewis",
            "title": {
                "fragments": [],
                "text": "Convex analysis and nonlinear optimization : theory and examples"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "Since this convex optimization problem admits separable constraints in the updated blocks (columns), convergence to a global optimum is guaranteed (Bertsekas, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 263
                            }
                        ],
                        "text": "\u2026a.s.  Proof sktech: The .rst step in the proof is to show that O 1 Dt - Dt-1 \n= which, although it does not ensure t the convergence of Dt, ensures the convergence of the se\u00ad 8 ries \n||Dt - Dt-1||2 , a classical condition in gradi\u00ad t=1 F ent descent convergence proofs (Bertsekas, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 183
                            }
                        ],
                        "text": "which, although it does not ensure the convergence of Dt, ensures the convergence of the series \u2211\u221e t=1 ||Dt \u2212Dt\u22121||(2)F , a classical condition in gradient descent convergence proofs (Bertsekas, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 151
                            }
                        ],
                        "text": "Since this convex optimization problem \nad\u00ad j mits separable constraints in the updated blocks (columns), convergence to a global optimum is \nguaranteed (Bertsekas, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": true,
            "numCitedBy": 6359,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Bottou \nand Bousquet (2008) have further shown both the\u00adoretically and experimentally that stochastic gradient \nalgo\u00adrithms, whose rate of convergence is not good in conven\u00adtional optimization terms, may infact in \ncertain settings be the fastest in reaching a solution with low expected cost."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 18
                            }
                        ],
                        "text": "As pointed out by Bottou and Bousquet (2008), however, one is usually not interested in a perfect minimization \nof 2The lp norm of a vector x in Rm is de.ned, for p = 1, by ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 18
                            }
                        ],
                        "text": "As pointed out by Bottou and Bousquet (2008), however, one is usually not interested in a perfect minimization of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 203
                            }
                        ],
                        "text": "In this situation, the same data points may be examined several times, and it is very common in on\u00adline \nalgorithms to simulate an i.i.d. sampling of p(x) by cycling over a randomly permuted training set (Bottou \n&#38; Bousquet, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The tradeoffs of large"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 117
                            }
                        ],
                        "text": "In this setting, online techniques based on stochastic approximations are an attractive alternative to batch methods (Bottou, 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "The main tools used in our proofs arethe \nconvergenceof empiricalprocesses (VanderVaart, 1998) and, following Bottou (1998), the convergence of \nquasi-martingales (Fisk, 1965)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "In this setting, online tech\u00adniques based on \nstochastic approximations are an attractive alternative to batch methods (Bottou, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 29
                            }
                        ],
                        "text": "At this point, and following \nBottou (1998), proving the conver\u00adgence of the sequence f t(Dt)amounts to showing that the stochastic \npositive process . ut = f t(Dt)= 0, (12) is a quasi-martingale."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Online algorithms and stochastic ap"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 138
                            }
                        ],
                        "text": "For example, \n.rst-order stochastic gradient descent with projections on the constraint set is sometimes used for dictionary \nlearn\u00ading (see Aharon and Elad (2008) for instance)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 107
                            }
                        ],
                        "text": "In the case of dictionary learning, classical projected firstorder stochastic gradient descent (as used by Aharon and Elad (2008) for instance) consists of a sequence of updates ofD:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 139
                            }
                        ],
                        "text": "For example, first-order stochastic gradient descent with projections on the constraint set is sometimes used for dictionary learning (see Aharon and Elad (2008) for instance)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "In the case of dictionary learning, classical projected .rst\u00adorder stochastic gradient descent (as used \nby Aharon and Elad (2008) for instance) consists of a sequence of updates of D: [] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sparse and redundant"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 259
                            }
                        ],
                        "text": "In turn, this reduces to showing that Dt minimizes a parametrized quadratic function over C with parameters \n1At and 1Bt, tt then showing that the solution is uniformly Lipschitz with respect to these parameters, \nborrowing some ideas from perturbation theory (Bonnans &#38; Shapiro, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 52
                            }
                        ],
                        "text": "Then, a classical theorem from perturbation theory (Bon\u00adnans &#38; Shapiro, \n1998, Theorem 4.1) shows that l(x,D) is C1 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization problems with perturbation: A guided tour"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Review"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121570279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c7c5595dc7a1f5d360acf5c360ca1ca49536ba5",
            "isKey": false,
            "numCitedBy": 7890,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates."
            },
            "slug": "Least-angle-regression-Efron-Hastie",
            "title": {
                "fragments": [],
                "text": "Least angle regression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "It is also possible \nto enforce this condition using an elastic net penalization (Zou &#38; Hastie, 2005), replacing ||a||1 \nby .2 ||a||1 + ||a||2 and thus improving the numerical stabil\u00ad 22 ity of homotopyalgorithms such as LARS."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularization and variable"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization problems with perturbation: A guided tour"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Review,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The KSVD : An algorithm for designing of overcomplete dictionaries for sparse representations"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans ."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Online algorithms and stochastic approximations"
            },
            "venue": {
                "fragments": [],
                "text": "Online learning and neural networks"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 43
                            }
                        ],
                        "text": "Using some results \non empirical pro\u00adcesses (Van der Vaart, 1998, Chap."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Asymptotic Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Asymptotic Statistics"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 85
                            }
                        ],
                        "text": "Problem Statement Classical dictionary \nlearning techniques (Olshausen &#38; Field, 1997; Aharon et al., 2006; Lee et al., 2007) consider a .nite \ntraining set of signals X =[x1,..., xn]in Rm\u00d7n and optimize the empirical cost function n n ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "Most recent algorithms for dictionary learning (Olshausen &#38; Field, \n1997; Aharon et al., 2006; Lee et al., 2007) are second-order iterative batch procedures, accessing the \nwhole training set at each iteration in order to minimize a cost function under some constraints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "\u2026\nproblem is to alter\u00adnate between the two variables, minimizing over one while keeping the other one .xed, \nas proposed by Lee et al. (2007) (see also Aharon et al. (2006), who use l0 rather than l1 penalties, \nfor related approaches).3 Since the computation of a dominates the cost of each\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The KSVD : An algorithm for designing of overcomplete dictionaries for sparse representations"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions Signal Processing"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Online algorithms and stochastic approximations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 215
                            }
                        ],
                        "text": "Ajj (10) 1 dj . uj. max(||uj||2,1) 4: endfor 5: \nuntil convergence 6: Return D (updated dictionary). number of recent methods for solving this type of \nprob\u00adlems are based on coordinate descent with soft threshold\u00ading (Fu, 1998; Friedman et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 114
                            }
                        ],
                        "text": "number of recent methods for solving this type of problems are based on coordinate descent with soft thresholding (Fu, 1998; Friedman et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Penalized Regressions: The Bridge Ver"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 77
                            }
                        ],
                        "text": "To do so, denoting by Ft the .ltra\u00adtion \nof the past information, a theorem by Fisk (1965) states 8 that if the positive sum E[max(E[ut+1 - ut|Ft],0)] \nt=1 converges, then ut is a quasi-martingale which converges with probability one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 162
                            }
                        ],
                        "text": "The main tools used in our proofs arethe \nconvergenceof empiricalprocesses (VanderVaart, 1998) and, following Bottou (1998), the convergence of \nquasi-martingales (Fisk, 1965)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 166
                            }
                        ],
                        "text": "The main tools used in our proofs are the convergence of empirical processes (Van der Vaart, 1998) and, following Bottou (1998), the convergence of quasi-martingales (Fisk, 1965)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quasi-martingale"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the American Mathematical Society."
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 132
                            }
                        ],
                        "text": "However, learning the dictionary instead of using off-the-shelf bases has been shown to dra\u00admatically \nimprove signal reconstruction (Elad &#38; Aharon, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026on wavelets (Mallat, 1999) for example has recently led to state-of-the-art \nresults for numerous low-level image pro\u00adcessing tasks such as denoising (Elad &#38; Aharon, 2006) as \nwell as higher-level tasks such as classi.cation (Raina et al., 2007; Mairal et al., 2009), showing that \nsparse\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image denoising via sparse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 51
                            }
                        ],
                        "text": "Using some results \non empirical pro\u00adcesses (Van der Vaart, 1998, Chap."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Asymptotic Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Cambridge University Press."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization problems with perturbation : A guided tour"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Review"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 77
                            }
                        ],
                        "text": "To do so, denoting by Ft the .ltra\u00adtion \nof the past information, a theorem by Fisk (1965) states 8 that if the positive sum E[max(E[ut+1 - ut|Ft],0)] \nt=1 converges, then ut is a quasi-martingale which converges with probability one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 162
                            }
                        ],
                        "text": "The main tools used in our proofs arethe \nconvergenceof empiricalprocesses (VanderVaart, 1998) and, following Bottou (1998), the convergence of \nquasi-martingales (Fisk, 1965)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quasi-martingale. Transactions of the"
            },
            "venue": {
                "fragments": [],
                "text": "Quasi-martingale. Transactions of the"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "This matrix is thus invertible and classical results (Fuchs, 2005) ensure the uniqueness of the sparse coding solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 56
                            }
                        ],
                        "text": "This matrix . is thus invertible and classical results (Fuchs, 2005) \nensure the uniqueness of the sparse coding solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recovery of exact sparse representations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 18,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 46,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Online-dictionary-learning-for-sparse-coding-Mairal-Bach/cf80cc34528273d8fbe17783efe802a6509e1562?sort=total-citations"
}