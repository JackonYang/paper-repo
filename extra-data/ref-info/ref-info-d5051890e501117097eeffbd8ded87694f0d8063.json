{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120654716,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5f75d859e750961d1d094f166fc3b564d9cfe99b",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The report presents classes of prior distributions for which the Bayes' estimate of an unknown function given certain observations is a spline function. (Author)"
            },
            "slug": "A-Correspondence-Between-Bayesian-Estimation-on-and-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 201
                            }
                        ],
                        "text": "To generalize the SV algorithm to that case, an analog of the soft margin is constructed in the space of the target values y (note that we now have y \u2208 R) by using Vapnik\u2019s \u03b5-insensitive loss function [35] (Figure 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 36
                            }
                        ],
                        "text": "26), one introduces slack variables [9, 35, 28]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "Statistical learning theory (Chapter ??, [39, 34, 35, 12, 36, 3]), or VC (VapnikChervonenkis) theory, shows that it is imperative to restrict the set of functions that f is chosen from to one which has a capacity that is suitable for the amount Capacity of available training data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "The top and the bottom lines indicate places where it takes the value 1, as enforced by the separation constraints (from [26])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9756494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55590f229e23a8e67af7d6d36f7456a595c251d1",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Developed only recently, support vector learning machines achieve high generalization ability by minimizing a bound on the expected test error; however, so far there existed no way of adding knowledge about invariances of a classification problem at hand. We present a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary."
            },
            "slug": "Incorporating-Invariances-in-Support-Vector-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Incorporating Invariances in Support Vector Learning Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work presents a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16697538"
                        ],
                        "name": "N. Aronszajn",
                        "slug": "N.-Aronszajn",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Aronszajn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Aronszajn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 33
                            }
                        ],
                        "text": "The Feature Space for PD Kernels [4, 2, 22] \u2022 define a feature map \u03a6 : X \u2192 R x 7\u2192 k(."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Learning with Kernels Support Vector Machines, Regularization, Optimization, and Beyond\nBernhard Scholkopf Alexander J. Smola\nThe MIT Press Cambridge, Massachusetts London, England\nContents\nSeries Foreword\nPreface\nxm\nxv\n1 A Tutorial Introduction 1 1.1 Data Representation and Similarity 1 1.2 A Simple Pattern Recognition Algorithm 4 1.3 Some Insights From Statistical Learning Theory 6 1.4 Hyperplane Classifiers 11 1.5 Support Vector Classification 15 1.6 Support Vector Regression 17 1.7 Kernel Principal Component Analysis 19 1.8 Empirical Results and Implementations 21\nI CONCEPTS AND TOOLS 23\nKernels 25 2.1 Product Features 26 2.2 The Representation of Similarities in Linear Spaces 29 2.3 2.4 2.5 2.6 Examples and Properties of Kernels 45 The Representation of Dissimilarities in Linear Spaces 48 Summary 55 Problems 55\nRisk and Loss Functions 61 3.1 Loss Functions 62 3.2 Test Error and Expected Risk 65 3.3 A Statistical Perspective 68 3.4 Robust Estimators 75 3.5 Summary 83 3.6 Problems 84\nRegularization 4.1 The Regularized Risk Functional\n87\nvui Contents\n4.2 The Representer Theorem 89 4.3 Regularization Operators 92 4.4 Translation Invariant Kernels 96 4.5 Translation Invariant Kernels in Higher Dimensions 105 4.6 Dot Product Kernels 110 4.7 Multi-Output Regularization 113 4.8 Semiparametric Regularization 115 4.9 Coefficient Based Regularization 118 4.10 Summary 121 4.11 Problems 122\nElements of Statistical Learning Theory 125 5.1 Introduction 125 5.2 The Law of Large Numbers 128 5.3 When Does Learning Work: the Question of Consistency 131 5.4 Uniform Convergence and Consistency 131 5.5 How to Derive a VC Bound 134 5.6 A Model Selection Example 144 5.7 Summary 146 5.8 Problems 146\nOptimization 149 6.1 Convex Optimization 150 6.2 Unconstrained Problems 154 6.3 Constrained Problems 165 6.4 Interior Point Methods 175 6.5 Maximum Search Problems 179 6.6 Summary 183 6.7 Problems 184\nII SUPPORT VECTOR MACHINES 187\nPattern Recognition 189 7.1 Separating Hyperplanes 189 7.2 The Role of the Margin 192 7.3 Optimal Margin Hyperplanes 196 7.4 Nonlinear Support Vector Classifiers 200 7.5 Soft Margin Hyperplanes 204 7.6 Multi-Class Classification 211 7.7 Variations on a Theme 214 7.8 Experiments 215 7.9 Summary 222 7.10 Problems 222\nContents\n8 Single-Class Problems: Quantile Estimation and Novelty Detection 227 8.1 Introduction 228 8.2 A Distribution's Support and Quantiles 229 8.3 Algorithms 230 8.4 Optimization 234 8.5 Theory 236 8.6 Discussion 241 8.7 Experiments 243 8.8 Summary 247 8.9 Problems 248\n9 Regression Estimation 251 9.1 Linear Regression with Insensitive Loss Function 251 9.2 Dual Problems 254 9.3 I/-SV Regression 260 9.4 Convex Combinations and ^i-Norms 266 9.5 Parametric Insensitivity Models 269 9.6 Applications 272 9.7 Summary 273 9.8 Problems 274\n10 Implementation 279 10.1 Tricks of the Trade 281 10.2 Sparse Greedy Matrix Approximation 288 10.3 Interior Point Algorithms 295 10.4 Subset Selection Methods 300 10.5 Sequential Minimal Optimization 305 10.6 Iterative Methods 312 10.7 Summary 327 10.8 Problems 329\n11 Incorporating Invariances 333 11.1 Prior Knowledge 333 11.2 Transformation Invariance 335 11.3 The Virtual SV Method 337 11.4 Constructing Invariance Kernels 343 11.5 The Jittered SV Method 354 11.6 Summary 356 11.7 Problems 357\n12 Learning Theory Revisited 359 12.1 Concentration of Measure Inequalities 360 12.2 Leave-One-Out Estimates 366 12.3 PAC-Bayesian Bounds 381 12.4 Operator-Theoretic Methods in Learning Theory 391\nContents\n12.5 Summary 403 12.6 Problems 404\nIII KERNEL METHODS 405\n13 Designing Kernels 407 13.1 Tricks for Constructing Kernels 408 13.2 String Kernels 412 13.3 Locality-Improved Kernels 414 13.4 Natural Kernels 418 13.5 Summary 423 13.6 Problems 423\n14 Kernel Feature Extraction 427 14.1 Introduction 427 14.2 Kernel PCA 429 14.3 Kernel PCA Experiments 437 14.4 A Framework for Feature Extraction 442 14.5 Algorithms for Sparse KFA 447 14.6 KFA Experiments 450 14.7 Summary 451 14.8 Problems 452\n15 Kernel Fisher Discriminant 457 15.1 Introduction 457 15.2 Fisher's Discriminant in Feature Space 458 15.3 Efficient Training of Kernel Fisher Discriminants 460 15.4 Probabilistic Outputs 464 15.5 Experiments 466 15.6 Summary 467 15.7 Problems 468\n16 Bayesian Kernel Methods 469 16.1 Bayesics 470 16.2 Inference Methods 475 16.3 Gaussian Processes 480 16.4 Implementation of Gaussian Processes 488 16.5 Laplacian Processes 499 16.6 Relevance Vector Machines \u2022 ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 54040858,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "fe697b4e2cb4c132da39aed8b8266a0e6113f9f2",
            "isKey": true,
            "numCitedBy": 5083,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie generale de noyaux reproduisants-Premiere partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we outlined the plan of papers which were to follow. In the meantime, however, the general theory has been developed in many directions, and our original plans have had to be changed. Due to wartime conditions we were not able, at the time of writing the first paper, to take into account all the earlier investigations which, although sometimes of quite a different character, were, nevertheless, related to our subject. Our investigation is concerned with kernels of a special type which have been used under different names and in different ways in many domains of mathematical research. We shall therefore begin our present paper with a short historical introduction in which we shall attempt to indicate the different manners in which these kernels have been used by various investigators, and to clarify the terminology. We shall also discuss the more important trends of the application of these kernels without attempting, however, a complete bibliography of the subject matter. (KAR) P. 2"
            },
            "slug": "Theory-of-Reproducing-Kernels.-Aronszajn",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35224059"
                        ],
                        "name": "S. Golowich",
                        "slug": "S.-Golowich",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Golowich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Golowich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Moreover, the SV method was applied to the solution of inverse function estimation problems ([40]; cf."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19196574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43ffa2c1a06a76e58a333f2e7d0bd498b24365ca",
            "isKey": false,
            "numCitedBy": 2602,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector (SV) method was recently proposed for estimating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presentation we report results of applying the SV method to these problems."
            },
            "slug": "Support-Vector-Method-for-Function-Approximation,-Vapnik-Golowich",
            "title": {
                "fragments": [],
                "text": "Support Vector Method for Function Approximation, Regression Estimation and Signal Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This presentation reports results of applying the Support Vector method to problems of estimating regressions, constructing multidimensional splines, and solving linear operator equations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 70866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2d13bc44e15fd93480e16305d37c025bc0818c2",
            "isKey": false,
            "numCitedBy": 1275,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples \u2013 in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case."
            },
            "slug": "Regularization-Networks-and-Support-Vector-Machines-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "Regularization Networks and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Both formulations of regularization and Support Vector Machines are reviewed in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics."
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput. Math."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125657"
                        ],
                        "name": "Phil Knirsch",
                        "slug": "Phil-Knirsch",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Knirsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phil Knirsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14669541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6a3e0028d99439ce2741d0e147b6e9a34bc4267",
            "isKey": false,
            "numCitedBy": 1231,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper collects some ideas targeted at advancing our understanding of the feature spaces associated with support vector (SV) kernel functions. We first discuss the geometry of feature space. In particular, we review what is known about the shape of the image of input space under the feature space map, and how this influences the capacity of SV methods. Following this, we describe how the metric governing the intrinsic geometry of the mapped surface can be computed in terms of the kernel, using the example of the class of inhomogeneous polynomial kernels, which are often used in SV pattern recognition. We then discuss the connection between feature space and input space by dealing with the question of how one can, given some vector in feature space, find a preimage (exact or approximate) in input space. We describe algorithms to tackle this issue, and show their utility in two applications of kernel methods. First, we use it to reduce the computational complexity of SV decision functions; second, we combine it with the Kernel PCA algorithm, thereby constructing a nonlinear statistical denoising technique which is shown to perform well on real-world data."
            },
            "slug": "Input-space-versus-feature-space-in-kernel-based-Sch\u00f6lkopf-Mika",
            "title": {
                "fragments": [],
                "text": "Input space versus feature space in kernel-based methods"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The geometry of feature space is reviewed, and the connection between feature space and input space is discussed by dealing with the question of how one can, given some vector in feature space, find a preimage in input space."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 334,
                                "start": 330
                            }
                        ],
                        "text": "are pd kernels, then so are \u2022 \u03b1k1, provided \u03b1 \u2265 0 \u2022 k1 + k2 \u2022 k1 \u00b7 k2 \u2022 k(x, x\u2032) := limn\u2192\u221e kn(x, x\u2032), provided it exists \u2022 k(A,B) := \u2211 x\u2208A,x\u2032\u2208B k1(x, x \u2032), where A,B are finite subsets of X (using the feature map \u03a6\u0303(A) := \u2211 x\u2208A\u03a6(x)) Further operations to construct kernels from kernels: tensor products, direct sums, convolutions [15]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17702358,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac",
            "isKey": false,
            "numCitedBy": 1371,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs. The method can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set. The family of kernels generated generalizes the family of radial basis kernels. It can also be used to deene kernels in the form of joint Gibbs probability distributions. Kernels can be built from hidden Markov random elds, generalized regular expressions, pair-HMMs, or ANOVA de-compositions. Uses of the method lead to open problems involving the theory of innnitely divisible positive deenite functions. Fundamentals of this theory and the theory of reproducing kernel Hilbert spaces are reviewed and applied in establishing the validity of the method."
            },
            "slug": "Convolution-kernels-on-discrete-structures-Haussler",
            "title": {
                "fragments": [],
                "text": "Convolution kernels on discrete structures"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs is introduced, which can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "For instance, more general loss functions can be used for \u03be, leading to problems that can still be solved efficiently [31, 29], cf."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11652139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0366ce5be03f003f8b28078f8e154a79baa80987",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. We present a kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion. Adopting a regularization-theoretic framework, the above are formulated as constrained optimization problems. Previous approaches such as ridge regression, support vector methods, and regularization networks are included as special cases. We show connections between the cost function and some properties up to now believed to apply to support vector machines only. For appropriately chosen cost functions, the optimal solution of all the problems described above can be found by solving a simple quadratic programming problem."
            },
            "slug": "On-a-Kernel-Based-Method-for-Pattern-Recognition,-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "On a Kernel-Based Method for Pattern Recognition, Regression, Approximation, and Operator Inversion"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion is presented, adopting a regularization-theoretic framework."
            },
            "venue": {
                "fragments": [],
                "text": "Algorithmica"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 36
                            }
                        ],
                        "text": "26), one introduces slack variables [9, 35, 28]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 207673395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d73c0d0c92446102fdb6cc728b5d69674a1a387",
            "isKey": false,
            "numCitedBy": 2613,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of , and report experimental results."
            },
            "slug": "New-Support-Vector-Algorithms-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "New Support Vector Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new class of support vector algorithms for regression and classification that eliminates one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "With the above example in mind, let us now consider the problem of pattern recognition in a slightly more formal setting [34, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "For instance, more general loss functions can be used for \u03be, leading to problems that can still be solved efficiently [31, 29], cf."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 601110,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8985a9637540daa0b7b8295f8a5bbda3a3be1dea",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-connection-between-regularization-operators-and-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "The connection between regularization operators and support vector kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 46
                            }
                        ],
                        "text": "Remark ??), the function k is called a kernel [19, 1, 5, 6, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12154028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7020c2455b74deda5af696248cc41c53e32c00e2",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Large VC-dimension classifiers can learn difficult tasks, but are usually impractical because they generalize well only if they are trained with huge quantities of data. In this paper we show that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension. This is achieved with a maximum margin algorithm (the Generalized Portrait). The technique is applicable to a wide variety of classifiers, including Perceptrons, polynomial classifiers (sigma-pi unit networks) and Radial Basis Functions. The effective number of parameters is adjusted automatically by the training algorithm to match the complexity of the problem. It is shown to equal the number of those training patterns which are closest patterns to the decision boundary (supporting patterns). Bounds on the generalization error and the speed of convergence of the algorithm are given. Experimental results on handwritten digit recognition demonstrate good generalization compared to other algorithms."
            },
            "slug": "Automatic-Capacity-Tuning-of-Very-Large-Classifiers-Guyon-Boser",
            "title": {
                "fragments": [],
                "text": "Automatic Capacity Tuning of Very Large VC-Dimension Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "Statistical learning theory (Chapter ??, [39, 34, 35, 12, 36, 3]), or VC (VapnikChervonenkis) theory, shows that it is imperative to restrict the set of functions that f is chosen from to one which has a capacity that is suitable for the amount Capacity of available training data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 52
                            }
                        ],
                        "text": "For overviews, the interested reader is referred to [7, 27, 30, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54174771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c900c66310a29bdb771270bb22440a4cf42958cb",
            "isKey": false,
            "numCitedBy": 1051,
            "numCiting": 265,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThe concept of large margins is a unifying principle for the analysis of many different approaches to the classification of data from examples, including boosting, mathematical programming, neural networks, and support vector machines. The fact that it is the margin, or confidence level, of a classification--that is, a scale parameter--rather than a raw training error that matters has become a key tool for dealing with classifiers. This book shows how this idea applies to both the theoretical analysis and the design of algorithms. \nThe book provides an overview of recent developments in large margin classifiers, examines connections with other methods (e.g., Bayesian inference), and identifies strengths and weaknesses of the method, as well as directions for future research. Among the contributors are Manfred Opper, Vladimir Vapnik, and Grace Wahba."
            },
            "slug": "Advances-in-Large-Margin-Classifiers-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Advances in Large Margin Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This book provides an overview of recent developments in large margin classifiers, examines connections with other methods, and identifies strengths and weaknesses of the method, as well as directions for future research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 590725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "caa4c10d496839702fd7089f1b5a3d47ef9fa5c8",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave-one-out error [Jaakkola and Haussler, 1999] proved for Support Vector Machines (SVMs) [Vapnik, 1995; 1998]. The new approach directly minimizes the expression given by the bound in an attempt to minimize leave-one-out error. This gives a convex optimization problem which constructs a sparse linear classifier in feature space using the kernel technique. As such the algorithm possesses many of the same properties as SVMs. The main novelty of the algorithm is that apart from the choice of kernel, it is parameterless - the selection of the number of training errors is inherent in the algorithm and not chosen by an extra free parameter as in SVMs. First experiments using the method on benchmark datasets from the UCI repository show results similar to SVMs which have been tuned to have the best choice of parameter."
            },
            "slug": "Leave-One-Out-Support-Vector-Machines-Weston",
            "title": {
                "fragments": [],
                "text": "Leave-One-Out Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The main novelty of the algorithm is that apart from the choice of kernel, it is parameterless - the selection of the number of training errors is inherent in the algorithm and not chosen by an extra free parameter as in SVMs."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 46
                            }
                        ],
                        "text": "Remark ??), the function k is called a kernel [19, 1, 5, 6, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "This substitution, which is sometimes referred to as the kernel trick, was used by Boser, Guyon, and Vapnik [6] to extend the Generalized Portrait hyperplane classifier of Vapnik and co-workers [41, 39] to nonlinear Support Vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10837,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722397"
                        ],
                        "name": "W. DuMouchel",
                        "slug": "W.-DuMouchel",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "DuMouchel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. DuMouchel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3146362"
                        ],
                        "name": "C. Volinsky",
                        "slug": "C.-Volinsky",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Volinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Volinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145633895"
                        ],
                        "name": "T. Johnson",
                        "slug": "T.-Johnson",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3027365"
                        ],
                        "name": "D. Pregibon",
                        "slug": "D.-Pregibon",
                        "structuredName": {
                            "firstName": "Daryl",
                            "lastName": "Pregibon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pregibon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "[3, 34]) \u2022X training set, Dirac measures pi = \u03b4xi: dataset squashing, [10] \u2022X test set, Dirac measures pi = \u03b4yi centered on the training points Y : covariate shift correction [16] B."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15625471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f69f87061959a8b6fec180096b440b65eb13c64a",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A feature of data mining that distinguishes it from \u201cclassical\u201d machine learning (ML) and statistical modeling (SM) is scale. The community seems to agree on this yet progress to this point has been limited. We present a methodology that addresses scale in a novel fashion that has the potential for revolutionizing the field. While the methodology applies most directly to flat (row by column) data sets we believe that it can be adapted to other representations. Our approach to the problem is not to scale up individual ML and SM methods. Rather we prefer to leverage the entire collection of existing methods by scaling down the data set. We call the method squashing. Our method demonstrably outperforms random sampling and a theoretical argument suggests how and why it works well. Squashing consists of three modular steps: grouping, momentizing, and generating (GMG). These three steps describe the squashing pipeline whereby the original (very large data set) is sectioned off into mutually exclusive groups (or bins); within each group a series of low-order moments are computed; and finally these moments are passed off to a routine that generates pseudo data that accurately reproduce the moments. The result of the GMG squashing pipeline is a squashed data set that has the same structure as the original data with the addition of a weight for each pseudo data point that reflects the distribution of the original data into the initial groups. Any ML or SM method that accepts weights can be used to analyze the weighted pseudo data. By construction the resulting analyses will mimic the corresponding analyses on the original data set. Squashing should appeal to many of the sub-disciplines of"
            },
            "slug": "Squashing-flat-files-flatter-DuMouchel-Volinsky",
            "title": {
                "fragments": [],
                "text": "Squashing flat files flatter"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a methodology that addresses scale in a novel fashion that has the potential for revolutionizing the field and demonstrably outperforms random sampling and a theoretical argument suggests how and why it works well."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 52
                            }
                        ],
                        "text": "For overviews, the interested reader is referred to [7, 27, 30, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60486887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d86e239a9e9741f22be1d8c1feed7a44da1bdc1",
            "isKey": false,
            "numCitedBy": 3135,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. The book also introduces Bayesian analysis of learning and relates SVMs to Gaussian Processes and other kernel based learning methods. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc. Their first introduction in the early 1990s lead to a recent explosion of applications and deepening theoretical analysis, that has now established Support Vector Machines along with neural networks as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and application of these techniques. The concepts are introduced gradually in accessible and self-contained stages, though in each stage the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally the book will equip the practitioner to apply the techniques and an associated web site will provide pointers to updated literature, new applications, and on-line software."
            },
            "slug": "An-introduction-to-Support-Vector-Machines-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "An introduction to Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This book is the first comprehensive introduction to Support Vector Machines, a new generation learning system based on recent advances in statistical learning theory, and introduces Bayesian analysis of learning and relates SVMs to Gaussian Processes and other kernel based learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6082464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d27c7569fdbcbb57ff511f5293e32b547acca7b3",
            "isKey": false,
            "numCitedBy": 572,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995). SVM is a technique that can be derived from the structural risk minimization principle (Vapnik, 1982) and can be used to estimate the parameters of several different approximation schemes, including radial basis functions, algebraic and trigonometric polynomials, B-splines, and some forms of multilayer perceptrons. Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary). We show that if the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem. In the appendix, we present a derivation of the SVM technique in the framework of regularization theory, rather than statistical learning theory, establishing a connection between SVM, sparse approximation, and regularization theory."
            },
            "slug": "An-Equivalence-Between-Sparse-Approximation-and-Girosi",
            "title": {
                "fragments": [],
                "text": "An Equivalence Between Sparse Approximation and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "If the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70011052"
                        ],
                        "name": "Weston J Stitson Mo",
                        "slug": "Weston-J-Stitson-Mo",
                        "structuredName": {
                            "firstName": "Weston",
                            "lastName": "Stitson Mo",
                            "middleNames": [
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weston J Stitson Mo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69902577"
                        ],
                        "name": "Sch\u00f6lkopf B Bottou L",
                        "slug": "Sch\u00f6lkopf-B-Bottou-L",
                        "structuredName": {
                            "firstName": "Sch\u00f6lkopf",
                            "lastName": "Bottou L",
                            "middleNames": [
                                "B"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sch\u00f6lkopf B Bottou L"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "Among the optimizers used for SVMs are LOQO [33], MINOS [22], and variants of conjugate gradient descent, such as the optimizers of Bottou [25] and Burges [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60691110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d36f7543109a8c859d423ddb98bf6d2bd4e13d4d",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector Machine (SVM) is a new type of learning machine. The SVM is a general architecture that can be applied to pattern recognition, regression estimation and other problems. The following researchers were involved in the development of the SVM: The Support Vector Machine (SVM) program allows a user to carry out pattern recognition and regression estimation, using support vector techniques on some given data. If you have any questions not answered by the documentation, you can e-mail us at:"
            },
            "slug": "Support-Vector-Machine-Reference-Manual-Saunders-Mo",
            "title": {
                "fragments": [],
                "text": "Support Vector Machine Reference Manual"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The Support Vector Machine (SVM) program allows a user to carry out pattern recognition and regression estimation, using support vector techniques on some given data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734327"
                        ],
                        "name": "N. Alon",
                        "slug": "N.-Alon",
                        "structuredName": {
                            "firstName": "Noga",
                            "lastName": "Alon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Alon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409749316"
                        ],
                        "name": "S. Ben-David",
                        "slug": "S.-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 144
                            }
                        ],
                        "text": "Another interesting capacity measure, which can be thought of as a scale-sensitive version of the VC dimension, is the fat shattering dimension [17, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8347198,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a5a86656448540a91ec06dbe017231d16862a502",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Learnability in Valiant's PAC learning model has been shown to be strongly related to the existence of uniform laws of large numbers. These laws define a distribution-free convergence property of means to expectations uniformly over classes of random variables. Classes of real-valued functions enjoying such a property are also known as uniform Gliveako-Cantelli classes. In this paper we prove, through a generalization of Sauer's lemma that may be interesting in its own right, a new characterization of uniform Glivenko-Cantelli classes. Our characterization yields Dudley, Gine, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a simple combinatorial quantity generalizing the Vapnik-Chervonenkis dimension. We apply this result to characterize PAC learnability in the statistical regression framework of probabilistic concepts, solving an open problem posed by Kearns and Schapire. Our characterization shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class.<<ETX>>"
            },
            "slug": "Scale-sensitive-dimensions,-uniform-convergence,-Alon-Ben-David",
            "title": {
                "fragments": [],
                "text": "Scale-sensitive dimensions, uniform convergence, and learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A characterization of learnability in the probabilistic concept model, solving an open problem posed by Kearns and Schapire, and shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "With the above example in mind, let us now consider the problem of pattern recognition in a slightly more formal setting [34, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62359231,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f1277592f221ea26fa1d2321a38b64c58b33d75b",
            "isKey": false,
            "numCitedBy": 8009,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises."
            },
            "slug": "Introduction-to-Statistical-Pattern-Recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Introduction to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition, which is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3702,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 54
                            }
                        ],
                        "text": "For this case, decomposition algorithms were proposed [23, 24], based on the observation that not only can we leave out the non-SV examples (the xi with \u03b1i = 0) from the current chunk, but also some of the SVs, especially those that hit the upper boundary (\u03b1i = C)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": false,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 467,
                                "start": 451
                            }
                        ],
                        "text": "The Kernel Trick \u2014 Summary \u2022 any algorithm that only depends on dot products can benefit from the kernel trick \u2022 this way, we can apply linear methods to vectorial as well as non-vectorial data \u2022 think of the kernel as a nonlinear similarity measure \u2022 examples of common kernels: Polynomial k(x, x\u2032) = ( \u3008 x, x\u2032 \u3009 + c)d Sigmoid k(x, x\u2032) = tanh(\u03ba \u3008 x, x\u2032 \u3009 + \u0398) Gaussian k(x, x\u2032) = exp(\u2212\u2016x\u2212 x\u2032\u20162/(2\u03c32)) \u2022 Kernels are also known as covariance functions [35, 32, 36, 19] B."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121858740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e786caa59202d923ccaae00ae6a4682eec92699b",
            "isKey": false,
            "numCitedBy": 5071,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword 1. Background 2. More splines 3. Equivalence and perpendicularity, or, what's so special about splines? 4. Estimating the smoothing parameter 5. 'Confidence intervals' 6. Partial spline models 7. Finite dimensional approximating subspaces 8. Fredholm integral equations of the first kind 9. Further nonlinear generalizations 10. Additive and interaction splines 11. Numerical methods 12. Special topics Bibliography Author index."
            },
            "slug": "Spline-Models-for-Observational-Data-Wahba",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 52
                            }
                        ],
                        "text": "For overviews, the interested reader is referred to [7, 27, 30, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5544,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 467,
                                "start": 451
                            }
                        ],
                        "text": "The Kernel Trick \u2014 Summary \u2022 any algorithm that only depends on dot products can benefit from the kernel trick \u2022 this way, we can apply linear methods to vectorial as well as non-vectorial data \u2022 think of the kernel as a nonlinear similarity measure \u2022 examples of common kernels: Polynomial k(x, x\u2032) = ( \u3008 x, x\u2032 \u3009 + c)d Sigmoid k(x, x\u2032) = tanh(\u03ba \u3008 x, x\u2032 \u3009 + \u0398) Gaussian k(x, x\u2032) = exp(\u2212\u2016x\u2212 x\u2032\u20162/(2\u03c32)) \u2022 Kernels are also known as covariance functions [35, 32, 36, 19] B."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": ", \u03a9(\u2016f\u2016) = \u03bb\u2016f\u20162 \u2014 Gaussian process prior [36] with covariance function k \u2022 minimizer of (4) = MAP estimate Kernel PCA (see below) can be shown to correspond to the case of c((xi, yi, f (xi))i=1,."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6884486,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "28667c276ba78ab1d855064d5456d50d9932b775",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes. We start from Bayesian linear regression, and show how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on priors over parameters. This leads in to a more general discussion of Gaussian processes in section 4. Section 5 deals with further issues, including hierarchical modelling and the setting of the parameters that control the Gaussian process, the covariance functions for neural network models and the use of Gaussian processes in classification problems."
            },
            "slug": "Prediction-with-Gaussian-Processes:-From-Linear-to-Williams",
            "title": {
                "fragments": [],
                "text": "Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes, starting from Bayesian linear regression, and showing how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on prior over parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3463572"
                        ],
                        "name": "M. O. Stitson",
                        "slug": "M.-O.-Stitson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Stitson",
                            "middleNames": [
                                "Oliver"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O. Stitson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793317"
                        ],
                        "name": "A. Gammerman",
                        "slug": "A.-Gammerman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gammerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gammerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675281"
                        ],
                        "name": "V. Vovk",
                        "slug": "V.-Vovk",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vovk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vovk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Very good results were obtained on the Boston housing benchmark [32], and on problems of times series prediction (see [21, 20, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118468304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "221663c3ec94babfaa0754a00d92ff69e2a4424a",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines using ANOVA Decomposition Kernels (SVAD) [Vapng] are a way of imposing a structure on multi-dimensional kernels which are generated as the tensor product of one-dimensional kernels. This gives more accurate control over the capacity of the learning machine (VCdimension). SVAD uses ideas from ANOVA decomposition methods and extends them to generate kernels which directly implement these ideas. SVAD is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel. The Boston housing data set from UCI has been tested on Bagging [Bre94] and Support Vector methods before [DBK97] and these results are compared to the SVAD method."
            },
            "slug": "Support-vector-regression-with-ANOVA-decomposition-Stitson-Gammerman",
            "title": {
                "fragments": [],
                "text": "Support vector regression with ANOVA decomposition kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Support Vector Machines using ANOVA Decomposition Kernels (SVAD) is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Properties of Kernel Matrices, I [23] Suppose we are given distinct training patterns x1, ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30545896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "356125478f5d06b564b420755a4944254045bbbe",
            "isKey": false,
            "numCitedBy": 627,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology."
            },
            "slug": "Support-vector-learning-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This book provides a comprehensive analysis of what can be done using Support vector Machines, achieving record results in real-life pattern recognition problems, and proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which it is considered as the most natural and elegant way for generalization of classical Principal Component analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "Seen in this light, it is not too surprising that it is possible to give a mechanical interpretation of optimal margin hyperplanes [8]: If we assume that each SV xi exerts a perpendicular force of size \u03b1i and sign yi on a solid plane sheet lying along the hyperplane, then the solution satisfies the requirements of mechanical stability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "This way, SV machines soon became competitive with the best available classifiers on OCR and other object recognition tasks [8], and later even achieved the world record on the main handwritten digit benchmark dataset [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9434141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40e5a40ae66d44e6c00d562d068d35db6922715d",
            "isKey": false,
            "numCitedBy": 431,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Learning Machines (SVM) are finding application in pattern recognition, regression estimation, and operator inversion for ill-posed problems. Against this very general backdrop, any methods for improving the generalization performance, or for improving the speed in test phase, of SVMs are of increasing interest. In this paper we combine two such techniques on a pattern recognition problem. The method for improving generalization performance (the \"virtual support vector\" method) does so by incorporating known invariances of the problem. This method achieves a drop in the error rate on 10,000 NIST test digit images of 1.4% to 1.0%. The method for improving the speed (the \"reduced set\" method) does so by approximating the support vector decision surface. We apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine. The combined approach yields a machine which is both 22 times faster than the original machine, and which has better generalization performance, achieving 1.1 % error. The virtual support vector method is applicable to any SVM problem with known invariances. The reduced set method is applicable to any support vector machine."
            },
            "slug": "Improving-the-Accuracy-and-Speed-of-Support-Vector-Burges-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Improving the Accuracy and Speed of Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper combines two techniques for improving generalization performance and speed on a pattern recognition problem by incorporating known invariances of the problem, and applies the reduced set method, applicable to any support vector machine."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052797964"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 54
                            }
                        ],
                        "text": "For this case, decomposition algorithms were proposed [23, 24], based on the observation that not only can we leave out the non-SV examples (the xi with \u03b1i = 0) from the current chunk, but also some of the SVs, especially those that hit the upper boundary (\u03b1i = C)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5667586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a61a3bf41fc770186a58fa34466af337e997ef6",
            "isKey": false,
            "numCitedBy": 1234,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of training a support vector machine (SVM) on a very large database in the case in which the number of support vectors is also very large. Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small (<3,000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series database with 110,000 data points that generates 100,000 support vectors."
            },
            "slug": "An-improved-training-algorithm-for-support-vector-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "An improved training algorithm for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "i=1 (yi \u2212 f (xi))(2), and \u03a9(\u2016f\u2016) = \u03bb\u2016f\u20162 (\u03bb > 0): [18] \u2022 generalization to non-quadratic cost functions: [7] \u2022 present form: [25]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 121062339,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3065c5e37a0c1f1be365e88ddf2d5cd02faa5db1",
            "isKey": false,
            "numCitedBy": 1330,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-results-on-Tchebycheffian-spline-functions-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "Some results on Tchebycheffian spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2637505"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423299574"
                        ],
                        "name": "F. O\u2019Sullivan",
                        "slug": "F.-O\u2019Sullivan",
                        "structuredName": {
                            "firstName": "Finbarr",
                            "lastName": "O\u2019Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. O\u2019Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "i=1 (yi \u2212 f (xi))(2), and \u03a9(\u2016f\u2016) = \u03bb\u2016f\u20162 (\u03bb > 0): [18] \u2022 generalization to non-quadratic cost functions: [7] \u2022 present form: [25]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 120908503,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5bf249445466ac041694525d8969d7879cd259ce",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A general approach to the first order asymptotic analysis ofpenalized likelihood and related estimators is described. The method gives expansions for the systematic and random error. Asymptotic convergence rates in a family of spectral norms are obtained. The theory applies to a broad range of function estimation prob~erns including non\"paxametric. dellSity, hazard and generalized regression curve estimation. Some examples are provided. AMS 1980 subject classifications. Primary, 62-G05, Secondary, 62J05, 41-A35, 41-A25, 47-A53, 45-LlO, 45-M05."
            },
            "slug": "Asymptotic-Analysis-of-Penalized-Likelihood-and-Cox-O\u2019Sullivan",
            "title": {
                "fragments": [],
                "text": "Asymptotic Analysis of Penalized Likelihood and Related Estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "Very good results were obtained on the Boston housing benchmark [32], and on problems of times series prediction (see [21, 20, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5398743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f43840dc1638a18eb6178f1060dc5f41af1c5ac7",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines are used for time series prediction and compared to radial basis function networks. We make use of two different cost functions for Support Vectors: training with (i) an e insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models. Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D). In both cases Support Vector Machines show an excellent performance. In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 29%."
            },
            "slug": "Predicting-Time-Series-with-Support-Vector-Machines-M\u00fcller-Smola",
            "title": {
                "fragments": [],
                "text": "Predicting Time Series with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two different cost functions for Support Vectors are made use: training with an e insensitive loss and Huber's robust loss function and how to choose the regularization parameters in these models are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47312726"
                        ],
                        "name": "J. Mercer",
                        "slug": "J.-Mercer",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Mercer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "i=1 \u03bbi\u03c8i(x)\u03c8i(x \u2032) using eigenfunctions \u03c8i and eigenvalues \u03bbi \u2265 0 [20]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121070291,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b48694cb275eba60b48026f3159373c92c1b286c",
            "isKey": false,
            "numCitedBy": 1592,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The present memoir is the outcome of an attempt to obtain the conditions under which a given symmetric and continuous function k ( s, t ) is definite, in the sense of Hilbert. At an early stage, however, it was found that the class of definite functions was too restricted to allow the determination of necessary and sufficient conditions in terms of the determinants of \u00a7 10. The discovery that this could be done for functions of positive or negative type, and the fact that almost all the theorems which are true of definite functions are, with slight modification, true of these, led finally to the abandonment of the original plan in favour of a discussion of the properties of functions belonging to the wider classes. The first part of the memoir is devoted to the definition of various terms employed, and to the re-statement of the consequences which follow from Hilbert\u2019s theorem."
            },
            "slug": "Functions-of-Positive-and-Negative-Type,-and-their-Mercer",
            "title": {
                "fragments": [],
                "text": "Functions of Positive and Negative Type, and their Connection with the Theory of Integral Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1909
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545803"
                        ],
                        "name": "M. Aizerman",
                        "slug": "M.-Aizerman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Aizerman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aizerman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 46
                            }
                        ],
                        "text": "Remark ??), the function k is called a kernel [19, 1, 5, 6, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Aizerman et al [1] called H the linearization space, and used in the context of the potential function classification method to express the dot product between elements of H in terms of elements of the input space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60493317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93",
            "isKey": false,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-Foundations-of-the-Potential-Function-Aizerman",
            "title": {
                "fragments": [],
                "text": "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "Statistical learning theory (Chapter ??, [39, 34, 35, 12, 36, 3]), or VC (VapnikChervonenkis) theory, shows that it is imperative to restrict the set of functions that f is chosen from to one which has a capacity that is suitable for the amount Capacity of available training data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116929976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fcdee6c6d885ac2bd32e122dbf282f93720c22",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 557,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface * Introduction * The Bayes Error * Inequalities and alternatedistance measures * Linear discrimination * Nearest neighbor rules *Consistency * Slow rates of convergence Error estimation * The regularhistogram rule * Kernel rules Consistency of the k-nearest neighborrule * Vapnik-Chervonenkis theory * Combinatorial aspects of Vapnik-Chervonenkis theory * Lower bounds for empirical classifier selection* The maximum likelihood principle * Parametric classification *Generalized linear discrimination * Complexity regularization *Condensed and edited nearest neighbor rules * Tree classifiers * Data-dependent partitioning * Splitting the data * The resubstitutionestimate * Deleted estimates of the error probability * Automatickernel rules * Automatic nearest neighbor rules * Hypercubes anddiscrete spaces * Epsilon entropy and totally bounded sets * Uniformlaws of large numbers * Neural networks * Other error estimates *Feature extraction * Appendix * Notation * References * Index"
            },
            "slug": "A-Probabilistic-Theory-of-Pattern-Recognition-Devroye-Gy\u00f6rfi",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Theory of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Bayes Error and Vapnik-Chervonenkis theory are applied as guide for empirical classifier selection on the basis of explicit specification and explicit enforcement of the maximum likelihood principle."
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Modelling and Applied Probability"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5216020"
                        ],
                        "name": "M. Casdagli",
                        "slug": "M.-Casdagli",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Casdagli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Casdagli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "Very good results were obtained on the Boston housing benchmark [32], and on problems of times series prediction (see [21, 20, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122236599,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "be946457d3f880d9ec836aee3d0d231ffa3bcc9a",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-prediction-of-chaotic-time-series-Casdagli",
            "title": {
                "fragments": [],
                "text": "Nonlinear prediction of chaotic time series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986103"
                        ],
                        "name": "R. Vanderbei",
                        "slug": "R.-Vanderbei",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Vanderbei",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vanderbei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Among the optimizers used for SVMs are LOQO [33], MINOS [22], and variants of conjugate gradient descent, such as the optimizers of Bottou [25] and Burges [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119722553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "640bb6a37dd6fe0b209373de4d5e32011f22d35a",
            "isKey": false,
            "numCitedBy": 1208,
            "numCiting": 123,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. Part 1: Basic Theory - The Simplex Method and Duality. 1. Introduction. 2. The Simplex Method. 3. Degeneracy. 4. Efficiency of the Simplex Method. 5. Duality Theory. 6. The Simplex Method in Matrix Notation. 7. Sensitivity and Parametric Analyses. 8. Implementation Issues. 9. Problems in General Form. 10. Convex Analysis. 11. Game Theory. 12. Regression. Part 2: Network-Type Problems. 13. Network Flow Problems. 14. Applications. 15. Structural Optimization. Part 3: Interior-Point Methods. 16. The Central Path. 17. A Path-Following Method. 18. The KKT System. 19. Implementation Issues. 20. The Affine-Scaling Method. 21. The Homogeneous Self-Dual Method. Part 4: Extensions. 22. Integer Programming. 23. Quadratic Programming. 24. Convex Programming. Appendix A: Source Listings. Answers to Selected Exercises. Bibliography. Index."
            },
            "slug": "Linear-Programming:-Foundations-and-Extensions-Vanderbei",
            "title": {
                "fragments": [],
                "text": "Linear Programming: Foundations and Extensions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The Simplex Method in Matrix Notation and Duality Theory, and Applications: Foundations of Convex Programming."
            },
            "venue": {
                "fragments": [],
                "text": "Kluwer international series in operations research and management service"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47934783"
                        ],
                        "name": "J. Davenport",
                        "slug": "J.-Davenport",
                        "structuredName": {
                            "firstName": "Janina",
                            "lastName": "Davenport",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Davenport"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 467,
                                "start": 451
                            }
                        ],
                        "text": "The Kernel Trick \u2014 Summary \u2022 any algorithm that only depends on dot products can benefit from the kernel trick \u2022 this way, we can apply linear methods to vectorial as well as non-vectorial data \u2022 think of the kernel as a nonlinear similarity measure \u2022 examples of common kernels: Polynomial k(x, x\u2032) = ( \u3008 x, x\u2032 \u3009 + c)d Sigmoid k(x, x\u2032) = tanh(\u03ba \u3008 x, x\u2032 \u3009 + \u0398) Gaussian k(x, x\u2032) = exp(\u2212\u2016x\u2212 x\u2032\u20162/(2\u03c32)) \u2022 Kernels are also known as covariance functions [35, 32, 36, 19] B."
                    },
                    "intents": []
                }
            ],
            "corpusId": 220072940,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5864512d1e03cb9072ec623c688336f1d8b5e9a3",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Editor-Davenport",
            "title": {
                "fragments": [],
                "text": "Editor"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Dietetic Association"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2292635"
                        ],
                        "name": "R. Fortet",
                        "slug": "R.-Fortet",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Fortet",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fortet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102415007"
                        ],
                        "name": "E. Mourier",
                        "slug": "E.-Mourier",
                        "structuredName": {
                            "firstName": "Edith",
                            "lastName": "Mourier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mourier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "Theorem 4 [12, 9] p = q \u21d0\u21d2 sup f\u2208C(X) \u2223 Ex\u223cp(f (x)) \u2212 Ex\u223cq(f (x)) \u2223 \u2223 = 0, where C(X) is the space of continuous bounded functions on X."
                    },
                    "intents": []
                }
            ],
            "corpusId": 125138309,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ae3b54bd876cf66b0bc16b5dde04137aac5038be",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "\u00a9 Gauthier-Villars (\u00c9ditions scientifiques et m\u00e9dicales Elsevier), 1953, tous droits r\u00e9serv\u00e9s. L\u2019acc\u00e8s aux archives de la revue \u00ab Annales scientifiques de l\u2019\u00c9.N.S. \u00bb (http://www. elsevier.com/locate/ansens) implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/conditions). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright."
            },
            "slug": "Convergence-de-la-r\u00e9partition-empirique-vers-la-Fortet-Mourier",
            "title": {
                "fragments": [],
                "text": "Convergence de la r\u00e9partition empirique vers la r\u00e9partition th\u00e9orique"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2734323"
                        ],
                        "name": "Y. Sawano",
                        "slug": "Y.-Sawano",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Sawano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sawano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087748"
                        ],
                        "name": "S. Saitoh",
                        "slug": "S.-Saitoh",
                        "structuredName": {
                            "firstName": "Saburou",
                            "lastName": "Saitoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Saitoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 33
                            }
                        ],
                        "text": "The Feature Space for PD Kernels [4, 2, 22] \u2022 define a feature map \u03a6 : X \u2192 R x 7\u2192 k(."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Learning with Kernels Support Vector Machines, Regularization, Optimization, and Beyond\nBernhard Scholkopf Alexander J. Smola\nThe MIT Press Cambridge, Massachusetts London, England\nContents\nSeries Foreword\nPreface\nxm\nxv\n1 A Tutorial Introduction 1 1.1 Data Representation and Similarity 1 1.2 A Simple Pattern Recognition Algorithm 4 1.3 Some Insights From Statistical Learning Theory 6 1.4 Hyperplane Classifiers 11 1.5 Support Vector Classification 15 1.6 Support Vector Regression 17 1.7 Kernel Principal Component Analysis 19 1.8 Empirical Results and Implementations 21\nI CONCEPTS AND TOOLS 23\nKernels 25 2.1 Product Features 26 2.2 The Representation of Similarities in Linear Spaces 29 2.3 2.4 2.5 2.6 Examples and Properties of Kernels 45 The Representation of Dissimilarities in Linear Spaces 48 Summary 55 Problems 55\nRisk and Loss Functions 61 3.1 Loss Functions 62 3.2 Test Error and Expected Risk 65 3.3 A Statistical Perspective 68 3.4 Robust Estimators 75 3.5 Summary 83 3.6 Problems 84\nRegularization 4.1 The Regularized Risk Functional\n87\nvui Contents\n4.2 The Representer Theorem 89 4.3 Regularization Operators 92 4.4 Translation Invariant Kernels 96 4.5 Translation Invariant Kernels in Higher Dimensions 105 4.6 Dot Product Kernels 110 4.7 Multi-Output Regularization 113 4.8 Semiparametric Regularization 115 4.9 Coefficient Based Regularization 118 4.10 Summary 121 4.11 Problems 122\nElements of Statistical Learning Theory 125 5.1 Introduction 125 5.2 The Law of Large Numbers 128 5.3 When Does Learning Work: the Question of Consistency 131 5.4 Uniform Convergence and Consistency 131 5.5 How to Derive a VC Bound 134 5.6 A Model Selection Example 144 5.7 Summary 146 5.8 Problems 146\nOptimization 149 6.1 Convex Optimization 150 6.2 Unconstrained Problems 154 6.3 Constrained Problems 165 6.4 Interior Point Methods 175 6.5 Maximum Search Problems 179 6.6 Summary 183 6.7 Problems 184\nII SUPPORT VECTOR MACHINES 187\nPattern Recognition 189 7.1 Separating Hyperplanes 189 7.2 The Role of the Margin 192 7.3 Optimal Margin Hyperplanes 196 7.4 Nonlinear Support Vector Classifiers 200 7.5 Soft Margin Hyperplanes 204 7.6 Multi-Class Classification 211 7.7 Variations on a Theme 214 7.8 Experiments 215 7.9 Summary 222 7.10 Problems 222\nContents\n8 Single-Class Problems: Quantile Estimation and Novelty Detection 227 8.1 Introduction 228 8.2 A Distribution's Support and Quantiles 229 8.3 Algorithms 230 8.4 Optimization 234 8.5 Theory 236 8.6 Discussion 241 8.7 Experiments 243 8.8 Summary 247 8.9 Problems 248\n9 Regression Estimation 251 9.1 Linear Regression with Insensitive Loss Function 251 9.2 Dual Problems 254 9.3 I/-SV Regression 260 9.4 Convex Combinations and ^i-Norms 266 9.5 Parametric Insensitivity Models 269 9.6 Applications 272 9.7 Summary 273 9.8 Problems 274\n10 Implementation 279 10.1 Tricks of the Trade 281 10.2 Sparse Greedy Matrix Approximation 288 10.3 Interior Point Algorithms 295 10.4 Subset Selection Methods 300 10.5 Sequential Minimal Optimization 305 10.6 Iterative Methods 312 10.7 Summary 327 10.8 Problems 329\n11 Incorporating Invariances 333 11.1 Prior Knowledge 333 11.2 Transformation Invariance 335 11.3 The Virtual SV Method 337 11.4 Constructing Invariance Kernels 343 11.5 The Jittered SV Method 354 11.6 Summary 356 11.7 Problems 357\n12 Learning Theory Revisited 359 12.1 Concentration of Measure Inequalities 360 12.2 Leave-One-Out Estimates 366 12.3 PAC-Bayesian Bounds 381 12.4 Operator-Theoretic Methods in Learning Theory 391\nContents\n12.5 Summary 403 12.6 Problems 404\nIII KERNEL METHODS 405\n13 Designing Kernels 407 13.1 Tricks for Constructing Kernels 408 13.2 String Kernels 412 13.3 Locality-Improved Kernels 414 13.4 Natural Kernels 418 13.5 Summary 423 13.6 Problems 423\n14 Kernel Feature Extraction 427 14.1 Introduction 427 14.2 Kernel PCA 429 14.3 Kernel PCA Experiments 437 14.4 A Framework for Feature Extraction 442 14.5 Algorithms for Sparse KFA 447 14.6 KFA Experiments 450 14.7 Summary 451 14.8 Problems 452\n15 Kernel Fisher Discriminant 457 15.1 Introduction 457 15.2 Fisher's Discriminant in Feature Space 458 15.3 Efficient Training of Kernel Fisher Discriminants 460 15.4 Probabilistic Outputs 464 15.5 Experiments 466 15.6 Summary 467 15.7 Problems 468\n16 Bayesian Kernel Methods 469 16.1 Bayesics 470 16.2 Inference Methods 475 16.3 Gaussian Processes 480 16.4 Implementation of Gaussian Processes 488 16.5 Laplacian Processes 499 16.6 Relevance Vector Machines \u2022 ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 117252811,
            "fieldsOfStudy": [],
            "id": "636b46471adea4916ec1b2e38c8e8265218f6952",
            "isKey": true,
            "numCitedBy": 616,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Reproducing-Kernels-and-Its-Applications-Sawano-Saitoh",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels and Its Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[41, 38] considered the class of hyperplanes in some dot product space H,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 228778895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c2d9e61b7f406f2ca35e63cbbb23e7cf7a85c2a",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-note-one-class-of-perceptrons-Vapnik",
            "title": {
                "fragments": [],
                "text": "A note one class of perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117967708,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4a7a9c568e050853609ae18f9b7733dbb756177d",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Geometry-and-invariance-in-kernel-based-methods-Burges",
            "title": {
                "fragments": [],
                "text": "Geometry and invariance in kernel based methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35080630"
                        ],
                        "name": "H. Weinert",
                        "slug": "H.-Weinert",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Weinert",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Weinert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118235835,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bfadc8299539818c21da34cda6c12aab1da3d744",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reproducing-kernel-Hilbert-spaces:-Applications-in-Weinert",
            "title": {
                "fragments": [],
                "text": "Reproducing kernel Hilbert spaces: Applications in statistical signal processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144403112"
                        ],
                        "name": "C. Berg",
                        "slug": "C.-Berg",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Berg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144027552"
                        ],
                        "name": "J. Christensen",
                        "slug": "J.-Christensen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Christensen",
                            "middleNames": [
                                "Peter",
                                "Reus"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Christensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101172055"
                        ],
                        "name": "P. Ressel",
                        "slug": "P.-Ressel",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Ressel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ressel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 46
                            }
                        ],
                        "text": "Remark ??), the function k is called a kernel [19, 1, 5, 6, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117450983,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b9f4febf74f3802df63f3d73dd49cabd37eece15",
            "isKey": false,
            "numCitedBy": 1020,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Harmonic-Analysis-on-Semigroups-Berg-Christensen",
            "title": {
                "fragments": [],
                "text": "Harmonic Analysis on Semigroups"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[41, 38] considered the class of hyperplanes in some dot product space H,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 194
                            }
                        ],
                        "text": "This substitution, which is sometimes referred to as the kernel trick, was used by Boser, Guyon, and Vapnik [6] to extend the Generalized Portrait hyperplane classifier of Vapnik and co-workers [41, 39] to nonlinear Support Vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115205884,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7cabbdf6a7288d15e26fa6ea504009bab3d1edf4",
            "isKey": false,
            "numCitedBy": 1137,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-recognition-using-generalized-portrait-Vapnik",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 112
                            }
                        ],
                        "text": "Hence there are theoretical arguments supporting the good generalization performance of the optimal hyperplane ([39, 34, 43, 4], cf."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61853586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "623888e2350c0abc08d36a71fcd9024f19039994",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-Performance-of-Support-Vector-and-Bartlett-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "Generalization Performance of Support Vector Machines and Other Pattern Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60760212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5a8e5782420e8c4e634fb2c7b8ad0ace4506b92",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Three-remarks-on-the-support-vector-method-of-Vapnik",
            "title": {
                "fragments": [],
                "text": "Three remarks on the support vector method of function estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713577"
                        ],
                        "name": "D. Mattera",
                        "slug": "D.-Mattera",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Mattera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mattera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "Very good results were obtained on the Boston housing benchmark [32], and on problems of times series prediction (see [21, 20, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58731659,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "466d91cd6c31a389cce0db80a82dbf71e6b916dd",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Support-vector-machines-for-dynamic-reconstruction-Mattera-Haykin",
            "title": {
                "fragments": [],
                "text": "Support vector machines for dynamic reconstruction of a chaotic system"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40805799"
                        ],
                        "name": "J. W. Humberston",
                        "slug": "J.-W.-Humberston",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Humberston",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Humberston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5482999,
            "fieldsOfStudy": [],
            "id": "f2dbab1960529561c9432600694607d59d6db694",
            "isKey": false,
            "numCitedBy": 8103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Classical-mechanics-Humberston",
            "title": {
                "fragments": [],
                "text": "Classical mechanics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 144
                            }
                        ],
                        "text": "Another interesting capacity measure, which can be thought of as a scale-sensitive version of the VC dimension, is the fat shattering dimension [17, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52817629,
            "fieldsOfStudy": [],
            "id": "f8a1f40e6b820e5f2fdd543dab32ad8fc74a750c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient Distribution-Free Learning of Probabilistic Concepts"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "Theorem 4 [12, 9] p = q \u21d0\u21d2 sup f\u2208C(X) \u2223 Ex\u223cp(f (x)) \u2212 Ex\u223cq(f (x)) \u2223 \u2223 = 0, where C(X) is the space of continuous bounded functions on X."
                    },
                    "intents": []
                }
            ],
            "corpusId": 116062098,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6c06333850b96721709b3162c6332939a2fdce31",
            "isKey": false,
            "numCitedBy": 2181,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Foundations: set theory 2. General topology 3. Measures 4. Integration 5. Lp spaces: introduction to functional analysis 6. Convex sets and duality of normed spaces 7. Measure, topology, and differentiation 8. Introduction to probability theory 9. Convergence of laws and central limit theorems 10. Conditional expectations and martingales 11. Convergence of laws on separable metric spaces 12. Stochastic processes 13. Measurability: Borel isomorphism and analytic sets Appendixes: A. Axiomatic set theory B. Complex numbers, vector spaces, and Taylor's theorem with remainder C. The problem of measure D. Rearranging sums of nonnegative terms E. Pathologies of compact nonmetric spaces Indices."
            },
            "slug": "Real-Analysis-and-Probability-Dudley",
            "title": {
                "fragments": [],
                "text": "Real Analysis and Probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 144
                            }
                        ],
                        "text": "Another interesting capacity measure, which can be thought of as a scale-sensitive version of the VC dimension, is the fat shattering dimension [17, 2]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6243824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d474299d7a51b89a1d7394d426cf881a89b8013d",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>"
            },
            "slug": "Efficient-distribution-free-learning-of-concepts-Kearns-Schapire",
            "title": {
                "fragments": [],
                "text": "Efficient distribution-free learning of probabilistic concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated, and an underlying theory of learning p-concepts is developed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644344103"
                        ],
                        "name": "J. C. BurgesChristopher",
                        "slug": "J.-C.-BurgesChristopher",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "BurgesChristopher",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. BurgesChristopher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 52
                            }
                        ],
                        "text": "For overviews, the interested reader is referred to [7, 27, 30, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "Among the optimizers used for SVMs are LOQO [33], MINOS [22], and variants of conjugate gradient descent, such as the optimizers of Bottou [25] and Burges [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215966761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6716697767fc601efc7690f40820d9ea7a7bf57c",
            "isKey": false,
            "numCitedBy": 13527,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, w..."
            },
            "slug": "A-Tutorial-on-Support-Vector-Machines-for-Pattern-BurgesChristopher",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Support Vector Machines for Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This tutorial starts with an overview of the concepts of VC dimension and structural risk minimization and describes linear Support Vector Machines (SVMs) for separable and non-separable data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61480753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edb62d05f8eeaa7e1921c6c25c544935a2b6b131",
            "isKey": false,
            "numCitedBy": 415,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theory-of-Pattern-Recognition-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "Statistical learning theory (Chapter ??, [39, 34, 35, 12, 36, 3]), or VC (VapnikChervonenkis) theory, shows that it is imperative to restrict the set of functions that f is chosen from to one which has a capacity that is suitable for the amount Capacity of available training data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 194
                            }
                        ],
                        "text": "This substitution, which is sometimes referred to as the kernel trick, was used by Boser, Guyon, and Vapnik [6] to extend the Generalized Portrait hyperplane classifier of Vapnik and co-workers [41, 39] to nonlinear Support Vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 112
                            }
                        ],
                        "text": "Hence there are theoretical arguments supporting the good generalization performance of the optimal hyperplane ([39, 34, 43, 4], cf."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition [in Russian"
            },
            "venue": {
                "fragments": [],
                "text": "Nauka, Moscow, 1974. "
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 46
                            }
                        ],
                        "text": "Remark ??), the function k is called a kernel [19, 1, 5, 6, 16]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Functions of positive and negative type and their connection with the theory of  Sch\u00f6lkopf and Smola: Learning with Kernels \u2014 Confidential draft"
            },
            "venue": {
                "fragments": [],
                "text": "please do not circulate \u2014 2001/03/02 20:32  24  REFERENCES integral equations. Philos. Trans. Roy. Soc. London, A 209:415\u2013446"
            },
            "year": 1909
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A note on one class of perceptrons. Automation and Remote Control"
            },
            "venue": {
                "fragments": [],
                "text": "A note on one class of perceptrons. Automation and Remote Control"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support Vector Learning. R. Oldenbourg Verlag"
            },
            "venue": {
                "fragments": [],
                "text": "Support Vector Learning. R. Oldenbourg Verlag"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Among the optimizers used for SVMs are LOQO [33], MINOS [22], and variants of conjugate gradient descent, such as the optimizers of Bottou [25] and Burges [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MINOS 5.4 user\u2019s guide"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report SOL 83.20,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "Statistical learning theory (Chapter ??, [39, 34, 35, 12, 36, 3]), or VC (VapnikChervonenkis) theory, shows that it is imperative to restrict the set of functions that f is chosen from to one which has a capacity that is suitable for the amount Capacity of available training data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Theory of Learning in Artificial Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Cambridge University Press"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training invariant support vector machines Accepted for publication"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method. Automation and Remote Control"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method. Automation and Remote Control"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Input space vs"
            },
            "venue": {
                "fragments": [],
                "text": "feature space in kernel-based methods. IEEE Transactions on Neural Networks, 10(5):1000\u20131017"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sup"
            },
            "venue": {
                "fragments": [],
                "text": "Sup"
            },
            "year": 1953
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 31,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 67,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-with-kernels-Smola/d5051890e501117097eeffbd8ded87694f0d8063?sort=total-citations"
}