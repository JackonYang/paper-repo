{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34929449"
                        ],
                        "name": "George H. John",
                        "slug": "George-H.-John",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "John",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George H. John"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30079006"
                        ],
                        "name": "Karl Pfleger",
                        "slug": "Karl-Pfleger",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Pfleger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl Pfleger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 106
                            }
                        ],
                        "text": "Another feature selection methodolgy which has recently received much more attention is the wrapper model (John et al. 1994) (Caruana & Freitag 1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 58
                            }
                        ],
                        "text": "The Corral dataset has been noted by previous researchers (John et al. 1994) as particularly di cult for lter methods since, of the 6 features in this domain, the target concept is a Boolean function of only four of the features: (A ^ B) _ (C ^D)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15089378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7cc43c9dbf6991a91db6314523bec861d087b86",
            "isKey": false,
            "numCitedBy": 2679,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Irrelevant-Features-and-the-Subset-Selection-John-Kohavi",
            "title": {
                "fragments": [],
                "text": "Irrelevant Features and the Subset Selection Problem"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46946909"
                        ],
                        "name": "K. Kira",
                        "slug": "K.-Kira",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Kira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1873631"
                        ],
                        "name": "L. Rendell",
                        "slug": "L.-Rendell",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Rendell",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rendell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 3
                            }
                        ],
                        "text": "In RELIEF, a sub-set of features in not directly selected, but rather eachfeature is given a weighting indicating its level of rele-vance to the class label."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "Two ofthe most well-known lter methods for feature selec-\ntion are RELIEF (Kira & Rendell 1992) and FOCUS(Almuallim & Dietterich 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "RELIEF is therefore ine ec-tive at removing redundant features as two predictivebut highly correlated features are both likely to behighly weighted."
                    },
                    "intents": []
                }
            ],
            "corpusId": 46457448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aded004c181e218a32cf7413de4ac38affe72d4a",
            "isKey": false,
            "numCitedBy": 1948,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "For real-world concept learning problems, feature selection is important to speed up learning and to improve concept quality. We review and analyze past approaches to feature selection and note their strengths and weaknesses. We then introduce and theoretically examine a new algorithm Rellef which selects relevant features using a statistical method. Relief does not depend on heuristics, is accurate even if features interact, and is noise-tolerant. It requires only linear time in the number of given features and the number of training instances, regardless of the target concept complexity. The algorithm also has certain limitations such as nonoptimal feature set size. Ways to overcome the limitations are suggested. We also report the test results of comparison between Relief and other feature selection algorithms. The empirical results support the theoretical analysis, suggesting a practical approach to feature selection for real-world problems."
            },
            "slug": "The-Feature-Selection-Problem:-Traditional-Methods-Kira-Rendell",
            "title": {
                "fragments": [],
                "text": "The Feature Selection Problem: Traditional Methods and a New Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new algorithm Rellef is introduced which selects relevant features using a statistical method and is accurate even if features interact, and is noise-tolerant, suggesting a practical approach to feature selection for real-world problems."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47829236"
                        ],
                        "name": "S. Sage",
                        "slug": "S.-Sage",
                        "structuredName": {
                            "firstName": "Stephanie",
                            "lastName": "Sage",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sage"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "Another feature selection methodolgy which has re-cently received much attention is the wrapper model(John, Kohavi, & P eger 1994) (Caruana & Freitag1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5075598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d63c7b3b86276a6085e8ff7a104a3fd8864b8c02",
            "isKey": false,
            "numCitedBy": 783,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Induction-of-Selective-Bayesian-Classifiers-Langley-Sage",
            "title": {
                "fragments": [],
                "text": "Induction of Selective Bayesian Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522754"
                        ],
                        "name": "H. Almuallim",
                        "slug": "H.-Almuallim",
                        "structuredName": {
                            "firstName": "Hussein",
                            "lastName": "Almuallim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Almuallim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12494914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7008bc0709341ae19d260dbeda6b65a4c4467ee1",
            "isKey": false,
            "numCitedBy": 766,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires \u0398(1/e ln 1/\u03b4+ 1/e[2p + p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that-- contrary to expectations--these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURES bias is appropriate. This suggests that, in practical applications, training data should be preprocessed to remove irrelevant features before being given to ID3 or FRINGE."
            },
            "slug": "Learning-with-Many-Irrelevant-Features-Almuallim-Dietterich",
            "title": {
                "fragments": [],
                "text": "Learning with Many Irrelevant Features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that any learning algorithm implementing the MIN-FEATURES bias requires \u0398(1/e ln 1/\u03b4+ 1/e[2p + p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features, and suggests that training data should be preprocessed to remove irrelevant features before being given to ID3 or FRINGE."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37838196"
                        ],
                        "name": "G. Provan",
                        "slug": "G.-Provan",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Provan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Provan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 112
                            }
                        ],
                        "text": "It is interesting to compare our approach to another,seemingly very similar one, often used in the litera-ture (Singh & Provan 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18894798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1c44a5f503880aea901de1eff1a2e6ad39ec0b7",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a computation-ally eecient method for inducing selective Bayesian network classiiers. Our approach is to use information-theoretic metrics to ef-ciently select a subset of attributes from which to learn the classiier. We explore three conditional, information-theoretic met-rics that are extensions of metrics used extensively in decision tree learning, namely Quin-lan's gain and gain ratio metrics and Man-taras's distance metric. We experimentally show that the algorithms based on gain ratio and distance metric learn selective Bayesian networks that have predictive accuracies as good as or better than those learned by existing selective Bayesian network induction approaches (K2-AS), but at a signiicantly lower computational cost. We prove that the subset-selection phase of these information-based algorithms has polynomial complexity, as compared to the worst-case exponential time complexity of the corresponding phase in K2-AS."
            },
            "slug": "Eecient-Learning-of-Selective-Bayesian-Network-Provan",
            "title": {
                "fragments": [],
                "text": "Eecient Learning of Selective Bayesian Network Classiiers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is proved that the subset-selection phase of these information-based algorithms has polynomial complexity, as compared to the worst-case exponential time complexity of the corresponding phase in K2-AS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706719"
                        ],
                        "name": "Wayne Iba",
                        "slug": "Wayne-Iba",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Iba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wayne Iba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123928894"
                        ],
                        "name": "K. Thompson",
                        "slug": "K.-Thompson",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15383317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1925bacaa10b4ec83a0509132091bb79243b41b6",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an average-case analysis of the Bayesian classiier, a simple probabilistic induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, Boolean attributes that are independent of each other and that follow a single distribution, and the absence of attribute noise. We rst calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions ; we then use this expression to compute the probability of correct classiication over the space of instances. The analysis takes into account the number of training instances, the number of relevant and irrelevant attributes, the distribution of these attributes, and the level of class noise. In addition, we explore the behavioral implications of the analysis by presenting predicted learning curves for a number of artiicial domains. We also give experimental results on these domains as a check on our reasoning. Finally, we discuss some unresolved questions about the behavior of Bayesian classiiers and outline directions for future research. we nd the current format more desirable. We have not submitted the paper to any other conference or journal."
            },
            "slug": "An-Analysis-of-Bayesian-Classiiers-Langley-Iba",
            "title": {
                "fragments": [],
                "text": "An Analysis of Bayesian Classiiers"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An average-case analysis of the Bayesian classiier is presented, a simple probabilistic induction algorithm that fares remarkably well on many learning tasks and explores the behavioral implications by presenting predicted learning curves for a number of artiicial domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 120
                            }
                        ],
                        "text": "Two ofthe most well-known lter methods for feature selec-\ntion are RELIEF (Kira & Rendell 1992) and FOCUS(Almuallim & Dietterich 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "By way of comparison, a roughestimate of the time required by a wrapper approachsuch as that of Caruana & Freitag (1994) or John etal (1994) to eliminate this many features is on the or-der of thousands of hours, assuming the method doesnot get caught in a local minima rst and prematurelystops\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 132
                            }
                        ],
                        "text": "Another feature selection methodolgy which has re-cently received much attention is the wrapper model(John, Kohavi, & P eger 1994) (Caruana & Freitag1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5238043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f82e86b853fcda968d63c0196bf2df6748b13233",
            "isKey": false,
            "numCitedBy": 605,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Greedy-Attribute-Selection-Caruana-Freitag",
            "title": {
                "fragments": [],
                "text": "Greedy Attribute Selection"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In some cases (like the DNA and Reuters data sets) this method is able to achieve notable improvements after removing a very large amount of features (even 2/3 of them or more!) It is also interesting to note how this method compares to another method proposed in a previous study ( Kohavi 1995 ) in terms of speed (using a Sun SPARC 10 machine, with the DNA data set)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 22
                            }
                        ],
                        "text": "By way of comparison, Kohavi (1995) obtainssimilar accuracy results on the DNA dataset for Naive-Bayes and C4.5 using the wrapper approach, but notesthat doing so takes 15 hours on a Sun sparc 10."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60538272,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "49f6fe73703ecad9271b23697da8902fe49348b4",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 360,
            "paperAbstract": {
                "fragments": [],
                "text": "In this doctoral dissertation, we study three basic problems in machine learning and two new hypothesis spaces with corresponding learning algorithms. The problems we investigate are: accuracy estimation, feature subset selection, and parameter tuning. The latter two problems are related and are studied under the wrapper approach. The hypothesis spaces we investigate are: decision tables with a default majority rule (DTMs) and oblivious read-once decision graphs (OODGs). For accuracy estimation, we investigate cross-validation and the~.632 bootstrap. We show examples where they fail and conduct a large scale study comparing them. We conclude that repeated runs of five-fold cross-validation give a good tradeoff between bias and variance for the problem of model selection used in later chapters. We define the wrapper approach and use it for feature subset selection and parameter tuning. We relate definitions of feature relevancy to the set of optimal features, which is defined with respect to both a concept and an induction algorithm. The wrapper approach requires a search space, operators, a search engine, and an evaluation function. We investigate all of them in detail and introduce compound operators for feature subset selection. Finally, we abstract the search problem into search with probabilistic estimates. We introduce decision tables with a default majority rule (DTMs) to test the conjecture that feature subset selection is a very powerful bias. The accuracy of induced DTMs is surprisingly powerful, and we concluded that this bias is extremely important for many real-world datasets. We show that the resulting decision tables are very small and can be succinctly displayed. We study properties of oblivious read-once decision graphs (OODGs) and show that they do not suffer from some inherent limitations of decision trees. We describe a a general framework for constructing OODGs bottom-up and specialize it using the wrapper approach. We show that the graphs produced are use less features than C4.5, the state-of-the-art decision tree induction algorithm, and are usually easier for humans to comprehend."
            },
            "slug": "Wrappers-for-Performance-Enhancements-and-Oblivious-Kohavi",
            "title": {
                "fragments": [],
                "text": "Wrappers for Performance Enhancements and Oblivious Decision Graphs."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This doctoral dissertation concludes that repeated runs of five-fold cross-validation give a good tradeoff between bias and variance for the problem of model selection used in later chapters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21898,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143672554"
                        ],
                        "name": "Denise Draper",
                        "slug": "Denise-Draper",
                        "structuredName": {
                            "firstName": "Denise",
                            "lastName": "Draper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denise Draper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38413017"
                        ],
                        "name": "S. Hanks",
                        "slug": "S.-Hanks",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1912808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25e2cc30c561f79a0c397638c53e23e0b56907a3",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Localized-Partial-Evaluation-of-Belief-Networks-Draper-Hanks",
            "title": {
                "fragments": [],
                "text": "Localized Partial Evaluation of Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145562276"
                        ],
                        "name": "A. Kozlov",
                        "slug": "A.-Kozlov",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kozlov",
                            "middleNames": [
                                "Vladimirovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kozlov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685479"
                        ],
                        "name": "J. Singh",
                        "slug": "J.-Singh",
                        "structuredName": {
                            "firstName": "Jaswinder",
                            "lastName": "Singh",
                            "middleNames": [
                                "Pal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1526790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2a42a383b086639b0231c1721803f0d9500c6c3",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We show an alternative way of representing a Bayesian belief network by sensitivities and probability distributions. This representation is equivalent to the traditional representation by conditional probabilities, but makes dependencies between nodes apparent and intuitively easy to understand. We also propose a QR matrix representation for the sensitivities and/or conditional probabilities which is more efficient, in both memory requirements and computational speed, than the traditional representation for computer-based implementations of probabilistic inference. We use sensitivities to show that for a certain class of binary networks, the computation time for approximate probabilistic inference with any positive upper bound on the error of the result is independent of the size of the network. Finally, as an alternative to traditional algorithms that use conditional probabilities, we describe an exact algorithm for probabilistic inference that uses the QR-representation for sensitivities and updates probability distributions of nodes in a network according to messages from the neighbors."
            },
            "slug": "Sensitivities:-An-Alternative-to-Conditional-for-Kozlov-Singh",
            "title": {
                "fragments": [],
                "text": "Sensitivities: An Alternative to Conditional Probabilities for Bayesian Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "An exact algorithm for probabilistic inference that uses the QR-representation for sensitivities and updates probability distributions of nodes in a network according to messages from the neighbors is described."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 106
                            }
                        ],
                        "text": "We describe a formalframework for understanding feature selection, basedon ideas from Information Theory (Cover & Thomas1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 11
                            }
                        ],
                        "text": "Elements ofInformation Theory."
                    },
                    "intents": []
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42795,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2692987"
                        ],
                        "name": "Huaiyu Zhu",
                        "slug": "Huaiyu-Zhu",
                        "structuredName": {
                            "firstName": "Huaiyu",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huaiyu Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 108
                            }
                        ],
                        "text": "As our distance metric, we use theinformation-theoretic measure of cross-entropy (alsoknown as KL-distance (Kullback & Leibler 1951))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116908168,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "87cbed883368d4a9efd42fdd91f47038f8d8fbe6",
            "isKey": false,
            "numCitedBy": 6005,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The information deviation between any two finite measures cannot be increased by any statistical operations (Markov morphisms). It is invarient if and only if the morphism is sufficient for these two measures"
            },
            "slug": "On-Information-and-Sufficiency-Zhu",
            "title": {
                "fragments": [],
                "text": "On Information and Sufficiency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 106
                            }
                        ],
                        "text": "It is well-known that additional informationcan cause correlations that were not present before toappear (Pearl 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 159
                            }
                        ],
                        "text": "Then Fi also has a Markovblanket within G fFjg.Proof: The proof is based on the basic indepen-dence properties of probability distributions, as de-scribed in (Pearl 1988, p. 84)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 1
                            }
                        ],
                        "text": "(Pearl 1988, p. 97)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "As we now show, we can utilize ideas from probabilistic reasoning (Pearl 1988) to circumvent this problem (to some extent)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "It is well-known that additional information can cause correlations that were not present before to appear (Pearl 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 66
                            }
                        ],
                        "text": "As we now show, we can utilize ideas from probabilisticreasoning (Pearl 1988) to circumvent this problem (tosome extent)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 222
                            }
                        ],
                        "text": ", Fi or C) are said to be conditionally independent given some set of variables X if, for any assignment of values a, b, and x to the variables A, B, and X respectively, Pr(A = a j X = x ; B = b) = Pr(A = a j X = x ) (see (Pearl 1988) for more details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57437891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bf6f01402e1648b7d1e6c9200ede6cb1af30123",
            "isKey": true,
            "numCitedBy": 4579,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 192
                            }
                        ],
                        "text": "\u2026datasets include:the Corral data which was arti cially constructed byJohn et al (1994) speci cally for research in featureselection; the LED24, Vote, and DNA datasets fromthe UCI repository (Murphy & Aha 1995); and twodatasets which are a subset of the Reuters documentcollection (Reuters 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "By way of comparison, Kohavi (1995) obtainssimilar accuracy results on the DNA dataset for Naive-Bayes and C4.5 using the wrapper approach, but notesthat doing so takes 15 hours on a Sun sparc 10."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "To test how our method of feature subset selec-tion a ected classi cation, we employed both a NaiveBayesian classi er (Duda & Hart 1973) and C4.5(Quinlan 1993) as induction algorithms; these were ap-plied both to the original datasets and to the datasets ltered through our feature selection\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "This poses a problem for cer-tain induction methods; C4.5, for example, is likelyto initially split on the correlated feature, thus frag-menting the data enough that the true target conceptcannot be recovered in the subtrees."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "C4.5: Programs for MachineLearning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1050,
                                "start": 1046
                            }
                        ],
                        "text": "Fwd. Bckwd.0 84.4 84.4 81.2 75.01 81.3 84.4 75.0 87.5Corral 6 = 4 2 90.6 81.3 87.5 81.2 75.0 100.03 81.3 87.5 81.2 100.04 81.3 87.5 81.2 100.00 72.1 1.0 72.1 1.0 71.3 1.2 71.3 1.3LED-24 24 = 14 1 72.1 2.1 71.9 0.9 72.1 0.7 71.1 1.2 71.0 1.0 70.9 1.22 72.2 1.4 72.4 1.4 71.9 1.0 71.3 0.90 72.8 1.5 72.8 1.5 72.1 0.9 72.1 0.9LED-24 24 = 7 1 72.1 2.1 72.2 1.8 72.2 1.8 71.1 1.2 71.3 1.9 71.3 1.92 72.1 1.5 72.1 0.8 71.6 1.3 71.5 1.10 90.1 1.8 90.1 1.8 95.7 1.5 95.7 1.5Vote 48 = 28 1 90.1 1.8 90.1 2.7 90.1 2.7 95.2 1.5 95.0 2.8 95.2 2.82 90.3 3.4 90.3 3.4 94.5 1.3 94.7 1.30 92.0 2.7 92.0 2.7 95.4 2.9 95.4 2.9Vote 48 = 8 1 90.1 1.8 93.6 1.8 92.7 2.5 95.2 1.5 95.9 1.5 95.7 1.52 95.2 2.6 93.1 5.4 95.0 2.5 96.0 2.20 95.0 0.5 95.0 0.5 93.6 0.7 93.5 0.7DNA 180 = 80 1 94.0 0.6 95.4 0.7 95.5 0.9 92.3 0.7 93.4 0.8 93.3 0.62 94.9 0.9 94.8 0.6 93.6 1.4 93.5 1.10 94.1 1.0 94.1 1.0 93.6 0.2 93.6 0.3DNA 180 = 30 1 94.0 0.6 94.2 1.1 94.3 1.1 92.3 0.7 93.6 0.8 93.4 0.72 92.3 1.8 93.8 1.0 91.7 1.5 93.3 1.0Table 2: Accuracy percentages for Naive-Bayes and C4.5 using feature selection.# Features Naive Bayes Accuracy C4.5 AccuracyDataset Orig.=Final K Orig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "As seen in the accuracy results for Corral (using C4.5),Vote (using Naive Bayes with aggressive feature elim-ination), and DNA, selection of the appropriate fea-ture set can have a large impact on classi cation ac-curacy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "We ran our feature selection algorithm on the Reuters\n# Features Naive Bayes Accuracy C4.5 AccuracyDataset Orig.=Final K Orig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "To test how our method of feature subset selec-tion a ected classi cation, we employed both a NaiveBayesian classi er (Duda & Hart 1973) and C4.5(Quinlan 1993) as induction algorithms; these were ap-plied both to the original datasets and to the datasets ltered through our feature selection algorithm (us-ing both forward selection and backward elimination)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 271
                            }
                        ],
                        "text": "A two-tailed paired T-test over thecross-validation folds reveals statistically signi cantimprovements (P < 0:10) in accuracy for the Vote do-main using Naive Bayes with aggressive feature selec-tion and in the DNA domain for backward eliminationused in conjunction with C4.5."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E \u000e cient learning of selective bayesian network classi ers , Submitted for publication . 14"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 106
                            }
                        ],
                        "text": "It is well-known that additional informationcan cause correlations that were not present before toappear (Pearl 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 159
                            }
                        ],
                        "text": "Then Fi also has a Markovblanket within G fFjg.Proof: The proof is based on the basic indepen-dence properties of probability distributions, as de-scribed in (Pearl 1988, p. 84)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 1
                            }
                        ],
                        "text": "(Pearl 1988, p. 97)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "As we now show, we can utilize ideas from probabilistic reasoning (Pearl 1988) to circumvent this problem (to some extent)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "It is well-known that additional information can cause correlations that were not present before to appear (Pearl 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 66
                            }
                        ],
                        "text": "As we now show, we can utilize ideas from probabilisticreasoning (Pearl 1988) to circumvent this problem (tosome extent)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic Reasoning in Intelligent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Two ofthe most well-known lter methods for feature selec-\ntion are RELIEF (Kira & Rendell 1992) and FOCUS(Almuallim & Dietterich 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 7
                            }
                        ],
                        "text": "In RELIEF, a sub-set of features in not directly selected, but rather eachfeature is given a weighting indicating its level of rele-vance to the class label."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "RELIEF is therefore ine ec-tive at removing redundant features as two predictivebut highly correlated features are both likely to behighly weighted."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "Another feature selection methodolgy which has re-cently received much attention is the wrapper model(John, Kohavi, & P eger 1994) (Caruana & Freitag1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Induction of selective b a y esian classiiers"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Tenth Conference on Uncertainty in Artiicial Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109503620"
                        ],
                        "name": "Temple F. Smith",
                        "slug": "Temple-F.-Smith",
                        "structuredName": {
                            "firstName": "Temple",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Temple F. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4276691,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "id": "0b4d43ef0051a225e07af8194e81007ebba8d787",
            "isKey": false,
            "numCitedBy": 705,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Occam's-razor-Smith",
            "title": {
                "fragments": [],
                "text": "Occam's razor"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "Another feature selection methodolgy which has re-cently received much attention is the wrapper model(John, Kohavi, & P eger 1994) (Caruana & Freitag1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Induction of selective bayesian classi ers, in \\Proceedings of the Tenth Conference on Uncertainty in Arti cial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 22
                            }
                        ],
                        "text": "By way of comparison, Kohavi (1995) obtainssimilar accuracy results on the DNA dataset for Naive-Bayes and C4.5 using the wrapper approach, but notesthat doing so takes 15 hours on a Sun sparc 10."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wrappers for Performance En"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 108
                            }
                        ],
                        "text": "As our distance metric, we use theinformation-theoretic measure of cross-entropy (alsoknown as KL-distance (Kullback & Leibler 1951))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On information and su ciency"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematical Statistics"
            },
            "year": 1951
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reuters collection available via anonymous ftp"
            },
            "venue": {
                "fragments": [],
                "text": "Reuters collection available via anonymous ftp"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On information and suuciency"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematical Statistics"
            },
            "year": 1951
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 112
                            }
                        ],
                        "text": "It is interesting to compare our approach to another,seemingly very similar one, often used in the litera-ture (Singh & Provan 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient learning of selective bayesian network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An analysis of bayesian classi ers, in \\Proceedings of the tenth national conference on arti cial intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 104
                            }
                        ],
                        "text": "Two ofthe most well-known lter methods for feature selec-\ntion are RELIEF (Kira & Rendell 1992) and FOCUS(Almuallim & Dietterich 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "This consistencycriterion makes FOCUS very sensitive to noise in thetraining data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The FOCUS algorithm conducts anexhaustive search of all feature subsets to determinethe minimal set of features that can provide a consis-tent labeling of the training data."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with many irrelevant features, in \\Ninth"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 120
                            }
                        ],
                        "text": "It is interesting to compare our approach to another,seemingly very similar one, often used in the litera-ture (Singh & Provan 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient learning of selective bayesian network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "Another feature selection methodolgy which has re-cently received much attention is the wrapper model(John, Kohavi, & P eger 1994) (Caruana & Freitag1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Induction of selec"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 22
                            }
                        ],
                        "text": "By way of comparison, Kohavi (1995) obtainssimilar accuracy results on the DNA dataset for Naive-Bayes and C4.5 using the wrapper approach, but notesthat doing so takes 15 hours on a Sun sparc 10."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wrappers for Performance Enhancement and Oblivious Decision Graphs, PhD"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "Another feature selection methodolgy which has re-cently received much attention is the wrapper model(John, Kohavi, & P eger 1994) (Caruana & Freitag1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Induction of selective bayesian classiiers, in \\Proceedings of the Tenth Conference on Uncertainty in Artiicial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "Induction of selective bayesian classiiers, in \\Proceedings of the Tenth Conference on Uncertainty in Artiicial Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 22
                            }
                        ],
                        "text": "By way of comparison, Kohavi (1995) obtainssimilar accuracy results on the DNA dataset for Naive-Bayes and C4.5 using the wrapper approach, but notesthat doing so takes 15 hours on a Sun sparc 10."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wrappers for Performance Enhancement and Oblivious Decision Graphs, PhD thesis, Stanford University, Computer Science department"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An analysis of bayesian classi ers, in \\Proceedings"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "By way of comparison, a roughestimate of the time required by a wrapper approachsuch as that of Caruana & Freitag (1994) or John etal (1994) to eliminate this many features is on the or-der of thousands of hours, assuming the method doesnot get caught in a local minima rst and prematurelystops\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 132
                            }
                        ],
                        "text": "Another feature selection methodolgy which has re-cently received much attention is the wrapper model(John, Kohavi, & P eger 1994) (Caruana & Freitag1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Greedy attributeselection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 106
                            }
                        ],
                        "text": "We describe a formalframework for understanding feature selection, basedon ideas from Information Theory (Cover & Thomas1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 11
                            }
                        ],
                        "text": "Elements ofInformation Theory."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory, Wiley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 108
                            }
                        ],
                        "text": "As our distance metric, we use theinformation-theoretic measure of cross-entropy (alsoknown as KL-distance (Kullback & Leibler 1951))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On information and su \u000e ciency"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematical Statistics"
            },
            "year": 1951
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An analysis of bayesian classiiers, in \\Proceedings of the tenth national conference on artiicial intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "An analysis of bayesian classiiers, in \\Proceedings of the tenth national conference on artiicial intelligence"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 128
                            }
                        ],
                        "text": "While other measures of separability (notably divergence) have been suggested in the statistics community for feature selection (Fukunaga 1990), these measures are often aimed at selecting features to enhance the separability of the data and may have di culty in very large dimensional spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 129
                            }
                        ],
                        "text": "While other measures of separa-bility (notably divergence) have been suggested in thestatistics community for feature selection (Fukunaga1990), these measures are often aimed at selecting fea-tures to enhance the separability of the data and mayhave di culty in very large dimensional spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Statistical Pat"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "Another feature selection methodolgy which has re-cently received much attention is the wrapper model(John, Kohavi, & P eger 1994) (Caruana & Freitag1994) (Langley & Sage 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Induction of selective bayesian classi ers, in \\Proceedings"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reuters collection available via anonymous ftp., Distribution for research purposes has been granted by Reuters and Carnegie Group. Arrangements for access were made by David Lewis"
            },
            "venue": {
                "fragments": [],
                "text": "Reuters"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12,
            "methodology": 18
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 41,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Toward-Optimal-Feature-Selection-Koller-Sahami/5ed4e1dbe10c0ac9fa00b30d1882cae1249a5a6a?sort=total-citations"
}