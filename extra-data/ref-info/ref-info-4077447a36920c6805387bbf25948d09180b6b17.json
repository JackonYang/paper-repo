{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 16
                            }
                        ],
                        "text": "It is seen from Shivakumara et al. (2013) that the method gives poor accuracy for ICDAR data when the method uses strict measures at the word level for text detection from natural scene images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 20
                            }
                        ],
                        "text": "As we observed from Shivakumara et al. (2013) for arbitrary video text detection where they exploit GVF for identifying dominant text pixels, we are inspired by this observation to propose a new Gradient Vector Symmetry (GVS) property to separate text pixels from non-text pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "It is seen from [30] that the method gives poor accuracy for ICDAR data when the method uses strict measures at the word level for text detection from natural scene images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "As we observed from [30] for arbitrary video text detection where they exploit GVF for identifying dominant text pixels, we are inspired by this observation to propose a new Gradient Vector Symmetry (GVS) property to separate text pixels from non-text pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[30] proposed a method by exploring gradient and gradient vector flow, respectively for arbitrary text detection from video without classifier\u2019s help."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 98
                            }
                        ],
                        "text": "The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11328251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67aba129ae7f19fcec1a74081768855811c1a586",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in videos is challenging due to low resolution and complex background of videos. Besides, an arbitrary orientation of scene text lines in video makes the problem more complex and challenging. This paper presents a new method that extracts text lines of any orientations based on gradient vector flow (GVF) and neighbor component grouping. The GVF of edge pixels in the Sobel edge map of the input frame is explored to identify the dominant edge pixels which represent text components. The method extracts edge components corresponding to dominant pixels in the Sobel edge map, which we call text candidates (TC) of the text lines. We propose two grouping schemes. The first finds nearest neighbors based on geometrical properties of TC to group broken segments and neighboring characters which results in word patches. The end and junction points of skeleton of the word patches are considered to eliminate false positives, which output the candidate text components (CTC). The second is based on the direction and the size of the CTC to extract neighboring CTC and to restore missing CTC, which enables arbitrarily oriented text line detection in video frame. Experimental results on different datasets, including arbitrarily oriented text data, nonhorizontal and horizontal text data, Hua's data and ICDAR-03 data (camera images), show that the proposed method outperforms existing methods in terms of recall, precision and f-measure."
            },
            "slug": "Gradient-Vector-Flow-and-Grouping-Based-Method-for-Shivakumara-Phan",
            "title": {
                "fragments": [],
                "text": "Gradient Vector Flow and Grouping-Based Method for Arbitrarily Oriented Scene Text Detection in Video Images"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A new method that extracts text lines of any orientations based on gradient vector flow (GVF) and neighbor component grouping and enables arbitrarily oriented text line detection in video frame is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 22
                            }
                        ],
                        "text": "The work presented in Phan et al. (2012) shows that the text pattern in both the Sobel and Canny edge images of the input image exhibit the same properties (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Phan et al. (2012) proposed the idea of symmetry using gradient vector flow (GVF) computed from the common information of the Sobel and the Canny edge images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 98
                            }
                        ],
                        "text": "However, when we compare the F-score, our proposed method achieves the best F-measure, similar to Phan et al. (2012) and Neumann and Matas\u2019s method (2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 22
                            }
                        ],
                        "text": "The work presented in Phan et al. (2012) shows that the text pattern in both the Sobel and Canny edge images of the input image exhibit the same properties (e.g. symmetry) while non-text in the image does not exhibit the same properties due to background complexity where the Sobel and Canny\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Phan et al. (2012) gives the highest recall compared to the other methods including our method."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 62
                            }
                        ],
                        "text": "However, it is noted from the experimental results of method (Phan et al., 2012), the precision is lower than the existing methods because the proposed GVF symmetry alone is not sufficient to eliminate non-text pixels as non-text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 25
                            }
                        ],
                        "text": "Therefore, the method in Phan et al. (2012) achieves the best recall compared to the state-of-the-art methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 102
                            }
                        ],
                        "text": "Hence this work aims to overcome these drawbacks by making use of the same basis that was proposed in Phan et al. (2012) and proposing new features that can work regardless of the text orientation to achieve better accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7268498,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "275b27f86971cb4380b1caac79ad5825c1730759",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of text detection in natural scene images is challenging because of the unconstrained sizes, colors, backgrounds and alignments of the characters. This paper proposes novel symmetry features for this task. Within a text line, the intra-character symmetry captures the correspondence between the inner contour and the outer contour of a character while the inter-character symmetry helps to extract information from the gap region between two consecutive characters. A formulation based on Gradient Vector Flow is used to detect both types of symmetry points. These points are then grouped into text lines using the consistency in sizes, colors, and stroke and gap thickness. Therefore, unlike most existing methods which use only character features, our method exploits both the text features and the gap features to improve the detection result. Experimentally, our method compares well to the state-of-the-art on public datasets for natural scenes and street-level images, an emerging category of image data. The proposed technique can be used in a wide range of multimedia applications such as content-based image/video retrieval, mobile visual search and sign translation."
            },
            "slug": "Detecting-text-in-the-real-world-Phan-Shivakumara",
            "title": {
                "fragments": [],
                "text": "Detecting text in the real world"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed novel symmetry features can be used in a wide range of multimedia applications such as content-based image/video retrieval, mobile visual search and sign translation."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38340927"
                        ],
                        "name": "Yi-Feng Pan",
                        "slug": "Yi-Feng-Pan",
                        "structuredName": {
                            "firstName": "Yi-Feng",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi-Feng Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761961"
                        ],
                        "name": "Xinwen Hou",
                        "slug": "Xinwen-Hou",
                        "structuredName": {
                            "firstName": "Xinwen",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinwen Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 91
                            }
                        ],
                        "text": "It has been shown that the combination of stroke width with gradient magnitude proposed in Pan et al. (2011) for text detection is good for classifying text and non-text components."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "The methods presented in Pan et al. (2011) and Yao et al. (2012) exploit linearity (regularity in direction of each components) in arbitrary text lines using a large number of heuristics with parameters at the component level which may limit its ability to work with arbitrary texts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 589,
                                "start": 25
                            }
                        ],
                        "text": "Therefore, the method in Phan et al. (2012) achieves the best recall compared to the state-of-the-art methods. However, it is noted from the experimental results of method (Phan et al., 2012), the precision is lower than the existing methods because the proposed GVF symmetry alone is not sufficient to eliminate non-text pixels as non-text. Besides, the scope of the method is limited to horizontal text detection but not arbitrary-oriented or curved text detection. Hence this work aims to overcome these drawbacks by making use of the same basis that was proposed in Phan et al. (2012) and proposing new features that can work regardless of the text orientation to achieve better accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 25
                            }
                        ],
                        "text": "The methods presented in Pan et al. (2011) and Yao et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 27
                            }
                        ],
                        "text": "Motivated from the work in Pan et al. (2011) where gradient was explored along with Stroke Width Transform for text detection, we propose a new Mutual Magnitude Symmetry (MMS) property for the selected pairs of pixels (more details in Section 3.1)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 44
                            }
                        ],
                        "text": "The third category, namely hybrid methods, (Pan et al., 2011) proposes both texture and region based methods for text detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 885,
                                "start": 27
                            }
                        ],
                        "text": "Motivated from the work in Pan et al. (2011) where gradient was explored along with Stroke Width Transform for text detection, we propose a new Mutual Magnitude Symmetry (MMS) property for the selected pairs of pixels (more details in Section 3.1). Thus, the objective of MMS is to check whether the gradient magnitude of a pair of pixels satisfies Mutual Magnitude Symmetry property or not to separate text pixels from non-text pixels. For instance, the same pair of pixels (pi and q) as shown in Fig. 3(b) with green color as discussed above for MDS, the proposed method checks whether gradient magnitude of these two pixels are same or not with the margin of threshold 0.15 magnitude difference. If the magnitude difference is less than 0.15 then pixels are said to satisfy MMS and hence classified considered as text pixel candidates. As we observed from Shivakumara et al. (2013) for arbitrary video text detection where they exploit GVF for identifying dominant text pixels, we are inspired by this observation to propose a new Gradient Vector Symmetry (GVS) property to separate text pixels from non-text pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 25
                            }
                        ],
                        "text": "Therefore, the method in Phan et al. (2012) achieves the best recall compared to the state-of-the-art methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 27
                            }
                        ],
                        "text": "Motivated from the work in Pan et al. (2011) where gradient was explored along with Stroke Width Transform for text detection, we propose a new Mutual Magnitude Symmetry (MMS) property for the selected pairs of pixels (more details in Section 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10564829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a79f43246bed540084ca2d1fcf99a68c69820747",
            "isKey": true,
            "numCitedBy": 410,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection and localization in natural scene images is important for content-based image analysis. This problem is challenging due to the complex background, the non-uniform illumination, the variations of text font, size and line orientation. In this paper, we present a hybrid approach to robustly detect and localize texts in natural scene images. A text region detector is designed to estimate the text existing confidence and scale information in image pyramid, which help segment candidate text components by local binarization. To efficiently filter out the non-text components, a conditional random field (CRF) model considering unary component properties and binary contextual component relationships with supervised parameter learning is proposed. Finally, text components are grouped into text lines/words with a learning-based energy minimization method. Since all the three stages are learning-based, there are very few parameters requiring manual tuning. Experimental results evaluated on the ICDAR 2005 competition dataset show that our approach yields higher precision and recall performance compared with state-of-the-art methods. We also evaluated our approach on a multilingual image dataset with promising results."
            },
            "slug": "A-Hybrid-Approach-to-Detect-and-Localize-Texts-in-Pan-Hou",
            "title": {
                "fragments": [],
                "text": "A Hybrid Approach to Detect and Localize Texts in Natural Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A hybrid approach to robustly detect and localize texts in natural scene images using a text region detector, a conditional random field model, and a learning-based energy minimization method are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257498"
                        ],
                        "name": "N. Sharma",
                        "slug": "N.-Sharma",
                        "structuredName": {
                            "firstName": "Nabin",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144167309"
                        ],
                        "name": "U. Pal",
                        "slug": "U.-Pal",
                        "structuredName": {
                            "firstName": "Umapada",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801266"
                        ],
                        "name": "M. Blumenstein",
                        "slug": "M.-Blumenstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Blumenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blumenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 86
                            }
                        ],
                        "text": "The clear differences among horizontal, non-horizontal and curved text is reported in Sharma et al. (2012). For instance, in \u2018\u2018STARBUCKS\u2019\u2019 \u2013 the name of a coffee house chain, one can see that the text is always in a circular form, embedded in a complex"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 19
                            }
                        ],
                        "text": "On the other hand, Sharma et al. (2012) requires a classification algorithm for separating horizontal text from other data and the proposed growing method is sensitive to character touching, small fonts, noise etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 86
                            }
                        ],
                        "text": "The clear differences among horizontal, non-horizontal and curved text is reported in Sharma et al. (2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17554528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "009480859dc865a19cc2ec37912f46fed3389e0f",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in video frames plays a vital role in enhancing the performance of information extraction systems because the text in video frames helps in indexing and retrieving video efficiently and accurately. This paper presents a new method for arbitrarily-oriented text detection in video, based on dominant text pixel selection, text representatives and region growing. The method uses gradient pixel direction and magnitude corresponding to Sobel edge pixels of the input frame to obtain dominant text pixels. Edge components in the Sobel edge map corresponding to dominant text pixels are then extracted and we call them text representatives. We eliminate broken segments of each text representatives to get candidate text representatives. Then the perimeter of candidate text representatives grows along the text direction in the Sobel edge map to group the neighboring text components which we call word patches. The word patches are used for finding the direction of text lines and then the word patches are expanded in the same direction in the Sobel edge map to group the neighboring word patches and to restore missing text information. This results in extraction of arbitrarily-oriented text from the video frame. To evaluate the method, we considered arbitrarily-oriented data, non-horizontal data, horizontal data, Hua's data and ICDAR-2003 competition data (Camera images). The experimental results show that the proposed method outperforms the existing method in terms of recall and f-measure."
            },
            "slug": "A-New-Method-for-Arbitrarily-Oriented-Text-in-Video-Sharma-Shivakumara",
            "title": {
                "fragments": [],
                "text": "A New Method for Arbitrarily-Oriented Text Detection in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed method outperforms the existing method in terms of recall and f-measure and results in extraction of arbitrarily-oriented text from the video frame."
            },
            "venue": {
                "fragments": [],
                "text": "2012 10th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 214
                            }
                        ],
                        "text": "\u2026robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 159
                            }
                        ],
                        "text": "Texture based methods (Chen & Yuille, 2004; FernandezCaballero et al., 2012; Shivakumara, Dutta, Tan, & Pal, 2013; Shivakumara et al., 2011; Yao et al., 2012; Yi & Tian, 2011) usually treat the pattern of text appearance as a special texture."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 146
                            }
                        ],
                        "text": "\u2026Cord, Fabrizio, & Marcotegui, 2010; Neumann & Matas, 2012; Pan, Hou, & Liu, 2011; Shivakumara, Phan, & Tan, 2011; Yao, Bai, Liu, Ma, & Tu, 2012; Yi & Tian, 2011; Yun, Jing, Yu, & Huang, 2010) focus on the use of classifier with training samples for classification of text pixels and text\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "KAIST and OSTD data provide incomplete ground truth (e.g. for small text) though it includes non-horizontal text lines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 401,
                                "start": 397
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206724376,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cb107a5b3b6539a9b9a758d91871f8b2519c79d",
            "isKey": true,
            "numCitedBy": 380,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Text information in natural scene images serves as important clues for many image-based applications such as scene understanding, content-based image retrieval, assistive navigation, and automatic geocoding. However, locating text from a complex background with multiple colors is a challenging task. In this paper, we explore a new framework to detect text strings with arbitrary orientations in complex natural scene images. Our proposed framework of text string detection consists of two steps: 1) image partition to find text character candidates based on local gradient features and color uniformity of character components and 2) character candidate grouping to detect text strings based on joint structural features of text characters in each text string such as character size differences, distances between neighboring characters, and character alignment. By assuming that a text string has at least three characters, we propose two algorithms of text string detection: 1) adjacent character grouping method and 2) text line grouping method. The adjacent character grouping method calculates the sibling groups of each character candidate as string segments and then merges the intersecting sibling groups into text string. The text line grouping method performs Hough transform to fit text line among the centroids of text candidates. Each fitted text line describes the orientation of a potential text string. The detected text string is presented by a rectangle region covering all characters whose centroids are cascaded in its text line. To improve efficiency and accuracy, our algorithms are carried out in multi-scales. The proposed methods outperform the state-of-the-art results on the public Robust Reading Dataset, which contains text only in horizontal orientation. Furthermore, the effectiveness of our methods to detect text strings with arbitrary orientations is evaluated on the Oriented Scene Text Dataset collected by ourselves containing text strings in nonhorizontal orientations."
            },
            "slug": "Text-String-Detection-From-Natural-Scenes-by-and-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Text String Detection From Natural Scenes by Structure-Based Partition and Grouping"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new framework to detect text strings with arbitrary orientations in complex natural scene images with outperform the state-of-the-art results on the public Robust Reading Dataset, which contains text only in horizontal orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056652334"
                        ],
                        "name": "Li Rong",
                        "slug": "Li-Rong",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Rong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Rong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49185019"
                        ],
                        "name": "Suyu Wang",
                        "slug": "Suyu-Wang",
                        "structuredName": {
                            "firstName": "Suyu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suyu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708019"
                        ],
                        "name": "Zhixin Shi",
                        "slug": "Zhixin-Shi",
                        "structuredName": {
                            "firstName": "Zhixin",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhixin Shi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Rong, Suyu, and Shi (2014) proposed a method for scene text extraction based on seed-based segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[33] proposed a method for scene text extraction based on seed-based segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37169805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "596471bd4666a8d292ffed8964bfc60f4c80ec1d",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a two-level method to detect text in natural scene images. In the first level, connected components (referred as CCs) are got from the images. Then candidate text lines are extracted and groups of connected components that align in horizontal or vertical direction are got. We think CCs in these groups have high probability are texts. To validate which CC is text, a SVM is trained to make an initial decision. The output of SVM is calibrated to posterior probability. Then we use the information of posterior probability of SVM and information of whether the connected component is in a group to divide the connected components into four classes: texts, non-texts, probable texts and undetermined CCs. In the second level, a conditional random field model is used to make final decision. Relationship between CCs is modeled by a network G(V, E), Vertices of the graph correspond to CCs. The determination in the first level will influence the second levels determination by giving different parameters of data term for the four classes of CCs. By this way, we not only use information of a single CCs feature, but also use the information of whether a CC is in a group to make final decision of whether the CC is text or non-text. Experiments show that the method is effective."
            },
            "slug": "A-Two-Level-Algorithm-for-Text-Detection-in-Natural-Rong-Wang",
            "title": {
                "fragments": [],
                "text": "A Two Level Algorithm for Text Detection in Natural Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A two-level method to detect text in natural scene images that not only uses information of a single CCs feature, but also uses the information of whether a CC is in a group to make final decision of whether the CC is text or non-text."
            },
            "venue": {
                "fragments": [],
                "text": "2014 11th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112721792"
                        ],
                        "name": "Quan Meng",
                        "slug": "Quan-Meng",
                        "structuredName": {
                            "firstName": "Quan",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quan Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682580"
                        ],
                        "name": "Yonghong Song",
                        "slug": "Yonghong-Song",
                        "structuredName": {
                            "firstName": "Yonghong",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonghong Song"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Meng and Song [27] proposed two steps for text detection based on salient regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Meng and Song (2012) proposed two steps for text detection based on salient regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2701672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "197d123d4f025a3876e08a8cbed95f5807f88675",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel approach to detect text in natural scenes. This approach is a type of bionic method, which imitates how human beings detect text exactly and robustly. Practically, human beings follow two steps to detect text: the first step is to find salient regions in a scene and the second step is to determine whether these salient regions are text or not. Therefore, two similar steps namely salient regions computation and text localization are used in our method. In the step of salient regions computation, a set of salient features including multi-sacle contrast, modified center-surround histogram, color spatial distribution and similarity of stroke width are used to describe an image, following with computation of salient regions based on the combination of Conditional Random Fields model and above features. Because sole letter rarely appear, in the step of text localization, salient regions are segmented and the connected components are grouped into text strings based on their features such as spatial relationships, color difference and stroke width. As an elementary unit, the text string is refined by connected component analysis. We tested the effectiveness of our method on the ICDAR 2003 database. The experimental results show that the proposed method provides promising performance in comparison with existing methods."
            },
            "slug": "Text-Detection-in-Natural-Scenes-with-Salient-Meng-Song",
            "title": {
                "fragments": [],
                "text": "Text Detection in Natural Scenes with Salient Region"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel approach to detect text in natural scenes using a type of bionic method that imitates how human beings detect text exactly and robustly and provides promising performance in comparison with existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "2012 10th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2867809"
                        ],
                        "name": "Yuning Du",
                        "slug": "Yuning-Du",
                        "structuredName": {
                            "firstName": "Yuning",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuning Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3168114"
                        ],
                        "name": "Genquan Duan",
                        "slug": "Genquan-Duan",
                        "structuredName": {
                            "firstName": "Genquan",
                            "lastName": "Duan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Genquan Duan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679380"
                        ],
                        "name": "H. Ai",
                        "slug": "H.-Ai",
                        "structuredName": {
                            "firstName": "Haizhou",
                            "lastName": "Ai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Du, Duan, and Ai (2012) developed a method based on context information of the text pixels to detect text in natural scene images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15196820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb00135ec2fe558f9469d8da1779c8949adfde13",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in natural scenes is fundamental for text image analysis. In this paper, we propose a context-based approach for robust and fast text detection. Our main contribution is that we introduce a new concept of key region, which is described with context according to stroke properties, appearance consistency and specific spatial distribution of text line. With such context descriptors, we adopt SVM to learn a context-based classifier to find key regions in candidate regions. Therein, candidate regions are connected components generated by local binarization algorithm in the areas, which are detected by an offline learned text patch detector. Experimental results on two benchmark datasets demonstrate that our approach has achieved competitive performances compared with the state-of-the-art algorithms including the stroke width transform (SWT) [1] and the hybrid approach based on CRFs [2] with speedup rates of about 1.7x~4.4x."
            },
            "slug": "Context-based-text-detection-in-natural-scenes-Du-Duan",
            "title": {
                "fragments": [],
                "text": "Context-based text detection in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new concept of key region is introduced, which is described with context according to stroke properties, appearance consistency and specific spatial distribution of text line, and adopted SVM to learn a context-based classifier to find key regions in candidate regions."
            },
            "venue": {
                "fragments": [],
                "text": "2012 19th IEEE International Conference on Image Processing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064458131"
                        ],
                        "name": "Bo Bai",
                        "slug": "Bo-Bai",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820427"
                        ],
                        "name": "Fei Yin",
                        "slug": "Fei-Yin",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] proposed a two level algorithm for text detection in natural scene images using connected component analysis and an SVM classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Bai, Yin, and Liu (2014) proposed a two level algorithm for text detection in natural scene images using connected component analysis and an SVM classifier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24123203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1296b8e309f80c9473286c3f737558cb58cdacc",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text extraction, i.e., segmenting text pixels from background, is an important step before the text can be recognized. It is a challenging problem due to the cluttered background and the variation of lighting. In this paper, we propose a seed-based segmentation method that can automatically judge the text polarity, extract seed points of text and background, and segment texts by semi-supervised learning (SSL). First, we estimate the text polarity and the stroke width using gradient local correlation. Then, all the points in the middle of stroke edge pairs satisfying the width and polarity are taken as foreground seeds, and the points in the middle of the edge pairs with opposite polarity are taken as background seeds. The whole image is then segmented into text and background using an SSL algorithm. Owing to the accurate estimate of text polarity and extraction of seed points, the proposed method yields good segmentation performance. Experimental results on the KAIST dataset demonstrate the superiority of the method."
            },
            "slug": "A-Seed-Based-Segmentation-Method-for-Scene-Text-Bai-Yin",
            "title": {
                "fragments": [],
                "text": "A Seed-Based Segmentation Method for Scene Text Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A seed-based segmentation method that can automatically judge the text polarity, extract seed points of text and background, and segment texts by semi-supervised learning (SSL) is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2014 11th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114172383"
                        ],
                        "name": "Qi Zheng",
                        "slug": "Qi-Zheng",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72387933"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46433090"
                        ],
                        "name": "Yi Zhou",
                        "slug": "Yi-Zhou",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1882300"
                        ],
                        "name": "Congcong Gu",
                        "slug": "Congcong-Gu",
                        "structuredName": {
                            "firstName": "Congcong",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Congcong Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145676478"
                        ],
                        "name": "Haibing Guan",
                        "slug": "Haibing-Guan",
                        "structuredName": {
                            "firstName": "Haibing",
                            "lastName": "Guan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haibing Guan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 45
                            }
                        ],
                        "text": "The SIFT features are used in several papers [7, 34, 35] for text recognition in scene images and they have shown that SIFT is useful for finding matches between the target and reference character images because of its invariance to scale, rotation, illumination, and viewpoint."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41534100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bba21702743cac9460b03c5f227806ae3781b35",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach using local features to resolve problems in text localization and recognition in complex scenes. Low image quality, complex background and variations of text make these problems challenging. Our approach includes the following stages: (1) Template images are generated automatically; (2) SIFT features are extracted and matched to template images; (3) Multiple single-character-areas are located using segmentation algorithm based upon multiple-size sliding subwindows; (4) An voting and geometric verification algorithm is used to identify final results. This framework thus is essentially simple by skipping many steps, such as normalization, binarization and OCR, which are required in previous methods. Moreover, this framework is robust as only SIFT feature is used. We evaluated our method using 200,000+ images in 3 scripts (Chinese, Japanese and Korean). We obtained average single-character success rate of 77.3% (highest 94.1%), average multiplecharacter success rate of 63.9% (highest 89.6%)."
            },
            "slug": "Text-Localization-and-Recognition-in-Complex-Scenes-Zheng-Chen",
            "title": {
                "fragments": [],
                "text": "Text Localization and Recognition in Complex Scenes Using Local Features"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An approach using local features to resolve problems in text localization and recognition in complex scenes by skipping many steps, such as normalization, binarization and OCR, which are required in previous methods is described."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[25] proposed a method based on character appearance and structure modelling to detect texts in natural scene images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Yi and Tian (2013) proposed a method based on character appearance and structure modeling to detect texts in natural scene images."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8284341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b05b01d031beb0c840279c96e4052b33bbb0742",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-extraction-from-scene-images-by-character-and-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Text extraction from scene images by character appearance and structure modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112558035"
                        ],
                        "name": "Y. Wei",
                        "slug": "Y.-Wei",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Wei",
                            "middleNames": [
                                "Cheng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116719879"
                        ],
                        "name": "C. Lin",
                        "slug": "C.-Lin",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Lin",
                            "middleNames": [
                                "Hong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 39
                            }
                        ],
                        "text": "The same conclusions can be drawn from Wei and Lin (2012) where robust video text detection system is proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 82
                            }
                        ],
                        "text": "Despite many efforts (Mishra et al., 2012; Smith et al., 2011; Wang et al., 2011; Wei & Lin, 2012; Weinman, Learned-Miller, & Hanson, 2009) for text detection from natural scene images, it is still considered a challenging and unsolved problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 195
                            }
                        ],
                        "text": "2 enhances the capability of content-based image retrieval models in order to retrieve meaningful events from the event database because it provides semantics to the content of images and videos [6-9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 276
                            }
                        ],
                        "text": "\u2026of content-based image retrieval models in order to retrieve meaningful events from the event database because it provides semantics to the content of images and videos (Mishra, Alahari, & Jawahar, 2012; Smith, Field, & Learned-Miller, 2011; Wang, Babenko, & Belongie, 2011; Wei & Lin, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "The same conclusions can be drawn from [9] where robust video text detection system is proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 21
                            }
                        ],
                        "text": "Despite many efforts [6-10] for text detection from natural scene images, it is still considered a challenging and unsolved problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5552719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c6778396386825243c7b8ed6be8f98519ad6c25",
            "isKey": true,
            "numCitedBy": 30,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-robust-video-text-detection-approach-using-SVM-Wei-Lin",
            "title": {
                "fragments": [],
                "text": "A robust video text detection approach using SVM"
            },
            "venue": {
                "fragments": [],
                "text": "Expert Syst. Appl."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108641611"
                        ],
                        "name": "Seonghun Lee",
                        "slug": "Seonghun-Lee",
                        "structuredName": {
                            "firstName": "Seonghun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seonghun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098811521"
                        ],
                        "name": "Min Su Cho",
                        "slug": "Min-Su-Cho",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cho",
                            "middleNames": [
                                "Su"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Su Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731707"
                        ],
                        "name": "Kyomin Jung",
                        "slug": "Kyomin-Jung",
                        "structuredName": {
                            "firstName": "Kyomin",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyomin Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152672892"
                        ],
                        "name": "J. H. Kim",
                        "slug": "J.-H.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 225
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 [37], ICDAR2005 [11], ICDAR2011 [38] robust reading competition data, Street View Text data (SVT) [39], KAIST scene text data [40], Microsoft data [23], Oriented Scene Text Database (OSTD) [20] and MSRA Text Detection 500 data (MSRA-TD500) [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16267173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e26c459cb7c6bcb261404ef18643b71ce15f369",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a framework for isolating text regions from natural scene images. The main algorithm has two functions: it generates text region candidates, and it verifies of the label of the candidates (text or non-text). The text region candidates are generated through a modified K-means clustering algorithm, which references texture features, edge information and color information. The candidate labels are then verified in a global sense by the Markov Random Field model where collinearity weight is added as long as most texts are aligned. The proposed method achieves reasonable accuracy for text extraction from moderately difficult examples from the ICDAR 2003 database."
            },
            "slug": "Scene-Text-Extraction-with-Edge-Constraint-and-Text-Lee-Cho",
            "title": {
                "fragments": [],
                "text": "Scene Text Extraction with Edge Constraint and Text Collinearity"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed method achieves reasonable accuracy for text extraction from moderately difficult examples from the ICDAR 2003 database."
            },
            "venue": {
                "fragments": [],
                "text": "2010 20th International Conference on Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149815080"
                        ],
                        "name": "Anjan Dutta",
                        "slug": "Anjan-Dutta",
                        "structuredName": {
                            "firstName": "Anjan",
                            "lastName": "Dutta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anjan Dutta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144167309"
                        ],
                        "name": "U. Pal",
                        "slug": "U.-Pal",
                        "structuredName": {
                            "firstName": "Umapada",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Pal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16567877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef436368a5b08681f0cdde65cce41bb397e80604",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address two complex issues: 1) Text frame classification and 2) Multi-oriented text detection in video text frame. We first divide a video frame into 16 blocks and propose a combination of wavelet and median-moments with k-means clustering at the block level to identify probable text blocks. For each probable text block, the method applies the same combination of feature with k-means clustering over a sliding window running through the blocks to identify potential text candidates. We introduce a new idea of symmetry on text candidates in each block based on the observation that pixel distribution in text exhibits a symmetric pattern. The method integrates all blocks containing text candidates in the frame and then all text candidates are mapped on to a Sobel edge map of the original frame to obtain text representatives. To tackle the multi-orientation problem, we present a new method called Angle Projection Boundary Growing (APBG) which is an iterative algorithm and works based on a nearest neighbor concept. APBG is then applied on the text representatives to fix the bounding box for multi-oriented text lines in the video frame. Directional information is used to eliminate false positives. Experimental results on a variety of datasets such as non-horizontal, horizontal, publicly available data (Hua\u2019s data) and ICDAR-03 competition data (camera images) show that the proposed method outperforms existing methods proposed for video and the state of the art methods for scene text as well."
            },
            "slug": "Multi-oriented-scene-text-detection-in-video-based-Shivakumara-Dutta",
            "title": {
                "fragments": [],
                "text": "Multi-oriented scene text detection in video based on wavelet and angle projection boundary growing"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Experimental results on a variety of datasets show that the proposed method outperforms existing methods proposed for video and the state of the art methods for scene text as well."
            },
            "venue": {
                "fragments": [],
                "text": "Multimedia Tools and Applications"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1959339"
                        ],
                        "name": "Cunzhao Shi",
                        "slug": "Cunzhao-Shi",
                        "structuredName": {
                            "firstName": "Cunzhao",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cunzhao Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683416"
                        ],
                        "name": "Chunheng Wang",
                        "slug": "Chunheng-Wang",
                        "structuredName": {
                            "firstName": "Chunheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658590"
                        ],
                        "name": "Baihua Xiao",
                        "slug": "Baihua-Xiao",
                        "structuredName": {
                            "firstName": "Baihua",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baihua Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145955846"
                        ],
                        "name": "Yang Zhang",
                        "slug": "Yang-Zhang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145284815"
                        ],
                        "name": "Song Gao",
                        "slug": "Song-Gao",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Gao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Shi et al [26] proposed a graph model based on MSER and then used colour and geometric features to classify text and non-text by minimizing a cost function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6801531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22064d411a128de1a91ad87f86055e254c9c5321",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scene-text-detection-using-graph-model-built-upon-Shi-Wang",
            "title": {
                "fragments": [],
                "text": "Scene text detection using graph model built upon maximally stable extremal regions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3029,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels. It then uses features at the character level to study the regularity of texts to locate text in the images. The main problems of texture based methods lie in the large number of features that heavily depend on the classifier in use and the number of training samples. In addition, most of the methods focus on horizontal and non-horizontal straight lines but not curved lines. Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem. The method is shown to work well for text detection in video as well as natural scene images. However, angle projection boundary growing assumes that text components in text lines are in one direction. As a result, the method is good for non-horizontal text lines but not text lines appearing in arc and ellipse shapes. In addition, the primary focus of that method is to detect text in video but not in scene images. The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012). These methods first identify text regions through edge detection or clustering followed by some heuristics to classify text components followed by a process to eliminate false positives. However, these methods heavily depend on heuristics and parameters setting. Epshtein, Ofek, and Wexler (2010) proposed an image operator called the Stroke Width Transform (SWT) on the Canny edge image to detect texts. This method looks for similar stroke widths to group text components and it studies the component properties to classify text components. Neumann and Matas (2012) exploited Maximally Stable Extremal Regions (MSER) to extract text components. The method uses geometrical properties of the components and a classifier to detect the text. Yao et al. (2012) proposed a method for arbitrary (non-horizontal straight lines but not curved text lines) text line detection. This method uses the SWT for extracting components and it studies the various features based on color, geometrical properties at the component level for classification of text and non-text components. To handle multi-oriented texts, this method uses the linearity of the text components (considering that characters in a text line have uniform orientation). Phan et al. (2012) proposed the idea of symmetry using gradient vector flow (GVF) computed from the common information of the Sobel and the Canny edge images. Grouping of text components is done based on the components\u2019 geometrical properties and text verification is done using a classifier with training samples to achieve good accuracy. Du, Duan, and Ai (2012) developed a method based on context information of the text pixels to detect text in natural scene images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3748,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels. It then uses features at the character level to study the regularity of texts to locate text in the images. The main problems of texture based methods lie in the large number of features that heavily depend on the classifier in use and the number of training samples. In addition, most of the methods focus on horizontal and non-horizontal straight lines but not curved lines. Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem. The method is shown to work well for text detection in video as well as natural scene images. However, angle projection boundary growing assumes that text components in text lines are in one direction. As a result, the method is good for non-horizontal text lines but not text lines appearing in arc and ellipse shapes. In addition, the primary focus of that method is to detect text in video but not in scene images. The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012). These methods first identify text regions through edge detection or clustering followed by some heuristics to classify text components followed by a process to eliminate false positives. However, these methods heavily depend on heuristics and parameters setting. Epshtein, Ofek, and Wexler (2010) proposed an image operator called the Stroke Width Transform (SWT) on the Canny edge image to detect texts. This method looks for similar stroke widths to group text components and it studies the component properties to classify text components. Neumann and Matas (2012) exploited Maximally Stable Extremal Regions (MSER) to extract text components. The method uses geometrical properties of the components and a classifier to detect the text. Yao et al. (2012) proposed a method for arbitrary (non-horizontal straight lines but not curved text lines) text line detection. This method uses the SWT for extracting components and it studies the various features based on color, geometrical properties at the component level for classification of text and non-text components. To handle multi-oriented texts, this method uses the linearity of the text components (considering that characters in a text line have uniform orientation). Phan et al. (2012) proposed the idea of symmetry using gradient vector flow (GVF) computed from the common information of the Sobel and the Canny edge images. Grouping of text components is done based on the components\u2019 geometrical properties and text verification is done using a classifier with training samples to achieve good accuracy. Du, Duan, and Ai (2012) developed a method based on context information of the text pixels to detect text in natural scene images. The context is studied using stroke properties and spatial distribution of the text line. The method uses an SVM classifier to learn the context and hence the performance of the method depends on the classifier and training samples. Yi and Tian (2013) proposed a method based on character appearance and structure modeling to detect texts in natural scene images. The method proposes a model by making use of corner and interest points detected by the Harris corner detector. This method also depends on the classifier in use and training samples to achieve good accuracy. Shi, Wang, Xiao, Zhang, and Gao (2013) proposed a graph model based on MSER and then used color and geometric features to classify text and non-text by minimizing a cost function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2005,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels. It then uses features at the character level to study the regularity of texts to locate text in the images. The main problems of texture based methods lie in the large number of features that heavily depend on the classifier in use and the number of training samples. In addition, most of the methods focus on horizontal and non-horizontal straight lines but not curved lines. Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem. The method is shown to work well for text detection in video as well as natural scene images. However, angle projection boundary growing assumes that text components in text lines are in one direction. As a result, the method is good for non-horizontal text lines but not text lines appearing in arc and ellipse shapes. In addition, the primary focus of that method is to detect text in video but not in scene images. The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012). These methods first identify text regions through edge detection or clustering followed by some heuristics to classify text components followed by a process to eliminate false positives. However, these methods heavily depend on heuristics and parameters setting. Epshtein, Ofek, and Wexler (2010) proposed an image operator called the Stroke Width Transform (SWT) on the Canny edge image to detect texts. This method looks for similar stroke widths to group text components and it studies the component properties to classify text components. Neumann and Matas (2012) exploited Maximally Stable Extremal Regions (MSER) to extract text components."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3388,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels. It then uses features at the character level to study the regularity of texts to locate text in the images. The main problems of texture based methods lie in the large number of features that heavily depend on the classifier in use and the number of training samples. In addition, most of the methods focus on horizontal and non-horizontal straight lines but not curved lines. Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem. The method is shown to work well for text detection in video as well as natural scene images. However, angle projection boundary growing assumes that text components in text lines are in one direction. As a result, the method is good for non-horizontal text lines but not text lines appearing in arc and ellipse shapes. In addition, the primary focus of that method is to detect text in video but not in scene images. The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012). These methods first identify text regions through edge detection or clustering followed by some heuristics to classify text components followed by a process to eliminate false positives. However, these methods heavily depend on heuristics and parameters setting. Epshtein, Ofek, and Wexler (2010) proposed an image operator called the Stroke Width Transform (SWT) on the Canny edge image to detect texts. This method looks for similar stroke widths to group text components and it studies the component properties to classify text components. Neumann and Matas (2012) exploited Maximally Stable Extremal Regions (MSER) to extract text components. The method uses geometrical properties of the components and a classifier to detect the text. Yao et al. (2012) proposed a method for arbitrary (non-horizontal straight lines but not curved text lines) text line detection. This method uses the SWT for extracting components and it studies the various features based on color, geometrical properties at the component level for classification of text and non-text components. To handle multi-oriented texts, this method uses the linearity of the text components (considering that characters in a text line have uniform orientation). Phan et al. (2012) proposed the idea of symmetry using gradient vector flow (GVF) computed from the common information of the Sobel and the Canny edge images. Grouping of text components is done based on the components\u2019 geometrical properties and text verification is done using a classifier with training samples to achieve good accuracy. Du, Duan, and Ai (2012) developed a method based on context information of the text pixels to detect text in natural scene images. The context is studied using stroke properties and spatial distribution of the text line. The method uses an SVM classifier to learn the context and hence the performance of the method depends on the classifier and training samples. Yi and Tian (2013) proposed a method based on character appearance and structure modeling to detect texts in natural scene images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 23
                            }
                        ],
                        "text": "Texture based methods (Chen & Yuille, 2004; FernandezCaballero et al., 2012; Shivakumara, Dutta, Tan, & Pal, 2013; Shivakumara et al., 2011; Yao et al., 2012; Yi & Tian, 2011) usually treat the pattern of text appearance as a special texture."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2684,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels. It then uses features at the character level to study the regularity of texts to locate text in the images. The main problems of texture based methods lie in the large number of features that heavily depend on the classifier in use and the number of training samples. In addition, most of the methods focus on horizontal and non-horizontal straight lines but not curved lines. Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem. The method is shown to work well for text detection in video as well as natural scene images. However, angle projection boundary growing assumes that text components in text lines are in one direction. As a result, the method is good for non-horizontal text lines but not text lines appearing in arc and ellipse shapes. In addition, the primary focus of that method is to detect text in video but not in scene images. The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012). These methods first identify text regions through edge detection or clustering followed by some heuristics to classify text components followed by a process to eliminate false positives. However, these methods heavily depend on heuristics and parameters setting. Epshtein, Ofek, and Wexler (2010) proposed an image operator called the Stroke Width Transform (SWT) on the Canny edge image to detect texts. This method looks for similar stroke widths to group text components and it studies the component properties to classify text components. Neumann and Matas (2012) exploited Maximally Stable Extremal Regions (MSER) to extract text components. The method uses geometrical properties of the components and a classifier to detect the text. Yao et al. (2012) proposed a method for arbitrary (non-horizontal straight lines but not curved text lines) text line detection. This method uses the SWT for extracting components and it studies the various features based on color, geometrical properties at the component level for classification of text and non-text components. To handle multi-oriented texts, this method uses the linearity of the text components (considering that characters in a text line have uniform orientation). Phan et al. (2012) proposed the idea of symmetry using gradient vector flow (GVF) computed from the common information of the Sobel and the Canny edge images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1734,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels. It then uses features at the character level to study the regularity of texts to locate text in the images. The main problems of texture based methods lie in the large number of features that heavily depend on the classifier in use and the number of training samples. In addition, most of the methods focus on horizontal and non-horizontal straight lines but not curved lines. Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem. The method is shown to work well for text detection in video as well as natural scene images. However, angle projection boundary growing assumes that text components in text lines are in one direction. As a result, the method is good for non-horizontal text lines but not text lines appearing in arc and ellipse shapes. In addition, the primary focus of that method is to detect text in video but not in scene images. The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012). These methods first identify text regions through edge detection or clustering followed by some heuristics to classify text components followed by a process to eliminate false positives. However, these methods heavily depend on heuristics and parameters setting. Epshtein, Ofek, and Wexler (2010) proposed an image operator called the Stroke Width Transform (SWT) on the Canny edge image to detect texts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 678,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels. It then uses features at the character level to study the regularity of texts to locate text in the images. The main problems of texture based methods lie in the large number of features that heavily depend on the classifier in use and the number of training samples. In addition, most of the methods focus on horizontal and non-horizontal straight lines but not curved lines. Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": "Generally, most existing methods (Chen & Yuille, 2004; Fernandez-Caballero, Lopez, & Castillo, 2012; Minetto, Thome, Cord, Fabrizio, & Marcotegui, 2010; Neumann & Matas, 2012; Pan, Hou, & Liu, 2011; Shivakumara, Phan, & Tan, 2011; Yao, Bai, Liu, Ma, & Tu, 2012; Yi & Tian, 2011; Yun, Jing, Yu, &\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 182
                            }
                        ],
                        "text": "Methods Precision Recall F-score\nProposed method 0.70 0.68 0.69 Yao et al. (2012) \u2013 mixture 0.63 0.63 0.63 Yao et al. (2012) \u2013 ICDAR 0.53 0.52 0.53 Epshtein et al. (2010) 0.25 0.25 0.25 Chen and Yuille (2004) 0.05 0.05 0.05\nThe bold values indicate that highest accuracy of the method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2196,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels. Yi and Tian (2011) proposed a partitioning method using gradient and color information of pixels. It then uses features at the character level to study the regularity of texts to locate text in the images. The main problems of texture based methods lie in the large number of features that heavily depend on the classifier in use and the number of training samples. In addition, most of the methods focus on horizontal and non-horizontal straight lines but not curved lines. Shivakumara et al. (2013) proposed a combination of wavelet and median moments to identify text candidates at the block level followed by an angle projection boundary growing method to deal with multi-oriented text problem. The method is shown to work well for text detection in video as well as natural scene images. However, angle projection boundary growing assumes that text components in text lines are in one direction. As a result, the method is good for non-horizontal text lines but not text lines appearing in arc and ellipse shapes. In addition, the primary focus of that method is to detect text in video but not in scene images. The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012). These methods first identify text regions through edge detection or clustering followed by some heuristics to classify text components followed by a process to eliminate false positives. However, these methods heavily depend on heuristics and parameters setting. Epshtein, Ofek, and Wexler (2010) proposed an image operator called the Stroke Width Transform (SWT) on the Canny edge image to detect texts. This method looks for similar stroke widths to group text components and it studies the component properties to classify text components. Neumann and Matas (2012) exploited Maximally Stable Extremal Regions (MSER) to extract text components. The method uses geometrical properties of the components and a classifier to detect the text. Yao et al. (2012) proposed a method for arbitrary (non-horizontal straight lines but not curved text lines) text line detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Chen and Yuille (2004) extracted 79 features for the text region given by a classifier and the method uses an adaptive binarization method to classify text and non-text pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61234963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37ba7b9a823e8a400046bd149b7756adf5d698da",
            "isKey": true,
            "numCitedBy": 512,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier. Commercial OCR software is used to read the text or reject it as a non-text region. The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "slug": "Detecting-and-reading-text-in-natural-scenes-Chen-Yuille",
            "title": {
                "fragments": [],
                "text": "Detecting and reading text in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "[12] and Neumann & Matas\u2019s method [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 35
                            }
                        ],
                        "text": "3 Generally, most existing methods [13-21] focus on the use of classifier with training samples for classification of text pixels and text components based on the characteristics of character shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 143
                            }
                        ],
                        "text": "\u2026most existing methods (Chen & Yuille, 2004; Fernandez-Caballero, Lopez, & Castillo, 2012; Minetto, Thome, Cord, Fabrizio, & Marcotegui, 2010; Neumann & Matas, 2012; Pan, Hou, & Liu, 2011; Shivakumara, Phan, & Tan, 2011; Yao, Bai, Liu, Ma, & Tu, 2012; Yi & Tian, 2011; Yun, Jing, Yu, & Huang,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 121
                            }
                        ],
                        "text": "However, when we compare the F-score, our proposed method achieves the best F-measure, similar to Phan et al. (2012) and Neumann and Matas\u2019s method (2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 42
                            }
                        ],
                        "text": "The next category is region-based methods [12, 17, 21, 22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Neumann and Matas (2012) exploited Maximally Stable Extremal Regions (MSER) to extract text components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 43
                            }
                        ],
                        "text": "The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 163
                            }
                        ],
                        "text": "Generally, most existing methods (Chen & Yuille, 2004; Fernandez-Caballero, Lopez, & Castillo, 2012; Minetto, Thome, Cord, Fabrizio, & Marcotegui, 2010; Neumann & Matas, 2012; Pan, Hou, & Liu, 2011; Shivakumara, Phan, & Tan, 2011; Yao, Bai, Liu, Ma, & Tu, 2012; Yi & Tian, 2011; Yun, Jing, Yu, & Huang, 2010) focus on the use of classifier with training samples for classification of text pixels and text components based on the characteristics of character shapes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Neumann and Matas [17] exploited Maximally Stable Extremal Regions (MSER) to extract text components."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206591895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b595c9e969e5605f62da51b6c16dad8aad3e0e",
            "isKey": true,
            "numCitedBy": 790,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "An end-to-end real-time scene text localization and recognition method is presented. The real-time performance is achieved by posing the character detection problem as an efficient sequential selection from the set of Extremal Regions (ERs). The ER detector is robust to blur, illumination, color and texture variation and handles low-contrast text. In the first classification stage, the probability of each ER being a character is estimated using novel features calculated with O(1) complexity per region tested. Only ERs with locally maximal probability are selected for the second stage, where the classification is improved using more computationally expensive features. A highly efficient exhaustive search with feedback loops is then applied to group ERs into words and to select the most probable character segmentation. Finally, text is recognized in an OCR stage trained using synthetic fonts. The method was evaluated on two public datasets. On the ICDAR 2011 dataset, the method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end-to-end text recognition. On the more challenging Street View Text dataset, the method achieves state-of-the-art recall. The robustness of the proposed method against noise and low contrast of characters is demonstrated by \u201cfalse positives\u201d caused by detected watermark text in the dataset."
            },
            "slug": "Real-time-scene-text-localization-and-recognition-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "Real-time scene text localization and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The proposed end-to-end real-time scene text localization and recognition method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end- to-end text recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146275501"
                        ],
                        "name": "Yi Ma",
                        "slug": "Yi-Ma",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 147
                            }
                        ],
                        "text": "The reported quantitative result in Table 5 shows that the proposed method achieves better recall, precision and F-measure compared to the method (Yao et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 47
                            }
                        ],
                        "text": "The methods presented in Pan et al. (2011) and Yao et al. (2012) exploit linearity (regularity in direction of each components) in arbitrary text lines using a large number of heuristics with parameters at the component level which may limit its ability to work with arbitrary texts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Yao et al. (2012) proposed a method for arbitrary (non-horizontal straight lines but not curved text lines) text line detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 150
                            }
                        ],
                        "text": "\u2026components was computed from SWT image using a simple rule that the ratio of SWT values of neighboring pixels is less than 3.0 which is suggested by Yao et al. (2012), with the additional component filtering, that is, width variation (the ratio of standard deviation and mean of stroke width),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 277
                            }
                        ],
                        "text": "\u2026robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 107
                            }
                        ],
                        "text": ", 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 141
                            }
                        ],
                        "text": "Texture based methods (Chen & Yuille, 2004; FernandezCaballero et al., 2012; Shivakumara, Dutta, Tan, & Pal, 2013; Shivakumara et al., 2011; Yao et al., 2012; Yi & Tian, 2011) usually treat the pattern of text appearance as a special texture."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 81
                            }
                        ],
                        "text": "For MSRA-TD500 and our data, we use the text line evaluation method suggested in Yao et al. (2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 29
                            }
                        ],
                        "text": "The definitions suggested in Yao et al. (2012) are used for calculating recall, precision and f-measure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 123
                            }
                        ],
                        "text": "Though SWT is invariant to the orientation of the text, it requires additional features to handle curved text as stated in Yao et al. (2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 64
                            }
                        ],
                        "text": "Methods Precision Recall F-score\nProposed method 0.70 0.68 0.69 Yao et al. (2012) \u2013 mixture 0.63 0.63 0.63 Yao et al. (2012) \u2013 ICDAR 0.53 0.52 0.53 Epshtein et al. (2010) 0.25 0.25 0.25 Chen and Yuille (2004) 0.05 0.05 0.05\nThe bold values indicate that highest accuracy of the method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 124
                            }
                        ],
                        "text": "The next category is region-based methods (Neumann & Matas, 2012; Phan, Shivakumara, & Tan, 2012; Shivakumara et al., 2013; Yao et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 82
                            }
                        ],
                        "text": "Therefore, the SWT is extended to non-horizontal straight text lines detection in Yao et al. (2012) where the authors proposed additional features based on linearity of text components in text lines."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14015069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "955028d46ab7237a30cfaab3a351c34f38ee0be5",
            "isKey": true,
            "numCitedBy": 635,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing popularity of practical vision systems and smart phones, text detection in natural scenes becomes a critical yet challenging task. Most existing methods have focused on detecting horizontal or near-horizontal texts. In this paper, we propose a system which detects texts of arbitrary orientations in natural images. Our algorithm is equipped with a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts. To better evaluate our algorithm and compare it with other competing algorithms, we generate a new dataset, which includes various texts in diverse real-world scenarios; we also propose a protocol for performance evaluation. Experiments on benchmark datasets and the proposed dataset demonstrate that our algorithm compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on texts of arbitrary orientations in complex natural scenes."
            },
            "slug": "Detecting-texts-of-arbitrary-orientations-in-images-Yao-Bai",
            "title": {
                "fragments": [],
                "text": "Detecting texts of arbitrary orientations in natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A system which detects texts of arbitrary orientations in natural images using a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts to better evaluate its algorithm and compare it with other competing algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110157976"
                        ],
                        "name": "\u00c1lvaro Gonzalez",
                        "slug": "\u00c1lvaro-Gonzalez",
                        "structuredName": {
                            "firstName": "\u00c1lvaro",
                            "lastName": "Gonzalez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c1lvaro Gonzalez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683950"
                        ],
                        "name": "L. M. Bergasa",
                        "slug": "L.-M.-Bergasa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Bergasa",
                            "middleNames": [
                                "Miguel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. Bergasa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 176
                            }
                        ],
                        "text": "In recent years, text detection and recognition from natural scene images has gained much research attention aiming to achieve comparable accuracy to that in document analysis [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Gonz\u00e1lez and Bergasa (2013) gives the highest precision but the worst recall compared to the other methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Gonzalez and Bergasa [1] proposed a text reading algorithm for natural scene images using a set of gradient and geometric features of text and dynamic programming for characters recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Gonz\u00e1lez and Bergasa (2013) proposed a text reading algorithm for natural scene images using a set of gradient and geometric features of text and dynamic programming for characters recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Gonzalez et al\u2019s method [1] gives the highest 0 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 177
                            }
                        ],
                        "text": "In recent years, text detection and recognition from natural scene images has gained much research attention aiming to achieve comparable accuracy to that in document analysis (Gonz\u00e1lez & Bergasa, 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1276308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fde74b961c71a418365a215da2d71f79fa7ef390",
            "isKey": true,
            "numCitedBy": 61,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-text-reading-algorithm-for-natural-images-Gonzalez-Bergasa",
            "title": {
                "fragments": [],
                "text": "A text reading algorithm for natural images"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796455"
                        ],
                        "name": "R. Minetto",
                        "slug": "R.-Minetto",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Minetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Minetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728523"
                        ],
                        "name": "Nicolas Thome",
                        "slug": "Nicolas-Thome",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Thome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Thome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51021910"
                        ],
                        "name": "M. Cord",
                        "slug": "M.-Cord",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Cord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715861"
                        ],
                        "name": "Jonathan Fabrizio",
                        "slug": "Jonathan-Fabrizio",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Fabrizio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Fabrizio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740693"
                        ],
                        "name": "B. Marcotegui",
                        "slug": "B.-Marcotegui",
                        "structuredName": {
                            "firstName": "Beatriz",
                            "lastName": "Marcotegui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Marcotegui"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14339518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6372065966fce1efcba0abafc50c1abc25152d85",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in natural images remains a very challenging task. For instance, in an urban context, the detection is very difficult due to large variations in terms of shape, size, color, orientation, and the image may be blurred or have irregular illumination, etc. In this paper, we describe a robust and accurate multiresolution approach to detect and classify text regions in such scenarios. Based on generation/validation paradigm, we first segment images to detect character regions with a multiresolution algorithm able to manage large character size variations. The segmented regions are then filtered out using shapebased classification, and neighboring characters are merged to generate text hypotheses. A validation step computes a region signature based on texture analysis to reject false positives. We evaluate our algorithm in two challenging databases, achieving very good results."
            },
            "slug": "Snoopertext:-A-multiresolution-system-for-text-in-Minetto-Thome",
            "title": {
                "fragments": [],
                "text": "Snoopertext: A multiresolution system for text detection in complex visual scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper describes a robust and accurate multiresolution approach to detect and classify text regions in such scenarios in an urban context with very good results."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Image Processing"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39719398"
                        ],
                        "name": "Anand Mishra",
                        "slug": "Anand-Mishra",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Mishra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Mishra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 22
                            }
                        ],
                        "text": "Despite many efforts (Mishra et al., 2012; Smith et al., 2011; Wang et al., 2011; Wei & Lin, 2012; Weinman, Learned-Miller, & Hanson, 2009) for text detection from natural scene images, it is still considered a challenging and unsolved problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5728901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66b71064b99331f908b60cb6d138f2ebea5bdcca",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text recognition has gained significant attention from the computer vision community in recent years. Recognizing such text is a challenging problem, even more so than the recognition of scanned documents. In this work, we focus on the problem of recognizing text extracted from street images. We present a framework that exploits both bottom-up and top-down cues. The bottom-up cues are derived from individual character detections from the image. We build a Conditional Random Field model on these detections to jointly model the strength of the detections and the interactions between them. We impose top-down cues obtained from a lexicon-based prior, i.e. language statistics, on the model. The optimal word represented by the text image is obtained by minimizing the energy function corresponding to the random field model. We show significant improvements in accuracies on two challenging public datasets, namely Street View Text (over 15%) and ICDAR 2003 (nearly 10%)."
            },
            "slug": "Top-down-and-bottom-up-cues-for-scene-text-Mishra-Karteek",
            "title": {
                "fragments": [],
                "text": "Top-down and bottom-up cues for scene text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a framework that exploits both bottom-up and top-down cues in the problem of recognizing text extracted from street images, and shows significant improvements in accuracies on two challenging public datasets, namely Street View Text and ICDAR 2003."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463454"
                        ],
                        "name": "H. Koo",
                        "slug": "H.-Koo",
                        "structuredName": {
                            "firstName": "Hyung",
                            "lastName": "Koo",
                            "middleNames": [
                                "Il"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111426782"
                        ],
                        "name": "Duck Hoon Kim",
                        "slug": "Duck-Hoon-Kim",
                        "structuredName": {
                            "firstName": "Duck",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hoon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duck Hoon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lastly, conclusion and future works are discussed in Section 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7486022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b45fdcdfb642262cf5b2b2b9e574ba47f471d8a",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new scene text detection algorithm based on two machine learning classifiers: one allows us to generate candidate word regions and the other filters out nontext ones. To be precise, we extract connected components (CCs) in images by using the maximally stable extremal region algorithm. These extracted CCs are partitioned into clusters so that we can generate candidate regions. Unlike conventional methods relying on heuristic rules in clustering, we train an AdaBoost classifier that determines the adjacency relationship and cluster CCs by using their pairwise relations. Then we normalize candidate word regions and determine whether each region contains text or not. Since the scale, skew, and color of each candidate can be estimated from CCs, we develop a text/nontext classifier for normalized images. This classifier is based on multilayer perceptrons and we can control recall and precision rates with a single free parameter. Finally, we extend our approach to exploit multichannel information. Experimental results on ICDAR 2005 and 2011 robust reading competition datasets show that our method yields the state-of-the-art performance both in speed and accuracy."
            },
            "slug": "Scene-Text-Detection-via-Connected-Component-and-Koo-Kim",
            "title": {
                "fragments": [],
                "text": "Scene Text Detection via Connected Component Clustering and Nontext Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new scene text detection algorithm based on two machine learning classifiers that allows us to generate candidate word regions and the other filters out nontext ones, and extends the approach to exploit multichannel information."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": "Texture based methods (Chen & Yuille, 2004; FernandezCaballero et al., 2012; Shivakumara, Dutta, Tan, & Pal, 2013; Shivakumara et al., 2011; Yao et al., 2012; Yi & Tian, 2011) usually treat the pattern of text appearance as a special texture."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 196066575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1755d99d89dee5915df1df4a7991b87138e93c78",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a method based on the Laplacian in the frequency domain for video text detection. Unlike many other approaches which assume that text is horizontally-oriented, our method is able to handle text of arbitrary orientation. The input image is first filtered with Fourier-Laplacian. K-means clustering is then used to identify candidate text regions based on the maximum difference. The skeleton of each connected component helps to separate the different text strings from each other. Finally, text string straightness and edge density are used for false positive elimination. Experimental results show that the proposed method is able to handle graphics text and scene text of both horizontal and nonhorizontal orientation."
            },
            "slug": "A-Laplacian-Approach-to-Multi-Oriented-Text-in-Shivakumara-Phan",
            "title": {
                "fragments": [],
                "text": "A Laplacian Approach to Multi-Oriented Text Detection in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results show that the proposed method is able to handle graphics text and scene text of both horizontal and nonhorizontal orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126798"
                        ],
                        "name": "B. Epshtein",
                        "slug": "B.-Epshtein",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Epshtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Epshtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20592981"
                        ],
                        "name": "E. Ofek",
                        "slug": "E.-Ofek",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Ofek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ofek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743988"
                        ],
                        "name": "Y. Wexler",
                        "slug": "Y.-Wexler",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Wexler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wexler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 694,
                                "start": 45
                            }
                        ],
                        "text": "Inspired by the Stroke Width Transform (SWT) Epshtein et al., 2010 which selects text components based on the gradient direction of each pixel, we extend the same idea in a novel way to propose three different symmetry properties to separate text pixels from the non-text pixels in edge images of the input image. In particular, the main idea of SWT is to identify text components but not text pixels by transforming input image to stroke width image. Though SWT is invariant to the orientation of the text, it requires additional features to handle curved text as stated in Yao et al. (2012). Therefore, the SWT is extended to non-horizontal straight text lines detection in Yao et al. (2012) where the authors proposed additional features based on linearity of text components in text lines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 120
                            }
                        ],
                        "text": "From the Sobel edge image, pairs of text pixels are determined using the gradient direction of edge pixels as stated in Epshtein et al. (2010). For each pair of pixels, we test three symmetry properties to identify the text pixel candidates as illustrated in the following."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 120
                            }
                        ],
                        "text": "From the Sobel edge image, pairs of text pixels are determined using the gradient direction of edge pixels as stated in Epshtein et al. (2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1954,
                                "start": 45
                            }
                        ],
                        "text": "Inspired by the Stroke Width Transform (SWT) Epshtein et al., 2010 which selects text components based on the gradient direction of each pixel, we extend the same idea in a novel way to propose three different symmetry properties to separate text pixels from the non-text pixels in edge images of the input image. In particular, the main idea of SWT is to identify text components but not text pixels by transforming input image to stroke width image. Though SWT is invariant to the orientation of the text, it requires additional features to handle curved text as stated in Yao et al. (2012). Therefore, the SWT is extended to non-horizontal straight text lines detection in Yao et al. (2012) where the authors proposed additional features based on linearity of text components in text lines. Unfortunately, this method is limited to non-horizontal straight text detection but not curved text. This is because the linearity of text components cannot be maintained due to varying orientation of character components in a curved text line. It has been shown that the combination of stroke width with gradient magnitude proposed in Pan et al. (2011) for text detection is good for classifying text and non-text components. However, experimental results have shown that the proposed features are not sufficient to overcome the problems of scene text detection especially the problems of curved text detection despite the feature are invariant to rotation. In this work, the method finds pairs of pixels by making use the concept of SWT and then checks whether these two pixels satisfy MDS property. For instance, let pi be an edge pixel on stroke as shown in Fig. 3(a) with green color, from which, the method traverses in perpendicular direction to stroke direction until it reaches edge pixel, say q as shown in Fig. 3(a) with green color on another stroke. The distance between pi and q gives stroke width distance as suggested in Epshtein et al. (2010), which we called pair of pixels in the proposed work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 963,
                                "start": 220
                            }
                        ],
                        "text": "The main basis for proposing three proposed symmetry features for text pixel candidates identification is the inadequacy of the Stroke Width Transform (SWT) which explores gradient direction to identify text components (Epshtein et al., 2010). To show that SWT alone is not sufficient for text detection especially text like curved shape, we compare the SWT results with the proposed features results for the image shown in Fig. 2(a). The results are shown in Fig. 10 where (a) shows the results of the proposed features after refinement method and (b) shows the results of SWT. For a fair comparison, the proposed method is computed using the three features (MDS, MMS, and GVS) and refinement using SIFT and k-means, but without false positives elimination. On the other hand, the SWT connected components was computed from SWT image using a simple rule that the ratio of SWT values of neighboring pixels is less than 3.0 which is suggested by Yao et al. (2012), with the additional component filtering, that is, width variation (the ratio of standard deviation and mean of stroke width), aspect ratio (the minimum ratio of the component width/height or height/width), and occupation ratio (the ratio of the number of component pixels and bounding box area)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 154
                            }
                        ],
                        "text": "\u2026robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 593,
                                "start": 45
                            }
                        ],
                        "text": "Inspired by the Stroke Width Transform (SWT) Epshtein et al., 2010 which selects text components based on the gradient direction of each pixel, we extend the same idea in a novel way to propose three different symmetry properties to separate text pixels from the non-text pixels in edge images of the input image. In particular, the main idea of SWT is to identify text components but not text pixels by transforming input image to stroke width image. Though SWT is invariant to the orientation of the text, it requires additional features to handle curved text as stated in Yao et al. (2012). Therefore, the SWT is extended to non-horizontal straight text lines detection in Yao et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1148,
                                "start": 45
                            }
                        ],
                        "text": "Inspired by the Stroke Width Transform (SWT) Epshtein et al., 2010 which selects text components based on the gradient direction of each pixel, we extend the same idea in a novel way to propose three different symmetry properties to separate text pixels from the non-text pixels in edge images of the input image. In particular, the main idea of SWT is to identify text components but not text pixels by transforming input image to stroke width image. Though SWT is invariant to the orientation of the text, it requires additional features to handle curved text as stated in Yao et al. (2012). Therefore, the SWT is extended to non-horizontal straight text lines detection in Yao et al. (2012) where the authors proposed additional features based on linearity of text components in text lines. Unfortunately, this method is limited to non-horizontal straight text detection but not curved text. This is because the linearity of text components cannot be maintained due to varying orientation of character components in a curved text line. It has been shown that the combination of stroke width with gradient magnitude proposed in Pan et al. (2011) for text detection is good for classifying text and non-text components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 45
                            }
                        ],
                        "text": "Inspired by the Stroke Width Transform (SWT) Epshtein et al., 2010 which selects text components based on the gradient direction of each pixel, we extend the same idea in a novel way to propose three different symmetry properties to separate text pixels from the non-text pixels in edge images of\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 230
                            }
                        ],
                        "text": ", 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 0
                            }
                        ],
                        "text": "Epshtein, Ofek, and Wexler (2010) proposed an image operator called the Stroke Width Transform (SWT) on the Canny edge image to detect texts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 220
                            }
                        ],
                        "text": "The main basis for proposing three proposed symmetry features for text pixel candidates identification is the inadequacy of the Stroke Width Transform (SWT) which explores gradient direction to identify text components (Epshtein et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 74
                            }
                        ],
                        "text": "The distance between pi and q gives stroke width distance as suggested in Epshtein et al. (2010), which we called pair of pixels in the proposed work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 144
                            }
                        ],
                        "text": "Methods Precision Recall F-score\nProposed method 0.70 0.68 0.69 Yao et al. (2012) \u2013 mixture 0.63 0.63 0.63 Yao et al. (2012) \u2013 ICDAR 0.53 0.52 0.53 Epshtein et al. (2010) 0.25 0.25 0.25 Chen and Yuille (2004) 0.05 0.05 0.05\nThe bold values indicate that highest accuracy of the method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8890220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39c4ae83b5c92e0fa55de1ec7e5cf12589c408db",
            "isKey": true,
            "numCitedBy": 1470,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel image operator that seeks to find the value of stroke width for each image pixel, and demonstrate its use on the task of text detection in natural images. The suggested operator is local and data dependent, which makes it fast and robust enough to eliminate the need for multi-scale computation or scanning windows. Extensive testing shows that the suggested scheme outperforms the latest published algorithms. Its simplicity allows the algorithm to detect texts in many fonts and languages."
            },
            "slug": "Detecting-text-in-natural-scenes-with-stroke-width-Epshtein-Ofek",
            "title": {
                "fragments": [],
                "text": "Detecting text in natural scenes with stroke width transform"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel image operator is presented that seeks to find the value of stroke width for each image pixel, and its use on the task of text detection in natural images is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10585219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6bb2d54b5d87c19607b7dc14e8aba7f51a62205",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "An end-to-end real-time scene text localization and recognition method is presented. The three main novel features are: (i) keeping multiple segmentations of each character until the very last stage of the processing when the context of each character in a text line is known, (ii) an efficient algorithm for selection of character segmentations minimizing a global criterion, and (iii) showing that, despite using theoretically scale-invariant methods, operating on a coarse Gaussian scale space pyramid yields improved results as many typographical artifacts are eliminated. The method runs in real time and achieves state-of-the-art text localization results on the ICDAR 2011 Robust Reading dataset. Results are also reported for end-to-end text recognition on the ICDAR 2011 dataset."
            },
            "slug": "On-Combining-Multiple-Segmentations-in-Scene-Text-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "On Combining Multiple Segmentations in Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An end-to-end real-time scene text localization and recognition method that achieves state-of-the-art text localization results on the ICDAR 2011 Robust Reading dataset and shows that, despite using theoretically scale-invariant methods, operating on a coarse Gaussian scale space pyramid yields improved results as many typographical artifacts are eliminated."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490700"
                        ],
                        "name": "Boris Babenko",
                        "slug": "Boris-Babenko",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Babenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Babenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 63
                            }
                        ],
                        "text": "Despite many efforts (Mishra et al., 2012; Smith et al., 2011; Wang et al., 2011; Wei & Lin, 2012; Weinman, Learned-Miller, & Hanson, 2009) for text detection from natural scene images, it is still considered a challenging and unsolved problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14136313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32b8f58a038df83138435b12a499c8bf0de13811",
            "isKey": false,
            "numCitedBy": 909,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition."
            },
            "slug": "End-to-end-scene-text-recognition-Wang-Babenko",
            "title": {
                "fragments": [],
                "text": "End-to-end scene text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "While scene text recognition has generally been treated with highly domain-specific methods, the results demonstrate the suitability of applying generic computer vision methods."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 82
                            }
                        ],
                        "text": "Comprehensive surveys on text detection in scene images and video can be found in Jung et al. (2004) and Liang et al. (2005). Most existing methods of text detection in natural scene images and video can be classified roughly into three categories: texture based methods, region based methods and hybrid methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 82
                            }
                        ],
                        "text": "Comprehensive surveys on text detection in scene images and video can be found in Jung et al. (2004) and Liang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 82
                            }
                        ],
                        "text": "Comprehensive surveys on text detection in scene images and video can be found in Jung et al. (2004) and Liang et al. (2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": true,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862313"
                        ],
                        "name": "J. Weinman",
                        "slug": "J.-Weinman",
                        "structuredName": {
                            "firstName": "Jerod",
                            "lastName": "Weinman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weinman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733922"
                        ],
                        "name": "A. Hanson",
                        "slug": "A.-Hanson",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 21
                            }
                        ],
                        "text": "Despite many efforts [6-10] for text detection from natural scene images, it is still considered a challenging and unsolved problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5416971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b2a523d48cee04c09c327e14fb8928c5feff03c",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text recognition (STR) is the recognition of text anywhere in the environment, such as signs and storefronts. Relative to document recognition, it is challenging because of font variability, minimal language context, and uncontrolled conditions. Much information available to solve this problem is frequently ignored or used sequentially. Similarity between character images is often overlooked as useful information. Because of language priors, a recognizer may assign different labels to identical characters. Directly comparing characters to each other, rather than only a model, helps ensure that similar instances receive the same label. Lexicons improve recognition accuracy but are used post hoc. We introduce a probabilistic model for STR that integrates similarity, language properties, and lexical decision. Inference is accelerated with sparse belief propagation, a bottom-up method for shortening messages by reducing the dependency between weakly supported hypotheses. By fusing information sources in one model, we eliminate unrecoverable errors that result from sequential processing, improving accuracy. In experimental results recognizing text from images of signs in outdoor scenes, incorporating similarity reduces character recognition error by 19 percent, the lexicon reduces word recognition error by 35 percent, and sparse belief propagation reduces the lexicon words considered by 99.9 percent with a 12X speedup and no loss in accuracy."
            },
            "slug": "Scene-Text-Recognition-Using-Similarity-and-a-with-Weinman-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "Scene Text Recognition Using Similarity and a Lexicon with Sparse Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A probabilistic model for scene text recognition is introduced that integrates similarity, language properties, and lexical decision and is fusing information sources in one model to eliminate unrecoverable errors that result from sequential processing, improving accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153155081"
                        ],
                        "name": "David L. Smith",
                        "slug": "David-L.-Smith",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Smith",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David L. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36389113"
                        ],
                        "name": "Jacqueline L. Feild",
                        "slug": "Jacqueline-L.-Feild",
                        "structuredName": {
                            "firstName": "Jacqueline",
                            "lastName": "Feild",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacqueline L. Feild"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 45
                            }
                        ],
                        "text": "The SIFT features are used in several papers (Guo, Gurrin, Lao, Foley, & Smeaton, 2011; Smith et al., 2011; Zheng, Chen, Zhou, Gu, & Guan, 2011) for text recognition in scene images and they have shown that SIFT is useful for finding matches between the target and reference character images because of its invariance to scale, rotation, illumination, and viewpoint."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 43
                            }
                        ],
                        "text": "Despite many efforts (Mishra et al., 2012; Smith et al., 2011; Wang et al., 2011; Wei & Lin, 2012; Weinman, Learned-Miller, & Hanson, 2009) for text detection from natural scene images, it is still considered a challenging and unsolved problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 88
                            }
                        ],
                        "text": "The SIFT features are used in several papers (Guo, Gurrin, Lao, Foley, & Smeaton, 2011; Smith et al., 2011; Zheng, Chen, Zhou, Gu, & Guan, 2011) for text recognition in scene images and they have shown that SIFT is useful for finding matches between the target and reference character images because\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 39
                            }
                        ],
                        "text": "The same conclusions can be drawn from Wei and Lin (2012) where robust video text detection system is proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14076273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "430a19e17471339d65ff56b1febef4114150626e",
            "isKey": true,
            "numCitedBy": 42,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The recognition of text in everyday scenes is made difficult by viewing conditions, unusual fonts, and lack of linguistic context. Most methods integrate a priori appearance information and some sort of hard or soft constraint on the allowable strings. Weinman and Learned-Miller [14] showed that the similarity among characters, as a supplement to the appearance of the characters with respect to a model, could be used to improve scene text recognition. In this work, we make further improvements to scene text recognition by taking a novel approach to the incorporation of similarity. In particular, we train a similarity expert that learns to classify each pair of characters as equivalent or not. After removing logical inconsistencies in an equivalence graph, we formulate the search for the maximum likelihood interpretation of a sign as an integer program. We incorporate the equivalence information as constraints in the integer program and build an optimization criterion out of appearance features and character bigrams. Finally, we take the optimal solution from the integer program, and compare all \u201cnearby\u201d solutions using a probability model for strings derived from search engine queries. We demonstrate word error reductions of more than 30% relative to previous methods on the same data set."
            },
            "slug": "Enforcing-similarity-constraints-with-integer-for-Smith-Feild",
            "title": {
                "fragments": [],
                "text": "Enforcing similarity constraints with integer programming for better scene text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work trains a similarity expert that learns to classify each pair of characters as equivalent or not and incorporates the equivalence information as constraints in the integer program and builds an optimization criterion out of appearance features and character bigrams."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324807"
                        ],
                        "name": "J. Park",
                        "slug": "J.-Park",
                        "structuredName": {
                            "firstName": "Jung",
                            "lastName": "Park",
                            "middleNames": [
                                "Guk"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709448"
                        ],
                        "name": "Kyung-Joong Kim",
                        "slug": "Kyung-Joong-Kim",
                        "structuredName": {
                            "firstName": "Kyung-Joong",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyung-Joong Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17412082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d073657dec44e2c5b0b852ade487543c9f981530",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Design-of-a-visual-perception-model-with-Gabor-and-Park-Kim",
            "title": {
                "fragments": [],
                "text": "Design of a visual perception model with edge-adaptive Gabor filter and support vector machine for traffic sign detection"
            },
            "venue": {
                "fragments": [],
                "text": "Expert Syst. Appl."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397896370"
                        ],
                        "name": "A. Fern\u00e1ndez-Caballero",
                        "slug": "A.-Fern\u00e1ndez-Caballero",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Fern\u00e1ndez-Caballero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fern\u00e1ndez-Caballero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143671051"
                        ],
                        "name": "Mar\u00eda T. L\u00f3pez",
                        "slug": "Mar\u00eda-T.-L\u00f3pez",
                        "structuredName": {
                            "firstName": "Mar\u00eda",
                            "lastName": "L\u00f3pez",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mar\u00eda T. L\u00f3pez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2439331"
                        ],
                        "name": "J. C. Castillo",
                        "slug": "J.-C.-Castillo",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Castillo",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Castillo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 35
                            }
                        ],
                        "text": "3 Generally, most existing methods [13-21] focus on the use of classifier with training samples for classification of text pixels and text components based on the characteristics of character shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 22
                            }
                        ],
                        "text": "Texture based methods [13, 14, 19, 20, 21, 22] usually treat the pattern of text appearance as a special texture."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 892763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2e784cda8ba8b3b37fa8a7a2616b143a8f1f433",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Display-text-segmentation-after-learning-OCR-Fern\u00e1ndez-Caballero-L\u00f3pez",
            "title": {
                "fragments": [],
                "text": "Display text segmentation after learning best-fitted OCR binarization parameters"
            },
            "venue": {
                "fragments": [],
                "text": "Expert Syst. Appl."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066406899"
                        ],
                        "name": "Michael Opitz",
                        "slug": "Michael-Opitz",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Opitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Opitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715229"
                        ],
                        "name": "Markus Diem",
                        "slug": "Markus-Diem",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Diem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Diem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732418"
                        ],
                        "name": "Stefan Fiel",
                        "slug": "Stefan-Fiel",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Fiel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Fiel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729636"
                        ],
                        "name": "Florian Kleber",
                        "slug": "Florian-Kleber",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Kleber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Kleber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706090"
                        ],
                        "name": "R. Sablatnig",
                        "slug": "R.-Sablatnig",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Sablatnig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sablatnig"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[31] proposed end-to-end text recognition using local ternary patterns, MSER and deep convolutional nets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 712599,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8815a9466095993dd62539353fd55c4fb7230401",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Text recognition in natural scene images is an application for several computer vision applications like licence plate recognition, automated translation of street signs, help for visually impaired people or image retrieval. In this work an end-to-end text recognition system is presented. For detection an AdaBoost ensemble with a modified Local Ternary Pattern (LTP) feature-set with a post-processing stage build upon Maximally Stable Extremely Region (MSER) is used. The text recognition is done using a deep Convolution Neural Network (CNN) trained with backpropagation. The system presented outperforms state of the art methods on the ICDAR 2003 dataset in the text-detection (F-Score: 74.2%), dictionary-driven cropped-word recognition (F-Score: 87.1%) and dictionary-driven end-to-end recognition (F-Score: 72.6%) tasks."
            },
            "slug": "End-to-End-Text-Recognition-Using-Local-Ternary-and-Opitz-Diem",
            "title": {
                "fragments": [],
                "text": "End-to-End Text Recognition Using Local Ternary Patterns, MSER and Deep Convolutional Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The system presented outperforms state of the art methods on the ICDAR 2003 dataset in the text-detection, dictionary-driven cropped-word recognition and Dictionary-driven end-to-end recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2014 11th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 246
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 146
                            }
                        ],
                        "text": "\u2026et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 202
                            }
                        ],
                        "text": "Out of these, we use\nICDAR2005, ICDAR2011 and MSRA-TD500 data for experimentation and evaluation because ICDAR2005 and ICDAR2011 data are widely used for text detection compared to the other data while SVT data is a more specific dataset mostly containing street view images and it does not expect any algorithm to detect all the texts in the image."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 [37], ICDAR2005 [11], ICDAR2011 [38] robust reading competition data, Street View Text data (SVT) [39], KAIST scene text data [40], Microsoft data [23], Oriented Scene Text Database (OSTD) [20] and MSRA Text Detection 500 data (MSRA-TD500) [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14911813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d307221fa52e3939d46180cb5921ebbd92c8adb",
            "isKey": true,
            "numCitedBy": 425,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for spotting words in the wild, i.e., in real images taken in unconstrained environments. Text found in the wild has a surprising range of difficulty. At one end of the spectrum, Optical Character Recognition (OCR) applied to scanned pages of well formatted printed text is one of the most successful applications of computer vision to date. At the other extreme lie visual CAPTCHAs - text that is constructed explicitly to fool computer vision algorithms. Both tasks involve recognizing text, yet one is nearly solved while the other remains extremely challenging. In this work, we argue that the appearance of words in the wild spans this range of difficulties and propose a new word recognition approach based on state-of-the-art methods from generic object recognition, in which we consider object categories to be the words themselves. We compare performance of leading OCR engines - one open source and one proprietary - with our new approach on the ICDAR Robust Reading data set and a new word spotting data set we introduce in this paper: the Street View Text data set. We show improvements of up to 16% on the data sets, demonstrating the feasibility of a new approach to a seemingly old problem."
            },
            "slug": "Word-Spotting-in-the-Wild-Wang-Belongie",
            "title": {
                "fragments": [],
                "text": "Word Spotting in the Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that the appearance of words in the wild spans this range of difficulties and a new word recognition approach based on state-of-the-art methods from generic object recognition is proposed, in which object categories are considered to be the words themselves."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 30
                            }
                        ],
                        "text": "In summary, we use ICDAR2005, ICDAR2011, MSRA-TD500 and CUTE80 to evaluate the proposed method performance for horizontal, non-horizontal and curved text line detection, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 [37], ICDAR2005 [11], ICDAR2011 [38] robust reading competition data, Street View Text data (SVT) [39], KAIST scene text data [40], Microsoft data [23], Oriented Scene Text Database (OSTD) [20] and MSRA Text Detection 500 data (MSRA-TD500) [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 57
                            }
                        ],
                        "text": "It is observed from Tables 3\u20136 that for the ICDAR2005 and ICDAR2011 data, the proposed method gives low accuracy compared to the results of MSRA-TD500 data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 31
                            }
                        ],
                        "text": "This is because ICDAR 2005 and ICDAR2011 data requires word segmentation to calculate the measures while for MSRA-TD500 data requires line segmentation to calculate the measures."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 84
                            }
                        ],
                        "text": "The proposed method is evaluated on three benchmark datasets, namely, ICDAR2005 and ICDAR2011 for horizontal text evaluation, MSRA-TD500 for non-horizontal straight text evaluation and on our own dataset (CUTE80) that consists of 80 images for curved text evaluation to show its effectiveness and superiority over existing methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 146
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 32
                            }
                        ],
                        "text": "Out of these, we use\nICDAR2005, ICDAR2011 and MSRA-TD500 data for experimentation and evaluation because ICDAR2005 and ICDAR2011 data are widely used for text detection compared to the other data while SVT data is a more specific dataset mostly containing street view images and it does not expect any algorithm to detect all the texts in the image."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1468345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f4edbb12d346e873ca1faeff959aa7d4809495f",
            "isKey": true,
            "numCitedBy": 431,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of text in natural scene images is becoming a prominent research area due to the widespread availablity of imaging devices in low-cost consumer products like mobile phones. To evaluate the performance of recent algorithms in detecting and recognizing text from complex images, the ICDAR 2011 Robust Reading Competition was organized. Challenge 2 of the competition dealt specifically with detecting/recognizing text in natural scene images. This paper presents an overview of the approaches that the participants used, the evaluation measure, and the dataset used in the Challenge 2 of the contest. We also report the performance of all participating methods for text localization and word recognition tasks and compare their results using standard methods of area precision/recall and edit distance."
            },
            "slug": "ICDAR-2011-Robust-Reading-Competition-Challenge-2:-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2011 Robust Reading Competition Challenge 2: Reading Text in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An overview of the approaches that the participants used, the evaluation measure, and the dataset used in the ICDAR 2011 Robust Reading Competition for detecting/recognizing text in natural scene images is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144752865"
                        ],
                        "name": "Jian Liang",
                        "slug": "Jian-Liang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5053740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82b6f95e805a92887f8efccf5a0dc8d5783676f5",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 131,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.The increasing availability of high-performance, low-priced, portable digital imaging devices has created a tremendous opportunity for supplementing traditional scanning for document image acquisition. Digital cameras attached to cellular phones, PDAs, or wearable computers, and standalone image or video devices are highly mobile and easy to use; they can capture images of thick books, historical manuscripts too fragile to touch, and text in scenes, making them much more versatile than desktop scanners. Should robust solutions to the analysis of documents captured with such devices become available, there will clearly be a demand in many domains. Traditional scanner-based document analysis techniques provide us with a good reference and starting point, but they cannot be used directly on camera-captured images. Camera-captured images can suffer from low resolution, blur, and perspective distortion, as well as complex layout and interaction of the content and background. In this paper we present a survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras. We begin by describing typical imaging devices and the imaging process. We discuss document analysis from a single camera-captured image as well as multiple frames and highlight some sample applications under development and feasible ideas for future development."
            },
            "slug": "Camera-based-analysis-of-text-and-documents:-a-Liang-Doermann",
            "title": {
                "fragments": [],
                "text": "Camera-based analysis of text and documents: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras, and some sample applications under development and feasible ideas for future development is presented."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3268932"
                        ],
                        "name": "M. Grafm\u00fcller",
                        "slug": "M.-Grafm\u00fcller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Grafm\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Grafm\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73125940"
                        ],
                        "name": "J. Beyerer",
                        "slug": "J.-Beyerer",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Beyerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Beyerer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 196
                            }
                        ],
                        "text": "\u2026real world applications, such as assisting visually impaired people, enhancing safe vehicle driving, helping tourists in navigation through reading road signs, visual inspection tasks and so on (Grafmuller & Beyerer, 2013; Jung, Kim, & Jain, 2004; Liang, Doermann, & Li, 2005; Park & Kim, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 240
                            }
                        ],
                        "text": "This is mainly due to its usefulness for many real world applications, such as assisting visually impaired people, enhancing safe vehicle driving, helping tourists in navigation through reading road signs, visual inspection tasks and so on [2, 3, 4, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35463362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ab3eb31faa041cf9cca54b8d954fedec494d26b",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Performance-improvement-of-character-recognition-in-Grafm\u00fcller-Beyerer",
            "title": {
                "fragments": [],
                "text": "Performance improvement of character recognition in industrial applications using prior knowledge for more reliable segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "Expert Syst. Appl."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 89
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 99
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 100
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6379469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the robust reading competitions forICDAR 2003. With the rapid growth in research over thelast few years on recognizing text in natural scenes, thereis an urgent need to establish some common benchmarkdatasets, and gain a clear understanding of the current stateof the art. We use the term robust reading to refer to text imagesthat are beyond the capabilities of current commercialOCR packages. We chose to break down the robust readingproblem into three sub-problems, and run competitionsfor each stage, and also a competition for the best overallsystem. The sub-problems we chose were text locating,character recognition and word recognition.By breaking down the problem in this way, we hope togain a better understanding of the state of the art in eachof the sub-problems. Furthermore, our methodology involvesstoring detailed results of applying each algorithm toeach image in the data sets, allowing researchers to study indepth the strengths and weaknesses of each algorithm. Thetext locating contest was the only one to have any entries.We report the results of this contest, and show cases wherethe leading algorithms succeed and fail."
            },
            "slug": "ICDAR-2003-robust-reading-competitions-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The robust reading problem was broken down into three sub-problems, and competitions for each stage, and also a competition for the best overall system, which was the only one to have any entries."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734206"
                        ],
                        "name": "Xu Chen",
                        "slug": "Xu-Chen",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144664548"
                        ],
                        "name": "Jerry L Prince",
                        "slug": "Jerry-L-Prince",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Prince",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jerry L Prince"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 49
                            }
                        ],
                        "text": "(1) by minimizing the following energy function (Xu & Prince, 1998), E \u00bc Z Z l\u00f0r2g\u00de \u00fe jrf j2jg rf j2dxdy \u00f01\u00de\nwhere g(x,y) = [u(x,y),v(x,y)] is the GVF field, while rf is the gradient of the edge map."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "The GVF can be calculated as defined in equation (1) by minimizing the following energy function [36],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6175091,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28ec1fe81dfc6eebe359898ff79960e24876032e",
            "isKey": false,
            "numCitedBy": 2949,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Snakes, or active contours, are used extensively in computer vision and image processing applications, particularly to locate object boundaries. Problems associated with initialization and poor convergence to boundary concavities, however, have limited their utility. This paper presents a new external force for active contours, largely solving both problems. This external force, which we call gradient vector flow (GVF), is computed as a diffusion of the gradient vectors of a gray-level or binary edge map derived from the image. It differs fundamentally from traditional snake external forces in that it cannot be written as the negative gradient of a potential function, and the corresponding snake is formulated directly from a force balance condition rather than a variational formulation. Using several two-dimensional (2-D) examples and one three-dimensional (3-D) example, we show that GVF has a large capture range and is able to move snakes into boundary concavities."
            },
            "slug": "Snakes,-shapes,-and-gradient-vector-flow-Chen-Prince",
            "title": {
                "fragments": [],
                "text": "Snakes, shapes, and gradient vector flow"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a new external force for active contours, which is computed as a diffusion of the gradient vectors of a gray-level or binary edge map derived from the image, and has a large capture range and is able to move snakes into boundary concavities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 19
                            }
                        ],
                        "text": "In summary, we use ICDAR2005, ICDAR2011, MSRA-TD500 and CUTE80 to evaluate the proposed method performance for horizontal, non-horizontal and curved text line detection, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 43
                            }
                        ],
                        "text": "It is observed from Tables 3\u20136 that for the ICDAR2005 and ICDAR2011 data, the proposed method gives low accuracy compared to the results of MSRA-TD500 data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 385,
                                "start": 81
                            }
                        ],
                        "text": "For evaluation on horizontal text detection, we follow the instructions given in Lucas (2005). Similarly, there are 300 images for training and 200 images for testing in the case of MSRA-TD500 data. Here, we use 200 images for evaluating the proposed method performance on non-horizontal text detection without using the training images. The definitions suggested in Yao et al. (2012) are used for calculating recall, precision and f-measure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 137
                            }
                        ],
                        "text": "Note that the\ndifferent threshold values of precision tp e [0, 1] and area recall tr e [0, 1] are chosen according to the description in Lucas (2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 132
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": "For evaluation on horizontal text detection, we follow the instructions given in Lucas (2005). Similarly, there are 300 images for training and 200 images for testing in the case of MSRA-TD500 data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 70
                            }
                        ],
                        "text": "The proposed method is evaluated on three benchmark datasets, namely, ICDAR2005 and ICDAR2011 for horizontal text evaluation, MSRA-TD500 for non-horizontal straight text evaluation and on our own dataset (CUTE80) that consists of 80 images for curved text evaluation to show its effectiveness and superiority over existing methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 81
                            }
                        ],
                        "text": "For evaluation on horizontal text detection, we follow the instructions given in Lucas (2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 892,
                                "start": 81
                            }
                        ],
                        "text": "For evaluation on horizontal text detection, we follow the instructions given in Lucas (2005). Similarly, there are 300 images for training and 200 images for testing in the case of MSRA-TD500 data. Here, we use 200 images for evaluating the proposed method performance on non-horizontal text detection without using the training images. The definitions suggested in Yao et al. (2012) are used for calculating recall, precision and f-measure. For curved text detection, we use the 80 images from CUTE80 and Eqs. (5)\u2013(7) for calculating recall, precision and f-measure. Note that since ICDAR performance measures are based on word level evaluation, we modify our method such that it segments text lines into words with the help of distances between words and characters during ellipse growing. For MSRA-TD500 and our data, we use the text line evaluation method suggested in Yao et al. (2012). In order to find optical thresholds to define overlapping region, we conduct experiments on 50 samples chosen randomly from"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 121
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 19
                            }
                        ],
                        "text": ", 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 21
                            }
                        ],
                        "text": "Out of these, we use\nICDAR2005, ICDAR2011 and MSRA-TD500 data for experimentation and evaluation because ICDAR2005 and ICDAR2011 data are widely used for text detection compared to the other data while SVT data is a more specific dataset mostly containing street view images and it does not expect any algorithm to detect all the texts in the image."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 137
                            }
                        ],
                        "text": "Note that the different threshold values of precision tp e [0, 1] and area recall tr e [0, 1] are chosen according to the description in Lucas (2005). The same values are used for all experiments to calculate recall, precision and F-measure in this work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1842569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf50fe5622253f401e892ed943a18033e18b7b9",
            "isKey": true,
            "numCitedBy": 318,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system."
            },
            "slug": "ICDAR-2005-text-locating-competition-results-Lucas",
            "title": {
                "fragments": [],
                "text": "ICDAR 2005 text locating competition results"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f- score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805712"
                        ],
                        "name": "Jinlin Guo",
                        "slug": "Jinlin-Guo",
                        "structuredName": {
                            "firstName": "Jinlin",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinlin Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737981"
                        ],
                        "name": "C. Gurrin",
                        "slug": "C.-Gurrin",
                        "structuredName": {
                            "firstName": "Cathal",
                            "lastName": "Gurrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gurrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716428"
                        ],
                        "name": "Songyang Lao",
                        "slug": "Songyang-Lao",
                        "structuredName": {
                            "firstName": "Songyang",
                            "lastName": "Lao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Songyang Lao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206696"
                        ],
                        "name": "Colum Foley",
                        "slug": "Colum-Foley",
                        "structuredName": {
                            "firstName": "Colum",
                            "lastName": "Foley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colum Foley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680223"
                        ],
                        "name": "A. Smeaton",
                        "slug": "A.-Smeaton",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Smeaton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeaton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "On the other hand, it is known that applying SIFT for all pixels is expensive [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 45
                            }
                        ],
                        "text": "The SIFT features are used in several papers [7, 34, 35] for text recognition in scene images and they have shown that SIFT is useful for finding matches between the target and reference character images because of its invariance to scale, rotation, illumination, and viewpoint."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11794246,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "add2c3a081f62867a731b4d9d3982d35a4c6710a",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In broadcast sports video, the scoreboard is attached at a fixed location in the video and generally the scoreboard always exists in all video frames in order to help viewers to understand the match's progression quickly. Based on these observations, we present a new localization and recognition method for scoreboard text in sport videos in this paper. The method first matches the Scale Invariant Feature Transform (SIFT) points using a modified matching technique between two frames extracted from a video clip and then localizes the scoreboard by computing a robust estimate of the matched point cloud in a two-stage non-scoreboard filter process based on some domain rules. Next some enhancement operations are performed on the localized scoreboard, and a Multi-frame Voting Decision is used. Both aim to increasing the OCR rate. Experimental results demonstrate the effectiveness and efficiency of our proposed method."
            },
            "slug": "Localization-and-Recognition-of-the-Scoreboard-in-Guo-Gurrin",
            "title": {
                "fragments": [],
                "text": "Localization and Recognition of the Scoreboard in Sports Video Based on SIFT Point Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The method first matches the Scale Invariant Feature Transform points using a modified matching technique between two frames extracted from a video clip and then localizes the scoreboard by computing a robust estimate of the matched point cloud in a two-stage non-scoreboard filter process based on some domain rules."
            },
            "venue": {
                "fragments": [],
                "text": "MMM"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3144205"
                        ],
                        "name": "Jiali Yun",
                        "slug": "Jiali-Yun",
                        "structuredName": {
                            "firstName": "Jiali",
                            "lastName": "Yun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiali Yun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144889532"
                        ],
                        "name": "L. Jing",
                        "slug": "L.-Jing",
                        "structuredName": {
                            "firstName": "Liping",
                            "lastName": "Jing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740321"
                        ],
                        "name": "Jian Yu",
                        "slug": "Jian-Yu",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746467"
                        ],
                        "name": "Houkuan Huang",
                        "slug": "Houkuan-Huang",
                        "structuredName": {
                            "firstName": "Houkuan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Houkuan Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 35
                            }
                        ],
                        "text": "3 Generally, most existing methods [13-21] focus on the use of classifier with training samples for classification of text pixels and text components based on the characteristics of character shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30023128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2da8558136e901ae89efd78ab0c0194025f7990",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-multi-layer-text-classification-framework-based-Yun-Jing",
            "title": {
                "fragments": [],
                "text": "A multi-layer text classification framework based on two-level representation model"
            },
            "venue": {
                "fragments": [],
                "text": "Expert Syst. Appl."
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Koo and Kim (2013) proposed two classifiers utilizing MSER."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scene Text Detection via Connected Component Clustering and Non text Filtering , \" Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "http://code.google.com/p/tesseract-ocr/. Point-to-Point Responses to Reviewers Reviewer #1"
            },
            "venue": {
                "fragments": [],
                "text": "http://code.google.com/p/tesseract-ocr/. Point-to-Point Responses to Reviewers Reviewer #1"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 89
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data (Lee, Cho, Jung, & Kim, 2010), Microsoft data (Epshtein et al., 2010), Oriented Scene Text Database (OSTD) Yi & Tian, 2011 and MSRA Text Detection 500 data (MSRA-TD500) (Yao et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 100
                            }
                        ],
                        "text": "There are several standard datasets for scene text detection available publicly, namely, ICDAR2003 (Lucas et al., 2003), ICDAR2005 (Lucas, 2005), ICDAR2011 (Shahab, Shafait, & Dengel, 2011) robust reading competition data, Street View Text data (SVT) Wang & Belongie, 2010, KAIST scene text data\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR2003 robust reading competitions Proc. ICDAR"
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR2003 robust reading competitions Proc. ICDAR"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On combining multiple segmentation in scene text recognition"
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "This shows that there is a great demand for robust systems that work for text in any orientation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Du, Duan, and Ai (2012) developed a method based on context information of the text pixels to detect text in natural scene images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Context-Based Text Detection in Natural Scene Scenes"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ICIP"
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 30,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 46,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/A-robust-arbitrary-text-detection-system-for-scene-Risnumawan-Shivakumara/4077447a36920c6805387bbf25948d09180b6b17?sort=total-citations"
}