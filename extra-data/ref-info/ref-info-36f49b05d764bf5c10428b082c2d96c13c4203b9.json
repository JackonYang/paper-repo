{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8195063"
                        ],
                        "name": "Martin A. Zinkevich",
                        "slug": "Martin-A.-Zinkevich",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Zinkevich",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Zinkevich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965406"
                        ],
                        "name": "Markus Weimer",
                        "slug": "Markus-Weimer",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Weimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Weimer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47681372"
                        ],
                        "name": "Lihong Li",
                        "slug": "Lihong-Li",
                        "structuredName": {
                            "firstName": "Lihong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lihong Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "This recursion underlies many convergence proofs for SGD where ak denotes the distance to the optimal solution after k iterations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "We experimentally compare lock-free SGD to several recently proposed methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "For this data set, we also implemented the approach in [27] which runs multiple SGD runs in parallel and averages their output."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "This result is interesting in of itself and shows that one need not settle for 1/ \u221a k rates to ensure robustness in SGD algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 139
                            }
                        ],
                        "text": "As many large data sets are currently pre-processed in a MapReduce-like parallel-processing framework, much of the recent work on parallel SGD has focused naturally on MapReduce implementations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 188
                            }
                        ],
                        "text": "Nevertheless, the recent emergence of inexpensive multicore processors and mammoth, web-scale data sets has motivated researchers to develop several clever parallelization schemes for SGD [4, 10, 12, 16, 27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "With its small memory footprint, robustness against noise, and rapid learning rates, Stochastic Gradient Descent (SGD) has proved to be well suited to data-intensive machine learning tasks [3,5,26]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "We do not analyze this \u201cwithout replacement\u201d procedure because no one has achieved tractable analyses for SGD in any without replacement sampling models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "For this data set, we also implemented the approach in [30] which runs multiple SGD runs in parallel and averages their output."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "We also note that constant stepsize protocols with backoff procedures are canonical in SGD practice, but perhaps not in theory."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "However, when the data access is sparse, meaning that individual SGD steps only modify a small part of the decision variable, we show that memory overwrites are rare and that they introduce barely any error into the computation when they do occur."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 184
                            }
                        ],
                        "text": "Nevertheless, the recent emergence of inexpensive multicore processors and mammoth, web-scale data sets has motivated researchers to develop several clever parallelization schemes for SGD [4, 10, 12, 16, 30]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "In MapReduce settings, Zinkevich et al proposed running many instances of stochastic gradient descent on different machines and averaging their output [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "These authors also show that SGD convergence is robust to a variety of models of delay in computation and communication in [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "In this work, we propose a simple strategy for eliminating the overhead associated with locking: run SGD in parallel without locks, a strategy that we call Hogwild!."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "Note that up to the log(1/ ) term in (4.6), our analysis nearly provides a 1/k rate of convergence for a constant stepsize SGD scheme, both in the serial and parallel cases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "However, SGD\u2019s scalability is limited by its inherently sequential nature; it is difficult to parallelize."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "That is, it may be possible to bias the SGD iterations to completely avoid memory contention between processors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "In the case that \u03c4 = 0, this reduces to precisely the rate achieved by the serial SGD protocol."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7885987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83e9565cede81b2b88a9fa241833135da142f4d3",
            "isKey": true,
            "numCitedBy": 1146,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique \u2014 contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8]."
            },
            "slug": "Parallelized-Stochastic-Gradient-Descent-Zinkevich-Weimer",
            "title": {
                "fragments": [],
                "text": "Parallelized Stochastic Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper presents the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence and introduces a novel proof technique \u2014 contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760896"
                        ],
                        "name": "K. Asanovi\u0107",
                        "slug": "K.-Asanovi\u0107",
                        "structuredName": {
                            "firstName": "Krste",
                            "lastName": "Asanovi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Asanovi\u0107"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1991345"
                        ],
                        "name": "R. Bod\u00edk",
                        "slug": "R.-Bod\u00edk",
                        "structuredName": {
                            "firstName": "Rastislav",
                            "lastName": "Bod\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bod\u00edk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053614"
                        ],
                        "name": "Joseph Gebis",
                        "slug": "Joseph-Gebis",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Gebis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Gebis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38602469"
                        ],
                        "name": "P. Husbands",
                        "slug": "P.-Husbands",
                        "structuredName": {
                            "firstName": "Parry",
                            "lastName": "Husbands",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Husbands"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701130"
                        ],
                        "name": "D. Patterson",
                        "slug": "D.-Patterson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Patterson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Patterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2160093"
                        ],
                        "name": "W. Plishker",
                        "slug": "W.-Plishker",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Plishker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Plishker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746446"
                        ],
                        "name": "J. Shalf",
                        "slug": "J.-Shalf",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shalf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shalf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145117071"
                        ],
                        "name": "Samuel Williams",
                        "slug": "Samuel-Williams",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731111"
                        ],
                        "name": "K. Yelick",
                        "slug": "K.-Yelick",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Yelick",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yelick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 147
                            }
                        ],
                        "text": "The high rates achievable by multicore systems move the bottlenecks in parallel computation to synchronization (or locking) amongst the processors [2,13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62143065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ceb3fd01e1d5ece659654638e1c7cde2c4704a7a",
            "isKey": false,
            "numCitedBy": 2303,
            "numCiting": 141,
            "paperAbstract": {
                "fragments": [],
                "text": "Author(s): Asanovic, K; Bodik, R; Catanzaro, B; Gebis, J; Husbands, P; Keutzer, K; Patterson, D; Plishker, W; Shalf, J; Williams, SW | Abstract: The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism. We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following: \u2022 The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems \u2022 The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar. \u2022 Instead of traditional benchmarks, use 13 \u201cDwarfs\u201d to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.) \u2022 \u201cAutotuners\u201d should play a larger role than conventional compilers in translating parallel programs. \u2022 To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. \u2022 To be successful, programming models should be independent of the number of processors. \u2022 To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism. 1 The Landscape of Parallel Computing Research: A View From Berkeley \u2022 Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters. \u2022 Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines. \u2022 To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost. Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems."
            },
            "slug": "The-Landscape-of-Parallel-Computing-Research:-A-Asanovi\u0107-Bod\u00edk",
            "title": {
                "fragments": [],
                "text": "The Landscape of Parallel Computing Research: A View from Berkeley"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The parallel landscape is frame with seven questions, and the following are recommended to explore the design space rapidly: \u2022 The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems \u2022 The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS each development dollar."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716876"
                        ],
                        "name": "O. Dekel",
                        "slug": "O.-Dekel",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Dekel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Dekel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388775848"
                        ],
                        "name": "Ran Gilad-Bachrach",
                        "slug": "Ran-Gilad-Bachrach",
                        "structuredName": {
                            "firstName": "Ran",
                            "lastName": "Gilad-Bachrach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ran Gilad-Bachrach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768909"
                        ],
                        "name": "O. Shamir",
                        "slug": "O.-Shamir",
                        "structuredName": {
                            "firstName": "Ohad",
                            "lastName": "Shamir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Shamir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149551306"
                        ],
                        "name": "Lin Xiao",
                        "slug": "Lin-Xiao",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Incremental gradient methods, Machine learning, Parallel computing, Multicore"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3043622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b197d60de05e14781d67a318b29a4d4600a7460",
            "isKey": false,
            "numCitedBy": 527,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem."
            },
            "slug": "Optimal-Distributed-Online-Prediction-Using-Dekel-Gilad-Bachrach",
            "title": {
                "fragments": [],
                "text": "Optimal Distributed Online Prediction Using Mini-Batches"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms that is asymptotically optimal for smooth convex loss functions and stochastic inputs and proves a regret bound for this method."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9229182"
                        ],
                        "name": "B. Recht",
                        "slug": "B.-Recht",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Recht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Recht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114485554"
                        ],
                        "name": "C. R\u00e9",
                        "slug": "C.-R\u00e9",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "R\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17109415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3986480be64dce5ae55c1f64df660d7d698f8fb",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops Jellyfish, an algorithm for solving data-processing problems with matrix-valued decision variables regularized to have low rank. Particular examples of problems solvable by Jellyfish include matrix completion problems and least-squares problems regularized by the nuclear norm or $$\\gamma _2$$-norm. Jellyfish implements a projected incremental gradient method with a biased, random ordering of the increments. This biased ordering allows for a parallel implementation that admits a speed-up nearly proportional to the number of processors. On large-scale matrix completion tasks, Jellyfish is orders of magnitude more efficient than existing codes. For example, on the Netflix Prize data set, prior art computes rating predictions in approximately 4\u00a0h, while Jellyfish solves the same problem in under 3\u00a0min on a 12 core workstation."
            },
            "slug": "Parallel-stochastic-gradient-algorithms-for-matrix-Recht-R\u00e9",
            "title": {
                "fragments": [],
                "text": "Parallel stochastic gradient algorithms for large-scale matrix completion"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Jellyfish, an algorithm for solving data-processing problems with matrix-valued decision variables regularized to have low rank, is developed, which is orders of magnitude more efficient than existing codes."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program. Comput."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "This recursion underlies many convergence proofs for SGD where ak denotes the distance to the optimal solution after k iterations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "We experimentally compare lock-free SGD to several recently proposed methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "This result is interesting in of itself and shows that one need not settle for 1/ \u221a k rates to ensure robustness in SGD algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 139
                            }
                        ],
                        "text": "As many large data sets are currently pre-processed in a MapReduce-like parallel-processing framework, much of the recent work on parallel SGD has focused naturally on MapReduce implementations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 188
                            }
                        ],
                        "text": "Nevertheless, the recent emergence of inexpensive multicore processors and mammoth, web-scale data sets has motivated researchers to develop several clever parallelization schemes for SGD [4, 10, 12, 16, 27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "With its small memory footprint, robustness against noise, and rapid learning rates, Stochastic Gradient Descent (SGD) has proved to be well suited to data-intensive machine learning tasks [3,5,26]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "We do not analyze this \u201cwithout replacement\u201d procedure because no one has achieved tractable analyses for SGD in any without replacement sampling models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "For this data set, we also implemented the approach in [30] which runs multiple SGD runs in parallel and averages their output."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "We also note that constant stepsize protocols with backoff procedures are canonical in SGD practice, but perhaps not in theory."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "However, when the data access is sparse, meaning that individual SGD steps only modify a small part of the decision variable, we show that memory overwrites are rare and that they introduce barely any error into the computation when they do occur."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 184
                            }
                        ],
                        "text": "Nevertheless, the recent emergence of inexpensive multicore processors and mammoth, web-scale data sets has motivated researchers to develop several clever parallelization schemes for SGD [4, 10, 12, 16, 30]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "These authors also show that SGD convergence is robust to a variety of models of delay in computation and communication in [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "In this work, we propose a simple strategy for eliminating the overhead associated with locking: run SGD in parallel without locks, a strategy that we call Hogwild!."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "Note that up to the log(1/ ) term in (4.6), our analysis nearly provides a 1/k rate of convergence for a constant stepsize SGD scheme, both in the serial and parallel cases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "However, SGD\u2019s scalability is limited by its inherently sequential nature; it is difficult to parallelize."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "That is, it may be possible to bias the SGD iterations to completely avoid memory contention between processors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 114
                            }
                        ],
                        "text": "Most schemes for parallelizing stochastic gradient descent are variants of ideas presented in the seminal text by Bertsekas and Tsitsiklis [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "In the case that \u03c4 = 0, this reduces to precisely the rate achieved by the serial SGD protocol."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 154
                            }
                        ],
                        "text": "5 Related Work Most schemes for parallelizing stochastic gradient descent are variants of ideas presented in the seminal text by Bertsekas and Tsitsiklis [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60492634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "638df1b831feb3647a9bf5496780b38890573d4d",
            "isKey": true,
            "numCitedBy": 5436,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "gineering, computer science, operations research, and applied mathematics. It is essentially a self-contained work, with the development of the material occurring in the main body of the text and excellent appendices on linear algebra and analysis, graph theory, duality theory, and probability theory and Markov chains supporting it. The introduction discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later. After the introduction, the text is organized in two parts: synchronous algorithms and asynchronous algorithms. The discussion of synchronous algorithms comprises four chapters, with Chapter 2 presenting both direct methods (converging to the exact solution within a finite number of steps) and iterative methods for linear"
            },
            "slug": "Parallel-and-Distributed-Computation:-Numerical-Bertsekas-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Parallel and Distributed Computation: Numerical Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682823"
                        ],
                        "name": "P. Tseng",
                        "slug": "P.-Tseng",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tseng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tseng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some theoretical work which has at least demonstrated convergence of these protocols can be found in [21, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18276525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a296a1577478654a54a9f801f93f71b7d853c53",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider an incremental gradient method with momentum term for minimizing the sum of continuously differentiable functions. This method uses a new adaptive stepsize rule that decreases the stepsize whenever sufficient progress is not made. We show that if the gradients of the functions are bounded and Lipschitz continuous over a certain level set, then every cluster point of the iterates generated by the method is a stationary point. In addition, if the gradient of the functions have a certain growth property, then the method is either linearly convergent in some sense or the stepsizes are bounded away from zero. The new stepsize rule is much in the spirit of heuristic learning rules used in practice for training neural networks via backpropagation. As such, the new stepsize rule may suggest improvements on existing learning rules. Finally, extension of the method and the convergence results to constrained minimization is discussed, as are some implementation issues and numerical experience."
            },
            "slug": "An-Incremental-Gradient(-Projection)-Method-with-Tseng",
            "title": {
                "fragments": [],
                "text": "An Incremental Gradient(-Projection) Method with Momentum Term and Adaptive Stepsize Rule"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An incremental gradient method with momentum term for minimizing the sum of continuously differentiable functions is considered, which uses a new adaptive stepsize rule that decreases the stepsize whenever sufficient progress is not made."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102259884"
                        ],
                        "name": "Luo Zhi-quan",
                        "slug": "Luo-Zhi-quan",
                        "structuredName": {
                            "firstName": "Luo",
                            "lastName": "Zhi-quan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luo Zhi-quan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053906112"
                        ],
                        "name": "Tseng Paul",
                        "slug": "Tseng-Paul",
                        "structuredName": {
                            "firstName": "Tseng",
                            "lastName": "Paul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tseng Paul"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 101
                            }
                        ],
                        "text": "Some theoretical work which has at least demonstrated convergence of these protocols can be found in [21, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122368112,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "51df39eb912759413bc31a7950408db580eecf09",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the convergence of an approximate gradient projection method for minimizing the sum of continuously differentiable functions over a nonempty closed convex set. In this method, the functions are aggregated and, at each iteration, a succession of gradient steps, one for each of the aggregate functions, is applied and the result is projected onto the convex set. We show that if the gradients of the functions are bounded and Lipschitz continuous over a certain level set and the stepsizes are chosen to be proportional to a certain residual squared or to be square summable, then every cluster point of the iterates is a stationary point. We apply these results to the backpropagation algorithm to obtain new deterministic convergence results for this algorithm. We also discuss the issues of parallel implementation and give a simple criterion for choosing the aggregation."
            },
            "slug": "Analysis-of-an-approximate-gradient-projection-with-Zhi-quan-Paul",
            "title": {
                "fragments": [],
                "text": "Analysis of an approximate gradient projection method with applications to the backpropagation algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71084179"
                        ],
                        "name": "Muthu Dayalan",
                        "slug": "Muthu-Dayalan",
                        "structuredName": {
                            "firstName": "Muthu",
                            "lastName": "Dayalan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muthu Dayalan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": ", \u201cfind all the urls from a 100TB of Web data\u201d) that was designed to ensure fault tolerance and to simplify the maintenance and programming of large clusters of machines [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67055872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "627be67feb084f1266cfc36e5aed3c3e7e6ce5f0",
            "isKey": false,
            "numCitedBy": 7853,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day."
            },
            "slug": "MapReduce:-simplified-data-processing-on-large-Dayalan",
            "title": {
                "fragments": [],
                "text": "MapReduce: simplified data processing on large clusters"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This presentation explains how the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800855"
                        ],
                        "name": "A. Nedi\u0107",
                        "slug": "A.-Nedi\u0107",
                        "structuredName": {
                            "firstName": "Angelia",
                            "lastName": "Nedi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nedi\u0107"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1) (see, for example [23])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13930560,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "1bcddc47cda03a28133d6fdbbd9f386de7d4b6c3",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a class of subgradient methods for minimizing a convex function that consists of the sum of a large number of component functions. This type of minimization arises in a dual context from Lagrangian relaxation of the coupling constraints of large scale separable problems. The idea is to perform the subgradient iteration incrementally, by sequentially taking steps along the subgradients of the component functions, with intermediate adjustment of the variables after processing each component function. This incremental approach has been very successful in solving large differentiable least squares problems, such as those arising in the training of neural networks, and it has resulted in a much better practical rate of convergence than the steepest descent method."
            },
            "slug": "Convergence-Rate-of-Incremental-Subgradient-Nedi\u0107-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Convergence Rate of Incremental Subgradient Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An incremental approach to minimizing a convex function that consists of the sum of a large number of component functions is considered, which has been very successful in solving large differentiable least squares problems, such as those arising in the training of neural networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734693"
                        ],
                        "name": "John C. Duchi",
                        "slug": "John-C.-Duchi",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Duchi",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Duchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40333747"
                        ],
                        "name": "Alekh Agarwal",
                        "slug": "Alekh-Agarwal",
                        "structuredName": {
                            "firstName": "Alekh",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alekh Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Nevertheless, the recent emergence of inexpensive multicore processors and mammoth, web-scale data sets has motivated researchers to develop several clever parallelization schemes for SGD [4, 10, 12, 17, 31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Schemes involving the averaging of gradients via a distributed protocol have also been proposed by several authors [10, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16349379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e42df274946c40dc370393f68feb67c91ae5347c",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks."
            },
            "slug": "Distributed-Dual-Averaging-In-Networks-Duchi-Agarwal",
            "title": {
                "fragments": [],
                "text": "Distributed Dual Averaging In Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work develops and analyzes distributed algorithms based on dual averaging of subgradients, and provides sharp bounds on their convergence rates as a function of the network size and topology."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47681372"
                        ],
                        "name": "Lihong Li",
                        "slug": "Lihong-Li",
                        "structuredName": {
                            "firstName": "Lihong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lihong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49104973"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The work most closely related to our own is a round-robin scheme proposed by Langford et al [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We ran numerical experiments on a variety of machine learning tasks, and compared against a round-robin approach proposed in [16] and implemented in Vowpal Wabbit [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1600289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eec1451c8964d6e5311ef63c4e2228188e7cdccd",
            "isKey": false,
            "numCitedBy": 446,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous\u2014a parameter controls the rate of sparsification from no sparsification to total sparsification. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1-regularization method in the batch setting. We prove small rates of sparsification result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and find for datasets with large numbers of features, substantial sparsity is discoverable."
            },
            "slug": "Sparse-Online-Learning-via-Truncated-Gradient-Langford-Li",
            "title": {
                "fragments": [],
                "text": "Sparse Online Learning via Truncated Gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work proposes a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss and finds for datasets with large numbers of features, substantial sparsity is discoverable."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421201"
                        ],
                        "name": "J. Lee",
                        "slug": "J.-Lee",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9229182"
                        ],
                        "name": "B. Recht",
                        "slug": "B.-Recht",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Recht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Recht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788107"
                        ],
                        "name": "J. Tropp",
                        "slug": "J.-Tropp",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tropp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tropp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 94
                            }
                        ],
                        "text": "Such problems arise in collaborative filtering, Euclidean distance estimation, and clustering [8,17,23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 712550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04e4134c41e95ec7654f3adc3651b373f038a682",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative filtering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable first-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas."
            },
            "slug": "Practical-Large-Scale-Optimization-for-Max-norm-Lee-Recht",
            "title": {
                "fragments": [],
                "text": "Practical Large-Scale Optimization for Max-norm Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work uses a factorization technique of Burer and Monteiro to devise scalable first-order algorithms for convex programs involving the max-norm and these algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "Problems involving minimum cuts in graphs frequently arise in machine learning (see [6] for a comprehensive survey)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5324521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c62fdf1e6a520d9fee8ca9981fb588d07f2c6fa",
            "isKey": false,
            "numCitedBy": 3563,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Minimum cut/maximum flow algorithms on graphs have emerged as an increasingly useful tool for exactor approximate energy minimization in low-level vision. The combinatorial optimization literature provides many min-cut/max-flow algorithms with different polynomial time complexity. Their practical efficiency, however, has to date been studied mainly outside the scope of computer vision. The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for applications in vision. We compare the running times of several standard algorithms, as well as a new algorithm that we have recently developed. The algorithms we study include both Goldberg-Tarjan style \"push -relabel\" methods and algorithms based on Ford-Fulkerson style \"augmenting paths.\" We benchmark these algorithms on a number of typical graphs in the contexts of image restoration, stereo, and segmentation. In many cases, our new algorithm works several times faster than any of the other methods, making near real-time performance possible. An implementation of our max-flow/min-cut algorithm is available upon request for research purposes."
            },
            "slug": "An-experimental-comparison-of-min-cut/max-flow-for-Boykov-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "An experimental comparison of min-cut/max- flow algorithms for energy minimization in vision"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for applications in vision, comparing the running times of several standard algorithms, as well as a new algorithm that is recently developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145853268"
                        ],
                        "name": "A. Nemirovski",
                        "slug": "A.-Nemirovski",
                        "structuredName": {
                            "firstName": "Arkadi",
                            "lastName": "Nemirovski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nemirovski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754887"
                        ],
                        "name": "A. Juditsky",
                        "slug": "A.-Juditsky",
                        "structuredName": {
                            "firstName": "Anatoli",
                            "lastName": "Juditsky",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Juditsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070945"
                        ],
                        "name": "Guanghui Lan",
                        "slug": "Guanghui-Lan",
                        "structuredName": {
                            "firstName": "Guanghui",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guanghui Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31474779"
                        ],
                        "name": "A. Shapiro",
                        "slug": "A.-Shapiro",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Shapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shapiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "In contrast, Nemirovski et al demonstrate that when the stepsize is inversely proportional to the iteration counter, an overestimate of c can result in exponential slow-down [21]! Robust 1/k rates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Nemirovski et al [23] show that the expected squared distance to the optimal solution, ak, satisfies\nak+1 \u2264 (1\u2212 2c\u03b3k)ak + 12\u03b3 2 kM 2 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "We follow the notation and assumptions of Nemirovski et al [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 13
                            }
                        ],
                        "text": "In contrast, Nemirovski et al demonstrate that when the stepsize is inversely proportional to the iteration counter, an overestimate of c can result in exponential slow-down [23]!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 42
                            }
                        ],
                        "text": "We follow the notation and assumptions of Nemirovski et al [23]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1767867,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "96167ed3ebc9a2c3270f6ae96043e6f086eed4de",
            "isKey": true,
            "numCitedBy": 1843,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments."
            },
            "slug": "Robust-Stochastic-Approximation-Approach-to-Nemirovski-Juditsky",
            "title": {
                "fragments": [],
                "text": "Robust Stochastic Approximation Approach to Stochastic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is intended to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "2 , (2) and we know a priori that the examples z \u21b5 are very sparse (see for example [14])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5155714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "427b168f490b56716f22b129ac93aba5425ea08f",
            "isKey": false,
            "numCitedBy": 2113,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s << N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets."
            },
            "slug": "Training-linear-SVMs-in-linear-time-Joachims",
            "title": {
                "fragments": [],
                "text": "Training linear SVMs in linear time"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n)) for ordinal regression problems and several orders of magnitude faster than decomposition methods like svm light for large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8195063"
                        ],
                        "name": "Martin A. Zinkevich",
                        "slug": "Martin-A.-Zinkevich",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Zinkevich",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Zinkevich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Nevertheless, the recent emergence of inexpensive multicore processors and mammoth, web-scale data sets has motivated researchers to develop several clever parallelization schemes for SGD [4, 10, 12, 17, 31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1881928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34050f81755e0f718cb924e460a4691e35722b2e",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Online learning algorithms have impressive convergence properties when it comes to risk minimization and convex games on very large problems. However, they are inherently sequential in their design which prevents them from taking advantage of modern multi-core architectures. In this paper we prove that online learning with delayed updates converges well, thereby facilitating parallel online learning."
            },
            "slug": "Slow-Learners-are-Fast-Zinkevich-Smola",
            "title": {
                "fragments": [],
                "text": "Slow Learners are Fast"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proves that online learning with delayed updates converges well, thereby facilitating parallel online learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145603701"
                        ],
                        "name": "M. Athans",
                        "slug": "M.-Athans",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Athans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Athans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "These authors also show that SGD convergence is robust to a variety of models of delay in computation and communication in [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17975552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "788ed979eee040e52ca558f6f42cbd6c8ec7da00",
            "isKey": false,
            "numCitedBy": 1503,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model for asynchronous distributed computation and then proceed to analyze the convergence of natural asynchronous distributed versions of a large class of deterministic and stochastic gradient-like algorithms. We show that such algorithms retain the desirable convergence properties of their centralized counterparts, provided that the time between consecutive communications between processors plus communication delays are not too large."
            },
            "slug": "Distributed-Asynchronous-Deterministic-and-Gradient-Tsitsiklis-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A model for asynchronous distributed computation is presented and it is shown that natural asynchronous distributed versions of a large class of deterministic and stochastic gradient-like algorithms retain the desirable convergence properties of their centralized counterparts."
            },
            "venue": {
                "fragments": [],
                "text": "1984 American Control Conference"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9229182"
                        ],
                        "name": "B. Recht",
                        "slug": "B.-Recht",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Recht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Recht"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 94
                            }
                        ],
                        "text": "Such problems arise in collaborative filtering, Euclidean distance estimation, and clustering [8,17,23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "This follows from a coupon collector argument [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8061516,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "040c161f21e0fa57ac192ac826310f55d60277b0",
            "isKey": false,
            "numCitedBy": 2735,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a problem of considerable practical interest: the recovery of a data matrix from a sampling of its entries. Suppose that we observe m entries selected uniformly at random from a matrix M. Can we complete the matrix and recover the entries that we have not seen?We show that one can perfectly recover most low-rank matrices from what appears to be an incomplete set of entries. We prove that if the number m of sampled entries obeys $$m\\ge C\\,n^{1.2}r\\log n$$ for some positive numerical constant C, then with very high probability, most n\u00d7n matrices of rank r can be perfectly recovered by solving a simple convex optimization program. This program finds the matrix with minimum nuclear norm that fits the data. The condition above assumes that the rank is not too large. However, if one replaces the 1.2 exponent with 1.25, then the result holds for all values of the rank. Similar results hold for arbitrary rectangular matrices as well. Our results are connected with the recent literature on compressed sensing, and show that objects other than signals and images can be perfectly reconstructed from very limited information."
            },
            "slug": "Exact-Matrix-Completion-via-Convex-Optimization-Cand\u00e8s-Recht",
            "title": {
                "fragments": [],
                "text": "Exact matrix completion via convex optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is proved that one can perfectly recover most low-rank matrices from what appears to be an incomplete set of entries, and that objects other than signals and images can be perfectly reconstructed from very limited information."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "With its small memory footprint, robustness against noise, and rapid learning rates, Stochastic Gradient Descent (SGD) has proved to be well suited to data-intensive machine learning tasks [3,5,27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7431525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5936754b5762260bf102ac95d7b26cfc9d31956a",
            "isKey": false,
            "numCitedBy": 1485,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation-estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways."
            },
            "slug": "The-Tradeoffs-of-Large-Scale-Learning-Bottou-Bousquet",
            "title": {
                "fragments": [],
                "text": "The Tradeoffs of Large Scale Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms and shows distinct tradeoffs for the case of small-scale and large-scale learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9229182"
                        ],
                        "name": "B. Recht",
                        "slug": "B.-Recht",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Recht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Recht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144651919"
                        ],
                        "name": "M. Fazel",
                        "slug": "M.-Fazel",
                        "structuredName": {
                            "firstName": "Maryam",
                            "lastName": "Fazel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fazel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731770"
                        ],
                        "name": "P. Parrilo",
                        "slug": "P.-Parrilo",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Parrilo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Parrilo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8249100,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0c9bb579d8ad6ac987f7a16b66ddace671fc57c5",
            "isKey": false,
            "numCitedBy": 3277,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "The affine rank minimization problem consists of finding a matrix of minimum rank that satisfies a given system of linear equality constraints. Such problems have appeared in the literature of a diverse set of fields including system identification and control, Euclidean embedding, and collaborative filtering. Although specific instances can often be solved with specialized algorithms, the general affine rank minimization problem is NP-hard because it contains vector cardinality minimization as a special case. \n \nIn this paper, we show that if a certain restricted isometry property holds for the linear transformation defining the constraints, the minimum-rank solution can be recovered by solving a convex optimization problem, namely, the minimization of the nuclear norm over the given affine space. We present several random ensembles of equations where the restricted isometry property holds with overwhelming probability, provided the codimension of the subspace is sufficiently large. \n \nThe techniques used in our analysis have strong parallels in the compressed sensing framework. We discuss how affine rank minimization generalizes this preexisting concept and outline a dictionary relating concepts from cardinality minimization to those of rank minimization. We also discuss several algorithmic approaches to minimizing the nuclear norm and illustrate our results with numerical examples."
            },
            "slug": "Guaranteed-Minimum-Rank-Solutions-of-Linear-Matrix-Recht-Fazel",
            "title": {
                "fragments": [],
                "text": "Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if a certain restricted isometry property holds for the linear transformation defining the constraints, the minimum-rank solution can be recovered by solving a convex optimization problem, namely, the minimization of the nuclear norm over the given affine space."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32440409"
                        ],
                        "name": "T. Rose",
                        "slug": "T.-Rose",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Rose",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146329150"
                        ],
                        "name": "Fan Li",
                        "slug": "Fan-Li",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fan Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "We tested our sparse SVM implementation on the Reuters RCV1 data set on the binary text classification task CCAT [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11027141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices."
            },
            "slug": "RCV1:-A-New-Benchmark-Collection-for-Text-Research-Lewis-Yang",
            "title": {
                "fragments": [],
                "text": "RCV1: A New Benchmark Collection for Text Categorization Research"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746764"
                        ],
                        "name": "G. Calinescu",
                        "slug": "G.-Calinescu",
                        "structuredName": {
                            "firstName": "Gruia",
                            "lastName": "Calinescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Calinescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735932"
                        ],
                        "name": "H. Karloff",
                        "slug": "H.-Karloff",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Karloff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Karloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763352"
                        ],
                        "name": "Y. Rabani",
                        "slug": "Y.-Rabani",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Rabani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Rabani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Our notion of sparsity allows us to provide theoretical guarantees of linear speedups in Section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 47141077,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0fd469a2954f8a3dad2de4438199443e54a1849c",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Given an undirected graph with edge costs and a subset of k nodes called terminals, a multiway cut is a subset of edges whose removal disconnects each terminal from the rest. Multiway Cut is the problem of finding a multiway cut of minimum cost. Previously, a very simple combinatorial algorithm due to Dahlhaus, Johnson, Papadimitriou, Seymour, and Yannakakis gave a performance guarantee of 2(1?1k). In this paper, we present a new linear programming relaxation for Multiway Cut and a new approximation algorithm based on it. The algorithm breaks the threshold of 2 for approximating Multiway Cut, achieving a performance ratio of at most 1.5?1k. This improves the previous result for every value of k. In particular, for k=3 we get a ratio of 76<43."
            },
            "slug": "An-improved-approximation-algorithm-for-multiway-Calinescu-Karloff",
            "title": {
                "fragments": [],
                "text": "An improved approximation algorithm for multiway cut"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new linear programming relaxation for Multiway Cut is presented and a new approximation algorithm based on it achieves a performance ratio of at most 1.5?1k, which improves the previous result for every value of k."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109590665"
                        ],
                        "name": "Haixun Wang",
                        "slug": "Haixun-Wang",
                        "structuredName": {
                            "firstName": "Haixun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haixun Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Here, two-way cuts use D = 2, but multiway-cuts with tens of thousands of classes also arise in entity resolution problems [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54527847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab14620f8a02dfdd26a0a11997666715787812ae",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Entity resolution has been extensively studied. Many approaches have been proposed, including using machine learning techniques to derive domain-specific lexical similarity measures, or rank entities\u2019 attributes by their discriminative power, etc. In this paper, we study the problem in the setting of matching two web scale taxonomies. Besides the scale, we address the challenge that the taxonomies may not contain enough context (such as attributes) for entity resolution, and traditional lexical similarity measures result in many false positive matches. To tackle this new task, we explore negative evidence in the structure of the taxonomy, as well as in external data sources such as the web. To integrate positive and negative evidence, we formulate the entity resolution problem as a problem of finding optimal multi-way cuts in a graph. We analyze the complexity of the problem, and propose a Monte Carlo algorithm for finding greedy cuts. We conduct extensive experiments that demonstrate the advantage of our approach."
            },
            "slug": "Web-Scale-Entity-Resolution-using-Relational-Wang",
            "title": {
                "fragments": [],
                "text": "Web Scale Entity Resolution using Relational Evidence"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper addresses the challenge that the taxonomies may not contain enough context for entity resolution, and traditional lexical similarity measures result in many false positive matches, and proposes a Monte Carlo algorithm for finding greedy cuts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "1 Introduction With its small memory footprint, robustness against noise, and rapid learning rates, Stochastic Gradient Descent (SGD) has proved to be well suited to data-intensive machine learning tasks [3,5,24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5771157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c103834c918d18b79a1794f26c3422435768fcd",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss how the runtime of SVM optimization should decrease as the size of the training data increases. We present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels."
            },
            "slug": "SVM-optimization:-inverse-dependence-on-training-Shalev-Shwartz-Srebro",
            "title": {
                "fragments": [],
                "text": "SVM optimization: inverse dependence on training set size"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Tests are presented demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels, as the runtime of SVM optimization should decrease as the size of the training data increases."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 147
                            }
                        ],
                        "text": "The high rates achievable by multicore systems move the bottlenecks in parallel computation to synchronization (or locking) amongst the processors [2, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208784962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ff65ac698013cdd9d61326cab49a1d75404e001",
            "isKey": false,
            "numCitedBy": 18721,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Editors",
            "title": {
                "fragments": [],
                "text": "Editors"
            },
            "venue": {
                "fragments": [],
                "text": "Brain Research Bulletin"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211659"
                        ],
                        "name": "Jason D. M. Rennie",
                        "slug": "Jason-D.-M.-Rennie",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Rennie",
                            "middleNames": [
                                "D.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. M. Rennie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 286
                            }
                        ],
                        "text": "\u2026low latency and high throughput shared main memory (a processor in such a system can write and read the shared physical memory at over 12GB/s with latency in the tens of nanoseconds); and (2) high bandwidth off multiple disks (a thousand-dollar RAID can pump data into main memory at over 1GB/s)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5048382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf154c28178370d95510112413dc8cb48120a8",
            "isKey": false,
            "numCitedBy": 1105,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them."
            },
            "slug": "Maximum-Margin-Matrix-Factorization-Srebro-Rennie",
            "title": {
                "fragments": [],
                "text": "Maximum-Margin Matrix Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A novel approach to collaborative prediction is presented, using low-norm instead of low-rank factorizations, inspired by, and has strong connections to, large-margin linear discrimination."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "1 Introduction With its small memory footprint, robustness against noise, and rapid learning rates, Stochastic Gradient Descent (SGD) has proved to be well suited to data-intensive machine learning tasks [3,5,24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": false,
            "numCitedBy": 6359,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143973671"
                        ],
                        "name": "S. Melnik",
                        "slug": "S.-Melnik",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Melnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Melnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144267830"
                        ],
                        "name": "Andrey Gubarev",
                        "slug": "Andrey-Gubarev",
                        "structuredName": {
                            "firstName": "Andrey",
                            "lastName": "Gubarev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrey Gubarev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9397383"
                        ],
                        "name": "J. J. Long",
                        "slug": "J.-J.-Long",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Long",
                            "middleNames": [
                                "Jing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2466970"
                        ],
                        "name": "Geoffrey Romer",
                        "slug": "Geoffrey-Romer",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Romer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey Romer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34715669"
                        ],
                        "name": "Shiva Shivakumar",
                        "slug": "Shiva-Shivakumar",
                        "structuredName": {
                            "firstName": "Shiva",
                            "lastName": "Shivakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiva Shivakumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2705551"
                        ],
                        "name": "Matt Tolton",
                        "slug": "Matt-Tolton",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Tolton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Tolton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2439539"
                        ],
                        "name": "Theo Vassilakis",
                        "slug": "Theo-Vassilakis",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Vassilakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Theo Vassilakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Indeed, even Google researchers themselves suggest that other systems, for example Dremel, are more appropriate than MapReduce for data analysis tasks [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1720268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03363ed04e9d4d2e8c9348551815e80615969611",
            "isKey": false,
            "numCitedBy": 720,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system."
            },
            "slug": "Dremel:-Interactive-Analysis-of-Web-Scale-Datasets-Melnik-Gubarev",
            "title": {
                "fragments": [],
                "text": "Dremel: Interactive Analysis of Web-Scale Datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The architecture and implementation of Dremel are described, and how it complements MapReduce-based computing is explained, and a novel columnar storage representation for nested records is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "1 is provided in the full version of this paper [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Incremental gradient methods, Machine learning, Parallel computing, Multicore"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallelized stochastic gradient descent. Neural Information Processing Systems (NIPS)"
            },
            "venue": {
                "fragments": [],
                "text": "Parallelized stochastic gradient descent. Neural Information Processing Systems (NIPS)"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 101
                            }
                        ],
                        "text": "The image has 512 \u00d7 512 \u00d7 551 voxels, and the associated graph is 6-connected with maximum capacity 10."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max-flow problem instances in vision"
            },
            "venue": {
                "fragments": [],
                "text": "Max-flow problem instances in vision"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Incremental gradient methods, Machine learning, Parallel computing, Multicore"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bertsekas. Nonlinear Programming. Athena Scientific"
            },
            "venue": {
                "fragments": [],
                "text": "Bertsekas. Nonlinear Programming. Athena Scientific"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 23,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 32,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Hogwild:-A-Lock-Free-Approach-to-Parallelizing-Recht-R\u00e9/36f49b05d764bf5c10428b082c2d96c13c4203b9?sort=total-citations"
}