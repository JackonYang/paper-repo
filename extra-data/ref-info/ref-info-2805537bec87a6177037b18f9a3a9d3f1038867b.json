{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1650980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1152582155acaa0e9d0ccbc900a4641504256d",
            "isKey": false,
            "numCitedBy": 1344,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a compact coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal compact code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of sparse distributed coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling wavelet transforms are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented."
            },
            "slug": "What-Is-the-Goal-of-Sensory-Coding-Field",
            "title": {
                "fragments": [],
                "text": "What Is the Goal of Sensory Coding?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway and suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 280
                            }
                        ],
                        "text": "The entire set of basis functions forms a complete image code that spans the jointspace of spatial position, orientation, and scale in a manner similar to wavelet codes,which have previously been shown to form sparse representations of natural images12\n(Field, 1987; Field, 1994; Daugman, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20021288,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "da9cee8647743711c1f8b4f61185bfbece0ad284",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In biological visual systems, it is not obvious whether coding efficiency as measured by mutual information among the neurons is a factor that explains any of their properties. The center/surround receptive field profiles of neurons in the retina and geniculate are far from an orthogonal set, but a given neuron can still be regarded as a decorrelator of the incoming signal in the sense that it responds primarily to changes in the image. At the level of the brain's visual cortex, the introduction of the new variable of orientation selectivity can be regarded not only as a means for providing orientation labels for image structure, but also more basically as an effective decorrelator of the neural representation. The present image coding simulations, based on quantitative neurobiological data about the code primitives, provide measures of the bit-rate efficiency of such oriented, quadrature, neural codes. Demonstrations of data compression to below 1 bit/pixel in cortically-based, quadrature self-similar wavelet image codes are also provided.<<ETX>>"
            },
            "slug": "Entropy-reduction-and-decorrelation-in-visual-by-Daugman",
            "title": {
                "fragments": [],
                "text": "Entropy reduction and decorrelation in visual coding by oriented neural receptive fields"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The present image coding simulations, based on quantitative neurobiological data about the code primitives, provide measures of the bit-rate efficiency of such oriented, quadrature, neural codes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Biomedical Engineering"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 171
                            }
                        ],
                        "text": "\u2026curved, fractal-like edges, give rise to statistical dependencies that are ofhigher-order than linear pairwise correlations (e.g., three-point correlations) (Field,1993; Olshausen and Field, 1996b), and so it is important to consider these forms ofstructure as well in developing an e cient code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "oriented lines and edges, especially curved, fractal-like edges, give rise to statistical dependencies that are of higher-order than linear pairwise correlations (e.g., threepoint correlations) (Field, 1993;  Olshausen & Field, 1996b ), and so it is important to consider these forms of structure as well in developing an efficient code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Previously, we ascertained the receptive field for each unit by spot-mapping and showed that they are basically similar in form to the basis functions, with somewhat tighter spatial localization [ Olshausen & Field, 1996b;  Fig. 4(b)]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 62
                            }
                        ],
                        "text": "The method and results of these tests are described elsewhere(Olshausen and Field, 1996a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 192
                            }
                        ],
                        "text": "Previously, we ascertained the receptive eldfor each unit by spot-mapping and showed that they are basically similar in form tothe basis functions, with somewhat tighter spatial localization (Olshausen and Field,1996, Figure 4b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 190
                            }
                        ],
                        "text": "Previously, we have shown that these receptive eld properties maybe accounted for in terms of a strategy for producing a sparse distribution ofoutput activity in response to natural images (Olshausen and Field, 1996a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9526302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e309e441a38ccee6456bd02e0f1e894e44180d53",
            "isKey": false,
            "numCitedBy": 618,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more efficiently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e. more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for finding efficient codes for natural images. We suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex."
            },
            "slug": "Natural-image-statistics-and-efficient-coding.-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Natural image statistics and efficient coding."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that a good objective for an efficient coding of natural Scenes is to maximize the sparseness of the representation, and it is shown that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 62
                            }
                        ],
                        "text": "The method and results of these tests are described elsewhere(Olshausen and Field, 1996a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 190
                            }
                        ],
                        "text": "Previously, we have shown that these receptive eld properties maybe accounted for in terms of a strategy for producing a sparse distribution ofoutput activity in response to natural images (Olshausen and Field, 1996a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 192
                            }
                        ],
                        "text": "Previously, we ascertained the receptive eldfor each unit by spot-mapping and showed that they are basically similar in form tothe basis functions, with somewhat tighter spatial localization (Olshausen and Field,1996, Figure 4b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 171
                            }
                        ],
                        "text": "\u2026curved, fractal-like edges, give rise to statistical dependencies that are ofhigher-order than linear pairwise correlations (e.g., three-point correlations) (Field,1993; Olshausen and Field, 1996b), and so it is important to consider these forms ofstructure as well in developing an e cient code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15125241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44352b35791ceaad3439b8ccf165cc9b4978d801",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more eeciently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e., more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for nding eecient codes for natural images. We suggest that a good objective for an eecient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive elds similar to those in the primate striate cortex."
            },
            "slug": "Natural-Image-Statistics-and-Eecient-Coding-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Natural Image Statistics and Eecient Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that a good objective for an eecient coding of natural Scenes is to maximize the sparseness of the representation, and it is shown that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive elds similar to those in the primate striate cortex."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 254
                            }
                        ],
                        "text": "The entire set of basis functions forms a complete image code that spans the jointspace of spatial position, orientation, and scale in a manner similar to wavelet codes,which have previously been shown to form sparse representations of natural images12\n(Field, 1987; Field, 1994; Daugman, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1600874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeb37769d72999bcbfb0582b73607fd8d23f4545",
            "isKey": false,
            "numCitedBy": 3273,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow's theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy."
            },
            "slug": "Relations-between-the-statistics-of-natural-images-Field",
            "title": {
                "fragments": [],
                "text": "Relations between the statistics of natural images and the response properties of cortical cells."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy into first- order redundancy."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742997"
                        ],
                        "name": "C. Fyfe",
                        "slug": "C.-Fyfe",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Fyfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fyfe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34931109"
                        ],
                        "name": "R. Baddeley",
                        "slug": "R.-Baddeley",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Baddeley",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baddeley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 79
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods (Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995; Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 132
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods(Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995;Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8604471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5ae37805bc9e0291c4c2c64cb2435f91849de74",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Some recent work has investigated the dichotomy between compact coding using dimensionality reduction and sparse-distributed coding in the context of understanding biological information processing. We introduce an artificial neural network which self-organizes on the basis of simple Hebbian learning and negative feedback of activation and show that it is capable both of forming compact codings of data distributions and of identifying filters most sensitive to sparse-distributed codes. The network is extremely simple and its biological relevance is investigated via its response to a set of images which are typical of everyday life. However, an analysis of the network's identification of the filter for sparse coding reveals that this coding may not be globally optimal and that there exists an innate limiting factor which cannot be transcended."
            },
            "slug": "Finding-compact-and-sparse-distributed-of-visual-Fyfe-Baddeley",
            "title": {
                "fragments": [],
                "text": "Finding compact and sparse-distributed representations of visual images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An artificial neural network which self-organizes on the basis of simple Hebbian learning and negative feedback of activation is introduced and it is shown that it is capable both of forming compact codings of data distributions and of identifying filters most sensitive to sparse-distributed codes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909068"
                        ],
                        "name": "Y. Dan",
                        "slug": "Y.-Dan",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Dan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Dan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46567235"
                        ],
                        "name": "R. Reid",
                        "slug": "R.-Reid",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Reid",
                            "middleNames": [
                                "Clay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Reid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 80
                            }
                        ],
                        "text": "Atick and colleagues (Atick & Redlich 1990, 1992; Atick 1992; Dong & Atick 1995;Dan et al., 1996) have achieved considerable success in showing how the principle ofredundancy reduction may be applied toward understanding the response propertiesof retinal ganglion cells in terms of a strategy for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 34
                            }
                        ],
                        "text": "Given the importance of using resourcese ciently in the competition for survival, it is reasonable to think that the cortex hasdiscovered e cient coding strategies for representing natural images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7752261,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "aa918a04a4e55283dcdbfbc8d3d7896026e67640",
            "isKey": false,
            "numCitedBy": 490,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A recent computational theory suggests that visual processing in the retina and the lateral geniculate nucleus (LGN) serves to recode information into an efficient form (Atick and Redlich, 1990). Information theoretic analysis showed that the representation of visual information at the level of the photoreceptors is inefficient, primarily attributable to a high degree of spatial and temporal correlation in natural scenes. It was predicted, therefore, that the retina and the LGN should recode this signal into a decorrelated form or, equivalently, into a signal with a \u201cwhite\u201d spatial and temporal power spectrum. In the present study, we tested directly the prediction that visual processing at the level of the LGN temporally whitens the natural visual input. We recorded the responses of individual neurons in the LGN of the cat to natural, time-varying images (movies) and, as a control, to white-noise stimuli. Although there is substantial temporal correlation in natural inputs (Dong and Atick, 1995b), we found that the power spectra of LGN responses were essentially white. Between 3 and 15 Hz, the power of the responses had an average variation of only \u00b110.3%. Thus, the signals that the LGN relays to visual cortex are temporarily decorrelated. Furthermore, the responses of X-cells to natural inputs can be well predicted from their responses to white-noise inputs. We therefore conclude that whitening of natural inputs can be explained largely by the linear filtering properties (Enroth-Cugell and Robson, 1966). Our results suggest that the early visual pathway is well adapted for efficient coding of information in the natural visual environment, in agreement with the prediction of the computational theory."
            },
            "slug": "Efficient-Coding-of-Natural-Scenes-in-the-Lateral-a-Dan-Atick",
            "title": {
                "fragments": [],
                "text": "Efficient Coding of Natural Scenes in the Lateral Geniculate Nucleus: Experimental Test of a Computational Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results suggest that the early visual pathway is well adapted for efficient coding of information in the natural visual environment, in agreement with the prediction of the computational theory."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615848"
                        ],
                        "name": "Rajesh P. N. Rao",
                        "slug": "Rajesh-P.-N.-Rao",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Rao",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajesh P. N. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7136784,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e3a83c2ed3af29a23ab342212d1ae9650a0c64a1",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "The responses of visual cortical neurons during fixation tasks can be significantly modulated by stimuli from beyond the classical receptive field. Modulatory effects in neural responses have also been recently reported in a task where a monkey freely views a natural scene. In this article, we describe a hierarchical network model of visual recognition that explains these experimental observations by using a form of the extended Kalman filter as given by the minimum description length (MDL) principle. The model dynamically combines input-driven bottom-up signals with expectation-driven top-down signals to predict current recognition state. Synaptic weights in the model are adapted in a Hebbian manner according to a learning rule also derived from the MDL principle. The resulting prediction-learning scheme can be viewed as implementing a form of the expectation-maximization (EM) algorithm. The architecture of the model posits an active computational role for the reciprocal connections between adjoining visual cortical areas in determining neural response properties. In particular, the model demonstrates the possible role of feedback from higher cortical areas in mediating neurophysiological effects due to stimuli from beyond the classical receptive field. Simulations of the model are provided that help explain the experimental observations regarding neural responses in both free viewing and fixating conditions."
            },
            "slug": "Dynamic-Model-of-Visual-Recognition-Predicts-Neural-Rao-Ballard",
            "title": {
                "fragments": [],
                "text": "Dynamic Model of Visual Recognition Predicts Neural Response Properties in the Visual Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A hierarchical network model of visual recognition that explains experimental observations regarding neural responses in both free viewing and fixating conditions by using a form of the extended Kalman filter as given by the minimum description length (MDL) principle is described."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 22
                            }
                        ],
                        "text": "Atick and colleagues (Atick & Redlich 1990, 1992; Atick 1992; Dong & Atick 1995;Dan et al., 1996) have achieved considerable success in showing how the principle ofredundancy reduction may be applied toward understanding the response propertiesof retinal ganglion cells in terms of a strategy for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Atick and Redlich (1992) have shown that such awhitening lter corresponds well to the response properties of retinal ganglioncells."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Atick and colleagues ( Atick & Redlich, 1990, 1992;"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17515861,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "841cd4a6cac86fa0cfb0e8542eac5ed164f23f50",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "By examining the experimental data on the statistical properties of natural scenes together with (retinal) contrast sensitivity data, we arrive at a first principle, theoretical hypothesis for the purpose of retinal processing and its relationship to an animal's environment. We argue that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow. The extent of this whitening of the input is limited, however, by the need to suppress input noise. Our explicit theoretical solutions for the retinal filters also show a simple dependence on mean stimulus luminance: they predict an approximate Weber law at low spatial frequencies and a De Vries-Rose law at high frequencies. Assuming that the dominant source of noise is quantum, we generate a family of contrast sensitivity curves as a function of mean luminance. This family is compared to psychophysical data."
            },
            "slug": "What-Does-the-Retina-Know-about-Natural-Scenes-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "What Does the Retina Know about Natural Scenes?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143718183"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 270
                            }
                        ],
                        "text": "\u2026the number ofhigh-frequency cells may have been substantially underestimated since these unitswill generally have smaller receptive elds and so will be much more di cult to iso-late than a low-frequency unit that exhibits a more prolonged response to bars andthe like (Olshausen and Anderson, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123869570,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a2cc34e721c82eb64538c4bfeb53d3534440d857",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The majority of cells in area V1 of the primate visual cortex have largely been characterized as oriented, bandpass filters. That is, for any given region of visual space, one finds cells tuned to a variety of different spatial frequency bands and orientations (De Valois et al., 1982; Parker and Hawken, 1988). In order to be consistent with our perceptual capabilities, one would expect these cells to be organized in a way that yields a veridical representation of spatial structure over a range of different scales. Here, we present a model of the spatial frequency organization that would be required in primate V1 in order to form a complete representation of spatial information provided by the optic nerve."
            },
            "slug": "A-Model-of-the-Spatial-Frequency-Organization-in-Olshausen-Anderson",
            "title": {
                "fragments": [],
                "text": "A Model of the Spatial-Frequency Organization in Primate Striate Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Here, a model of the spatial frequency organization that would be required in primate V1 in order to form a complete representation of spatial information provided by the optic nerve is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858054"
                        ],
                        "name": "P. F\u00f6ldi\u00e1k",
                        "slug": "P.-F\u00f6ldi\u00e1k",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "F\u00f6ldi\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. F\u00f6ldi\u00e1k"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50555333"
                        ],
                        "name": "M. Young",
                        "slug": "M.-Young",
                        "structuredName": {
                            "firstName": "Malcolm",
                            "lastName": "Young",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18763154,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "552e26d425c6cd6d79e428716adcd2897ee62e0e",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A central goal in the study of cortical function is to understand how states of the environment are represented by firing patterns of cortical neurons. Electrophysiological recordings from single cells have revealed a remarkably close relationship among stimuli, neural activity, and perceptual states. The nature of this relationship and interpretations of experimental results are fiercely debated. Is sensory information represented by the activity of single, individually meaningful cells, or is it only the global activity pattern across a whole cell population that corresponds to interpretable states? There are now strong theoretical reasons and experimental evidence suggesting that the brain adopts a compromise between these extremes which is often referred to as sparse coding."
            },
            "slug": "Sparse-coding-in-the-primate-cortex-F\u00f6ldi\u00e1k-Young",
            "title": {
                "fragments": [],
                "text": "Sparse coding in the primate cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "There are now strong theoretical reasons and experimental evidence suggesting that the brain adopts a compromise between these extremes which is often referred to as sparse coding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The formal relationship described in Appendix I (see also  Olshausen, 1996 ), shows that both algorithms are solving the same maximum-likelihood problem, but by making different simplifying assumptions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "The formal relationship described inAppendix A (see also Olshausen, 1996), shows that both algorithms are solving thesame maximum-likelihood problem, described in Section 3, but by making di erentsimplifying assumptions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15214722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69d713c63575947e4ef78b4dc04ed3a686cc06a4",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In previous work (Olshausen \\& Field 1996), an algorithm was described for learning linear sparse codes which, when trained on natural images, produces a set of basis functions that are spatially localized, oriented, and bandpass (i.e., wavelet-like). This note shows how the algorithm may be interpreted within a maximum-likelihood framework. Several useful insights emerge from this connection: it makes explicit the relation to statistical independence (i.e., factorial coding), it shows a formal relationship to the algorithm of Bell and Sejnowski (1995), and it suggests how to adapt parameters that were previously fixed."
            },
            "slug": "Learning-linear,-sparse,-factorial-codes-Olshausen",
            "title": {
                "fragments": [],
                "text": "Learning linear, sparse, factorial codes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This note shows how the algorithm for learning linear sparse codes may be interpreted within a maximum-likelihood framework and makes explicit the relation to statistical independence and shows a formal relationship to Bell and Sejnowski's algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195869"
                        ],
                        "name": "D. Dong",
                        "slug": "D.-Dong",
                        "structuredName": {
                            "firstName": "Dawei",
                            "lastName": "Dong",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 62
                            }
                        ],
                        "text": "Atick and colleagues (Atick & Redlich 1990, 1992; Atick 1992; Dong & Atick 1995;Dan et al., 1996) have achieved considerable success in showing how the principle ofredundancy reduction may be applied toward understanding the response propertiesof retinal ganglion cells in terms of a strategy for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6969376,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "790b5be49f5c7ad02b4ebd4db9967f1d2c26fcae",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural time-varying images possess significant temporal correlations when sampled frame by frame by the photoreceptors. These correlations persist even after retinal processing and hence, under natural activation conditions, the signal sent to the lateral geniculate nucleus (LGN) is temporally redundant or inefficient. We explore the hypothesis that the LGN is concerned, among other things, with improving efficiency of visual representation through active temporal decorrelation of the retinal signal much in the same way that the retina improves efficiency by spatially decorrelating incoming images. Using some recently measured statistical properties of time-varying images, we predict the spatio-temporal receptive fields that achieve this decorrelation. It is shown that, because of neuronal nonlinearities, temporal decorrelation requires two response types, the lagged and nonlagged, just as spatial decorrelation requires on and off response types. The tuning and response properties of the predicted LGN ce..."
            },
            "slug": "Temporal-decorrelation:-a-theory-of-lagged-and-in-Dong-Atick",
            "title": {
                "fragments": [],
                "text": "Temporal decorrelation: a theory of lagged and nonlagged responses in the lateral geniculate nucleus"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The hypothesis that the lateral geniculate nucleus is concerned with improving efficiency of visual representation through active temporal decorrelation of the retinal signal much in the same way that the retina improves efficiency by spatially decorrelating incoming images is explored."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 22
                            }
                        ],
                        "text": "Atick and colleagues (Atick & Redlich 1990, 1992; Atick 1992; Dong & Atick 1995;Dan et al., 1996) have achieved considerable success in showing how the principle ofredundancy reduction may be applied toward understanding the response propertiesof retinal ganglion cells in terms of a strategy for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Atick and colleagues ( Atick & Redlich, 1990, 1992;"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28154878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03128ecdd3c3dfb9752861d8555b97535e1cfc14",
            "isKey": false,
            "numCitedBy": 531,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a theory of the early processing in the mammalian visual pathway. The theory is formulated in the language of information theory and hypothesizes that the goal of this processing is to recode in order to reduce a generalized redundancy subject to a constraint that specifies the amount of average information preserved. In the limit of no noise, this theory becomes equivalent to Barlow's redundancy reduction hypothesis, but it leads to very different computational strategies when noise is present. A tractable approach for finding the optimal encoding is to solve the problem in successive stages where at each stage the optimization is performed within a restricted class of transfer functions. We explicitly find the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations. The solution shows agreement with the experimentally observed transfer functions at all levels of signal to noise."
            },
            "slug": "Towards-a-Theory-of-Early-Visual-Processing-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "Towards a Theory of Early Visual Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A theory of the early processing in the mammalian visual pathway is proposed and the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations is found."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3434639"
                        ],
                        "name": "Y. Tadmor",
                        "slug": "Y.-Tadmor",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Tadmor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tadmor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34941108"
                        ],
                        "name": "D. Tolhurst",
                        "slug": "D.-Tolhurst",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tolhurst",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tolhurst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 191
                            }
                        ],
                        "text": "This is what one usually thinks of as thestandard model of a cortical simple cell, although the physiological data show thatthere are interesting forms of non-linearity in these cells (e.g., Tadmor and Tolhurst,1989) that are not captured by such a straightforward, linear model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 52
                            }
                        ],
                        "text": "A similar e ect has been observed in corticalcells (Tadmor and Tolhurst, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27762429,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b95394e9881b87f4a2e2841ab9fab5727c07aacb",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "It is believed that spatial summation in most simple cells is a linear process. If this were so, then the Fourier transform of a simple cell's line weighting function should predict the cell's spatial frequency tuning curve. We have compared such predictions with experimental measurements and have found a consistent discrepancy: the predicted tuning curve is much too broad. We show qualitatively that this kind of discrepancy is consistent with the well-known threshold nonlinearity shown by most cortical cells. We have tested quantitatively whether a response threshold could explain the observed disagreements between predictions and measurements: a least-squares minimization routine was used to fit the inverse Fourier Transform of the measured frequency tuning curve to the measured line weighting function. The fitting procedure permitted us to introduce a threshold to the reconstructed line weighting function. The results of the analysis show that, for all of the cells tested, the Fourier transforms produced better predictions when a response threshold was included in the model. For some cells, the actual magnitude of the response threshold was measured independently and found to be compatible with that suggested by the model. The effects of nonlinearities of spatial summation are considered."
            },
            "slug": "The-effect-of-threshold-on-the-relationship-between-Tadmor-Tolhurst",
            "title": {
                "fragments": [],
                "text": "The effect of threshold on the relationship between the receptive-field profile and the spatial-frequency tuning curve in simple cells of the cat's striate cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A response threshold could explain the observed disagreements between predictions and measurements and it is shown qualitatively that this kind of discrepancy is consistent with the well-known threshold nonlinearity shown by most cortical cells."
            },
            "venue": {
                "fragments": [],
                "text": "Visual neuroscience"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064184853"
                        ],
                        "name": "C. C. Law",
                        "slug": "C.-C.-Law",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Law",
                            "middleNames": [
                                "Charles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. C. Law"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8884630"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 110
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods(Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995;Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Another class of efficient coding methods is based on projection pursuit methods (Friedman, 1987; Intrator, 1992;  Law & Cooper, 1994;  Fyfe & Baddeley, 1995; Press & Lee, 1996; Lu, Chubb, & Sperling, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 25243472,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "486babc6bea3568bf1ef0ae8ef5d469c212bcb41",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bienenstock, Cooper, and Munro (BCM) theory of synaptic plasticity has successfully reproduced the development of orientation selectivity and ocular dominance in kitten visual cortex in normal, as well as deprived, visual environments. To better compare the consequences of this theory with experiment, previous abstractions of the visual environment are replaced in this work by real visual images with retinal processing. The visual environment is represented by 24 gray-scale natural images that are shifted across retinal fields. In this environment, the BCM neuron develops receptive fields similar to the fields of simple cells found in kitten striate cortex. These fields display adjacent excitatory and inhibitory bands when tested with spot stimuli, orientation selectivity when tested with bar stimuli, and spatial-frequency selectivity when tested with sinusoidal gratings. In addition, their development in various deprived visual environments agrees with experimental results."
            },
            "slug": "Formation-of-receptive-fields-in-realistic-visual-Law-Cooper",
            "title": {
                "fragments": [],
                "text": "Formation of receptive fields in realistic visual environments according to the Bienenstock, Cooper, and Munro (BCM) theory."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Bienenstock, Cooper, and Munro (BCM) theory of synaptic plasticity has successfully reproduced the development of orientation selectivity and ocular dominance in kitten visual cortex in normal, as well as deprived, visual environments."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583773"
                        ],
                        "name": "L. Parra",
                        "slug": "L.-Parra",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Parra",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Parra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 47
                            }
                        ],
                        "text": "Thisconnection has also been shown recently by Pearlmutter & Parra (1996) and Mackay(1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6469440,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e2b06b4fcd3f806c014fb05ca642cc6fa3cc664",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Source separation arises in a surprising number of signal processing applications, from speech \nrecognition to EEG analysis. In the square linear blind source separation problem without time delays, \none must find an unmixing matrix which can detangle the result of mixing n unknown independent sources \nthrough an unknown n x n mixing matrix. The recently introduced ICA blind source separation algorithm \n(Baram and Roth 1994; Bell and Sejnowski 1995) is a powerful and surprisingly simple technique for solving \nthis problem. ICA is all the more remarkable for performing so well despite making absolutely no use of the \ntemporal structure of its input! This paper presents a new algorithm, contextual ICA, which derives from a \nmaximum likelihood density estimation formulation of the problem. cICA can incorporate arbitrarily complex \nadaptive history-sensitive source models, and thereby make use of the temporal structure of its input. \nThis allows it to separate in a number of situations where standard ICA cannot, including sources with low \nkurtosis, colored gaussian sources, and sources which have gaussian histograms. Since ICA is a special case \nof cICA, the MLE derivation provides as a corollary a rigorous derivation of classic ICA."
            },
            "slug": "A-Context-Sensitive-Generalization-of-ICA-Pearlmutter-Parra",
            "title": {
                "fragments": [],
                "text": "A Context-Sensitive Generalization of ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new algorithm, contextual ICA, is presented, which derives from a maximum likelihood density estimation formulation of the problem, which can incorporate arbitrarily complex adaptive history-sensitive source models, and thereby make use of the temporal structure of its input."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 147
                            }
                        ],
                        "text": "\u2026hhlogP (I 1)i+ log j det 1ji (26)= argmin \"h Xi S(( 1)i I)i log j det 1j# : (27)By making the following de nitions according to the convention of Bell and Sejnowski(1995), W = 1 (28)ui = Wi I (29)then, the gradient descent learning rule forW becomes Wij / S 0(ui)Ij + cofWijdetW : (30)This is\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 145
                            }
                        ],
                        "text": "\u2026function) of the prior onthe ai, i.e., yi = g(ui) (31)g(ui) = Z ui 1 1Z e S(x)dx : (32)18\nThus, the independent component analysis algorithm of Bell and Sejnowski (1995)is formally equivalent to maximum likelihood in the case of no noise and a squaresystem (dimensionality of output =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8754,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 137
                            }
                        ],
                        "text": "the basis of coding strategies such as the Discrete Cosine Transform (used in JPEG image compression), or orthonormal wavelet transforms (Mallat, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "This is4\nthe basis of coding strategies such as the Discrete Cosine Transform (used in JPEGimage compression), or orthonormal wavelet transforms (Mallat, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2356353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b78626ce1a562c05b1c06f9c805e839f9760b9ab",
            "isKey": false,
            "numCitedBy": 20813,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiresolution representations are effective for analyzing the information content of images. The properties of the operator which approximates a signal at a given resolution were studied. It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/sup j/ (where j is an integer) can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions. In L/sup 2/(R), a wavelet orthonormal basis is a family of functions which is built by dilating and translating a unique function psi (x). This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror filters. Wavelet representation lies between the spatial and Fourier domains. For images, the wavelet representation differentiates several spatial orientations. The application of this representation to data compression in image coding, texture discrimination and fractal analysis is discussed. >"
            },
            "slug": "A-Theory-for-Multiresolution-Signal-Decomposition:-Mallat",
            "title": {
                "fragments": [],
                "text": "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/Sup j/ can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6059616"
                        ],
                        "name": "G. F. Harpur",
                        "slug": "G.-F.-Harpur",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Harpur",
                            "middleNames": [
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. F. Harpur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680968"
                        ],
                        "name": "R. Prager",
                        "slug": "R.-Prager",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Prager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prager"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13911187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0590940043b88992c4417d239c01c282cd31309",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an unsupervised neural network which exhibits competition between units via inhibitory feedback. The operation is such as to minimize reconstruction error, both for individual patterns, and over the entire training set. A key difference from networks which perform principal components analysis, or one of its variants, is the ability to converge to non-orthogonal weight values. We discuss the network's operation in relation to the twin goals of maximizing information transfer and minimizing code entropy, and show how the assignment of prior probabilities to network outputs can help to reduce entropy. We present results from two binary coding problems, and from experiments with image coding."
            },
            "slug": "Development-of-low-entropy-coding-in-a-recurrent-Harpur-Prager",
            "title": {
                "fragments": [],
                "text": "Development of low entropy coding in a recurrent network."
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An unsupervised neural network which exhibits competition between units via inhibitory feedback is presented, and it is shown how the assignment of prior probabilities to network outputs can help to reduce entropy."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40468115"
                        ],
                        "name": "Joel L. Davis",
                        "slug": "Joel-L.-Davis",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Davis",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 102
                            }
                        ],
                        "text": "The form of this computation is very similar toan analysis/synthesis loop, which has been proposed by Mumford (1994) as a waythat cortical feedback could be used to perform inference on images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "This process is analogous to the analysis-synthesisloop proposed by Mumford (1994) for performing inference on images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10395533,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4a1a91cdf12c63e66e79321120de2ff510355a21",
            "isKey": false,
            "numCitedBy": 665,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This book originated at a small and informal workshop held in December of 1992 in Idyllwild, a relatively secluded resort village situated amid forests in the San Jacinto Mountains above Palm Springs in Southern California. Eighteen colleagues from a broad range of disciplines, including biophysics, electrophysiology, neuroanatomy, psychophysics, clinical studies, mathematics and computer vision, discussed 'Large Scale Models of the Brain,' that is, theories and models that cover a broad range of phenomena, including early and late vision, various memory systems, selective attention, and the neuronal code underlying figure-ground segregation and awareness (for a brief summary of this meeting, see Stevens 1993). The bias in the selection of the speakers toward researchers in the area of visual perception reflects both the academic background of one of the organizers as well as the (relative) more mature status of vision compared with other modalities. This should not be surprising given the emphasis we humans place on'seeing' for orienting ourselves, as well as the intense scrutiny visual processes have received due to their obvious usefullness in military, industrial, and robotic applications. JMD"
            },
            "slug": "Large-Scale-Neuronal-Theories-of-the-Brain-Koch-Davis",
            "title": {
                "fragments": [],
                "text": "Large-Scale Neuronal Theories of the Brain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 195
                            }
                        ],
                        "text": "Surely, there will be statistical dependencies among the elements of the single-stage model, and it would be desirable to have these modeled by a second and thirdstage in a hierarchical fashion (Dayan et al., 1995; Lewicki and Sejnowski, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Surely, there will be statistical dependencies among the elements of the single-stage model, and it would be desirable to have these modeled by a second and third stage in a hierarchical fashion ( Dayan et al., 1995;  Lewicki & Sejnowski, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1890561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "isKey": false,
            "numCitedBy": 1172,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "slug": "The-Helmholtz-Machine-Dayan-Hinton",
            "title": {
                "fragments": [],
                "text": "The Helmholtz Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations is described, viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735689"
                        ],
                        "name": "A. Parker",
                        "slug": "A.-Parker",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Parker",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Parker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2480424"
                        ],
                        "name": "M. Hawken",
                        "slug": "M.-Hawken",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hawken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hawken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20372548,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "11a437b768ff54b6af01b5568087b38d97942f00",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Measurements of the spatial contrast sensitivity function and orientation selectivity of visual neurons in the foveal striate cortex (V1) of primates were interpreted within the context of a model of the two-dimensional spatial structure of their receptive fields. Estimates of the spatial dimensions of the receptive fields along the axis of preferred orientation were derived from the application of the model and were compared with estimates of the smallest spatial subunit in the dimension orthogonal to the preferred orientation. Some measure of agreement was found with corresponding estimates of parameters for psychophysical channels in human foveal vision."
            },
            "slug": "Two-dimensional-spatial-structure-of-receptive-in-Parker-Hawken",
            "title": {
                "fragments": [],
                "text": "Two-dimensional spatial structure of receptive fields in monkey striate cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Measurements of the spatial contrast sensitivity function and orientation selectivity of visual neurons in the foveal striate cortex (V1) of primates were interpreted within the context of a model of the two-dimensional spatial structure of their receptive fields."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This point has been emphasized by  Simoncelli, Freeman, Adelson, & Heeger (1992) , who show that overcomplete codes allow for small translations or scaling of local image features to result in a smooth and graceful change in the distribution of activity among coefficients."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 271
                            }
                        ],
                        "text": "\u2026for blind separation problems where the number of independentsources (dimensionality of a) is less than or equal to the number of mixed signals(dimensionality of I), it will become a concern in the representation of images, whereovercompleteness is a desirable feature (Simoncelli et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43701174,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8515604037444b3f079a9d328b0c560f33da0a19",
            "isKey": false,
            "numCitedBy": 1427,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >"
            },
            "slug": "Shiftable-multiscale-transforms-Simoncelli-Freeman",
            "title": {
                "fragments": [],
                "text": "Shiftable multiscale transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two examples of jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored and the usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Among these are the algorithms of  Comon (1994) , Amari, Cichocki, & Yang (1996), and Bell & Sejnowski (1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "Among these are the algorithms of Comon (1994), Amari (1995), and Belland Sejnowski (1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Other methods for learning sparse codes have been described by Foldiak (1990) and  Zemel (1993) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117036852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b2bffdf5b62305bec4c0f1ea7e3c1ba66fccb5",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental problem in learning and reasoning about a set of information is finding the right representation. The primary goal of an unsupervised learning procedure is to optimize the quality of a system's internal representation. In this thesis, we present a general framework for describing unsupervised learning procedures based on the Minimum Description Length (MDL) principle. The MDL principle states that the best model is one that minimizes the summed description length of the model and the data with respect to the model. Applying this approach to the unsupervised learning problem makes explicit a key trade off between the accuracy of a representation (i.e., how concise a description of the input may be generated from it) and its succinctness (i.e., how compactly the representation itself can be described). \nViewing existing unsupervised learning procedures in terms of the framework exposes their implicit assumptions about the type of structure assumed to underlie the data. While these existing algorithms typically minimize the data description using a fixed length representation, we use the framework to derive a class of objective functions for training self-supervised neural networks, where the goal is to minimize the description length of the representation simultaneously with that of the data. Formulating a description of the representation forces assumptions about the structure of the data to be made explicit, which in turn leads to a particular network configuration as well as an objective function that can be used to optimize the network parameters. We describe three new learning algorithms derived in this manner from the MDL framework. Each algorithm embodies a different scheme for describing the internal representation, and is therefore suited to a range of datasets based on the structure underlying the data. Simulations demonstrate the applicability of these algorithms on some simple computational vision tasks."
            },
            "slug": "A-minimum-description-length-framework-for-learning-Zemel",
            "title": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis presents a general framework for describing unsupervised learning procedures based on the Minimum Description Length (MDL) principle, and describes three new learning algorithms derived in this manner from the MDL framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704693"
                        ],
                        "name": "J. Bower",
                        "slug": "J.-Bower",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bower",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bower"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37908783,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "93d06cd7fa7d1048724e769820db399214e8ed33",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Since the introduction of the alpha function by RaIl in 1967 (12), there has been significant progress in our understanding of the molecular events underlying synaptic transmission. Particular receptor types have been identified and their activation kinetics characterized. It is now possible to develop models of these receptors, using a formalism similar to that introduced by Hodgkin and Huxley [9). In this paper, we present recently-introduced models obtained by simplifying more detailed biophysical models of postsynaptic receptors [7]. The simplified models are fully compatible with the Hodgkin-Huxley formalism, are very efficient to simulate, and account for important phenomena such &8 synaptic summation and desensitization. These models should be useful in large-scale network simulations."
            },
            "slug": "The-Neurobiology-of-Computation-Bower",
            "title": {
                "fragments": [],
                "text": "The Neurobiology of Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Recently-introduced models obtained by simplifying more detailed biophysical models of postsynaptic receptors are presented, fully compatible with the Hodgkin-Huxley formalism, are very efficient to simulate, and account for important phenomena such as synaptic summation and desensitization."
            },
            "venue": {
                "fragments": [],
                "text": "Springer US"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2204972"
                        ],
                        "name": "M. V. Rossum",
                        "slug": "M.-V.-Rossum",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Rossum",
                            "middleNames": [
                                "C.",
                                "W.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. V. Rossum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 195
                            }
                        ],
                        "text": "Surely, there will be statistical dependencies among the elements of the single-stage model, and it would be desirable to have these modeled by a second and thirdstage in a hierarchical fashion (Dayan et al., 1995; Lewicki and Sejnowski, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2281536,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Lecture Notes for the MSc/DTC module. The brain is a complex computing machine which has evolved to give the ttest output to a given input. Neural computation has as goal to describe the function of the nervous system in mathematical and computational terms. By analysing or simulating the resulting equations, one can better understand its function, research how changes in parameters would eect the function, and try to mimic the nervous system in hardware or software implementations. Neural Computation is a bit like physics, that has been successful in describing numerous physical phenomena. However, approaches developed in those elds not always work for neural computation, because: 1. Physical systems are best studied in reduced, simplied circumstances, but the nervous system is hard to study in isolation. Neurons require a narrow range of operating conditions (temperature, oxygen, presence of other neurons, ion concentrations, ...) under which they work as they should. These conditions are hard to reproduce outside the body. Secondly, the neurons form a highly interconnected network. The function of the nervous systems depends on this connectivity and interaction, by trying to isolate the components, you are likely to alter the function. 2. It is not clear how much detail one needs to describe the computations in the brain. In these lectures we shall see various description levels. 3. Neural signals and neural connectivity are hard to measure, especially, if disturbance and damage to the nervous system is to be kept minimal. Perhaps Neural Computation has more in common with trying to gure out how a complicated machine, such as a computer or car works. Knowledge of the basic physics helps, but is not sucient. Luckily there are factors which perhaps make understanding the brain easier than understanding an arbitrary complicated machine: 1. There is a high degree of conservation across species. This means that animal studies can be used to gain information about the human brain. Furthermore, study of, say, the visual system might help to understand the auditory system. 2. The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives. Therefore it might be possible to nd the organising principles and develop a brain from there. This would be easier than guring out the complete 'wiring diagram'. 3. The nervous system is exible and robust, neurons die everyday. This stands \u2026"
            },
            "slug": "Neural-Computation-Rossum",
            "title": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives, and it might be possible to develop a brain from there."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8909922"
                        ],
                        "name": "N. Intrator",
                        "slug": "N.-Intrator",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Intrator",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Intrator"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15475544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "136fc611e49e5f3676265a288b78e473a752783b",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel unsupervised neural network for dimensionality reduction that seeks directions emphasizing multimodality is presented, and its connection to exploratory projection pursuit methods is discussed. This leads to a new statistical insight into the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982). The importance of a dimensionality reduction principle based solely on distinguishing features is demonstrated using a phoneme recognition experiment. The extracted features are compared with features extracted using a backpropagation network."
            },
            "slug": "Feature-Extraction-Using-an-Unsupervised-Neural-Intrator",
            "title": {
                "fragments": [],
                "text": "Feature Extraction Using an Unsupervised Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A novel unsupervised neural network for dimensionality reduction that seeks directions emphasizing multimodality is presented, and its connection to exploratory projection pursuit methods is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 252
                            }
                        ],
                        "text": "However, it should be notedthat the code being utilized here is a sparse, distributed code, which actually occupies amiddle ground between dense population codes at one end and local representations(i.e., grandmother cells) at the other (Foldiak 1995; Hinton & Ghahramani 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, it should be noted that the code being utilized here is a sparse, distributed code, which actually occupies a middle ground between dense population codes at one end and local representations (i.e., grandmother cells) at the other (Foldiak, 1995;  Hinton & Ghahramani, 1997 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17706343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e08b47eadfac97fab508485ed5fbef9dbbbd9a3",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network. The model uses bottom-up, top-down and lateral connections to perform Bayesian perceptual inference correctly. Once perceptual inference has been performed the connection strengths can be updated using a very simple learning rule that only requires locally available information. We demonstrate that the network learns to extract sparse, distributed, hierarchical representations."
            },
            "slug": "Generative-models-for-discovering-sparse-Hinton-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Generative models for discovering sparse distributed representations."
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network that learns to extract sparse, distributed, hierarchical representations is described."
            },
            "venue": {
                "fragments": [],
                "text": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763321"
                        ],
                        "name": "E. Saund",
                        "slug": "E.-Saund",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Saund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 27
                            }
                        ],
                        "text": "For the case of occlusion, Saund (1995) has described a\\soft-or\" function for dealing with feature overlaps in a binary image domain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For the case of occlusion,  Saund (1995)  has described a \"soft-or\" function for dealing with feature overlaps in a binary image domain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18231498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd7d7767129ed180db39d38be28c1ae389481d2f",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data. Unlike the hard k-means clustering algorithm and the soft mixture model, each of which assumes that a single hidden event generates each data point, a multiple cause model accounts for observed data by combining assertions from many hidden causes, each of which can pertain to varying degree to any subset of the observable dimensions. We employ an objective function and iterative gradient descent learning algorithm resembling the conventional mixture model. A crucial issue is the mixing function for combining beliefs from different cluster centers in order to generate data predictions whose errors are minimized both during recognition and learning. The mixing function constitutes a prior assumption about underlying structural regularities of the data domain; we demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing, and offer alternative forms of the nonlinearity for two types of data domain. Results are presented demonstrating the algorithm's ability successfully to discover coherent multiple causal representations in several experimental data sets."
            },
            "slug": "A-Multiple-Cause-Mixture-Model-for-Unsupervised-Saund",
            "title": {
                "fragments": [],
                "text": "A Multiple Cause Mixture Model for Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data, which employs an objective function and iterative gradient descent learning algorithm resembling the conventional mixture model and demonstrates its ability to discover coherent multiple causal representations in several experimental data sets."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 194
                            }
                        ],
                        "text": "Surely, there will be statistical dependencies among the elements of the singlestage model, and it would be desirable to have these modeled by a second and third stage in a hierarchical fashion (Dayan et al., 1995; Lewicki and Sejnowski, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 215
                            }
                        ],
                        "text": "Surely, there will be statistical dependencies among the elements of the single-stage model, and it would be desirable to have these modeled by a second and thirdstage in a hierarchical fashion (Dayan et al., 1995; Lewicki and Sejnowski, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7756318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16e0150a5257e78b028ca633c907489aa1693926",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer architectures such as those used in Bayesian belief networks and Helmholtz machines provide a powerful framework for representing and learning higher order statistical relations among inputs. Because exact probability calculations with these models are often intractable, there is much interest in finding approximate algorithms. We present an algorithm that efficiently discovers higher order structure using EM and Gibbs sampling. The model can be interpreted as a stochastic recurrent network in which ambiguity in lower-level states is resolved through feedback from higher levels. We demonstrate the performance of the algorithm on benchmark problems."
            },
            "slug": "Bayesian-Unsupervised-Learning-of-Higher-Order-Lewicki-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Bayesian Unsupervised Learning of Higher Order Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work presents an algorithm that efficiently discovers higher order structure using EM and Gibbs sampling and can be interpreted as a stochastic recurrent network in which ambiguity in lower-level states is resolved through feedback from higher levels."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795076"
                        ],
                        "name": "M. Arbib",
                        "slug": "M.-Arbib",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Arbib",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Arbib"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16312222,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "501ffe07846b34c91b3dd6fdb4fc02f22087add2",
            "isKey": false,
            "numCitedBy": 3459,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "A circular cribbage board having a circular base plate on which a circular counter disc, bearing a circular scale having 122 divisions numbered consecutively from 0, is mounted for rotation. A transparent cover plate which is fixedly mounted on the base plate over the counter plate and through which the counter disc scale can be viewed has an arcuate slot the length of which exposes thirty holes of a row of holes in the counter plate adjacent the circular scale thereon. A circular scale numbered from 0 to 30 is displayed on the cover plate adjacent the slot thereon so that a player to record his score, extends a pointed instrument into a hole of the counter disc adjacent a selected number on the cover plate and rotates the counter disc, until the instrument meets the end of the slot, the cumulative score of the player being indicated by comparing a registration mark on the cover plate with the counter disc scale."
            },
            "slug": "The-handbook-of-brain-theory-and-neural-networks-Arbib",
            "title": {
                "fragments": [],
                "text": "The handbook of brain theory and neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A circular cribbage board having a circular base plate on which a circular counter disc, bearing a circular scale having 122 divisions numbered consecutively from 0, is mounted for rotation."
            },
            "venue": {
                "fragments": [],
                "text": "A Bradford book"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079246689"
                        ],
                        "name": "Axthonv G. Oettinger",
                        "slug": "Axthonv-G.-Oettinger",
                        "structuredName": {
                            "firstName": "Axthonv",
                            "lastName": "Oettinger",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axthonv G. Oettinger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 271
                            }
                        ],
                        "text": "\u2026for blind separation problems where the number of independentsources (dimensionality of a) is less than or equal to the number of mixed signals(dimensionality of I), it will become a concern in the representation of images, whereovercompleteness is a desirable feature (Simoncelli et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207762321,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "04a5241e96d0e7ac63eb44ed8cfbcdcda9df1583",
            "isKey": false,
            "numCitedBy": 1598,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since at most T of these equations can come from (7), at least (m T) of them come from (8\u2019). Thus, for the distribution (PI\u2019, . . . , p,\u2018), at most T of the probabilities are nonzero. The proof of our contention now follows by letting plo, . . . , 10~0 in the above argument be a distribution for which channel capacity is attained. It should be remarked that this proof provides (via the Simplex Method of linear programming) a means for reducing any transmitter with more than r symbols, to another, equally good or better, with at most T symbols. For a discussion of the possibility of reducing the number of receiver symbols, see Feinstein.5"
            },
            "slug": "IEEE-TRANSACTIONS-ON-INFORMATION-THEORY-Oettinger",
            "title": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON INFORMATION THEORY"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40574103"
                        ],
                        "name": "S. Klinke",
                        "slug": "S.-Klinke",
                        "structuredName": {
                            "firstName": "Sigbert",
                            "lastName": "Klinke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Klinke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549124"
                        ],
                        "name": "J. Polzehl",
                        "slug": "J.-Polzehl",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Polzehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Polzehl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60628897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43d7cce568c4d6ec9cc2d95fe54dd5fa8f51e936",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques. This class contains methods designed for analyzing high dimensional data using low-dimensional projections. The main idea is to describe \u201cinteresting\u201d projections by maximizing an objective function or projection pursuit index."
            },
            "slug": "Exploratory-Projection-Pursuit-Klinke-Polzehl",
            "title": {
                "fragments": [],
                "text": "Exploratory Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques that contains methods designed for analyzing high dimensional data using low-dimensional projections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 189
                            }
                        ],
                        "text": "Thisresult should not come as a surprise, because it simply re ects the fact that naturalimages contain localized, oriented structures with limited phase alignment acrossspatial-frequency (Field, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 225
                            }
                        ],
                        "text": "In addition, one can see evidence for sparse structure in images by ltering them with a set of log-Gabor lters and collecting histograms of the resulting output distributions; these distributions typically show high kurtosis (Field, 1993), which is indicative of sparse structure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 159
                            }
                        ],
                        "text": "\u2026curved, fractal-like edges, give rise to statistical dependencies that are ofhigher-order than linear pairwise correlations (e.g., three-point correlations) (Field,1993; Olshausen and Field, 1996b), and so it is important to consider these forms ofstructure as well in developing an e cient code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 28
                            }
                        ],
                        "text": ", three-point correlations) (Field, 1993; Olshausen and Field, 1996b), and so it is important to consider these forms of structure as well in developing an e cient code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 226
                            }
                        ],
                        "text": "In addition, one can see evidence for sparse structure in imagesby ltering them with a set of log-Gabor lters and collecting histograms of the re-sulting output distributions; these distributions typically show high kurtosis (Field,1993), which is indicative of sparse structure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 191
                            }
                        ],
                        "text": "This result should not come as a surprise, because it simply re ects the fact that natural images contain localized, oriented structures with limited phase alignment across spatial-frequency (Field, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scale-invariance and self-similar `wavelet' transforms: an analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 147
                            }
                        ],
                        "text": "\u2026hhlogP (I 1)i+ log j det 1ji (26)= argmin \"h Xi S(( 1)i I)i log j det 1j# : (27)By making the following de nitions according to the convention of Bell and Sejnowski(1995), W = 1 (28)ui = Wi I (29)then, the gradient descent learning rule forW becomes Wij / S 0(ui)Ij + cofWijdetW : (30)This is\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 145
                            }
                        ],
                        "text": "\u2026function) of the prior onthe ai, i.e., yi = g(ui) (31)g(ui) = Z ui 1 1Z e S(x)dx : (32)18\nThus, the independent component analysis algorithm of Bell and Sejnowski (1995)is formally equivalent to maximum likelihood in the case of no noise and a squaresystem (dimensionality of output =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60478593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46909441dd40badf6f2f1a815dcdfb295bb194ef",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-information-maximization-approach-to-blind-and-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "An information-maximization approach to blind separation and blind deconvolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34931109"
                        ],
                        "name": "R. Baddeley",
                        "slug": "R.-Baddeley",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Baddeley",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baddeley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Note that these reasons for desiring sparseness are separate from those that have been written about elsewhere, such as increasing capacity in associative memory (Baum, Moody, & Wilczek, 1988), minimizing wiring length and ease of forming associations (Foldiak, 1995), or metabolic efficiency ( Baddeley, 1996 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 284
                            }
                        ],
                        "text": "\u20261b.Note that these reasons for desiring sparseness are separate from those thathave been written about elsewhere, such as increasing capacity in associative mem-ory (Baum et al., 1988), minimizing wiring length and ease of forming associations(Foldiak, 1995), or metabolic e ciency (Baddeley, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6216947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d40d9c3d403903a6e8bf19ec7f3bcf73bf4a8ced",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-efficient-code-in-V1-Baddeley",
            "title": {
                "fragments": [],
                "text": "An efficient code in V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2102692030"
                        ],
                        "name": "RussLL L. Ds Vnlos",
                        "slug": "RussLL-L.-Ds-Vnlos",
                        "structuredName": {
                            "firstName": "RussLL",
                            "lastName": "Vnlos",
                            "middleNames": [
                                "L.",
                                "Ds"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "RussLL L. Ds Vnlos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081557442"
                        ],
                        "name": "Duaxs G. ALSREcHT",
                        "slug": "Duaxs-G.-ALSREcHT",
                        "structuredName": {
                            "firstName": "Duaxs",
                            "lastName": "ALSREcHT",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duaxs G. ALSREcHT"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103028334"
                        ],
                        "name": "Lrse G. Tsonrll",
                        "slug": "Lrse-G.-Tsonrll",
                        "structuredName": {
                            "firstName": "Lrse",
                            "lastName": "Tsonrll",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lrse G. Tsonrll"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9306470,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d54f243c31a5797b059c945fec65502e47d5e879",
            "isKey": false,
            "numCitedBy": 701,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SPATIAL-FREQUENCY-SELECTIVITY-OF-CELLS-IN-MACAQUE-Vnlos-ALSREcHT",
            "title": {
                "fragments": [],
                "text": "SPATIAL FREQUENCY SELECTIVITY OF CELLS IN MACAQUE VISUAL CORTEX"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145251724"
                        ],
                        "name": "G. Sperling",
                        "slug": "G.-Sperling",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Sperling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sperling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 177
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods(Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995;Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 96
                            }
                        ],
                        "text": "Some of these were trained on natural images,but with the exception of Press and Lee (1996) and Lu et al. (1996), they did notshow a full family of receptive elds for forming a complete image code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8770215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "408294dbf1389e0f3e08c587c81730d1034af59c",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MBS-96-15-Independence-Rejection:-An-Unsupervised-Sperling",
            "title": {
                "fragments": [],
                "text": "MBS 96-15 Independence Rejection: An Unsupervised Learning Algorithm for Extracting Latent Source Structures from Arbitrary Image Populations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082087031"
                        ],
                        "name": "David Mumford",
                        "slug": "David-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Mumford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 102
                            }
                        ],
                        "text": "The form of this computation is very similar toan analysis/synthesis loop, which has been proposed by Mumford (1994) as a waythat cortical feedback could be used to perform inference on images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "This process is analogous to the analysis-synthesisloop proposed by Mumford (1994) for performing inference on images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17904453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd4b33ba26cd28dc8c3869f5ddbd0a87dbe625e0",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neuronal-Architectures-for-Pattern-theoretic-Mumford",
            "title": {
                "fragments": [],
                "text": "Neuronal Architectures for Pattern-theoretic Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6059616"
                        ],
                        "name": "G. F. Harpur",
                        "slug": "G.-F.-Harpur",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Harpur",
                            "middleNames": [
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. F. Harpur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Harpur (1997) has devised a modi cation to thealgorithm that speeds up the learning without requiring whitening, and the resultslook very similar to those shown here."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12399276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be08da912362bf40fe3ded78bdadc644f921b4e7",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Low-Entropy-Coding-with-Unsupervised-Neural-Harpur",
            "title": {
                "fragments": [],
                "text": "Low Entropy Coding with Unsupervised Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Peter Dayan, and Federico Girosi for very useful conversations during the development of this work"
            },
            "venue": {
                "fragments": [],
                "text": "Both authors were supported by grants from NIMH: F32-MHl1062 (BAO) and R29- MH50588 (DJF). Part of this work was carried out at the Center for Biological and Computational Learning at MIT"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information theory and statistics Law CC, Cooper LN (1994) \\Formation of receptive elds in realistic visual environments according to the Bienenstock, Cooper, and Munro (BCM) theory"
            },
            "venue": {
                "fragments": [],
                "text": "Proc Natl Acad Sci"
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 102
                            }
                        ],
                        "text": "The form of this computation is very similar toan analysis/synthesis loop, which has been proposed by Mumford (1994) as a waythat cortical feedback could be used to perform inference on images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "This process is analogous to the analysis-synthesisloop proposed by Mumford (1994) for performing inference on images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronal architectures for patterntheoretic problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scale-invariance and self-similar \"wavelet\" transforms: an analysis of natural scenes and mammalian visual systems"
            },
            "venue": {
                "fragments": [],
                "text": "Wavelets, fractals, and Fourier transforms"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning linear, sparse, factorial codes. AI Memo 1580"
            },
            "venue": {
                "fragments": [],
                "text": "Learning linear, sparse, factorial codes. AI Memo 1580"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "The formal relationship described inAppendix A (see also Olshausen, 1996), shows that both algorithms are solving thesame maximum-likelihood problem, described in Section 3, but by making di erentsimplifying assumptions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Field DJ (1996b) \\Natural image statistics and e cient coding,"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 81
                            }
                        ],
                        "text": "Other methods for learning sparse codes have been described by Foldiak (1990)and Zemel (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised leaming"
            },
            "venue": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised leaming"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatial frequency selectivity of cells"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 81
                            }
                        ],
                        "text": "Other methods for learning sparse codes have been described by Foldiak (1990)and Zemel (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 284
                            }
                        ],
                        "text": "\u20261b.Note that these reasons for desiring sparseness are separate from those thathave been written about elsewhere, such as increasing capacity in associative mem-ory (Baum et al., 1988), minimizing wiring length and ease of forming associations(Foldiak, 1995), or metabolic e ciency (Baddeley, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An efficient code in VI ?"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 166
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods(Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995;Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 81
                            }
                        ],
                        "text": "Some of these were trained on natural images,but with the exception of Press and Lee (1996) and Lu et al. (1996), they did notshow a full family of receptive elds for forming a complete image code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projection pursuit analysis of the statistical structure in natural scenes"
            },
            "venue": {
                "fragments": [],
                "text": "Paper presented at CNS"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 102
                            }
                        ],
                        "text": "The form of this computation is very similar toan analysis/synthesis loop, which has been proposed by Mumford (1994) as a waythat cortical feedback could be used to perform inference on images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "This process is analogous to the analysis-synthesisloop proposed by Mumford (1994) for performing inference on images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronal architectures for pattern-theoretic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 52
                            }
                        ],
                        "text": "A similar e ect has been observed in cortical cells (Tadmor and Tolhurst, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 191
                            }
                        ],
                        "text": "This is what one usually thinks of as thestandard model of a cortical simple cell, although the physiological data show thatthere are interesting forms of non-linearity in these cells (e.g., Tadmor and Tolhurst,1989) that are not captured by such a straightforward, linear model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 52
                            }
                        ],
                        "text": "A similar e ect has been observed in corticalcells (Tadmor and Tolhurst, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The e ect of threshold on the relationship between"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Comon P"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 270
                            }
                        ],
                        "text": "\u2026the number ofhigh-frequency cells may have been substantially underestimated since these unitswill generally have smaller receptive elds and so will be much more di cult to iso-late than a low-frequency unit that exhibits a more prolonged response to bars andthe like (Olshausen and Anderson, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A model of the spatialfrequency organization in primate striate cortex The neurobioIogy of computation"
            },
            "venue": {
                "fragments": [],
                "text": "A model of the spatialfrequency organization in primate striate cortex The neurobioIogy of computation"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "Among these are the algorithms of Comon (1994), Amari (1995), and Belland Sejnowski (1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Scale-invariance and self-similarsimilar`wavelet' transforms: an analysis of natural scenes and mammalian visual systems"
            },
            "venue": {
                "fragments": [],
                "text": "Wavelets, Fractals, and Fourier Transforms"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods(Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995;Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "Some of these were trained on natural images,but with the exception of Press and Lee (1996) and Lu et al. (1996), they did notshow a full family of receptive elds for forming a complete image code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projection pursuit analysis of the statistical structure in natural scenes. Paper presented at CNS96"
            },
            "venue": {
                "fragments": [],
                "text": "Projection pursuit analysis of the statistical structure in natural scenes. Paper presented at CNS96"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 79
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods (Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995; Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 110
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods(Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995;Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Formation of receptive elds in realistic visual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 79
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods (Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995; Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 177
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods(Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995;Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 96
                            }
                        ],
                        "text": "Some of these were trained on natural images,but with the exception of Press and Lee (1996) and Lu et al. (1996), they did notshow a full family of receptive elds for forming a complete image code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independence rejection: An unsupervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\An eecient code in V1"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 284
                            }
                        ],
                        "text": "\u20261b.Note that these reasons for desiring sparseness are separate from those thathave been written about elsewhere, such as increasing capacity in associative mem-ory (Baum et al., 1988), minimizing wiring length and ease of forming associations(Foldiak, 1995), or metabolic e ciency (Baddeley, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An efficient code in VI? Nature"
            },
            "venue": {
                "fragments": [],
                "text": "An efficient code in VI? Nature"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 79
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods (Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995; Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "Another class of e cient coding methods is based on projection pursuit methods(Friedman 1987; Intrator, 1992; Law and Cooper, 1994; Fyfe and Baddeley, 1995;Press and Lee, 1996; Lu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "Some of these were trained on natural images,but with the exception of Press and Lee (1996) and Lu et al. (1996), they did notshow a full family of receptive elds for forming a complete image code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projection pursuit analysis of the statistical structure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 162
                            }
                        ],
                        "text": "Note that these reasons for desiring sparseness are separate from those that have been written about elsewhere, such as increasing capacity in associative memory (Baum et al., 1988), minimizing wiring length and ease of forming associations (Foldiak, 1995), or metabolic e ciency (Baddeley, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 167
                            }
                        ],
                        "text": "\u20261b.Note that these reasons for desiring sparseness are separate from those thathave been written about elsewhere, such as increasing capacity in associative mem-ory (Baum et al., 1988), minimizing wiring length and ease of forming associations(Foldiak, 1995), or metabolic e ciency (Baddeley, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Internal representations for associative"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "The formal relationship described inAppendix A (see also Olshausen, 1996), shows that both algorithms are solving thesame maximum-likelihood problem, described in Section 3, but by making di erentsimplifying assumptions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Field DJ (1996a) \\Emergence of simple-cell receptive eld properties"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two-dimensional spatial structure of receptive elds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 252
                            }
                        ],
                        "text": "However, it should be notedthat the code being utilized here is a sparse, distributed code, which actually occupies amiddle ground between dense population codes at one end and local representations(i.e., grandmother cells) at the other (Foldiak 1995; Hinton & Ghahramani 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generative models for discovering sparse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 80
                            }
                        ],
                        "text": "Atick and colleagues (Atick & Redlich 1990, 1992; Atick 1992; Dong & Atick 1995;Dan et al., 1996) have achieved considerable success in showing how the principle ofredundancy reduction may be applied toward understanding the response propertiesof retinal ganglion cells in terms of a strategy for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 21
                            }
                        ],
                        "text": "Atick and colleagues (Atick & Redlich 1990, 1992; Atick 1992; Dong & Atick 1995; Dan et al., 1996) have achieved considerable success in showing how the principle of redundancy reduction may be applied toward understanding the response properties of retinal ganglion cells in terms of a strategy for \\whitening,\" or decorrelating, a set of outputs in response to the 1=f amplitude spectrum of natural images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient coding of natural scenes in the lateral"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 32,
            "methodology": 23
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 72,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field/2805537bec87a6177037b18f9a3a9d3f1038867b?sort=total-citations"
}