{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754328"
                        ],
                        "name": "D. Hush",
                        "slug": "D.-Hush",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Hush",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30731558"
                        ],
                        "name": "P. Kelly",
                        "slug": "P.-Kelly",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Kelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790221"
                        ],
                        "name": "C. Scovel",
                        "slug": "C.-Scovel",
                        "structuredName": {
                            "firstName": "Clint",
                            "lastName": "Scovel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Scovel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782193"
                        ],
                        "name": "Ingo Steinwart",
                        "slug": "Ingo-Steinwart",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Steinwart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ingo Steinwart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "... and entertain general asymptotic convergence properties [8], the time complexity of most of the algorithms in this family is typically super linear in the training set size m. Moreover, since decomposition methods find a feasible dual solution and their goal is to maximize the dual objective function, they often result in a rather slow convergence rate to the optimum of the primal objective function. (See also the discussion in [ 19 ].)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18644544,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "75a4608c9317cbdb7c4c5e0685fc1818533798cd",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe polynomial--time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classifiers. These algorithms employ a two--stage process where the first stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an approximate dual solution with accuracy (2(2Km)1/2+8(\u03bb)1/2)-2 \u03bb ep2 to an approximate primal solution with accuracy ep where n is the number of data samples, Kn is the maximum kernel value over the data and \u03bb > 0 is the SVM regularization parameter. For the first stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for \u03c4-rate certifying decomposition algorithms we establish the optimality of \u03c4 = 1/(n-1). In addition we extend the recent \u03c4 = 1/(n-1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the \u03c4 = 1/(n-1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the \u03c4-rate certifying property of these algorithms to produce new stopping rules that are computationally efficient and that guarantee a specified accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classification problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2) bound on the overall run time for the first stage. Combining the first and second stages gives an overall run time of O(n2(ck + 1)) where ck is an upper bound on the computation to perform a kernel evaluation. Pseudocode is presented for a complete algorithm that inputs an accuracy ep and produces an approximate solution that satisfies this accuracy in low order polynomial time. Experiments are included to illustrate the new stopping rules and to compare the Simon and composite decomposition algorithms."
            },
            "slug": "QP-Algorithms-with-Guaranteed-Accuracy-and-Run-Time-Hush-Kelly",
            "title": {
                "fragments": [],
                "text": "QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations are described and general conditions forwhich a matching lower bound exists for any decomposition algorithm that uses working sets of size 2 are described."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042259"
                        ],
                        "name": "Chuong B. Do",
                        "slug": "Chuong-B.-Do",
                        "structuredName": {
                            "firstName": "Chuong",
                            "lastName": "Do",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuong B. Do"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121484"
                        ],
                        "name": "Chuan-Sheng Foo",
                        "slug": "Chuan-Sheng-Foo",
                        "structuredName": {
                            "firstName": "Chuan-Sheng",
                            "lastName": "Foo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuan-Sheng Foo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This phenomenon has been observed before and there have been rather successful attempts to improve Pegasos when \u03bb is small (see for example [ 13 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12115491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aaee35181928fa35c68ca0b7598829f8afb1bfe",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Many learning algorithms rely on the curvature (in particular, strong convexity) of regularized objective functions to provide good theoretical performance guarantees. In practice, the choice of regularization penalty that gives the best testing set performance may result in objective functions with little or even no curvature. In these cases, algorithms designed specifically for regularized objectives often either fail completely or require some modification that involves a substantial compromise in performance.\n We present new online and batch algorithms for training a variety of supervised learning models (such as SVMs, logistic regression, structured prediction models, and CRFs) under conditions where the optimal choice of regularization parameter results in functions with low curvature. We employ a technique called proximal regularization, in which we solve the original learning problem via a sequence of modified optimization tasks whose objectives are chosen to have greater curvature than the original problem. Theoretically, our algorithms achieve low regret bounds in the online setting and fast convergence in the batch setting. Experimentally, our algorithms improve upon state-of-the-art techniques, including Pegasos and bundle methods, on medium and large-scale SVM and structured learning tasks."
            },
            "slug": "Proximal-regularization-for-online-and-batch-Do-Le",
            "title": {
                "fragments": [],
                "text": "Proximal regularization for online and batch learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "P proximal regularization is employed, in which the original learning problem is solved via a sequence of modified optimization tasks whose objectives are chosen to have greater curvature than the original problem."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The parameters for the Reuters dataset are taken from [2], while those for the Adult dataset are from [ 29 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, we compare its runtime on three large datasets to the runtimes of the state-of-the-art solver SVM-Perf [21], a cutting plane algorithm designed specifically for use with sparse feature vectors, as well as of two more conventional SVM solvers: LASVM [2] and SVM-Light [ 20 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Decomposition methods To overcome the quadratic memory requirement of IP methods, decomposition methods such as SMO [29] and SVM-Light [ 20 ] tackle the dual representation of the SVM optimization problem, and employ an active set of constraints thus working on a subset of dual variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Decomposition methods To overcome the quadratic memory requirement of IP methods, decomposition methods such as SMO [ 29 ] and SVM-Light [20] tackle the dual representation of the SVM optimization problem, and employ an active set of constraints thus working on a subset of dual variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": true,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Following [16, 24 ,10], the approach we take here is to directly minimize the primal problem while still using kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Two concrete algorithms that are closely related to the Pegasos algorithm and are also variants of stochastic sub-gradient methods are the NORMA algorithm [ 24 ] and a stochastic gradient algorithm due to Zhang [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, we compare Pegasos to two previously proposed methods that are based on stochastic gradient descent: Norma [ 24 ] by Kivinen et al. and to the method by Zhang [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In our last set of experiments, we compared Pegasos to Norma [ 24 ] and to a variant of stochastic gradient descent due to Zhang [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "7. The convergence rate given in [ 24 ] implies that the number of iterations required to achieve \ufffd -accurate solution is O(1/(\u03bb \ufffd) 2 ). This bound is inferior to the corresponding bound of Pegasos."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, most methods we compare to, including [18,21, 24 ,37], do not incorporate a bias term either."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Both methods share similarities with Pegasos when k = 1, and differ in their schemes for setting the learning rate \u03b7t . Theorem 4 from [ 24 ], suggests to set \u03b7t = p/(\u03bb \u221a t), where p \u2208 (0, 1). Based on the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Plugging the optimal value of p into Theorem 4 in [ 24 ] yields the bound O(1/(\u03bb \u221a T )). We therefore conjectured that Pegasos"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2959806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4",
            "isKey": true,
            "numCitedBy": 1044,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection."
            },
            "slug": "Online-learning-with-kernels-Kivinen-Smola",
            "title": {
                "fragments": [],
                "text": "Online learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper considers online learning in a reproducing kernel Hilbert space, and allows the exploitation of the kernel trick in an online setting, and examines the value of large margins for classification in the online setting with a drifting target."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143676697"
                        ],
                        "name": "Y. Nesterov",
                        "slug": "Y.-Nesterov",
                        "structuredName": {
                            "firstName": "Yurii",
                            "lastName": "Nesterov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nesterov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another related recent work is Nesterov\u2019s general primal-dual subgradient method for the minimization of non-smooth functions [ 28 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Intuitively, the ideas presented in [ 28 ] can be combined with the stochastic regime of Pegasos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14935076,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "73f583aad5195324ee75eb981b8b5f1fed6f9d38",
            "isKey": false,
            "numCitedBy": 796,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds."
            },
            "slug": "Primal-dual-subgradient-methods-for-convex-problems-Nesterov",
            "title": {
                "fragments": [],
                "text": "Primal-dual subgradient methods for convex problems"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure that is primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445783"
                        ],
                        "name": "A. Conconi",
                        "slug": "A.-Conconi",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Conconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895207"
                        ],
                        "name": "C. Gentile",
                        "slug": "C.-Gentile",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gentile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[16]). Such algorithms can be used to obtain a predictor with low generalization error using an online-to-batch conversion scheme [ 9 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 437093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78396e535101308d4431c08f0e85b18c920ee44f",
            "isKey": false,
            "numCitedBy": 522,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, it is shown how to extract a hypothesis with small risk from the ensemble of hypotheses generated by an arbitrary on-line learning algorithm run on an independent and identically distributed (i.i.d.) sample of data. Using a simple large deviation argument, we prove tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic M/sub n/ associated with the on-line performance of the ensemble. Via sharp pointwise bounds on M/sub n/, we then obtain risk tail bounds for kernel perceptron algorithms in terms of the spectrum of the empirical kernel matrix. These bounds reveal that the linear hypotheses found via our approach achieve optimal tradeoffs between hinge loss and margin size over the class of all linear functions, an issue that was left open by previous results. A distinctive feature of our approach is that the key tools for our analysis come from the model of prediction of individual sequences; i.e., a model making no probabilistic assumptions on the source generating the data. In fact, these tools turn out to be so powerful that we only need very elementary statistical facts to obtain our final risk bounds."
            },
            "slug": "On-the-generalization-ability-of-on-line-learning-Cesa-Bianchi-Conconi",
            "title": {
                "fragments": [],
                "text": "On the generalization ability of on-line learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proves tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic M/sub n/ associated with the on-line performance of the ensemble, and obtains risk tail bounds for kernel perceptron algorithms interms of the spectrum of the empirical kernel matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684722"
                        ],
                        "name": "S. Fine",
                        "slug": "S.-Fine",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Fine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005127"
                        ],
                        "name": "K. Scheinberg",
                        "slug": "K.-Scheinberg",
                        "structuredName": {
                            "firstName": "Katya",
                            "lastName": "Scheinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 15 ]). However, the dependence on m remains super linear."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13899309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db7af02be7cbadc029f9104a8c784d02de42df7",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SVM training is a convex optimization problem which scales with the training set size rather than the feature space dimension. While this is usually considered to be a desired quality, in large scale problems it may cause training to be impractical. The common techniques to handle this difficulty basically build a solution by solving a sequence of small scale subproblems. Our current effort is concentrated on the rank of the kernel matrix as a source for further enhancement of the training procedure. We first show that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity. We then suggest an efficient use of a known factorization technique to approximate a given kernel matrix by a low rank matrix, which in turn will be used to feed the optimizer. Finally, we derive an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors). This bound is general in the sense that it holds regardless of the approximation method."
            },
            "slug": "Efficient-SVM-Training-Using-Low-Rank-Kernel-Fine-Scheinberg",
            "title": {
                "fragments": [],
                "text": "Efficient SVM Training Using Low-Rank Kernel Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity and derives an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors)."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144833733"
                        ],
                        "name": "S. Sundararajan",
                        "slug": "S.-Sundararajan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sundararajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sundararajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, most methods we compare to, including [ 18 ,21,24,37], do not incorporate a bias term either."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Ascent (DCA). Following Pegasos\u2019s initial presentation [31], stochastic DCA was suggested as an alternative optimization method for SVMs [ 18 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7880266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0389a414c5d0ef50e06fe0c15f6102f374ce1b04",
            "isKey": false,
            "numCitedBy": 930,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1-and L2-loss functions. The proposed method is simple and reaches an \u03b5-accurate solution in O(log(1/\u03b5)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, TRON, SVMperf, and a recent primal coordinate descent implementation."
            },
            "slug": "A-dual-coordinate-descent-method-for-large-scale-Hsieh-Chang",
            "title": {
                "fragments": [],
                "text": "A dual coordinate descent method for large-scale linear SVM"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel dual coordinate descent method for linear SVM with L1-and L2-loss functions that reaches an \u03b5-accurate solution in O(log(1/\u03b5)) iterations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37564609"
                        ],
                        "name": "Karthik Sridharan",
                        "slug": "Karthik-Sridharan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Sridharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Sridharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 To do so, we can simply calculate the objective on the entire data set or estimate it according to a sample of size O(1/(\u03bb \ufffd)) ,w here\ufffd is the desired accuracy (see [ 35 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7065301,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a4cf35b6772b57aa838f7d6d0daad83d3e36cbb6",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We study convergence properties of empirical minimization of a stochastic strongly convex objective, where the stochastic component is linear. We show that the value attained by the empirical minimizer converges to the optimal value with rate 1/n. The result applies, in particular, to the SVM objective. Thus, we obtain a rate of 1/n on the convergence of the SVM objective (with fixed regularization parameter) to its infinite data limit. We demonstrate how this is essential for obtaining certain type of oracle inequalities for SVMs. The results extend also to approximate minimization as well as to strong convexity with respect to an arbitrary norm, and so also to objectives regularized using other lp norms."
            },
            "slug": "Fast-Rates-for-Regularized-Objectives-Sridharan-Shalev-Shwartz",
            "title": {
                "fragments": [],
                "text": "Fast Rates for Regularized Objectives"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the value attained by the empirical minimizer converges to the optimal value with rate 1/n, which is essential for obtaining certain type of oracle inequalities for SVMs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713876"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Cutting planes approach Recently, Joachims [21] proposed SVM-Perf, which uses a cutting planes method to find a solution with accuracy \ufffd in time O(md/(\u03bb\ufffd 2 )). This bound was later improved by Smola et al. [ 33  ]t oO(md/(\u03bb\ufffd)) . The complexity guarantee for Pegasos avoids the dependence on the data set size m. In addition, while SVM-Perf yields very significant improvements over decomposition methods for large data sets, our experiments (see ..."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5789659,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a67f1dcf735a1c52708a9a5392ff585658dbc8ce",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1/\u220a) steps to \u220a precision for general convex problems and in O(log(1/\u220a)) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach."
            },
            "slug": "Bundle-Methods-for-Machine-Learning-Smola-Vishwanathan",
            "title": {
                "fragments": [],
                "text": "Bundle Methods for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work presents a globally convergent method that applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem and presents tight convergence bounds, which show that the algorithm converges in O(1/\u220a) steps to \u220a precision for general convex problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 0,
        "totalPages": 0
    },
    "page_url": "https://www.semanticscholar.org/paper/Pegasos:-primal-estimated-sub-gradient-solver-for-Shalev-Shwartz-Singer/9691f67f5075bde2fd70da0135a4a70f25ef042b?sort=total-citations"
}