{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700821"
                        ],
                        "name": "F. Esposito",
                        "slug": "F.-Esposito",
                        "structuredName": {
                            "firstName": "Floriana",
                            "lastName": "Esposito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Esposito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738657"
                        ],
                        "name": "D. Malerba",
                        "slug": "D.-Malerba",
                        "structuredName": {
                            "firstName": "Donato",
                            "lastName": "Malerba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malerba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467353"
                        ],
                        "name": "G. Semeraro",
                        "slug": "G.-Semeraro",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Semeraro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Semeraro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "In any case, the reader can find some results on the Iris and Glass data in [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10071197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82005d22a7ffeec471f46d4d4a9e8764ebb66367",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a study of one particular problem of decision tree induction, namely (post-)pruning, with the aim of finding a common framework for the plethora of pruning methods appeared in literature. Given a tree Tmax to prune, a state space is defined as the set of all subtrees of Tmax to which only one operator, called any-depth branch pruning operator, can be applied in several ways in order to move from one state to another. By introducing an evaluation function f defined on the set of subtrees, the problem of tree pruning can be cast as an optimization problem, and it is also possible to classify each post-pruning method according to both its search strategy and the kind of information exploited by f. Indeed, while some methods use only the training set in order to evaluate the accuracy of a decision tree, other methods exploit an additional pruning set that allows them to get less biased estimates of the predictive accuracy of apruned tree. The introduction of the state space shows that very simple search strategies are used by the postpruning methods considered. Finally, some empirical results allow theoretical observations on strengths and weaknesses of pruning methods to be better understood."
            },
            "slug": "Decision-Tree-Pruning-as-a-Search-in-the-State-Esposito-Malerba",
            "title": {
                "fragments": [],
                "text": "Decision Tree Pruning as a Search in the State Space"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The introduction of the state space shows that very simple search strategies are used by the postpruning methods considered, and some empirical results allow theoretical observations on strengths and weaknesses of pruning methods to be better understood."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738657"
                        ],
                        "name": "D. Malerba",
                        "slug": "D.-Malerba",
                        "structuredName": {
                            "firstName": "Donato",
                            "lastName": "Malerba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malerba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700821"
                        ],
                        "name": "F. Esposito",
                        "slug": "F.-Esposito",
                        "structuredName": {
                            "firstName": "Floriana",
                            "lastName": "Esposito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Esposito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467353"
                        ],
                        "name": "G. Semeraro",
                        "slug": "G.-Semeraro",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Semeraro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Semeraro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A brief survey can be found in [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16395269,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b142715b38ae772a1bdb483a85efc04a3aa7373a",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an empirical investigation of eight well-known simplification methods for decision trees induced from training data. Twelve data sets are considered to compare both the accuracy and the complexity of simplified trees. The computation of optimally pruned trees is used in order to give a clear definition of bias of the methods towards overpruning and underpruning. The results indicate that the simplification strategies which exploit an independent pruning set do not perform better than the others. Furthermore, some methods show an evident bias towards either underpruning or overpruning."
            },
            "slug": "A-Further-Comparison-of-Simplification-Methods-for-Malerba-Esposito",
            "title": {
                "fragments": [],
                "text": "A Further Comparison of Simplification Methods for Decision-Tree Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An empirical investigation of eight well-known simplification methods for decision trees induced from training data indicates that the simplification strategies which exploit an independent pruning set do not perform better than the others."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695784"
                        ],
                        "name": "U. Fayyad",
                        "slug": "U.-Fayyad",
                        "structuredName": {
                            "firstName": "Usama",
                            "lastName": "Fayyad",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Fayyad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849755"
                        ],
                        "name": "K. Irani",
                        "slug": "K.-Irani",
                        "structuredName": {
                            "firstName": "Keki",
                            "lastName": "Irani",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Irani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Indeed, when the selection measure belongs to the families of impurity measures [2] or C-SEP [10], stopping Rule 5 may fire, although some tests could be useful combined with others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Other proposals can be found in [10], [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26648065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e65e227474358c43565f0969f958f218a49c640",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of selecting an attribute and some of its values for branching during the top-down generation of decision trees. We study the class of impurity measures, members of which are typically used in the literature for selecting attributes during decision tree generation (e.g. entropy in ID3, GID3*, and CART; Gini Index in CART). We argue that this class of measures is not particularly suitable for use in classification learning. We define a new class of measures, called C-SEP, that we argue is better suited for the purposes of class separation. A new measure from C-SEP is formulated and some of its desirable properties are shown. Finally, we demonstrate empirically that the new algorithm, O-BTree, that uses this measure indeed produces better decision trees than algorithms that use impurity measures."
            },
            "slug": "The-Attribute-Selection-Problem-in-Decision-Tree-Fayyad-Irani",
            "title": {
                "fragments": [],
                "text": "The Attribute Selection Problem in Decision Tree Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated empirically that the new algorithm, O-BTree, that uses a new measure, called C-SEP, that is better suited for the purposes of class separation produces better decision trees than algorithms that use impurity measures."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476128"
                        ],
                        "name": "B. Cestnik",
                        "slug": "B.-Cestnik",
                        "structuredName": {
                            "firstName": "Bojan",
                            "lastName": "Cestnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Cestnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725040"
                        ],
                        "name": "I. Bratko",
                        "slug": "I.-Bratko",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Bratko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Bratko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 267
                            }
                        ],
                        "text": "When a new observation reaching t is classified, the expected error rate is given by:\nEER t min 1 p t\nmin n t n t 1 p m / n t m\ni i\ni i ai\na f a fm r a f a f c h a f{ } = - = - + - \u25ca +\nThis formula is a generalization of the expected error rate computed by Niblett and Bratko [23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "Indeed, both the original version and the improved one reported in [6] exploit only information in the training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "Bohanec and Bratko developed an algorithm, named OPT, that generates a sequence of optimally pruned trees, decreasing in size, in time O T 2 max L F HG I KJ ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "Niblett and Bratko [23] proposed a bottom-up approach seeking for a single tree that minimizes \u201cthe expected error rate on an independent data set.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 47
                            }
                        ],
                        "text": "In the original method proposed by Niblett and Bratko, the weights ps were estimated by the proportion of training examples reaching the sth child."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "Cestnik and Bratko name pi(t) as m-probability estimate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "Later, Cestnik and Bratko [6] suggested an m-probability estimate with m = 2 for ps, although they admitted having chosen m arbitrarily."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "Cestnik and Bratko suggest the intervention of a domain expert who can choose the right value for m according to the level of noise in the data or even study the selection of trees produced."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 161
                            }
                        ],
                        "text": "Finally, we observe that the most recent version of minimum error pruning seems to have overcome two problems that affected the original proposal by Niblett and Bratko: optimistic bias [33] and dependence of the expected error rate on the number of classes [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 87
                            }
                        ],
                        "text": "More precisely, given a decision tree that accurately specifies a concept, Bohanec and Bratko set the problem of finding a smallest pruned tree that still represents a concept within a specified accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 21
                            }
                        ],
                        "text": "The same Bohanec and Bratko observe that no significant gains in classification accuracy can be expected in general."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38549636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97d4fcd68dffceb203df8263c7a12e3ee001236d",
            "isKey": true,
            "numCitedBy": 180,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a new method for decision tree pruning, based on the minimisation of the expected classification error method by Niblett and Bratko. The original Niblett-Bratko pruning algorithm uses Laplace probability estimates. Here we introduce a new, more general Bayesian approach to estimating probabilities which we call m-probability-estimation. By varying a parameter m in this method, tree pruning can be adjusted to particular properties of the learning domain, such as level of noise. The resulting pruning method improves on the original Niblett-Bratko pruning in the following respects: apriori probabilities can be incorporated into error estimation, several trees pruned to various degrees can be generated, and the degree of pruning is not affected by the number of classes. These improvements are supported by experimental findings. m-probability-estimation also enables the combination of learning data obtained from various sources."
            },
            "slug": "On-Estimating-Probabilities-in-Tree-Pruning-Cestnik-Bratko",
            "title": {
                "fragments": [],
                "text": "On Estimating Probabilities in Tree Pruning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The resulting pruning method improves on the original Niblett-Bratko pruning in the following respects: apriori probabilities can be incorporated into error estimation, several trees pruned to various degrees can be generated, and the degree of pruning is not affected by the number of classes."
            },
            "venue": {
                "fragments": [],
                "text": "EWSL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143622829"
                        ],
                        "name": "C. Schaffer",
                        "slug": "C.-Schaffer",
                        "structuredName": {
                            "firstName": "Cullen",
                            "lastName": "Schaffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schaffer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "As Schaffer [31] has already shown, the benefits in terms of accuracy of pruning are inversely proportional to the degree of sparseness, hence the worse performance of some methods on the Heart data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Another related empirical comparison can be found in [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 160
                            }
                        ],
                        "text": "It is also worthwhile observing that the average size/error reported in the column \u201ctrained\u201d of Table 3 for the Led-200 data are concordant with that observed by Schaffer [30, Table 1] under the same conditions, although in that case the trees were built using the Gini index [2] instead of the gain-ratio."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "In the light of such considerations, it is not difficult to accept Schaffer\u2019s claim [32] that tree pruning is a form of bias (here intended as a set of factors that influence hypothesis selection) rather than a statistical improvement of the classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 171
                            }
                        ],
                        "text": "Actually, the positive result for the analogous problem Led200 is quite surprising, even though it agrees with figures reported in the book by Breiman et al. As proven by Schaffer [30], this result is actually an exception to the general behavior shown by cost complexity pruning in the digit recognition problem."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2511072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0ad45ff3098f5ebfe7e9f4ea14dac553600a251",
            "isKey": true,
            "numCitedBy": 31,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Overfitting avoidance in induction has often been treated as if it statistically increases expected predictive accuracy. In fact, there is no statistical basis for believing it will have this effect. Overfitting avoidance is simply a form of bias and, as such, its effect on expected accuracy depends, not on statistics, but on the degree to which this bias is appropriate to a problem-generating domain. This paper identifies one important factor that affects the degree to which the bias of overfitting avoidance is appropriate--the abundance of training data relative to the complexity of the relationship to be induced--and shows empirically how it determines whether such methods as pessimistic and cross-validated cost-complexity pruning will increase or decrease predictive accuracy in decision tree induction. The effect of sparse data is illustrated first in an artificial domain and then in more realistic examples drawn from the UCI machine learning database repository."
            },
            "slug": "Sparse-Data-and-the-Effect-of-Overfitting-Avoidance-Schaffer",
            "title": {
                "fragments": [],
                "text": "Sparse Data and the Effect of Overfitting Avoidance in Decision Tree Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper identifies one important factor that affects the degree to which the bias of overfitting avoidance is appropriate--the abundance of training data relative to the complexity of the relationship to be induced--and shows empirically how it determines whether such methods as pessimistic and cross-validated cost-complexity pruning will increase or decrease predictive accuracy in decision tree induction."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143622829"
                        ],
                        "name": "C. Schaffer",
                        "slug": "C.-Schaffer",
                        "structuredName": {
                            "firstName": "Cullen",
                            "lastName": "Schaffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schaffer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Actually, the positive result for the analogous problem Led- 200 is quite surprising, even though it agrees with figures reported in the book by Breiman et al. As proven by Schaffer [ 30 ], this result is actually an exception to the general behavior shown by cost complexity pruning in the digit recognition problem."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 18429888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f97f5a43a0f3d5696be24bcc8dc54ed8bf0987d7",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deconstructing-the-Digit-Recognition-Problem-Schaffer",
            "title": {
                "fragments": [],
                "text": "Deconstructing the Digit Recognition Problem"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34929449"
                        ],
                        "name": "George H. John",
                        "slug": "George-H.-John",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "John",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George H. John"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This observation, which should be regarded as true for this section, can actually be extended to any resampling method, including nonparametric bootstrap methods [8], \u201cexact\u201d permutation analysis [7] and repeated crossvalidation [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7594891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20c25424a20bc4f88accb35fbfd98d2ca7ffc525",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Parameter-Selection-by-Minimizing-Error-Kohavi-John",
            "title": {
                "fragments": [],
                "text": "Automatic Parameter Selection by Minimizing Estimated Error"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744109"
                        ],
                        "name": "S. Salzberg",
                        "slug": "S.-Salzberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Salzberg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Salzberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800681"
                        ],
                        "name": "Alberto Maria Segre",
                        "slug": "Alberto-Maria-Segre",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Segre",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Maria Segre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60499165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7feb0fc888cd55360949554db032d7d1cba9e947",
            "isKey": false,
            "numCitedBy": 7025,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students."
            },
            "slug": "Programs-for-Machine-Learning-Salzberg-Segre",
            "title": {
                "fragments": [],
                "text": "Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments, which will be a welcome addition to the library of many researchers and students."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 162
                            }
                        ],
                        "text": "All databases are available in the UCI Machine Learning Repository(2) [21], and some of them have even been used to compare different pruning methods [25], [20], [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Another work that considers several selection measures is Buntine\u2019s thesis [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60943338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f44fbc2fb3df4425654ae429c6cd1e175c3a522d",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "The main contributions of this thesis are a Bayesian theory of learning classi cation rules, the uni cation and comparison of this theory with some previous theories of learning, and two extensive applications of the theory to the problems of learning class probability trees and bounding error when learning logical rules. The thesis is motivated by considering some current research issues in machine learning such as bias, over tting and search, and considering the requirements placed on a learning system when it is used for knowledge acquisition. Basic Bayesian decision theory relevant to the problem of learning classi cation rules is reviewed, then a Bayesian framework for such learning is presented. The framework has three components: the hypothesis space, the learning protocol, and criteria for successful learning. Several learning protocols are analysed in detail: queries, logical, noisy, uncertain and positive-only examples. The analysis is done by interpreting a protocol as a likelihood function and by performing conditional likelihood analysis. The aim of this framework and theoretical treatment is to consider how three important questions should be addressed: what actions a learner should take, whether the learner's initial subjective knowledge is appropriate, and how con dent the learner can be in results obtained. The resultant learning framework is compared and uni ed with the corresponding learning frameworks of Gold, Valiant, minimum encoding approaches such as MDL and MML, and classical methods from statistics and pattern recognition. Finally, two extensive case studies are reported. The rst case study works through conjunctive and complete hypothesis spaces to illustrate problems such as the inherent worst-case and sampleindependent analysis of the Valiantmodel and how this can be overcome in the Bayesian framework. Experiments reported indicate considerable improvements can be achieved. The second case study presents a Bayesian approach to learning decision trees based on the theory presented for the uncertain examples protocol. Experiments reported indicate the approach compares favourably with existing decision tree methods. The Bayesian method presented should readily transfer to the learning of disjunctive rules or other more exible representations, and to more exible search strategies. The Bayesian method should also provide much stronger feedback for an interactive style of learning."
            },
            "slug": "A-theory-of-learning-classification-rules-Buntine",
            "title": {
                "fragments": [],
                "text": "A theory of learning classification rules"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A Bayesian theory of learning classi cation rules, the comparison and comparison of this theory with some previous theories of learning, and two extensive applications of the theory to the problems of learningclass probability trees and bounding error when learning logical rules are reported."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "The first of them was made by Quinlan [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "This method, proposed by Quinlan [25], is conceptually the simplest and uses the pruning set to evaluate the efficacy of a subtree of Tmax."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "All databases are available in the UCI Machine Learning Repository(2) [21], and some of them have even been used to compare different pruning methods [25], [20], [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "This pruning method, proposed by Quinlan [25], like the previous one, is characterized by the fact that the same training set is used for both growing and pruning a tree."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2522623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "952c12ff973c2a800809bf357c73941c7c0aa387",
            "isKey": true,
            "numCitedBy": 825,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Many systems have been developed for constructing decision trees from collections of examples. Although the decision trees generated by these methods are accurate and efficient, they often suffer the disadvantage of excessive complexity and are therefore incomprehensible to experts. It is questionable whether opaque structures of this kind can be described as knowledge, no matter how well they function. This paper discusses techniques for simplifying decision trees while retaining their accuracy. Four methods are described, illustrated, and compared on a test-bed of decision trees from a variety of domains."
            },
            "slug": "Simplifying-decision-trees-Quinlan",
            "title": {
                "fragments": [],
                "text": "Simplifying decision trees"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Hum. Comput. Stud."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21898,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796214"
                        ],
                        "name": "R. Holte",
                        "slug": "R.-Holte",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Holte",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Holte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144621590"
                        ],
                        "name": "L. Acker",
                        "slug": "L.-Acker",
                        "structuredName": {
                            "firstName": "Liane",
                            "lastName": "Acker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Acker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47363174"
                        ],
                        "name": "B. Porter",
                        "slug": "B.-Porter",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Porter",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Porter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "Rule 3 is based on the idea that small disjuncts can be eliminated since they are error-prone, but an immediate objection is that in this way we cannot deal with exceptions [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1800604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e41d77b221a4a7c3eb1ffd1ceaf6dd1259afca71",
            "isKey": false,
            "numCitedBy": 436,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Ideally, definitions induced from examples should consist of all, and only, disjuncts that are meaningful (e.g., as measured by a statistical significance test) and have a low error rate. Existing inductive systems create definitions that are ideal with regard to large disjuncts, but far from ideal with regard to small disjuncts, where a small (large) disjunct is one that correctly classifies few (many) training examples. The problem with small disjuncts is that many of them have high rates of misclassification, and it is difficult to eliminate the errorprone small disjuncts from a definition without adversely affecting other disjuncts in the definition. Various approaches to this problem are evaluated, including the novel approach of using a bias different than the \"maximum generality\" bias. This approach, and some others, prove partly successful, but the problem of small disjuncts remains open."
            },
            "slug": "Concept-Learning-and-the-Problem-of-Small-Disjuncts-Holte-Acker",
            "title": {
                "fragments": [],
                "text": "Concept Learning and the Problem of Small Disjuncts"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Various approaches to this problem are evaluated, including the novel approach of using a bias different than the \"maximum generality\" bias, which prove partly successful, but the problem of small disjuncts remains open."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700484"
                        ],
                        "name": "S. Gelfand",
                        "slug": "S.-Gelfand",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Gelfand",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelfand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36729903"
                        ],
                        "name": "C. Ravishankar",
                        "slug": "C.-Ravishankar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Ravishankar",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ravishankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741483"
                        ],
                        "name": "E. Delp",
                        "slug": "E.-Delp",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Delp",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Delp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "19, NO. 5, MAY 1997 Ravishankar, and Delp [ 11 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Consequently, if the most accurate subtree with respect to the pruning set is not in {T0, T1, T2, \u2026, TL}, it cannot be selected [ 11 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43158592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da15bf953fd7bab529046c5ba3826e48288f1272",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient iterative method is proposed to grow and prune classification trees. This method divides the data sample into two subsets and iteratively grows a tree with one subset and prunes it with the other subset, successively interchanging the roles of the two subsets. The convergence and other properties of the algorithm are established. Theoretical and practical considerations suggest that the iterative tree growing and pruning algorithm should perform better and require less computation than other widely used tree growing and pruning algorithms. Numerical results on a waveform recognition problem are presented to support this view.<<ETX>>"
            },
            "slug": "An-iterative-growing-and-pruning-algorithm-for-tree-Gelfand-Ravishankar",
            "title": {
                "fragments": [],
                "text": "An iterative growing and pruning algorithm for classification tree design"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Numerical results on a waveform recognition problem are presented to support the theory and practical considerations suggest that the iterative tree growing and pruning algorithm should perform better and require less computation than other widely used tree grow and prune algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Proceedings., IEEE International Conference on Systems, Man and Cybernetics"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721048"
                        ],
                        "name": "S. R. Safavian",
                        "slug": "S.-R.-Safavian",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Safavian",
                            "middleNames": [
                                "Rasoul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. R. Safavian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773449"
                        ],
                        "name": "D. Landgrebe",
                        "slug": "D.-Landgrebe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Landgrebe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Landgrebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Various heuristic methods have been proposed for designing a decision tree [29], the best known being the topdown method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "frame their pruning method into a growing-pruning approach [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6966739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df50278b090b306f210eea77c2496d3fc790e7ea",
            "isKey": false,
            "numCitedBy": 2700,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "A survey is presented of current methods for decision tree classifier (DTC) designs and the various existing issues. After considering potential advantages of DTCs over single-state classifiers, the subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed. The relation between decision trees and neutral networks (NN) is also discussed. >"
            },
            "slug": "A-survey-of-decision-tree-classifier-methodology-Safavian-Landgrebe",
            "title": {
                "fragments": [],
                "text": "A survey of decision tree classifier methodology"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed, and the relation between decision trees and neutral networks (NN) is also discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922810"
                        ],
                        "name": "P. Devijver",
                        "slug": "P.-Devijver",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Devijver",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Devijver"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "The evaluation of the error rate is always made on the independent test set, using the empirical error count [14], which is an unbiased estimator."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Kittler and Devijver [14] have shown that the standard deviation of the empirical error count estimator eC, used with independent sets, is given by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Kittler and Devijver [14] have shown that the standard deviation of the empirical error count estimator eC, used with independent sets, is given by\ns (e ) = (e(1 \u2013 e) / N)c 1/2\nwhere:\n\u2022 e is the true expected error rate of the classifier (in this case Tmax), \u2022 N is the size of the independent set used for computing the error rate estimate, eC."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12224709,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c39f136dc6c5d54d16ca2275919541facd4f3070",
            "isKey": true,
            "numCitedBy": 35,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of estimating the error probability of a given classification system is considered. Statistical properties of the empirical error count (C) and the average conditional error (R) estimators are studied. It is shown that in the large sample case the R estimator is unbiased and its variance is less than that of the C estimator. In contrast to conventional methods of Bayes error estimation the unbiasedness of the R estimator for a given classifier can be obtained only at the price of an additional set of classified samples. On small test sets the R estimator may be subject to a pessimistic bias caused by the averaging phenomenon characterizing the functioning of conditional error estimators."
            },
            "slug": "Statistical-Properties-of-Error-Estimators-in-of-Kittler-Devijver",
            "title": {
                "fragments": [],
                "text": "Statistical Properties of Error Estimators in Performance Assessment of Recognition Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "In contrast to conventional methods of Bayes error estimation the unbiasedness of the R estimator for a given classifier can be obtained only at the price of an additional set of classified samples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "In the light of such considerations, it is not difficult to accept Schaffer\u2019s claim [32] that tree pruning is a form of bias (here intended as a set of factors that influence hypothesis selection) rather than a statistical improvement of the classifier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8919068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c871e8935afccc52c83e8166117be9b1420c01cb",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In supervising learning it is commonly believed that penalizing complex functions help one avoid ``overfitting'' functions to data, and therefore improves generalization. It is also commonly believed that cross-validation is an effective way to choose amongst algorithms for fitting functions to data. In a recent paper, Schaffer (1993) presents experimental evidence disputing these claims. The current paper consists of a formal analysis of these contentions of Schaffer's. It proves that his contentions are valid, although some of his experiments must be interpreted with caution."
            },
            "slug": "On-Overfitting-Avoidance-as-Bias-Wolpert",
            "title": {
                "fragments": [],
                "text": "On Overfitting Avoidance as Bias"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A formal analysis of contentions of Schaffer (1993) proves that his contentions are valid, although some of his experiments must be interpreted with caution."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "One of them [27] is part of a decision tree induction process based on the minimum description length (MDL) principle [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122569569,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ae7beb7920485aca9c252ce3ecc3972c52eb3c37",
            "isKey": false,
            "numCitedBy": 1832,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "of the number of bits required to write down the observed data, has been reformulated to extend the classical maximum likelihood principle. The principle permits estimation of the number of the parameters in statistical models in addition to their values and even of the way the parameters appear in the models; i.e., of the model structures. The principle rests on a new way to interpret and construct a universal prior distribution for the integers, which makes sense even when the parameter is an individual object. Truncated realvalued parameters are converted to integers by dividing them by their precision, and their prior is determined from the universal prior for the integers by optimizing the precision. 1. Introduction. In this paper we study estimation based upon the principle of minimizing the total number of binary digits required to rewrite the observed data, when each observation is given with some precision. Instead of attempting at an absolutely shortest description, which would be futile, we look for the optimum relative to a class of parametrically given distributions. This Minimum Description Length (MDL) principle, which we introduced in a less comprehensive form in [25], turns out to degenerate to the more familiar Maximum Likelihood (ML) principle in case the number of parameters in the models is fixed, so that the description length of the parameters themselves can be ignored. In another extreme case, where the parameters determine the data, it similarly degenerates to Jaynes's principle of maximum entropy, [14]. But the main power of the new criterion is that it permits estimates of the entire model, its parameters, their number, and even the way the parameters appear in the model; i.e., the model structure. Hence, there will be no need to supplement the estimated parameters with a separate hypothesis test to decide whether a model is adequately parameterized or, perhaps, over parameterized."
            },
            "slug": "A-UNIVERSAL-PRIOR-FOR-INTEGERS-AND-ESTIMATION-BY-Rissanen",
            "title": {
                "fragments": [],
                "text": "A UNIVERSAL PRIOR FOR INTEGERS AND ESTIMATION BY MINIMUM DESCRIPTION LENGTH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "One of them [27] is part of a decision tree induction process based on the minimum description length (MDL) principle [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 613410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84dae6a2870c68005732b9db6890f375490f2d4e",
            "isKey": false,
            "numCitedBy": 804,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inferring-Decision-Trees-Using-the-Minimum-Length-Quinlan-Rivest",
            "title": {
                "fragments": [],
                "text": "Inferring Decision Trees Using the Minimum Description Length Principle"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed, when the selection measure belongs to the families of impurity measures [2] or C-SEP [10], stopping Rule 5 may fire, although some tests could be useful combined with others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It is also worthwhile observing that the average size/error reported in the column \u201ctrained\u201d of Table 3 for the Led-200 data are concordant with that observed by Schaffer [30, Table 1] under the same conditions, although in that case the trees were built using the Gini index [2] instead of the gain-ratio."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This method is also known as the CART pruning algorithm [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[2] and Quinlan [24]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29458883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8017699564136f93af21575810d557dba1ee6fc6",
            "isKey": true,
            "numCitedBy": 16315,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index."
            },
            "slug": "Classification-and-Regression-Trees-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Classification and Regression Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This chapter discusses tree classification in the context of medicine, where right Sized Trees and Honest Estimates are considered and Bayes Rules and Partitions are used as guides to optimal pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777085"
                        ],
                        "name": "J. Mingers",
                        "slug": "J.-Mingers",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Mingers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mingers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 59
                            }
                        ],
                        "text": "This claim is at variance with the conclusions reported by Mingers [20]; this discrepancy should be attributed to the different design of the experiments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "This post-pruning method, proposed by Mingers [18], is very similar to a pre-pruning technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 88
                            }
                        ],
                        "text": "As to the choice of the best tree in the sequence, one of the alternatives suggested by Mingers consists of estimating the error rate on an independent pruning set [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 7
                            }
                        ],
                        "text": "Unlike Mingers\u2019 previous empirical comparison of pruning methods [20], we will not rely on the Analysis of Variance (ANOVA) to detect statistically significant differences between pruning methods, since the ANOVA test is based on the assumption that the standard deviation is constant for all the experiments, whereas this is not so in our case since we compare the algorithms on different data sets, each of which has its own standard deviation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Mingers [19] reviewed some selection measures based on statistics and information theory."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "On the other hand, Mingers\u2019 study involves four selection measures, and investigates possible interactions with the pruning method, while our analysis is limited to only one of those measures, namely the gain-ratio [24]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 135
                            }
                        ],
                        "text": "Our experimental design, mostly based on holdout resampling, has been used in many other empirical studies, such as those performed by Mingers [20], Buntine and Niblett [4], and Holte [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 73
                            }
                        ],
                        "text": "It can be proved that the optimality property is no longer guaranteed in Mingers' version of REP [20], in particular when among all the internal\nPROOF."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 25
                            }
                        ],
                        "text": "The same problem affects Mingers\u2019 empirical comparison as well [20], and justifies the discrepancy with some of our findings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 23
                            }
                        ],
                        "text": "The method proposed by Mingers consists of two main steps:\n1) Prune Tmax for increasing critical values."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62752831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4c46de835f0b488017abd3809a615716307a8f1",
            "isKey": true,
            "numCitedBy": 186,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Rule induction has been proposed as a way of speeding up the acquisition of knowledge for expert systems. Quinlan's ID3 algorithm has been used successfully but can only deal with determinate data. This paper explores extensions to deal with statistical data."
            },
            "slug": "Expert-Systems\u2014Rule-Induction-with-Statistical-Data-Mingers",
            "title": {
                "fragments": [],
                "text": "Expert Systems\u2014Rule Induction with Statistical Data"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Rule induction has been proposed as a way of speeding up the acquisition of knowledge for expert systems but can only deal with determinate data and extensions to deal with statistical data are explored."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "All databases are available in the UCI Machine Learning Repository(2) [21], and some of them have even been used to compare different pruning methods [25], [20], [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 35
                            }
                        ],
                        "text": "All databases are available in the UCI Machine Learning Repository2 [21], and some of them have even been used to compare different pruning methods [25], [20], [3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13445,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064561358"
                        ],
                        "name": "Gail Gong",
                        "slug": "Gail-Gong",
                        "structuredName": {
                            "firstName": "Gail",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gail Gong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 162
                            }
                        ],
                        "text": "This observation, which should be regarded as true for this section, can actually be extended to any resampling method, including nonparametric bootstrap methods [8], \u201cexact\u201d permutation analysis [7] and repeated crossvalidation [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45944920,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d69c5d6a37d8cad463a9606282deb139f6ba35ca",
            "isKey": false,
            "numCitedBy": 3128,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An apparatus for the in-situ detection of ions in a beam of an ion implanter device includes a mass spectrometer device having inner and outer walls and, a system for generating and directing an ion implant beam through the mass spectrometer device. The mass spectrometer device generates a magnetic field for directing ions of the ion implant beam of a desirable type through an aperture for implanting into a semiconductor wafer, and causing ions of undesirable type to collide with the inner or outer wall. For in-situ detection, a detector device is disposed on the inner and outer walls of the mass spectrometer for detecting the undesirable type of ions deflected. In one embodiment, the detector device comprises electronic sensor devices for detecting a concentration of the undesirable type ions which comprise undesirable elements and compounds. In another embodiment, the detector device comprises Faraday cup devices for detecting a concentration of ions of the undesirable type, or, may comprise a moving Faraday device positioned along tracks disposed respectively along the inner and outer wall, the Faraday being driven for reciprocal movement along a respective track. Data is collected from the sensors corresponding to the positions of undesirable ion detection and is processed, in real-time, during wafer processing. In this manner potential contaminants in the ion implant beam may be determined and corrective action may be taken in response."
            },
            "slug": "A-Leisurely-Look-at-the-Bootstrap,-the-Jackknife,-Efron-Gong",
            "title": {
                "fragments": [],
                "text": "A Leisurely Look at the Bootstrap, the Jackknife, and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353436"
                        ],
                        "name": "T. Niblett",
                        "slug": "T.-Niblett",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Niblett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Niblett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725040"
                        ],
                        "name": "I. Bratko",
                        "slug": "I.-Bratko",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Bratko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Bratko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 255
                            }
                        ],
                        "text": "When a new observation reaching t is classified, the expected error rate is given by:\nEER t min 1 p t\nmin n t n t 1 p m / n t m\ni i\ni i ai\na f a fm r a f a f c h a f{ } = - = - + - \u25ca +\nThis formula is a generalization of the expected error rate computed by Niblett and Bratko [23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "Bohanec and Bratko developed an algorithm, named OPT, that generates a sequence of optimally pruned trees, decreasing in size, in time O T 2 max L F HG I KJ ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Niblett and Bratko [23] proposed a bottom-up approach seeking for a single tree that minimizes \u201cthe expected error rate on an independent data set.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 35
                            }
                        ],
                        "text": "In the original method proposed by Niblett and Bratko, the weights ps were estimated by the proportion of training examples reaching the sth child."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "Cestnik and Bratko name pi(t) as m-probability estimate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "Later, Cestnik and Bratko [6] suggested an m-probability estimate with m = 2 for ps, although they admitted having chosen m arbitrarily."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Niblett and Bratko [23] proposed a bottom-up approach seeking for a single tree that minimizes \u201cthe expected error rate on an independent data set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "Cestnik and Bratko suggest the intervention of a domain expert who can choose the right value for m according to the level of noise in the data or even study the selection of trees produced."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "This formula is a generalization of the expected error rate computed by Niblett and Bratko [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "Finally, we observe that the most recent version of minimum error pruning seems to have overcome two problems that affected the original proposal by Niblett and Bratko: optimistic bias [33] and dependence of the expected error rate on the number of classes [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 87
                            }
                        ],
                        "text": "More precisely, given a decision tree that accurately specifies a concept, Bohanec and Bratko set the problem of finding a smallest pruned tree that still represents a concept within a specified accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 21
                            }
                        ],
                        "text": "The same Bohanec and Bratko observe that no significant gains in classification accuracy can be expected in general."
                    },
                    "intents": []
                }
            ],
            "corpusId": 54126012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1418eea19ccb9170e4af15d3c960b98e332e2607",
            "isKey": true,
            "numCitedBy": 225,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-decision-rules-in-noisy-domains-Niblett-Bratko",
            "title": {
                "fragments": [],
                "text": "Learning decision rules in noisy domains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "As Schaffer [31] has already shown, the benefits in terms of accuracy of pruning are inversely proportional to the degree of sparseness, hence the worse performance of some methods on the Heart data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 160
                            }
                        ],
                        "text": "It is also worthwhile observing that the average size/error reported in the column \u201ctrained\u201d of Table 3 for the Led-200 data are concordant with that observed by Schaffer [30, Table 1] under the same conditions, although in that case the trees were built using the Gini index [2] instead of the gain-ratio."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "As proven by Schaffer [30], this result is actually an exception to the general behavior shown by cost complexity pruning in the digit recognition problem."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "In the light of such considerations, it is not difficult to accept Schaffer\u2019s claim [32] that tree pruning is a form of bias (here intended as a set of factors that influence hypothesis selection) rather than a statistical improvement of the classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 171
                            }
                        ],
                        "text": "Actually, the positive result for the analogous problem Led200 is quite surprising, even though it agrees with figures reported in the book by Breiman et al. As proven by Schaffer [30], this result is actually an exception to the general behavior shown by cost complexity pruning in the digit recognition problem."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Digit Recognition Problem,"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Ninth Int\u2019l Workshop on Machine Learning,"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065449379"
                        ],
                        "name": "Christopher J. C. H. Watkins",
                        "slug": "Christopher-J.-C.-H.-Watkins",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Watkins",
                            "middleNames": [
                                "J.",
                                "C.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher J. C. H. Watkins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 185
                            }
                        ],
                        "text": "Finally, we observe that the most recent version of minimum error pruning seems to have overcome two problems that affected the original proposal by Niblett and Bratko: optimistic bias [33] and dependence of the expected error rate on the number of classes [20]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 722184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09d53cc7db6f873bfade813183f02db0991aa454",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-Cross-Validation-and-Search-Watkins",
            "title": {
                "fragments": [],
                "text": "Combining Cross-Validation and Search"
            },
            "venue": {
                "fragments": [],
                "text": "EWSL"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476128"
                        ],
                        "name": "B. Cestnik",
                        "slug": "B.-Cestnik",
                        "structuredName": {
                            "firstName": "Bojan",
                            "lastName": "Cestnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Cestnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143986204"
                        ],
                        "name": "I. Kononenko",
                        "slug": "I.-Kononenko",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Kononenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kononenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725040"
                        ],
                        "name": "I. Bratko",
                        "slug": "I.-Bratko",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Bratko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Bratko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 240
                            }
                        ],
                        "text": "There are two different ways to cope with this: either by prospectively deciding when to stop the growth of a tree (prepruning) or by retrospectively reducing the size of a fully expanded tree, Tmax, by pruning some branches (post-pruning) [5]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39569406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22ffbb97b6a55603bc0bccda60ac3b2ef5427153",
            "isKey": false,
            "numCitedBy": 527,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ASSISTANT-86:-A-Knowledge-Elicitation-Tool-for-Cestnik-Kononenko",
            "title": {
                "fragments": [],
                "text": "ASSISTANT 86: A Knowledge-Elicitation Tool for Sophisticated Users"
            },
            "venue": {
                "fragments": [],
                "text": "EWSL"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353436"
                        ],
                        "name": "T. Niblett",
                        "slug": "T.-Niblett",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Niblett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Niblett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "In this case, a tree Tmax is grown even when it seems worthless and is then retrospectively pruned of those branches that seem superfluous with respect to predictive accuracy [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43140291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9414d6f32edefe583ed98300f97e1d355584a43",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Constructing-Decision-Trees-in-Noisy-Domains-Niblett",
            "title": {
                "fragments": [],
                "text": "Constructing Decision Trees in Noisy Domains"
            },
            "venue": {
                "fragments": [],
                "text": "EWSL"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining Cross-Validation and Search Progress in Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. EWSL 87, I. Bratko and N. Lavrac"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "This means that cross-validation may provide us with an error rate estimate whose amount of bias is unpredictable [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Choosing the Best Pruned Decision Tree: A Matter of Bias"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Fifth Italian Workshop on Machine Learning, Parma, Italy, pp. 33-37, 1994."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2726 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "5: Programs for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "5: Programs for Machine Learning"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 13,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 30,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Comparative-Analysis-of-Methods-for-Pruning-Trees-Esposito-Malerba/508571db5d2f77c17d2829878bb1dc645010dda8?sort=total-citations"
}