{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 13
                            }
                        ],
                        "text": "For instance Lewicki and Sejnowski (2000) argued that by choosing the priors to be Laplacian the problem can be mapped to a standard linear program."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 122
                            }
                        ],
                        "text": "Thus at every iteration of learning and for every data vector the maximization in Equation (8) needs to be performed.5 In Lewicki and Sejnowski (2000) it was argued that the approximation can be significantly improved if a Gaussian distribution around this MAP value was constructed by matching the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6254191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42d906c733f273109c0ed716a5ef6e2a379beb26",
            "isKey": false,
            "numCitedBy": 1255,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be sparser, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual cortex. Previous work has focused on finding the best representation of a signal using a fixed overcomplete basis (or dictionary). We present an algorithm for learning an overcomplete basis by viewing it as probabilistic model of the observed data. We show that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency. This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "slug": "Learning-Overcomplete-Representations-Lewicki-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency and provide a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 173
                            }
                        ],
                        "text": "Therefore, for causal generative models, overcomplete representations are expected to produce very compact (or sparse) codes, a fact which is often emphasized as desirable (Olshausen and Field, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": false,
            "numCitedBy": 3573,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8755,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 746481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2307fd6058ab4f7554a0b1f188507150ddb5b9a2",
            "isKey": false,
            "numCitedBy": 596,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the independent factor analysis (IFA) method for recovering independent hidden sources from their observed mixtures. IFA generalizes and unifies ordinary factor analysis (FA), principal component analysis (PCA), and independent component analysis (ICA), and can handle not only square noiseless mixing but also the general case where the number of mixtures differs from the number of sources and the data are noisy. IFA is a two-step procedure. In the first step, the source densities, mixing matrix, and noise covariance are estimated from the observed data by maximum likelihood. For this purpose we present an expectation-maximization (EM) algorithm, which performs unsupervised learning of an associated probabilistic model of the mixing situation. Each source in our model is described by a mixture of gaussians; thus, all the probabilistic calculations can be performed analytically. In the second step, the sources are reconstructed from the observed data by an optimal nonlinear estimator. A variational approximation of this algorithm is derived for cases with a large number of sources, where the exact algorithm becomes intractable. Our IFA algorithm reduces to the one for ordinary FA when the sources become gaussian, and to an EM algorithm for PCA in the zero-noise limit. We derive an additional EM algorithm specifically for noiseless IFA. This algorithm is shown to be superior to ICA since it can learn arbitrary source densities from the data. Beyond blind separation, IFA can be used for modeling multidimensional data by a highly constrained mixture of gaussians and as a tool for nonlinear signal encoding."
            },
            "slug": "Independent-Factor-Analysis-Attias",
            "title": {
                "fragments": [],
                "text": "Independent Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An expectation-maximization (EM) algorithm is presented, which performs unsupervised learning of an associated probabilistic model of the mixing situation and is shown to be superior to ICA since it can learn arbitrary source densities from the data."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061249"
                        ],
                        "name": "K. Millman",
                        "slug": "K.-Millman",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Millman",
                            "middleNames": [
                                "Jarrod"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Millman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17527214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ec3f33d81ee1b795cacb102ee1a05f36b7c7971",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for learning an overcomplete set of basis functions for the purpose of modeling sparse structure in images. The sparsity of the basis function coefficients is modeled with a mixture-of-Gaussians distribution. One Gaussian captures nonactive coefficients with a small-variance distribution centered at zero, while one or more other Gaussians capture active coefficients with a large-variance distribution. We show that when the prior is in such a form, there exist efficient methods for learning the basis functions as well as the parameters of the prior. The performance of the algorithm is demonstrated on a number of test cases and also on natural images. The basis functions learned on natural images are similar to those obtained with other methods, but the sparse form of the coefficient distribution is much better described. Also, since the parameters of the prior are adapted to the data, no assumption about sparse structure in the images need be made a priori, rather it is learned from the data."
            },
            "slug": "Learning-Sparse-Codes-with-a-Mixture-of-Gaussians-Olshausen-Millman",
            "title": {
                "fragments": [],
                "text": "Learning Sparse Codes with a Mixture-of-Gaussians Prior"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work describes a method for learning an overcomplete set of basis functions for the purpose of modeling sparse structure in images and shows that when the prior is in such a form, there exist efficient methods for learning the basis functions as well as the parameters of the prior."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583773"
                        ],
                        "name": "L. Parra",
                        "slug": "L.-Parra",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Parra",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Parra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 194
                            }
                        ],
                        "text": "For square representations the information maximization approach turns out to be equivalent to the causal generative one if we interpret fi(\u00b7) to be the cumulative distribution function of pi(\u00b7) (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 31
                            }
                        ],
                        "text": "In the causal generative view (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997), on the\nc\u00a92003 Yee Whye Teh, Max Welling, Simon Osindero and Geoffrey E. Hinton.\nother hand, the aim is to build a density model in which independent, non-Gaussian sources are linearly combined to produce the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Independent Components Analysis, Density Estimation, Overcomplete Representations, Sparse Representations"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 104
                            }
                        ],
                        "text": "Rather we will focus on reviewing two general approaches to ICA, namely the causal generative approach (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997) and the information maximization approach (Bell and Sejnowski, 1995, Shriki et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6469440,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e2b06b4fcd3f806c014fb05ca642cc6fa3cc664",
            "isKey": true,
            "numCitedBy": 225,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Source separation arises in a surprising number of signal processing applications, from speech \nrecognition to EEG analysis. In the square linear blind source separation problem without time delays, \none must find an unmixing matrix which can detangle the result of mixing n unknown independent sources \nthrough an unknown n x n mixing matrix. The recently introduced ICA blind source separation algorithm \n(Baram and Roth 1994; Bell and Sejnowski 1995) is a powerful and surprisingly simple technique for solving \nthis problem. ICA is all the more remarkable for performing so well despite making absolutely no use of the \ntemporal structure of its input! This paper presents a new algorithm, contextual ICA, which derives from a \nmaximum likelihood density estimation formulation of the problem. cICA can incorporate arbitrarily complex \nadaptive history-sensitive source models, and thereby make use of the temporal structure of its input. \nThis allows it to separate in a number of situations where standard ICA cannot, including sources with low \nkurtosis, colored gaussian sources, and sources which have gaussian histograms. Since ICA is a special case \nof cICA, the MLE derivation provides as a corollary a rigorous derivation of classic ICA."
            },
            "slug": "A-Context-Sensitive-Generalization-of-ICA-Pearlmutter-Parra",
            "title": {
                "fragments": [],
                "text": "A Context-Sensitive Generalization of ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new algorithm, contextual ICA, is presented, which derives from a maximum likelihood density estimation formulation of the problem, which can incorporate arbitrarily complex adaptive history-sensitive source models, and thereby make use of the temporal structure of its input."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 94
                            }
                        ],
                        "text": "To further analyze the set of learned filters, we fitted a Gabor function of the form used by Lewicki and Olshausen (1999) to each feature and extracted parameters like frequency, location and extent in the spatial and frequency domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 88
                            }
                        ],
                        "text": "This effect has been observed in previous papers (van Hateren and van der Schaaf, 1998, Lewicki and Olshausen, 1999), and is probably due to pixellation."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6482128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87e0d75a8c17e464cf8e95a0466533e14b97c5e",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply a Bayesian method for inferring an optimal basis to the problem of finding efficient image codes for natural scenes. The basis functions learned by the algorithm are oriented and localized in both space and frequency, bearing a resemblance to two-dimensional Gabor functions, and increasing the number of basis functions results in a greater sampling density in position, orientation, and scale. These properties also resemble the spatial receptive fields of neurons in the primary visual cortex of mammals, suggesting that the receptive-field structure of these neurons can be accounted for by a general efficient coding principle. The probabilistic framework provides a method for comparing the coding efficiency of different bases objectively by calculating their probability given the observed data or by measuring the entropy of the basis function coefficients. The learned bases are shown to have better coding efficiency than traditional Fourier and wavelet bases. This framework also provides a Bayesian solution to the problems of image denoising and filling in of missing pixels. We demonstrate that the results obtained by applying the learned bases to these problems are improved over those obtained with traditional techniques."
            },
            "slug": "PROBABILISTIC-FRAMEWORK-FOR-THE-ADAPTATION-AND-OF-Lewicki-Olshausen",
            "title": {
                "fragments": [],
                "text": "PROBABILISTIC FRAMEWORK FOR THE ADAPTATION AND COMPARISON OF IMAGE CODES"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The learned bases are shown to have better coding efficiency than traditional Fourier and wavelet bases and to provide a Bayesian solution to the problems of image denoising and filling in of missing pixels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2489930"
                        ],
                        "name": "O. Shriki",
                        "slug": "O.-Shriki",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Shriki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Shriki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720547"
                        ],
                        "name": "H. Sompolinsky",
                        "slug": "H.-Sompolinsky",
                        "structuredName": {
                            "firstName": "Haim",
                            "lastName": "Sompolinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sompolinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115651440"
                        ],
                        "name": "Daniel D. Lee",
                        "slug": "Daniel-D.-Lee",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lee",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel D. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4109189,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1337a15f45f8d1b0a99727c5c0e3d652ee88159",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The principle of maximizing mutual information is applied to learning overcomplete and recurrent representations. The underlying model consists of a network of input units driving a larger number of output units with recurrent interactions. In the limit of zero noise, the network is deterministic and the mutual information can be related to the entropy of the output units. Maximizing this entropy with respect to both the feedforward connections as well as the recurrent interactions results in simple learning rules for both sets of parameters. The conventional independent components (ICA) learning algorithm can be recovered as a special case where there is an equal number of output units and no recurrent connections. The application of these new learning rules is illustrated on a simple two-dimensional input example."
            },
            "slug": "An-Information-Maximization-Approach-to-and-Shriki-Sompolinsky",
            "title": {
                "fragments": [],
                "text": "An Information Maximization Approach to Overcomplete and Recurrent Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The conventional independent components (ICA) learning algorithm can be recovered as a special case where there is an equal number of output units and no recurrent connections and the application of these new learning rules is illustrated on a simple two-dimensional input example."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1845853"
                        ],
                        "name": "M. Inki",
                        "slug": "M.-Inki",
                        "structuredName": {
                            "firstName": "Mika",
                            "lastName": "Inki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Inki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 88
                            }
                        ],
                        "text": "A notably different variation on the generative theme is the Bayesian approach taken by Hyvarinen and Inki (2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 5
                            }
                        ],
                        "text": "5 In Lewicki and Sejnowski (2000) it was argued that the approximation can be significantly improved if a Gaussian distribution around this MAP value was constructed by matching the second derivatives locally (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8121779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94565856cc7f2fb3a1e046eec3c805a1c84b85ef",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimating overcomplete ICA bases for image windows is a difficult problem. Most algorithms require the estimation of values of the independent components which leads to computationally heavy procedures. Here we first review the existing methods, and then introduce two new algorithms that estimate an approximate overcomplete basis quite fast in a high-dimensional space. The first algorithm is based on the prior assumption that the basis vectors are randomly distributed in the space, and therefore close to orthogonal. The second replaces the conventional orthogonalization procedure by a transformation of the marginal density to gaussian."
            },
            "slug": "Estimating-Overcomplete-Independent-Component-Bases-Hyv\u00e4rinen-Inki",
            "title": {
                "fragments": [],
                "text": "Estimating Overcomplete Independent Component Bases for Image Windows"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two new algorithms are introduced that estimate an approximate overcomplete basis quite fast in a high-dimensional space and replace the conventional orthogonalization procedure by a transformation of the marginal density to gaussian."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Mathematical Imaging and Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34212067"
                        ],
                        "name": "S. Abdallah",
                        "slug": "S.-Abdallah",
                        "structuredName": {
                            "firstName": "Samer",
                            "lastName": "Abdallah",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Abdallah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804703"
                        ],
                        "name": "Mark D. Plumbley",
                        "slug": "Mark-D.-Plumbley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Plumbley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Plumbley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 36
                            }
                        ],
                        "text": "This phenomenon is also reported by Abdallah and Plumbley (2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16832634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be3c3b8091e09d8569508b08bd3240153a02cfb2",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that various flavours of Independent Component Analysis, when applied to natural images, all result in broadly similar localised, oriented band-pass feature detectors, which have been likened to wavelets or edge detectors. In this paper, we present a similar analysis of \u2018natural\u2019 sounds drawn from two radio stations: one broadcasting mainly speech; the other mainly classical music. Many of the resulting basis vectors are quite wavelet-like, and can easily be characterised in terms of their position and spread in the time-frequency plane. Some of them, however, particularly from the set trained on music, do not fit that interpretation very well. The Wigner-Ville Distribution can be used to gain a clearer picture of time-frequency localisation of these basis vectors. We conclude by suggesting that these results be compared with other widely used auditory representations such as short-term Fourier transforms, wavelet transforms, and physiologically derived models based on the auditory filterbank. 1. REDUNDANCY REDUCTION AS A GOAL OF PERCEPTION It has been suggested [2, 1, 7] that the processing of sensory data in biological perceptual systems is best understood in the language of information theory. The wealth of structure present in natural phenomena means that sensory signals are highly redundant; characterising this structure in order to develop efficient, non-redundant representations might be an effective processing strategy. In a distributed code, a major source of redundancy is statistical dependency between units; independent or factorial coding will be an important tool in dealing with this. ICA is Reduncany Reduction via Linear Transformation. If we restrict ourselves to instantaneous linear methods, then the best we can do is aim for a matrix operation e-mail: samer.abdallah@kcl.ac.uk e-mail: mark.plumbley@kcl.ac.uk that results in a vector whose elements are as independent as possible\u2014that is, precisely the ICA problem. If the observed data is represented as an -element vector , then we wish to find the linear transformation"
            },
            "slug": "IF-THE-INDEPENDENT-COMPONENTS-OF-NATURAL-IMAGES-ARE-Abdallah-Plumbley",
            "title": {
                "fragments": [],
                "text": "IF THE INDEPENDENT COMPONENTS OF NATURAL IMAGES ARE EDGES, WHAT ARE THE INDEPENDENT COMPONENTS OF NATURAL SOUNDS?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a similar analysis of \u2018natural\u2019 sounds drawn from two radio stations: one broadcasting mainly speech; the other mainly classical music, and suggests that these results be compared with other widely used auditory representations such as short-term Fourier transforms, wavelet transforms, and physiologically derived models based on the auditory filterbank."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 3
                            }
                        ],
                        "text": "In Welling et al. (2003), a two-layer model was studied where the second layer performed a local averaging of the non-linearly transformed activities of the first layer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7406434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14d2d9b2e4c29fe105bfbb31f9749b60690303a7",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs. We encourage the system to find sparse features by using a Student-t distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the \"iterated Wiener filter\" for the purpose of denoising images."
            },
            "slug": "Learning-Sparse-Topographic-Representations-with-of-Welling-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning Sparse Topographic Representations with Products of Student-t Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs is proposed and used as a prior to derive the \"iterated Wiener filter\" for the purpose of denoising images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 75
                            }
                        ],
                        "text": "There is also an interesting link between EBMs and maximum entropy models (Della Pietra et al., 1997, Zhu et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 233
                            }
                        ],
                        "text": "Apart from greater model flexibility, reported advantages include improved robustness in the presence of noise (Simoncelli et al., 1992), more compact and more easily interpretable codes (Mallat and Zhang, 1993) and superresolution (Chen et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 103
                            }
                        ],
                        "text": ", 1992), more compact and more easily interpretable codes (Mallat and Zhang, 1993) and superresolution (Chen et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9739,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218905"
                        ],
                        "name": "M. Bartlett",
                        "slug": "M.-Bartlett",
                        "structuredName": {
                            "firstName": "Marian",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "Stewart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741200"
                        ],
                        "name": "J. Movellan",
                        "slug": "J.-Movellan",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Movellan",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movellan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3240791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d2225253964183b14abcd168d024efe00422dd3",
            "isKey": false,
            "numCitedBy": 2145,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of current face recognition algorithms use face representations found by unsupervised statistical methods. Typically these methods find a set of basis images and represent faces as a linear combination of those images. Principal component analysis (PCA) is a popular example of such methods. The basis images found by PCA depend only on pairwise relationships between pixels in the image database. In a task such as face recognition, in which important information may be contained in the high-order relationships among pixels, it seems reasonable to expect that better basis images may be found by methods sensitive to these high-order statistics. Independent component analysis (ICA), a generalization of PCA, is one such method. We used a version of ICA derived from the principle of optimal information transfer through sigmoidal neurons. ICA was performed on face images in the FERET database under two different architectures, one which treated the images as random variables and the pixels as outcomes, and a second which treated the pixels as random variables and the images as outcomes. The first architecture found spatially local basis images for the faces. The second architecture produced a factorial face code. Both ICA representations were superior to representations based on PCA for recognizing faces across days and changes in expression. A classifier that combined the two ICA representations gave the best performance."
            },
            "slug": "Face-recognition-by-independent-component-analysis-Bartlett-Movellan",
            "title": {
                "fragments": [],
                "text": "Face recognition by independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Independent component analysis (ICA), a generalization of PCA, was used, using a version of ICA derived from the principle of optimal information transfer through sigmoidal neurons, which was superior to representations based on PCA for recognizing faces across days and changes in expression."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 158
                            }
                        ],
                        "text": "We note that the additive form of the energy leads to a product form for the probability distribution, which was called a \u201cproduct of experts\u201d (PoE) model in (Hinton, 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 157
                            }
                        ],
                        "text": "We note that the additive form of the energy leads to a product form for the probability distribution, which was called a \u201cproduct of experts\u201d (PoE) model in (Hinton, 2002).\nwhere u\u0302i(x,wi) is the feature computed in Equation (13)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 73
                            }
                        ],
                        "text": "This idea, called contrastive divergence learning, was first proposed by Hinton (2002) to improve both computational efficiency and reduce the variance at the expense of introducing a bias for the estimates of the parameters with respect to the maximum likelihood solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 85
                            }
                        ],
                        "text": "Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Hinton and Teh (2001) interpreted these filters as linear constraints, with the energies serving as costs for violating the constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 91
                            }
                        ],
                        "text": "This term is hard to compute but fortunately it is typically very small and simulations by Hinton (2002) suggest that it can be safely ignored."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 48
                            }
                        ],
                        "text": "Define the contrastive divergence cost function (Hinton, 2002) as CD = KL(p0\u2016p\u221e)\u2212KL(pn\u2016p\u221e)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1130,
                                "start": 173
                            }
                        ],
                        "text": "Since this algorithm applies backpropagation in an unsupervised setting and combines it with contrastive divergence learning we have named it \u201ccontrastive backpropagation\u201d (Hinton et al., 2004). Indeed, the contrastive backpropagation learning procedure is quite flexible. It puts no constraints other than smoothness on the activation functions or the energy functions.17 The procedure can be easily modified to use recurrent neural networks that contain directed cycles by running each forward pass for some predetermined number of steps and defining the energy to be any smooth function of the time history of the activations. Backpropagation through time (Rumelhart et al., 1986, Werbos, 1990) can then be used to obtain the required derivatives. The data-vector can also change during the forward pass through a recurrent network. This makes it possible to model sequential data, such as video sequences, by running the network forward in time for a whole sequence and then running it backward in time to compute the derivatives required for hybrid Monte Carlo sampling and for updating the weights. In Welling et al. (2003), a two-layer model was studied where the second layer performed a local averaging of the non-linearly transformed activities of the first layer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "In light of this, we propose another estimation method for energy-based models called contrastive divergence (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "The energy-based approach to ICA we have presented stems from previous work on products of experts (PoEs) (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "Define the contrastive divergence cost function (Hinton, 2002) as\nCD = KL(p0\u2016p\u221e)\u2212KL(pn\u2016p\u221e)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4570,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 18
                            }
                        ],
                        "text": "Attias (1999) and Girolami (2001) use a variational approach which replaces the true posterior with a tractable approximation which is itself adapted to better approximate the posterior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28449499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bfbf789df18c8a6f825ef4f8777a0f145bf3b0f",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "An expectation-maximization algorithm for learning sparse and overcomplete data representations is presented. The proposed algorithm exploits a variational approximation to a range of heavy-tailed distributions whose limit is the Laplacian. A rigorous lower bound on the sparse prior distribution is derived, which enables the analytic marginalization of a lower bound on the data likelihood. This lower bound enables the development of an expectation-maximization algorithm for learning the overcomplete basis vectors and inferring the most probable basis coefficients."
            },
            "slug": "A-Variational-Method-for-Learning-Sparse-and-Girolami",
            "title": {
                "fragments": [],
                "text": "A Variational Method for Learning Sparse and Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An expectation-maximization algorithm for learning sparse and overcomplete data representations is presented, which exploits a variational approximation to a range of heavy-tailed distributions whose limit is the Laplacian."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6302770,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7c032d555a9d7096f7bb88441f10e33d3302d5be",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 130,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-two-layer-sparse-coding-model-learns-simple-and-Hyv\u00e4rinen-Hoyer",
            "title": {
                "fragments": [],
                "text": "A two-layer sparse coding model learns simple and complex cell receptive fields and topography from natural images"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39092098"
                        ],
                        "name": "Y. Wu",
                        "slug": "Y.-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": [
                                "Nian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 102
                            }
                        ],
                        "text": "There is also an interesting link between EBMs and maximum entropy models (Della Pietra et al., 1997, Zhu et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd9ed76e8b9fa8b69257d3fc61fbc38bee973016",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "This article proposes a general theory and methodology, called the minimax entropy principle, for building statistical models for images (or signals) in a variety of applications. This principle consists of two parts. The first is the maximum entropy principle for feature binding (or fusion): for a given set of observed feature statistics, a distribution can be built to bind these feature statistics together by maximizing the entropy over all distributions that reproduce them. The second part is the minimum entropy principle for feature selection: among all plausible sets of feature statistics, we choose the set whose maximum entropy distribution has the minimum entropy. Computational and inferential issues in both parts are addressed; in particular, a feature pursuit procedure is proposed for approximately selecting the optimal set of features. The minimax entropy principle is then corrected by considering the sample variation in the observed feature statistics, and an information criterion for feature pursuit is derived. The minimax entropy principle is applied to texture modeling, where a novel Markov random field (MRF) model, called FRAME (filter, random field, and minimax entropy), is derived, and encouraging results are obtained in experiments on a variety of texture images. The relationship between our theory and the mechanisms of neural computation is also discussed."
            },
            "slug": "Minimax-Entropy-Principle-and-Its-Application-to-Zhu-Wu",
            "title": {
                "fragments": [],
                "text": "Minimax Entropy Principle and Its Application to Texture Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The minimax entropy principle is applied to texture modeling, where a novel Markov random field model, called FRAME, is derived, and encouraging results are obtained in experiments on a variety of texture images."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 153
                            }
                        ],
                        "text": "Apart from this, it may also happen that for certain Markov chains spurious fixed points exist in contrastive divergence learning (for some examples see MacKay, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6951034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6069ec0d0387b3f3516ae83ff108a581f10a76e4",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The Hinton network (Hinton, 2001, personal communication) is a deterministic mapping from an observable space x to an energy function E(x; w), parameterized by parameters w. The energy deflnes a probability P(xjw) = exp(iE(x; w))=Z(w). A maximum likelihood learning algorithm for this density model takes steps \u00a2w/ihgi 0 + hgi 1 where hgi 0 is the average of the gradient g = @E=@w evaluated at points x drawn from the data density, and hgi 1 is the average gradient for points x drawn from P(xjw). If T is a Markov chain in x-space that has P(xjw) as its unique invariant density then we can approximate hgi 1 by taking the data points x and hitting each of them I times with T, where I is a large integer. In the one-step learning algorithm of Hinton (2001), we set I to 1. In this paper I give examples of models E(x; w) and Markov chains T for which the true likelihood is unimodal in the parameters, but the one-step algorithm does not necessarily converge to the maximum likelihood parameters. It is hoped that these negative examples will help pin down the conditions for the one-step algorithm to be a correctly convergent algorithm. The Hinton network (Hinton, 2001, personal communication) is a deterministic mapping from anobservablespace xofdimension Dtoanenergyfunction E(x;w),parameterizedbyparameters w. The energy deflnesa probability"
            },
            "slug": "Failures-of-the-One-Step-Learning-Algorithm-Mackay",
            "title": {
                "fragments": [],
                "text": "Failures of the One-Step Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Examples of models E(x; w) and Markov chains T for which the true likelihood is unimodal in the parameters are given, but the one-step algorithm does not necessarily converge to the maximum likelihood parameters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 58
                            }
                        ],
                        "text": ", 1992), more compact and more easily interpretable codes (Mallat and Zhang, 1993) and superresolution (Chen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 188
                            }
                        ],
                        "text": "Apart from greater model flexibility, reported advantages include improved robustness in the presence of noise (Simoncelli et al., 1992), more compact and more easily interpretable codes (Mallat and Zhang, 1993) and superresolution (Chen et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": false,
            "numCitedBy": 8851,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3288675"
                        ],
                        "name": "F. Agakov",
                        "slug": "F.-Agakov",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Agakov",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Agakov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14004203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9b93a69f2ff9d1e5243a4a0cbc71813f8ea1d02",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The Boltzmann machine (BM) learning rule for random field models with latent variables can be problematic to use in practice. These problems have (at least partially) been attributed to the negative phase in BM learning where a Gibbs sampling chain should be run to equilibrium. Hinton (1999, 2000) has introduced an alternative called contrastive divergence (CD) learning where the chain is run for only 1 step. In this paper we analyse the mean and variance of the parameter update obtained after steps of Gibbs sampling for a simple Gaussian BM. For this model our analysis shows that CD learning produces (as expected) a biased estimate of the true parameter update. We also show that the variance does usually increase with and quantify this behaviour."
            },
            "slug": "An-analysis-of-contrastive-divergence-learning-in-Williams-Agakov",
            "title": {
                "fragments": [],
                "text": "An analysis of contrastive divergence learning in gaussian boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper analyses the mean and variance of the parameter update obtained after steps of Gibbs sampling for a simple Gaussian BM and shows that CD learning produces (as expected) a biased estimate of the true parameter update."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3741160"
                        ],
                        "name": "J. V. van Hateren",
                        "slug": "J.-V.-van-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "van Hateren",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. van Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51922762"
                        ],
                        "name": "A. van der Schaaf",
                        "slug": "A.-van-der-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "van der Schaaf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. van der Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 50
                            }
                        ],
                        "text": "This effect has been observed in previous papers (van Hateren and van der Schaaf, 1998, Lewicki and Olshausen, 1999), and is probably due to pixellation."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1789554,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "117175a54263cc2ae693fe82c9b3fe0553931cd8",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. Series B: Biological Sciences"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "Apart from greater model flexibility, reported advantages include improved robustness in the presence of noise (Simoncelli et al., 1992), more compact and more easily interpretable codes (Mallat and Zhang, 1993) and superresolution (Chen et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43701174,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8515604037444b3f079a9d328b0c560f33da0a19",
            "isKey": false,
            "numCitedBy": 1427,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >"
            },
            "slug": "Shiftable-multiscale-transforms-Simoncelli-Freeman",
            "title": {
                "fragments": [],
                "text": "Shiftable multiscale transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two examples of jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored and the usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 54
                            }
                        ],
                        "text": "Backpropagation through time (Rumelhart et al., 1986, Werbos, 1990) can then be used to obtain the required derivatives."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18470994,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1a3d22599028a05669e884f3eaf19a342e190a87",
            "isKey": false,
            "numCitedBy": 4036,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for backpropagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, i t describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed."
            },
            "slug": "Backpropagation-Through-Time:-What-It-Does-and-How-Werbos",
            "title": {
                "fragments": [],
                "text": "Backpropagation Through Time: What It Does and How to Do It"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis, and describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14149261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aaf352dc0b8c02e22bf0e11dc7bbcbed90e4f16f",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for the blind separation of sources can be derived from several different principles. This article shows that the infomax (information-maximization) principle is equivalent to the maximum likelihood. The application of the infomax principle to source separation consists of maximizing an output entropy."
            },
            "slug": "Infomax-and-maximum-likelihood-for-blind-source-Cardoso",
            "title": {
                "fragments": [],
                "text": "Infomax and maximum likelihood for blind source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article shows that the infomax (information-maximization) principle is equivalent to the maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 241
                            }
                        ],
                        "text": "For all the experiments described in this section we use an energy function of the form\nEi(ui(x,wi)) = \u03b3i log ( 1+(wTi x) 2) ,\nwhich corresponds to modelling the data with a product of one-dimensional student-t distributions of degree (2\u03b3\u2212 1) (Hinton and Teh, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Hinton and Teh (2001) interpreted these filters as linear constraints, with the energies serving as costs for violating the constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 116
                            }
                        ],
                        "text": "which corresponds to modelling the data with a product of one-dimensional student-t distributions of degree (2\u03b3\u2212 1) (Hinton and Teh, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9951467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66e65f81f1f76fb3a7c8ab2d813362b924e2fa9b",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations."
            },
            "slug": "Discovering-Multiple-Constraints-that-are-Satisfied-Hinton-Teh",
            "title": {
                "fragments": [],
                "text": "Discovering Multiple Constraints that are Frequently Approximately Satisfied"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work describes three methods of learning products of constraints using a heavy-tailed probability distribution for the violations of constraint violations."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 30
                            }
                        ],
                        "text": "Backpropagation through time (Rumelhart et al., 1986, Werbos, 1990) can then be used to obtain the required derivatives."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 223
                            }
                        ],
                        "text": "For square representations the information maximization approach turns out to be equivalent to the causal generative one if we interpret fi(\u00b7) to be the cumulative distribution function of pi(\u00b7) (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 60
                            }
                        ],
                        "text": "In the causal generative view (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997), on the\nc\u00a92003 Yee Whye Teh, Max Welling, Simon Osindero and Geoffrey E. Hinton.\nother hand, the aim is to build a density model in which independent, non-Gaussian sources are linearly combined to produce the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Independent Components Analysis, Density Estimation, Overcomplete Representations, Sparse Representations"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 133
                            }
                        ],
                        "text": "Rather we will focus on reviewing two general approaches to ICA, namely the causal generative approach (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997) and the information maximization approach (Bell and Sejnowski, 1995, Shriki et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood and covariant algorithms for independent components analysis. Available electronically at http"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum likelihood and covariant algorithms for independent components analysis. Available electronically at http"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 223
                            }
                        ],
                        "text": "For square representations the information maximization approach turns out to be equivalent to the causal generative one if we interpret fi(\u00b7) to be the cumulative distribution function of pi(\u00b7) (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 60
                            }
                        ],
                        "text": "In the causal generative view (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997), on the\nc\u00a92003 Yee Whye Teh, Max Welling, Simon Osindero and Geoffrey E. Hinton.\nother hand, the aim is to build a density model in which independent, non-Gaussian sources are linearly combined to produce the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 3
                            }
                        ],
                        "text": "In Olshausen and Field (1996) the posterior is approximated by a delta function at its MAP value."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 133
                            }
                        ],
                        "text": "Rather we will focus on reviewing two general approaches to ICA, namely the causal generative approach (Pearlmutter and Parra, 1996, MacKay, 1996, Cardoso, 1997) and the information maximization approach (Bell and Sejnowski, 1995, Shriki et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood and covariant algorithms for independent components analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Available electronically at http://www.inference.phy.cam.ac.uk/mackay/abstracts/ica.html,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 111
                            }
                        ],
                        "text": "Apart from greater model flexibility, reported advantages include improved robustness in the presence of noise (Simoncelli et al., 1992), more compact and more easily interpretable codes (Mallat and Zhang, 1993) and superresolution (Chen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "Apart from greater model flexibility, reported advantages include improved robustness in the presence of noise (Simoncelli et al., 1992), more compact and more easily interpretable codes (Mallat and Zhang, 1993) and superresolution (Chen et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shiftable multi-scale transforms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions Information Theory,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 36
                            }
                        ],
                        "text": "This phenomenon is also reported by Abdallah and Plumbley (2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "If edges are the independent components of natural images, what are the independent components of natural sounds"
            },
            "venue": {
                "fragments": [],
                "text": "In International Conference On Independent Component Analysis and Blind Signal Separation,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shiftablemulti - scale transforms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions Information Theory"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Field . Sparse coding with an overcomplete basisset : A strategy employed by V 1"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning sparse topographicrepresentations with products of studentt distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 171
                            }
                        ],
                        "text": "Since this algorithm applies backpropagation in an unsupervised setting and combines it with contrastive divergence learning we have named it \u201ccontrastive backpropagation\u201d (Hinton et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contrastive backpropagation"
            },
            "venue": {
                "fragments": [],
                "text": "In preparation,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 18
                            }
                        ],
                        "text": "Attias (1999) and Girolami (2001) use a variational approach which replaces the true posterior with a tractable approximation which is itself adapted to better approximate the posterior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A variational method for learning overcomplete representations"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 37
                            }
                        ],
                        "text": "One of the first expositions on ICA (Comon, 1994) used the entropy of linearly transformed input vectors as a contrast function to find statistically independent directions in input space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis: A new concept? Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Independent component analysis: A new concept? Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component filt rs of natural images compared with simple cells in primary visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London B"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Field . Sparse coding with an overcomplete basis set : A strategy employed by V 1"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 11,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Energy-Based-Models-for-Sparse-Overcomplete-Teh-Welling/b95799a25def71b100bd12e7ebb32cbcee6590bf?sort=total-citations"
}