{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703592"
                        ],
                        "name": "Y. Gotoh",
                        "slug": "Y.-Gotoh",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Gotoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gotoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "In previous work, we [1] and others [2], [5] have suggested the use of Latent Semantic Analysis (LSA) [3] as a model of semantic knowledge to be applied to ASR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5759432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04e71ae0a7b82c7529f2d8e9b4b89ed8184b3f2c",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, an approach for constructing mixture language models (LMs) based on some notion of semantics is discussed. To this end, a technique known as latent semantic analysis (LSA) is used. The approach encapsulates corpus-derived semantic information and is able to model the varying style of the text. Using such information , the corpus texts are clustered in an unsuper-vised manner and mixture LMs are automatically created. This work builds on previous work in the eld of information retrieval which was recently applied by Bel-legarda et. al. to the problem of clustering words by semantic categories. The principal contribution of this work is to characterize the document space resulting from the LSA modeling and to demonstrate the approach for mixture LM application. Comparison is made between manual and automatic clustering in order to elucidate how the semantic information is expressed in the space. It is shown that, using semantic information, mixture LMs performs better than a conventional single LM with slight increase of computational cost."
            },
            "slug": "Document-space-models-using-latent-semantic-Gotoh-Renals",
            "title": {
                "fragments": [],
                "text": "Document space models using latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that, using semantic information, mixture LMs performs better than a conventional single LM with slight increase of computational cost and compared to manual clustering, this work builds on previous work in the eld of information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748407"
                        ],
                        "name": "J. Bellegarda",
                        "slug": "J.-Bellegarda",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Bellegarda",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bellegarda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148249"
                        ],
                        "name": "J. Butzberger",
                        "slug": "J.-Butzberger",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Butzberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Butzberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2727234"
                        ],
                        "name": "Y. Chow",
                        "slug": "Y.-Chow",
                        "structuredName": {
                            "firstName": "Yen-lu",
                            "lastName": "Chow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145632116"
                        ],
                        "name": "N. Coccaro",
                        "slug": "N.-Coccaro",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Coccaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Coccaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100925254"
                        ],
                        "name": "D. Naik",
                        "slug": "D.-Naik",
                        "structuredName": {
                            "firstName": "Devang",
                            "lastName": "Naik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Naik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "In previous work, we [1] and others [2], [5] have suggested the use of Latent Semantic Analysis (LSA) [3] as a model of semantic knowledge to be applied to ASR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28860281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8ad76470164e7ac1d374bcfb393983c0113725f",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new approach is proposed for the clustering of words in a given vocabulary. The method is based on a paradigm first formulated in the context of information retrieval, called latent semantic analysis. This paradigm leads to a parsimonious vector representation of each word in a suitable vector space, where familiar clustering techniques can be applied. The distance measure selected in this space arises naturally from the problem formulation. Preliminary experiments indicate that, the clusters produced are intuitively satisfactory. Because these clusters are semantic in nature, this approach may prove useful as a complement to conventional class-based statistical language modeling techniques."
            },
            "slug": "A-novel-word-clustering-algorithm-based-on-latent-Bellegarda-Butzberger",
            "title": {
                "fragments": [],
                "text": "A novel word clustering algorithm based on latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new approach is proposed for the clustering of words in a given vocabulary based on a paradigm first formulated in the context of information retrieval, called latent semantic analysis, which leads to a parsimonious vector representation of each word in a suitable vector space."
            },
            "venue": {
                "fragments": [],
                "text": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13110923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "076fa8d095c37c657f2aff39cf90bc2ea883b7cb",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the ME solution. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The ME framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the ME approach was trained on theWall Street Journalcorpus, and showed a 32\u201339% perplexity reduction over the baseline. When interfaced to SPHINX-II, Carnegie Mellon's speech recognizer, it reduced its error rate by 10\u201314%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "slug": "A-maximum-entropy-approach-to-adaptive-statistical-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources, and shows the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 171
                            }
                        ],
                        "text": "Our confidence metric is a \u2018global term weighting\u2019 found to be useful in IR applications: the entropy of the frequency of a word over all documents in the training corpus [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62541440,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3f869c5c74212c68e8879b2e7123394441f0706",
            "isKey": false,
            "numCitedBy": 719,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "A major barrier to successful retrieval from external sources (e.g., electronic databases) is the tremendous variability in the words that people use to describe objects of interest. The fact that different authors use different words to describe essentially the same idea means that relevant objects will be missed; conversely, the fact that the same word can be used to refer to many different things means that irrelevant objects will be retrieved. We describe a statistical method called latent semantic indexing, which models the implicit higher order structure in the association of words and objects and improves retrieval performance by up to 30%. Additional large performance improvements of 40% and 67% can be achieved through the use of differential term weighting and iterative retrieval methods."
            },
            "slug": "Improving-the-retrieval-of-information-from-sources-Dumais",
            "title": {
                "fragments": [],
                "text": "Improving the retrieval of information from external sources"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A statistical method is described called latent semantic indexing, which models the implicit higher order structure in the association of words and objects and improves retrieval performance by up to 30%."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118415117"
                        ],
                        "name": "Michael P. Jones",
                        "slug": "Michael-P.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael P. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10796472"
                        ],
                        "name": "James H. Martin",
                        "slug": "James-H.-Martin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "LSA is a model of word semantic similarity based on word co-occurrence tendencies, and has been successful in IR and NLP applications, such as spelling correction [7]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8592573,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "232e66748382ded9d217de554574fbf70df0f6b6",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Contextual spelling errors are defined as the use of an incorrect, though valid, word in a particular sentence or context. Traditional spelling checkers flag misspelled words, but they do not typically attempt to identify words that are used incorrectly in a sentence. We explore the use of Latent Semantic Analysis for correcting these incorrectly used words and the results are compared to earlier work based on a Bayesian classifier."
            },
            "slug": "Contextual-Spelling-Correction-Using-Latent-Jones-Martin",
            "title": {
                "fragments": [],
                "text": "Contextual Spelling Correction Using Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The use of Latent Semantic Analysis for correcting incorrectly used words is explored and the results are compared to earlier work based on a Bayesian classifier."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748407"
                        ],
                        "name": "J. Bellegarda",
                        "slug": "J.-Bellegarda",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Bellegarda",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bellegarda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "In previous work, we [1] and others [2], [5] have suggested the use of Latent Semantic Analysis (LSA) [3] as a model of semantic knowledge to be applied to ASR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12976399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2928de5400a920a6a29af41821c680cef5d35f91",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-latent-semantic-analysis-framework-for-large-Span-Bellegarda",
            "title": {
                "fragments": [],
                "text": "A latent semantic analysis framework for large-Span language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38232647"
                        ],
                        "name": "R. Iyer",
                        "slug": "R.-Iyer",
                        "structuredName": {
                            "firstName": "Rukmini",
                            "lastName": "Iyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Iyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59661910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c294104a5add3010805e93102b3194a613f6454",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-And-Predicting-Performance-Of-Statistical-Iyer",
            "title": {
                "fragments": [],
                "text": "Improving And Predicting Performance Of Statistical Language Models In Sparse Domains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "LSA is a model of word semantic similarity based on word co-occurrence tendencies, and has been successful in IR and NLP applications, such as spelling correction [7]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contextual spelling correcti  on using latent semantic analysis"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Fifth Conference on Applied Natural Language Processing"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Indexing by latent semantic analy"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Society for Information Science"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving the retrieval of informationfrom external sources"
            },
            "venue": {
                "fragments": [],
                "text": "Behavior Resarch Methods , Instruments and Computers"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Indexing by latent semantic analy"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Society for Information Science"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 11,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Towards-better-integration-of-semantic-predictors-Coccaro-Jurafsky/b888cae7e6e288b108f9d119fc23b84b4d447029?sort=total-citations"
}