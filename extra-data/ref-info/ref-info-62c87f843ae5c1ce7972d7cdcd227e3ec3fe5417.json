{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166946041"
                        ],
                        "name": "J. Ma",
                        "slug": "J.-Ma",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "The model expressed in (2) provides clear contrast to the trajectory or trended models where the time-varying acoustic features are approximated as an explicit temporal function of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15922975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3702e71dd9a051808f2cb30a3564fb96da6d56b4",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A statistical coarticulatory model is presented for spontaneous speech recognition, where knowledge of the dynamic, target-directed behavior in the vocal tract resonance is incorporated into the model design, training, and in likelihood computation. The principal advantage of the new model over the conventional HMM is the use of a compact, internal structure that parsimoniously represents long-span context dependence in the observable domain of speech acoustics without using additional, context-dependent model parameters. The new model is formulated mathematically as a constrained, nonstationary, and nonlinear dynamic system, for which a version of the generalized EM algorithm is developed and implemented for automatically learning the compact set of model parameters. A series of experiments for speech recognition and model synthesis using spontaneous speech data from the Switchboard corpus are reported. The promise of the new model is demonstrated by showing its consistently superior performance over a state-of-the-art benchmark HMM system under controlled experimental conditions. Experiments on model synthesis and analysis shed insight into the mechanism underlying such superiority in terms of the target-directed behavior and of the long-span context-dependence property, both inherent in the designed structure of the new dynamic model of speech."
            },
            "slug": "Spontaneous-speech-recognition-using-a-statistical-Deng-Ma",
            "title": {
                "fragments": [],
                "text": "Spontaneous speech recognition using a statistical coarticulatory model for the vocal-tract-resonance dynamics."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The promise of the new model is demonstrated by showing its consistently superior performance over a state-of-the-art benchmark HMM system under controlled experimental conditions, and insight is shed into the mechanism underlying such superiority in terms of the target-directed behavior and of the long-span context-dependence property, both inherent in the designed structure of thenew dynamic model of speech."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2867071"
                        ],
                        "name": "Jian-Lai Zhou",
                        "slug": "Jian-Lai-Zhou",
                        "structuredName": {
                            "firstName": "Jian-Lai",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Lai Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745715"
                        ],
                        "name": "F. Seide",
                        "slug": "F.-Seide",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Seide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Seide"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Key advantages of using VTRs as the \u201ctask\u201d are their direct correlate to the acoustic information, and the lower dimensionality in the VTR vector compared with the counterpart hidden vectors either in the articulatory dynamic model or in the task-dynamic model with articulatorily defined goals or\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15123441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f02a7547364310474c4fdd580952faece1c94721",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose and evaluate a new acoustic model that combines HMM and a special type of the hidden dynamic model (HDM) a target-directed hidden trajectory model - into a single integrated model named HTHMM. The new model provides a computational model of coarticulation by representing the internal dynamics of human speech based on the hidden trajectory of the vocal-tract resonances. This paper focuses on the general structure of the new model and the EM training procedure. The corresponding MAP decoding algorithm and more detailed evaluation are given in Seide et al. (2003). Speech recognition experimental results on the Aurora2 task demonstrated that the new model, although using only context-independent phoneme units (no context-dependent parameters), is still slightly superior in word error rate to the corresponding crossword triphone HMM. This provides the evidence that the coarticulatory mechanism represented by the HTHMM via the model structure matches the traditional context-dependent modeling approach based on enumeration of model parameters."
            },
            "slug": "Coarticulation-modeling-by-embedding-a-hidden-model-Zhou-Seide",
            "title": {
                "fragments": [],
                "text": "Coarticulation modeling by embedding a target-directed hidden trajectory model into HMM - model and training"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Speech recognition experimental results on the Aurora2 task demonstrated that the new model, although using only context-independent phoneme units (no context-dependent parameters), is still slightly superior in word error rate to the corresponding crossword triphone HMM, provides the evidence that the coarticulatory mechanism represented by the HTHMM via the model structure matches the traditional context- dependent modeling approach based on enumeration of model parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "The model expressed in (2) provides clear contrast to the trajectory or trended models where the time-varying acoustic features are approximated as an explicit temporal function of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "Since the underlying speech structure represented by the hidden dynamic model links a sequence of segments via continuity in the hidden dynamic variables, it can also be appropriately termed as the super-segmental model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7347692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac720f8756d88307b4c587eb229835d808a64b96",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-dynamic,-feature-based-approach-to-the-interface-Deng",
            "title": {
                "fragments": [],
                "text": "A dynamic, feature-based approach to the interface between phonology and phonetics for speech modeling and recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606816"
                        ],
                        "name": "Jeff Z. Ma",
                        "slug": "Jeff-Z.-Ma",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Ma",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Z. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Various simplified implementations of the aforementioned generic nonlinear system model have appeared in the literature (e.g., [4], [15], [17], [20], [24], [26], [ 37 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15833028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc0cfa2396ab8a8596dc64ea8e7370003bf89f8f",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a novel mixture linear dynamic model (MLDM) for speech recognition is developed and evaluated, where several linear dynamic models are combined (mixed) to represent different vocal-tract-resonance (VTR) dynamic behaviors and the mapping relationships between the VTRs and the acoustic observations. Each linear dynamic model is formulated as the state-space equations, where the VTRs target-directed property is incorporated in the state equation and a linear regression function is used for the observation equation that approximates the nonlinear mapping relationship. A version of the generalized EM algorithm is developed for learning the model parameters, where the constraint that the VTR targets change at the segmental level (rather than at the frame level) is imposed in the parameter learning and model scoring algorithms. Speech recognition experiments are carried out to evaluate the new model using the N-best re-scoring paradigm in a Switchboard task. Compared with a baseline recognizer using the triphone HMM acoustic model, the new recognizer demonstrated improved performance under several experimental conditions. The performance was shown to increase with an increased number of the mixture components in the model."
            },
            "slug": "Target-directed-mixture-dynamic-models-for-speech-Ma-Deng",
            "title": {
                "fragments": [],
                "text": "Target-directed mixture dynamic models for spontaneous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A novel mixture linear dynamic model (MLDM) for speech recognition is developed and evaluated, where several linear dynamic models are combined to represent different vocal-tract-resonance (VTR) dynamic behaviors and the mapping relationships between the VTRs and the acoustic observations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Speech and Audio Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793645"
                        ],
                        "name": "H. Gish",
                        "slug": "H.-Gish",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Gish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Gish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40446176"
                        ],
                        "name": "Kenney Ng",
                        "slug": "Kenney-Ng",
                        "structuredName": {
                            "firstName": "Kenney",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenney Ng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57374607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29abffb7bf0e428c8b48532181ad85d435c22b16",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a segmental speech model that explicitly models the dynamics in a variable-duration speech segment by using a time-varying trajectory model of the speech features in the segment. Each speech segment is represented by a set of statistics which includes a time-varying trajectory, a residual error covariance around the trajectory, and the number of frames in the segment. These statistics replace the frames in the segment and become the data that are modeled by either HMMs (hidden Markov models) or mixture models. This segment model is used to develop a secondary processing algorithm that rescores putative events hypothesized by a primary HMM word spotter to try to improve performance by discriminating true keywords from false alarms. This algorithm is evaluated on a keyword spotting task using the Road Rally Database, and performance is shown to improve significantly over that of the primary word spotter. The segmental model is also used on a TIMIT vowel classification task to evaluate its modeling capability.<<ETX>>"
            },
            "slug": "A-segmental-speech-model-with-applications-to-word-Gish-Ng",
            "title": {
                "fragments": [],
                "text": "A segmental speech model with applications to word spotting"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A segmental speech model is used to develop a secondary processing algorithm that rescores putative events hypothesized by a primary HMM word spotter to try to improve performance by discriminating true keywords from false alarms."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805005"
                        ],
                        "name": "H. Sheikhzadeh",
                        "slug": "H.-Sheikhzadeh",
                        "structuredName": {
                            "firstName": "Hamid",
                            "lastName": "Sheikhzadeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sheikhzadeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1114809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7d7e8671c1f7857a46f4d88768b7e7c34588d19",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a novel approach to speech recognition by directly modeling the statistical characteristics of the speech waveforms. This approach allows them to remove the need for using speech preprocessors, which conventionally serve a role of converting speech waveforms into frame-based speech data subject to a subsequent modeling process. Central to their method is the representation of the speech waveforms as the output of a time-varying filter excited by a Gaussian source time-varying in its power. In order to formulate a speech recognition algorithm based on this representation, the time variation in the characteristics of the filter and of the excitation source is described in a compact and parametric form of the Markov chain. They analyze in detail the comparative roles played by the filter modeling and by the source modeling in speech recognition performance. Based on the result of the analysis, they propose and evaluate a normalization procedure intended to remove the sensitivity of speech recognition accuracy to often uncontrollable speech power variations. The effectiveness of the proposed speech-waveform modeling approach is demonstrated in a speaker-dependent, discrete-utterance speech recognition task involving 18 highly confusable stop consonant-vowel syllables. The high accuracy obtained shows promising potentials of the proposed time-domain waveform modeling technique for speech recognition. >"
            },
            "slug": "Waveform-based-speech-recognition-using-hidden-and-Sheikhzadeh-Deng",
            "title": {
                "fragments": [],
                "text": "Waveform-based speech recognition using hidden filter models: parameter selection and sensitivity to power normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "The authors describe a novel approach to speech recognition by directly modeling the statistical characteristics of the speech waveforms, which allows them to remove the need for using speech preprocessors, which conventionally serve a role of converting speech waves into frame-based speech data subject to a subsequent modeling process."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 232
                            }
                        ],
                        "text": "\u2026direct correlate to the acoustic information, and the lower dimensionality in the VTR vector compared with the counterpart hidden vectors either in the articulatory dynamic model or in the task-dynamic model with articulatorily defined goals or \u201ctasks\u201d such as vocal tract constriction properties."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2449761,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "4917c57f8e070553c24209d40a4b39a05edef799",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We report our new development of a hidden trajectory model for co-articulated, time-varying patterns of speech. The model uses bi-directional filtering of vocal tract resonance targets to jointly represent contextual variation and phonetic reduction in speech acoustics. A novel maximum-likelihood-based learning algorithm is presented that accurately estimates the distributional parameters of the resonance targets. The results of the estimates are analyzed and shown to be consistent with all the relevant acoustic-phonetic facts and intuitions. Phonetic recognition experiments demonstrate that the model with more rigorous target training outperforms the most recent earlier version of the model, producing 17.5% fewer errors in N-best rescoring."
            },
            "slug": "Learning-statistically-characterized-resonance-in-a-Deng-Yu",
            "title": {
                "fragments": [],
                "text": "Learning statistically characterized resonance targets in a hidden trajectory model of speech coarticulation and reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel maximum-likelihood-based learning algorithm is presented that accurately estimates the distributional parameters of the resonance targets in a hidden trajectory model for co-articulated, time-varying patterns of speech."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121786"
                        ],
                        "name": "V. Digalakis",
                        "slug": "V.-Digalakis",
                        "structuredName": {
                            "firstName": "Vassilios",
                            "lastName": "Digalakis",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Digalakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3353221"
                        ],
                        "name": "O. Kimball",
                        "slug": "O.-Kimball",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Kimball",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kimball"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15742861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8658d645b716d59c5edc80e33e1936e33574bc26",
            "isKey": false,
            "numCitedBy": 702,
            "numCiting": 147,
            "paperAbstract": {
                "fragments": [],
                "text": "Many alternative models have been proposed to address some of the shortcomings of the hidden Markov model (HMM), which is currently the most popular approach to speech recognition. In particular, a variety of models that could be broadly classified as segment models have been described for representing a variable-length sequence of observation vectors in speech recognition applications. Since there are many aspects in common between these approaches, including the general recognition and training problems, it is useful to consider them in a unified framework. The paper describes a general stochastic model that encompasses most of the models proposed in the literature, pointing out similarities of the models in terms of correlation and parameter tying assumptions, and drawing analogies between segment models and HMMs. In addition, we summarize experimental results assessing different modeling assumptions and point out remaining open questions."
            },
            "slug": "From-HMM's-to-segment-models:-a-unified-view-of-for-Ostendorf-Digalakis",
            "title": {
                "fragments": [],
                "text": "From HMM's to segment models: a unified view of stochastic modeling for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A general stochastic model is described that encompasses most of the models proposed in the literature for speech recognition, pointing out similarities in terms of correlation and parameter tying assumptions, and drawing analogies between segment models and HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144438953"
                        ],
                        "name": "Xiang Li",
                        "slug": "Xiang-Li",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 226
                            }
                        ],
                        "text": "\u2026direct correlate to the acoustic information, and the lower dimensionality in the VTR vector compared with the counterpart hidden vectors either in the articulatory dynamic model or in the task-dynamic model with articulatorily defined goals or \u201ctasks\u201d such as vocal tract constriction properties."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2918028,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "719a570a71106a7de868f2084f1049d69c8f06a4",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel acoustic model of speech, based on statistical hidden trajectory modeling (HTM) with bi-directional vocal tract resonance (VTR) target filtering, for speech recognition. The HTM consists of two stages of the generative process of speech: from the phone sequence to VTR dynamics and then to the cepstrum-based acoustic observation. Two types of model implementation are detailed, one with straightforward two-stage cascading, and another which integrates over the statistical distribution of VTR in model construction and in computing acoustic likelihood. With the use of first-order Taylor series approximation to the nonlinearity in the VTR-to-cepstrum prediction component of HTM, the acoustic likelihood is established in an analytical form. It is a Gaussian with the time-varying mean that gives structured long-span context dependence over the entire utterance, and with the dynamically adjusted variance proportional to the squared \"local slope\" in the nonlinear mapping function from VTR to cepstrum. When the HTM parameters are trained via maximizing this \"integrated\" likelihood, dramatic reduction of an upper error bound is achieved in the standard TIMIT phonetic recognition task using a large-scale N-best rescoring paradigm."
            },
            "slug": "A-hidden-trajectory-model-with-bi-directional-vs.-Deng-Li",
            "title": {
                "fragments": [],
                "text": "A hidden trajectory model with bi-directional target filtering: cascaded vs. integrated implementation for phonetic recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A novel acoustic model of speech, based on statistical hidden trajectory modeling (HTM) with bi-directional vocal tract resonance (VTR) target filtering, with dramatic reduction of an upper error bound is achieved in the standard TIMIT phonetic recognition task using a large-scale N-best rescoring paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49266188"
                        ],
                        "name": "Yuanjie Dong",
                        "slug": "Yuanjie-Dong",
                        "structuredName": {
                            "firstName": "Yuanjie",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanjie Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17932569,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "65dd1eca58adbacaea632cfa8dc16b095628f41e",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative model of coarticulation is presented that accurately predicts formant dynamics in fluent speech using the prior information of resonance targets in the phone sequence, in absence of actual acoustic data. Realistic formant undershoot (reduction) and \u201cstatic\u201d sound confusion is produced naturally from the model for fast-rate speech in a contextually assimilated manner. The model developed is capable of resolving the confusion with dynamic speech specification. As a source of a-priori knowledge about the speech structure, the model is a central component of our Bayesian generative modeling approach to automatic recognition of conversational speech, where varying degrees of sound reduction abound due to the free-varying speaking style and rate. We present details of the model simulation that demonstrates quantitative effects of speaking rate and segment duration on the magnitude of reduction, agreeing closely with experimental measurement results in the acoustic-phonetic literature. The model simulation also gives quantitative effects of varying the \u201cstiffness\u2019 parameter in the model."
            },
            "slug": "A-quantitative-model-for-formant-dynamics-and-in-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "A quantitative model for formant dynamics and contextually assimilated reduction in fluent speech"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A quantitative model of coarticulation is presented that accurately predicts formant dynamics in fluent speech using the prior information of resonance targets in the phone sequence, in absence of actual acoustic data."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 221
                            }
                        ],
                        "text": "\u2026direct correlate to the acoustic information, and the lower dimensionality in the VTR vector compared with the counterpart hidden vectors either in the articulatory dynamic model or in the task-dynamic model with articulatorily defined goals or \u201ctasks\u201d such as vocal tract constriction properties."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7492431,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "2ffd476d481fa644b8721b43427138f858e2d9ae",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A structured generative model of speech coarticulation and reduction is described with a novel two-stage implementation. At the first stage, the dynamics of formants or vocal tract resonances (VTRs) in fluent speech is generated using prior information of resonance targets in the phone sequence, in absence of acoustic data. Bidirectional temporal filtering with finite-impulse response (FIR) is applied to the segmental target sequence as the FIR filter's input, where forward filtering produces anticipatory coarticulation and backward filtering produces regressive coarticulation. The filtering process is shown also to result in realistic resonance-frequency undershooting or reduction for fast-rate and low-effort speech in a contextually assimilated manner. At the second stage, the dynamics of speech cepstra are predicted analytically based on the FIR-filtered and speaker-adapted VTR targets, and the prediction residuals are modeled by Gaussian random variables with trainable parameters. The combined system of these two stages, thus, generates correlated and causally related VTR and cepstral dynamics, where phonetic reduction is represented explicitly in the hidden resonance space and implicitly in the observed cepstral space. We present details of model simulation demonstrating quantitative effects of speaking rate and segment duration on the magnitude of reduction, agreeing closely with experimental measurement results in the acoustic-phonetic literature. This two-stage model is implemented and applied to the TIMIT phonetic recognition task. Using the N-best (N=2000) rescoring paradigm, the new model, which contains only context-independent parameters, is shown to significantly reduce the phone error rate of a standard hidden Markov model (HMM) system under the same experimental conditions."
            },
            "slug": "A-bidirectional-target-filtering-model-of-speech-Deng-Yu",
            "title": {
                "fragments": [],
                "text": "A bidirectional target-filtering model of speech coarticulation and reduction: two-stage implementation for phonetic recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The new model, which contains only context-independent parameters, is shown to significantly reduce the phone error rate of a standard hidden Markov model (HMM) system under the same experimental conditions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13869184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8142355a9b964fc270f16e5111ba3d969b01c3db",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A long-contextual-span Hidden Trajectory Model (HTM) developed recently captures underlying dynamic structure of speech coarticulation and reduction using a highly compact set of context-independent parameters. However, the longspan nature of the HTM makes it difficult to develop efficient search algorithms for its full evaluation. In this paper, we describe our initial effort in meeting this challenge. The basic search algorithm is time-asynchronous A*. Given the structural complexity of the long-span HTM, special considerations are needed to take into account the fact that the HTM score for each frame depends on the model parameters associated with a variable number of adjacent phones. Specifically, we present details on how the nodes and links in the lattices are expanded via look-ahead, how the A* heuristics are estimated, and what pruning strategies are applied to speed up the search. The experiments on TIMIT phonetic recognition show the capability of our newly developed lattice search algorithm in evaluating billions of hypotheses based on long-span HTM scores. The results significantly extend our earlier work from N-best rescoring to A* search over lattices."
            },
            "slug": "Evaluation-of-a-long-contextual-Span-hidden-model-Yu-Deng",
            "title": {
                "fragments": [],
                "text": "Evaluation of a long-contextual-Span hidden trajectory model and phonetic recognizer using a* lattice search"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Details on how the nodes and links in the lattices are expanded via look-ahead, how the A* heuristics are estimated, and what pruning strategies are applied to speed up the search are presented."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 191
                            }
                        ],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7203896,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "d45b8dcc1f929a43f4dae4dbd69a12d163aa8ed8",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-generalized-hidden-Markov-model-with-trend-of-for-Deng",
            "title": {
                "fragments": [],
                "text": "A generalized hidden Markov model with state-conditioned trend functions of time for the speech signal"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145354930"
                        ],
                        "name": "G. Ramsay",
                        "slug": "G.-Ramsay",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Ramsay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ramsay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145622870"
                        ],
                        "name": "Don X. Sun",
                        "slug": "Don-X.-Sun",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Sun",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don X. Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "The model expressed in (2) provides clear contrast to the trajectory or trended models where the time-varying acoustic features are approximated as an explicit temporal function of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12065771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd645591ac4c0c72baf39d52e6a13c6cf2de7fe1",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Production-models-as-a-structural-basis-for-speech-Deng-Ramsay",
            "title": {
                "fragments": [],
                "text": "Production models as a structural basis for automatic speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47531272"
                        ],
                        "name": "W. Holmes",
                        "slug": "W.-Holmes",
                        "structuredName": {
                            "firstName": "Wendy",
                            "lastName": "Holmes",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Holmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145235286"
                        ],
                        "name": "M. Russell",
                        "slug": "M.-Russell",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Russell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8409534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf76eb1602be91a7734efed003258acbdb57ace1",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract \u201cSegmental hidden Markov models\u201d (SHMMs) are intended to overcome important speech-modelling limitations of the conventional-HMM approach by representing sequences (or segments) of features and incorporating the concept of trajectories to describe how features change over time. A novel feature of the approach presented in this paper is thatextra-segmentalvariability between different examples of a sub-phonemic speech segment is modelled separately fromintra-segmentalvariability within any one example. The extra-segmental component of the model is represented in terms of variability in the trajectory parameters, and these models are therefore referred to as \u201cprobabilistic-trajectory segmental HMMs\u201d (PTSHMMs). This paper presents the theory of PTSHMMs using a linear trajectory description characterized by slope and mid-point parameters, and presents theoretical and experimental comparisons between different types of PTSHMMs, simpler SHMMs and conventional HMMs. Experiments have demonstrated that, for any given feature set, a linear PTSHMM can substantially reduce the error rate in comparison with a conventional HMM, both for a connected-digit recognition task and for a phonetic classification task. Performance benefits have been demonstrated from incorporating a linear trajectory description and additionally from modelling variability in the mid-point parameter."
            },
            "slug": "Probabilistic-trajectory-segmental-HMMs-Holmes-Russell",
            "title": {
                "fragments": [],
                "text": "Probabilistic-trajectory segmental HMMs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Performance benefits have been demonstrated from incorporating a linear trajectory description and additionally from modelling variability in the mid-point parameter, and theoretical and experimental comparisons between different types of PTSHMMs, simpler SHMMs and conventional HMMs are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745715"
                        ],
                        "name": "F. Seide",
                        "slug": "F.-Seide",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Seide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Seide"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2867071"
                        ],
                        "name": "Jian-Lai Zhou",
                        "slug": "Jian-Lai-Zhou",
                        "structuredName": {
                            "firstName": "Jian-Lai",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Lai Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Key advantages of using VTRs as the \u201ctask\u201d are their direct correlate to the acoustic information, and the lower dimensionality in the VTR vector compared with the counterpart hidden vectors either in the articulatory dynamic model or in the task-dynamic model with articulatorily defined goals or\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5715404,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d96ddec72f578e1f905fa4ccb9b92b0d0630199",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The hidden dynamic model (HDM) has been an attractive acoustic modeling approach because it provides a computational model for coarticulation and the dynamics of human speech. However, the lack of a direct decoding algorithm has been a barrier to research progress on HDM. We have developed a new HDM-based acoustic model, the hidden-trajectory HMM (HTHMM), which combines the state/mixture topology of a traditional monophone HMM with a target-directed hidden-trajectory model (a special form of HDM) for coarticulation modeling. Because the classical Viterbi algorithm is not admissible, we have developed a novel MAP decoding algorithm for HTHMM that correctly takes the hidden continuous trajectory into account. This paper introduces our new HTHMM decoder that allows us for the first time to evaluate an HDM-type model by direct decoding instead of N-best rescoring. Using direct decoding, we demonstrate that the coarticulatory mechanism of our HTHMM matches traditional context-dependent modeling (enumeration of model parameters): The context-independent HTHMM has slightly better accuracy than a crossword-triphone HMM on the Aurora2 task. The decoder also enables us to include state-boundary optimization into the HDM/HTHMM training procedure. This paper presents the detailed decoding algorithm and evaluation results, while in Zhou et al. (2003) we present the HTHMM model itself and parameter training."
            },
            "slug": "Coarticulation-modeling-by-embedding-a-hidden-model-Seide-Zhou",
            "title": {
                "fragments": [],
                "text": "Coarticulation modeling by embedding a target-directed hidden trajectory model into HMM - MAP decoding and evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces the new HTHMM decoder that allows for the first time to evaluate an HDM-type model by direct decoding instead of N-best rescoring, and demonstrates that the coarticulatory mechanism of the H THMM matches traditional context-dependent modeling (enumeration of model parameters)."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114339"
                        ],
                        "name": "Joe Frankel",
                        "slug": "Joe-Frankel",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Frankel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joe Frankel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783569"
                        ],
                        "name": "Simon King",
                        "slug": "Simon-King",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon King"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "The model expressed in (2) provides clear contrast to the trajectory or trended models where the time-varying acoustic features are approximated as an explicit temporal function of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2387071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edd93314ceea5d36a2f51dd3dfe645e77b413981",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The hidden Markov model (HMM) has proven to be the model which has made large-vocabulary automatic speech recognition (ASR) possible. The HMM is robust, versatile and has at its disposal a host of efficient algorithms which deal with training, speaker adaptation and recognition. However, there is nothing uniquely speech orientated about the HMM. In fact, certain assumptions are made of speech which are known to be untrue. For example, speech is modelled as a piecewise stationary process when we know it to be continuous. Also, co-articulation, which should be a rich source of information, simply provides unwanted variation. This variation is generally taken into account by modelling every phone in every context which in turn leads to problems of data sparcity, making elaborate parameter tying schemes necessary. Speech is generally modelled in a parametrised version of the acoustic domain, which is natural given that this is the data we have most ready access to. Any practical speech recogniser must of course take acoustic waveforms as input, however to take these in isolation from the production mechanism which created them ignores a rich source of prior knowledge. We propose that modelling speech in the articulatory domain using linear dynamic models (see section 4) will address some of these issues. The data here consists of trajectories which evolve smoothly over time, namely coordinates of points on the articulators. Effects such as coarticulation and assimilation are most simply described in articulatory terms, as opposed to in acoustic terms where they are confounded with the representation. Models that work in the articulatory domain are therefore able to explicitly model these phenomena. We have access to real articulatory data, collected by Alan Wrench at Queen Margaret College, Edinburgh (see [1] for further details). This has been used to train neural networks to recover articulatory traces from the acoustics. In our experiments we have used both real and automatically recovered articulation."
            },
            "slug": "ASR-articulatory-speech-recognition-Frankel-King",
            "title": {
                "fragments": [],
                "text": "ASR - articulatory speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proposed that modelling speech in the articulatory domain using linear dynamic models (see section 4) will address some of the issues of data sparcity, making elaborate parameter tying schemes necessary."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "While our overall research program has been aimed at a comprehensive framework, where a detailed hierarchy in speech generation is exploited (see some descriptions in [9], [13], [16]), the approach presented in this paper is a significantly simplified version."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16224396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15754483b95c5386fb145751ad3b054761207451",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Major speech production models from speech science literature and a number of popular statistical \u201cgenerative\u201d models of speech used in speech technology are surveyed. Strengths and weaknesses of these two styles of speech models are analyzed, pointing to the need to integrate the respective strengths while eliminating the respective weaknesses. As an example, a statistical task-dynamic model of speech production is described, motivated by the original deterministic version of the model and targeted for integrated-multilingual speech recognition applications. Methods for model parameter learning (training) and for likelihood computation (recognition) are described based on statistical optimization principles integrated in neural network and dynamic system theories."
            },
            "slug": "Computational-Models-for-Speech-Production-Deng",
            "title": {
                "fragments": [],
                "text": "Computational Models for Speech Production"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A statistical task-dynamic model of speech production is described, motivated by the original deterministic version of the model and targeted for integrated-multilingual speech recognition applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318623"
                        ],
                        "name": "M. Aksmanovic",
                        "slug": "M.-Aksmanovic",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Aksmanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aksmanovic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16554353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "093c5822ebb8a02e8d1fd327f77a058871781432",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend the nonstationary-state or trended hidden Markov model (HMM) from the previous single-trend formulation (Deng, 1992; Deng et al., 1994) to the current mixture-trended one. This extension is motivated by the observation of wide variations in the trajectories of the acoustic data in fluent, speaker-independent speech associated with a fixed underlying linguistic unit. It is also motivated by potential use of mixtures of trend functions to characterize heterogeneous time-varying data generated from distinctive sources such as the speech signals collected from different microphones or from different telephone channels. We show how HMMs with mixtures of trend functions can be implemented simply in the already well-established single-trend HMM framework via the device of expanding each state into a set of parallel states. Details of a maximum-likelihood-based (ML-based) algorithm are given for estimating state-dependent mixture trajectory parameters in the model. Experimental results on the task of classifying speaker-independent vowels excised from the TIMIT data base demonstrate consistent performance improvement using phonemic mixture-trended HMMs over their single-trend counterpart."
            },
            "slug": "Speaker-independent-phonetic-classification-using-Deng-Aksmanovic",
            "title": {
                "fragments": [],
                "text": "Speaker-independent phonetic classification using hidden Markov models with mixtures of trend functions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown how HMMs with mixtures of trend functions can be implemented simply in the already well-established single-trend HMM framework via the device of expanding each state into a set of parallel states."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47825073"
                        ],
                        "name": "Weiqi Wang",
                        "slug": "Weiqi-Wang",
                        "structuredName": {
                            "firstName": "Weiqi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiqi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1999488"
                        ],
                        "name": "M. Harper",
                        "slug": "M.-Harper",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Harper",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Harper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13908253,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "1ac8987534ad3be87d4b70195c1beb039b102409",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Structured language models have recently been shown to give significant improvements in large-vocabulary recognition relative to traditional word N-gram models, but typically imply a heavy computational burden and have not been applied to large training sets or complex recognition systems. Previously, we developed a linguistically motivated and computationally efficient almost-parsing language model, using a data structure derived from constraint dependency grammar parsing, that tightly integrates knowledge of words, lexical features, and syntactic constraints. We show that such a model can be used effectively and efficiently in all stages of a complex, multi-pass conversational telephone speech recognition system. Compared to a state-of-the-art 4-gram interpolated word- and class-based language model, we obtained a 6.2% relative word error reduction (a 1.6% absolute reduction) on a recent NIST evaluation set."
            },
            "slug": "The-use-of-a-linguistically-motivated-language-in-Wang-Stolcke",
            "title": {
                "fragments": [],
                "text": "The use of a linguistically motivated language model in conversational speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A linguistically motivated and computationally efficient almost-parsing language model is developed, using a data structure derived from constraint dependency grammar parsing, that tightly integrates knowledge of words, lexical features, and syntactic constraints in all stages of a complex, multi-pass conversational telephone speech recognition system."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702029"
                        ],
                        "name": "R. Chengalvarayan",
                        "slug": "R.-Chengalvarayan",
                        "structuredName": {
                            "firstName": "Rathinavelu",
                            "lastName": "Chengalvarayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chengalvarayan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2239066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eaed58bf31f03e69d27817fec2562ece1bd67943",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In this paper we report our development of a new class of hidden Markov models (HMMs) with each state characterized by a time series model which is non-stationary up to the second order. A close-form solution for the model parameter estimation is obtained based on the EM algorithm and on the matrix-calculus implementation technique. In the first set of evaluation experiments, we adopt the residual square sum, over states and over time frames within state bounds, as a quantitative measure for goodness of fit between the model and the speech data. It is observed that inclusion of state-conditioned second-order non-stationarity, implemented by use of time-varying regression coefficients, has substantially greater effects on reducing data-fitting error than increase of the regression terms while maintaining the coefficients of each term constant. In the second set, isolated-word recognition experiments, it is found that use of mix of first-order and second-order non-stationarities consistently produces higher recognition accuracy than the conventional, stationary-state HMMs."
            },
            "slug": "A-Markov-model-containing-state-conditioned-to-Deng-Chengalvarayan",
            "title": {
                "fragments": [],
                "text": "A Markov model containing state-conditioned second-order non-stationarity: application to speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A close-form solution for the model parameter estimation is obtained based on the EM algorithm and on the matrix-calculus implementation technique and it is found that use of mix of first-order and second-order non-stationarities consistently produces higher recognition accuracy than the conventional, stationary-state HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318623"
                        ],
                        "name": "M. Aksmanovic",
                        "slug": "M.-Aksmanovic",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Aksmanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aksmanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49816456"
                        ],
                        "name": "Xiaodong Sun",
                        "slug": "Xiaodong-Sun",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111535064"
                        ],
                        "name": "C. F. J. Wu",
                        "slug": "C.-F.-J.-Wu",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Wu",
                            "middleNames": [
                                "F.",
                                "Jeff"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. F. J. Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "However, the lengths of such segments are typically short."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17594966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef9cfb852ad0a494ee90e4bcd4f34022819d5359",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Proposes, implements, and evaluates a class of nonstationary-state hidden Markov models (HMMs) having each state associated with a distinct polynomial regression function of time plus white Gaussian noise. The model represents the transitional acoustic trajectories of speech in a parametric manner, and includes the standard stationary-state HMM as a special, degenerated case. The authors develop an efficient dynamic programming technique which includes the state sojourn time as an optimization variable, in conjunction with a state-dependent orthogonal polynomial regression method, for estimating the model parameters. Experiments on fitting models to speech data and on limited-vocabulary speech recognition demonstrate consistent superiority of these nonstationary-state HMMs over the traditional stationary-state HMMs. >"
            },
            "slug": "Speech-recognition-using-hidden-Markov-models-with-Deng-Aksmanovic",
            "title": {
                "fragments": [],
                "text": "Speech recognition using hidden Markov models with polynomial regression functions as nonstationary states"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The authors develop an efficient dynamic programming technique which includes the state sojourn time as an optimization variable, in conjunction with a state-dependent orthogonal polynomial regression method, for estimating the model parameters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748118"
                        ],
                        "name": "J. Bilmes",
                        "slug": "J.-Bilmes",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Bilmes",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bilmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777223"
                        ],
                        "name": "C. Bartels",
                        "slug": "C.-Bartels",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Bartels",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bartels"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18153514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da6f34e4cec54aa107cf6e64b501750b808d4035",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This article discusses the foundations of the use of graphical models for speech recognition as presented in J. R. Deller et al. (1993), X. D. Huang et al. (2001), F. Jelinek (19970, L. R. Rabiner and B. -H. Juang (1993) and S. Young et al. (1990) giving detailed accounts of some of the more successful cases. Our discussion employs dynamic Bayesian networks (DBNs) and a DBN extension using the Graphical Model Toolkit's (GMTK's) basic template, a dynamic graphical model representation that is more suitable for speech and language systems. While this article concentrates on speech recognition, it should be noted that many of the ideas presented here are also applicable to natural language processing and general time-series analysis."
            },
            "slug": "Graphical-model-architectures-for-speech-Bilmes-Bartels",
            "title": {
                "fragments": [],
                "text": "Graphical model architectures for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This discussion employs dynamic Bayesian networks (DBNs) and a DBN extension using the Graphical Model Toolkit's (GMTK's) basic template, a dynamic graphical model representation that is more suitable for speech and language systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145058181"
                        ],
                        "name": "H. Hon",
                        "slug": "H.-Hon",
                        "structuredName": {
                            "firstName": "Hsiao-Wuen",
                            "lastName": "Hon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748169"
                        ],
                        "name": "Kuansan Wang",
                        "slug": "Kuansan-Wang",
                        "structuredName": {
                            "firstName": "Kuansan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuansan Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14420673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dc2ba471eb1a8af21c7232b3f08f52dfccc5c24",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an analytically tractable framework that integrates the frame and segment based acoustic modeling techniques. We combine the two approaches by jointly modeling their respective hidden Markov processes. Since the joint process is based on the same mathematical framework, conventional search and training techniques, such as Viterbi and EM algorithms, can be directly applied. It also allows the score from either model to contribute to the training and decoding of the other, reaching a jointly optimal decision. We conducted two series of experiments to verify our hypotheses. In the phone-pair classification experiments, our segment models show a 24% error reduction over state-of-the-art HMM-based system. The superior quality of segment models contributes to an 8.2% reduction in word error rates for the unified system on the WSJ dictation task."
            },
            "slug": "Unified-frame-and-segment-based-models-for-speech-Hon-Wang",
            "title": {
                "fragments": [],
                "text": "Unified frame and segment based models for automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An analytically tractable framework that integrates the frame and segment based acoustic modeling techniques by jointly modeling their respective hidden Markov processes is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693612"
                        ],
                        "name": "K. Sim",
                        "slug": "K.-Sim",
                        "structuredName": {
                            "firstName": "Khe",
                            "lastName": "Sim",
                            "middleNames": [
                                "Chai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740397"
                        ],
                        "name": "M. Gales",
                        "slug": "M.-Gales",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gales",
                            "middleNames": [
                                "John",
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gales"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9130108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4150b925883111297d92fbad5aa0af45997dd494",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Many forms of time varying acoustic models have been applied to the area of speech recognition. However, there has been little success in applying these models to Large Vocabulary Continuous Speech Recognition (LVCSR). Recently, fMPE was introduced as a discriminative feature space estimation scheme for the HMM-based LVCSR. This method estimates a projection matrix from a high dimensional space (\u223c 100,000) down to a standard feature space (typically 39). This projection is then added on to the original feature vector (e.g. MFCC or PLP) to yield a feature vector to train the final model. This paper considers fMPE as a time varying model for the mean vectors by applying the time varying feature offset to the Gaussian mean vectors. This approach naturally yields the update formulae for fMPE and motivates an alternative style of training systems. This concept is then extended to the temporal precision matrix modelling (pMPE). In pMPE, a temporally varying positive scale is applied to each element of the diagonal precision matrices. Experimental results are presented on a conversational telephone speech English task."
            },
            "slug": "Temporally-varying-model-parameters-for-large-Sim-Gales",
            "title": {
                "fragments": [],
                "text": "Temporally varying model parameters for large vocabulary continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "FMPE is considered as a time varying model for the mean vectors by applying the time varying feature offset to the Gaussian mean vectors and this approach naturally yields the update formulae for fMPE and motivates an alternative style of training systems."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222107"
                        ],
                        "name": "O. Ghitza",
                        "slug": "O.-Ghitza",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Ghitza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Ghitza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2022 Nonparametric trended HMM: The trend function is determined by the training data after performing dynamic time warping [ 28 ], rather than by any parametric form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206560859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23948218cabf80c3ee0f9efd0c77c3ffc042f18a",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In most implementations of hidden Markov models (HMMs) a state is assumed to be a stationary random sequence of observation vectors whose mean and covariance are estimated. Successive observations in a state are assumed to be independent and identically distributed. These assumptions are reasonable when each state represents a short segment of the speech signal. When states represent longer portions of the signal (e.g. phonemes, diphones, etc.) both assumptions are inaccurate. Recently, some attempts have been made to incorporate correlations between successive observations in a state. But to our knowledge, non-stationarity has not been dealt with. We propose an alternative representation in which a state of an HMM is defined as a template, i.e. a \"typical\" sequence of observations. The template for a state is derived from an ensemble of segments corresponding to that state. In our present implementation, the observations are 11th-order cepstrum vectors plus energy, states represent diphones and ensembles of the diphones are obtained from a hand-labeled speaker-dependent database of 2000 sentences spoken fluently. The probability of a test sequence being generated in a given state is obtained by time-warping the test utterance to the template, and assuming the differences between the corresponding observations to have a joint distribution. Tests on 50 sentences (outside the training set) indicate a correct recognition rate for phonemes of about 70%."
            },
            "slug": "Hidden-Markov-models-with-templates-as-states:-an-Ghitza-Sondhi",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models with templates as non-stationary states: an application to speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work proposes an alternative representation of hidden Markov models in which a state of an HMM is defined as a template, i.e. a \"typical\" sequence of observations, derived from an ensemble of segments corresponding to that state."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17319939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b66595abb4f737e2352482aa31b41b5b8a3ddf21",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-network-structures-and-inference-for-Zweig",
            "title": {
                "fragments": [],
                "text": "Bayesian network structures and inference techniques for automatic speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786323"
                        ],
                        "name": "R. Rose",
                        "slug": "R.-Rose",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Rose",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144276222"
                        ],
                        "name": "J. Schroeter",
                        "slug": "J.-Schroeter",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Schroeter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schroeter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 502726,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d574ddf984be1456e1df7dc5cdd16be569cd9e6d",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the issues that are associated with applying speech production models to automatic speech recognition (ASR). Here the applicability of articulatory representations to ASR is considered independently of the role of articulatory representations in speech perception. While the question of whether it is necessary or even possible for human listeners to recover the state of the articulators during the process of perceiving speech is an important one, it is not considered here. Hence, the authors refrain from posing completely new paradigms for ASR which more closely parallel the relationship between speech production and human speech understanding. Instead, work aimed at integrating speech production models into existing ASR formalisms is described."
            },
            "slug": "The-potential-role-of-speech-production-models-in-Rose-Schroeter",
            "title": {
                "fragments": [],
                "text": "The potential role of speech production models in automatic speech recognition."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors refrain from posing completely new paradigms for ASR which more closely parallel the relationship between speech production and human speech understanding, and work aimed at integrating speech production models into existing ASR formalisms is described."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5847709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6df5c972ed687aefe41637f3b0433c7dd44a0b6",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-probabilistic-framework-for-segment-based-speech-Glass",
            "title": {
                "fragments": [],
                "text": "A probabilistic framework for segment-based speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702029"
                        ],
                        "name": "R. Chengalvarayan",
                        "slug": "R.-Chengalvarayan",
                        "structuredName": {
                            "firstName": "Rathinavelu",
                            "lastName": "Chengalvarayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chengalvarayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15465982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27a81dc3932c5d84e3522664deba7b89d185c939",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A formulation of the maximum a posteriori (MAP) approach to speaker adaptation is presented with use of the trended or nonstationary-state hidden Markov model (HMM), where the Gaussian means in each HMM state are characterized by time-varying polynomial trend functions of the state sojourn time. Assuming uncorrelatedness among the polynomial coefficients in the trend functions, we have obtained analytical results for the MAP estimates of the parameters including time-varying means and time-invariant precisions. We have implemented a speech recognizer based on these results in speaker adaptation experiments using the TI46 corpora. The experimental evaluation demonstrates that the trended HMM, with use of either the linear or the quadratic polynomial trend function, consistently outperforms the conventional, stationary-state HMM. The evaluation also shows that the unadapted, speaker-independent models are outperformed by the models adapted by the MAP procedure under supervision with as few as a single adaptation token. Further, adaptation of polynomial coefficients alone is shown to be better than adapting both polynomial coefficients and precision matrices when fewer than four adaptation tokens are used, while the reverse is found with a greater number of adaptation tokens."
            },
            "slug": "A-maximum-a-posteriori-approach-to-speaker-using-Chengalvarayan-Deng",
            "title": {
                "fragments": [],
                "text": "A maximum a posteriori approach to speaker adaptation using the trended hidden Markov model"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Approximate results for the maximum a posteriori estimates of the parameters including time-varying means and time-invariant precisions are obtained and a speech recognizer is implemented based on these results."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554435"
                        ],
                        "name": "Issam Bazzi",
                        "slug": "Issam-Bazzi",
                        "structuredName": {
                            "firstName": "Issam",
                            "lastName": "Bazzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Issam Bazzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17753064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd940012d89155126dde0534955c977cdd1e6905",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new technique for high-accuracy tracking of vocal-tract resonances (which coincide with formants for nonnasalized vowels) in natural speech. The technique is based on a discretized nonlinear prediction function, which is embedded in a temporal constraint on the quantized input values over adjacent time frames as the prior knowledge for their temporal behavior. The nonlinear prediction is constructed, based on its analytical form derived in detail in this paper, as a parameter-free, discrete mapping function that approximates the \u201cforward\u201d relationship from the resonance frequencies and bandwidths to the Linear Predictive Coding (LPC) cepstra of real speech. Discretization of the function permits the \u201cinversion\u201d of the function via a search operation. We further introduce the nonlinear-prediction residual, characterized by a multivariate Gaussian vector with trainable mean vectors and covariance matrices, to account for the errors due to the functional approximation. We develop and describe an expectation\u2013maximization (EM)-based algorithm for training the parameters of the residual, and a dynamic programming-based algorithm for resonance tracking. Details of the algorithm implementation for computation speedup are provided. Experimental results are presented which demonstrate the effectiveness of our new paradigm for tracking vocal-tract resonances. In particular, we show the effectiveness of training the prediction-residual parameters in obtaining high-accuracy resonance estimates, especially during consonantal closure."
            },
            "slug": "Tracking-Vocal-Tract-Resonances-Using-a-Quantized-a-Deng-Acero",
            "title": {
                "fragments": [],
                "text": "Tracking Vocal Tract Resonances Using a Quantized Nonlinear Function Embedded in a Temporal Constraint"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new technique for high-accuracy tracking of vocal-tract resonances (which coincide with formants for nonnasalized vowels) in natural speech is presented, based on a discretized nonlinear prediction function which is embedded in a temporal constraint on the quantized input values over adjacent time frames as the prior knowledge for their temporal behavior."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102981024"
                        ],
                        "name": "A. Poritz",
                        "slug": "A.-Poritz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Poritz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Poritz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62479678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6627d8efde3ed55e34ccee059eb6cdac99bb2fe",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov modeling is a probabilistic technique for the study of time series. Hidden Markov theory permits modeling with any of the classical probability distributions. The costs of implementation are linear in the length of data. Models can be nested to reflect hierarchical sources of knowledge. These and other desirable features have made hidden Markov methods increasingly attractive for problems in language, speech and signal processing. The basic ideas are introduced by elementary examples in the spirit of the Polya urn models. The main tool in hidden Markov modeling is the Baum-Welch (or forward-backward) algorithm for maximum likelihood estimation of the model parameters. This iterative algorithm is discussed both from an intuitive point of view as an exercise in the art of counting and from a formal point of view via the information-theoretic Q-function. Selected examples drawn from the literature illustrate how the Baum-Welch technique places a rich variety of computational models at the disposal of the researcher.<<ETX>>"
            },
            "slug": "Hidden-Markov-models:-a-guided-tour-Poritz",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models: a guided tour"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The main tool in hidden Markov modeling is the Baum-Welch algorithm for maximum likelihood estimation of the model parameters, which is discussed both from an intuitive point of view as an exercise in the art of counting and from a formalpoint of view via the information-theoretic Q-function."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144179113"
                        ],
                        "name": "K. Stevens",
                        "slug": "K.-Stevens",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stevens",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1811670,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8f0da8dd0dec86f20d6879d86392935ff81240e8",
            "isKey": false,
            "numCitedBy": 524,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a model in which the acoustic speech signal is processed to yield a discrete representation of the speech stream in terms of a sequence of segments, each of which is described by a set (or bundle) of binary distinctive features. These distinctive features specify the phonemic contrasts that are used in the language, such that a change in the value of a feature can potentially generate a new word. This model is a part of a more general model that derives a word sequence from this feature representation, the words being represented in a lexicon by sequences of feature bundles. The processing of the signal proceeds in three steps: (1) Detection of peaks, valleys, and discontinuities in particular frequency ranges of the signal leads to identification of acoustic landmarks. The type of landmark provides evidence for a subset of distinctive features called articulator-free features (e.g., [vowel], [consonant], [continuant]). (2) Acoustic parameters are derived from the signal near the landmarks to provide evidence for the actions of particular articulators, and acoustic cues are extracted by sampling selected attributes of these parameters in these regions. The selection of cues that are extracted depends on the type of landmark and on the environment in which it occurs. (3) The cues obtained in step (2) are combined, taking context into account, to provide estimates of \"articulator-bound\" features associated with each landmark (e.g., [lips], [high], [nasal]). These articulator-bound features, combined with the articulator-free features in (1), constitute the sequence of feature bundles that forms the output of the model. Examples of cues that are used, and justification for this selection, are given, as well as examples of the process of inferring the underlying features for a segment when there is variability in the signal due to enhancement gestures (recruited by a speaker to make a contrast more salient) or due to overlap of gestures from neighboring segments."
            },
            "slug": "Toward-a-model-for-lexical-access-based-on-acoustic-Stevens",
            "title": {
                "fragments": [],
                "text": "Toward a model for lexical access based on acoustic landmarks and distinctive features."
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A model in which the acoustic speech signal is processed to yield a discrete representation of the speech stream in terms of a sequence of segments, each of which is described by a set (or bundle) of binary distinctive features."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47431383"
                        ],
                        "name": "Y. Minami",
                        "slug": "Y.-Minami",
                        "structuredName": {
                            "firstName": "Yasuhiro",
                            "lastName": "Minami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Minami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452389"
                        ],
                        "name": "E. McDermott",
                        "slug": "E.-McDermott",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "McDermott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726970"
                        ],
                        "name": "A. Nakamura",
                        "slug": "A.-Nakamura",
                        "structuredName": {
                            "firstName": "Atsushi",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715709"
                        ],
                        "name": "S. Katagiri",
                        "slug": "S.-Katagiri",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Katagiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katagiri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5123412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3977ec8e0d5e7165ca86b26761623ffcb694315",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We have proposed a new speech recognition technique that generates a speech trajectory from HMMs by maximizing the likelihood of the trajectory, while accounting for the relation between the cepstrum and the dynamic cepstrum coefficients. This method has the major advantage that the relation, which is ignored in conventional speech recognition, is directly used in the speech recognition phase. This paper describes an extension of the method for dealing with HMMs whose distributions are mixture Gaussian distributions. The method chooses the sequence of Gaussian distributions by selecting the best Gaussian distribution in the state during Viterbi decoding. Speaker-independent speech recognition experiments were carried out. The proposed method obtained an 18.2% reduction in error rate for the task, proving that the proposed method is effective even for Gaussian mixture HMMs."
            },
            "slug": "Recognition-method-with-parametric-trajectory-from-Minami-McDermott",
            "title": {
                "fragments": [],
                "text": "Recognition method with parametric trajectory generated from mixture distribution HMMs"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An extension of the method for dealing with HMMs whose distributions are mixture Gaussian distributions, which chooses the sequence ofGaussian distributions by selecting the best Gaussian distribution in the state during Viterbi decoding, achieves an 18.2% reduction in error rate."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143632131"
                        ],
                        "name": "P. Kenny",
                        "slug": "P.-Kenny",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Kenny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kenny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2959613"
                        ],
                        "name": "Matthew Lennig",
                        "slug": "Matthew-Lennig",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Lennig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Lennig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791674"
                        ],
                        "name": "P. Mermelstein",
                        "slug": "P.-Mermelstein",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Mermelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mermelstein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9833220,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "8c2218c2cd3efb1454b6c4f512fd9f7a61029ddf",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a new type of Markov model developed to account for the correlations between successive frames of a speech signal. The idea is to treat the sequence of frames as a nonstationary autoregressive process whose parameters are controlled by a hidden Markov chain. It is shown that this type of model performs better than the standard multivariate Gaussian HMM (hidden Markov model) when it is incorporated into a large-vocabulary isolated-word recognizer. >"
            },
            "slug": "A-linear-predictive-HMM-for-vector-valued-with-to-Kenny-Lennig",
            "title": {
                "fragments": [],
                "text": "A linear predictive HMM for vector-valued observations with applications to speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A new type of Markov model developed to account for the correlations between successive frames of a speech signal that performs better than the standard multivariate Gaussian HMM (hidden Markov models) when it is incorporated into a large-vocabulary isolated-word recognizer."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076687306"
                        ],
                        "name": "L. Elliot",
                        "slug": "L.-Elliot",
                        "structuredName": {
                            "firstName": "L",
                            "lastName": "Elliot",
                            "middleNames": [
                                "Saltzman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Elliot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66756916"
                        ],
                        "name": "G. Kevin",
                        "slug": "G.-Kevin",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "Kevin",
                            "middleNames": [
                                "Munhall"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kevin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8093044,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d2c16e1973d832e507bcda4c3a731385b8aa0d2c",
            "isKey": false,
            "numCitedBy": 867,
            "numCiting": 170,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we attempt to reconcile the linguistic hypothesis that speech involves an underlying sequencing of abstract, discrete, context-independent units, with the empirical observation of continuous, context-dependent interleaving of articulatory movements. To this end, we first review a previously proposed task-dynamic model for the coordination and control of the speech articulators. We then describe an extension of this model in which invariant speech units (gestural primitives) are identified with context-independent sets of parameters in a dynamical system having two functionally distinct but interacting levels. The intergestural level is defined according to a set of activation coordinates; the interarticulator level is defined according to both model articulator and tractvariable coordinates. In the framework of this extended model, coproduction effects in speech are described in terms of the blending dynamics defined among a set of temporally overlapping active units; the relative timing ..."
            },
            "slug": "A-Dynamical-Approach-to-Gestural-Patterning-in-Elliot-Kevin",
            "title": {
                "fragments": [],
                "text": "A Dynamical Approach to Gestural Patterning in Speech Production"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This article attempts to reconcile the linguistic hypothesis that speech involves an underlying sequencing of abstract, discrete, context-independent units, with the empirical observation of continuous, Context-dependent interleaving of articulatory movements."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40031121"
                        ],
                        "name": "Yuqing Gao",
                        "slug": "Yuqing-Gao",
                        "structuredName": {
                            "firstName": "Yuqing",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuqing Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762024"
                        ],
                        "name": "R. Bakis",
                        "slug": "R.-Bakis",
                        "structuredName": {
                            "firstName": "Raimo",
                            "lastName": "Bakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145739831"
                        ],
                        "name": "Jing Huang",
                        "slug": "Jing-Huang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028698"
                        ],
                        "name": "Bing Xiang",
                        "slug": "Bing-Xiang",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "The model expressed in (2) provides clear contrast to the trajectory or trended models where the time-varying acoustic features are approximated as an explicit temporal function of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17948231,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8b94245dbed475d4218dfa988b38f0534ac1a3bf",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a multi-stage speech production model containing a linear, phoneme-independent coarticulation lter, followed by a nonlinear component. The latter generates two cepstra which are then additively combined: one corresponding to a relatively smooth background spectrum, and the other representing three formant-like spectral peaks. A neural net is used for both parts, but the second part also utilizes a hard-coded function that generates exactly three spectral peaks. A uni ed model of training, adaptation, and decoding is developed, each operation di ering only with respect to prior probability distributions. Prior probabilities can be introduced at each stage of the model, providing a exible framework for utilizing both speci c and general prior knowledge. We demonstrate the use of this model for speech synthesis as well as recognition."
            },
            "slug": "Multistage-coarticulation-model-combining-formant-Gao-Bakis",
            "title": {
                "fragments": [],
                "text": "Multistage coarticulation model combining articulatory, formant and cepstral features"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A multi-stage speech production model containing a linear, phoneme-independent coarticulation lter, followed by a nonlinear component that generates two cepstra which are then additively combined: one corresponding to a relatively smooth background spectrum, and the other representing three formant-like spectral peaks."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393499"
                        ],
                        "name": "Chak-Fai Li",
                        "slug": "Chak-Fai-Li",
                        "structuredName": {
                            "firstName": "Chak-Fai",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chak-Fai Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143882614"
                        ],
                        "name": "M. Siu",
                        "slug": "M.-Siu",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Siu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Siu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 203
                            }
                        ],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 56
                            }
                        ],
                        "text": "However, the lengths of such segments are typically short."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206738255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04d48396a03d7a65b7be87054f712258ef685400",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The polynomial segment model (PSM), which was first proposed in Gish et al. (1993) and subsequently studied by other researchers, has opened up an alternative research direction for speech recognition. In PSM, speech frames within a segment are jointly modeled such that any change in the boundaries of a segment would require the re-computation of the likelihood of all the frames within the segment. While estimation of the best segment boundaries are possible, the computation consideration typically constrains the PSM model to limit the search to center around some pre-segmentation typically obtained by using another model such as an HMM, in effect limiting the possibility of using PSM itself. In this paper we introduce a new approach to evaluate the likelihood of a PSM segment by efficiently \"accumulating\" segment likelihood incrementally, i.e. one frame at a time. Based on this incremental likelihood evaluation, an efficient PSM search and training algorithm are also introduced. We show the effectiveness of the incremental likelihood evaluation by building a PSM-based TIMIT recognition system (both training and test) without the need of using another model for pre-segmentation."
            },
            "slug": "An-efficient-incremental-likelihood-evaluation-for-Li-Siu",
            "title": {
                "fragments": [],
                "text": "An efficient incremental likelihood evaluation for polynomial trajectory model using with application to model training and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new approach to evaluate the likelihood of a PSM segment by efficiently \"accumulating\" segment likelihood incrementally, i.e. one frame at a time is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802969"
                        ],
                        "name": "Ciprian Chelba",
                        "slug": "Ciprian-Chelba",
                        "structuredName": {
                            "firstName": "Ciprian",
                            "lastName": "Chelba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciprian Chelba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14339957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1c3748820d6b5ab4e7334524815df9bb6d20aed",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood re-estimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal and Switchboard corpora show improvement in both perplexity and word error rate?word lattice rescoring?over the standard 3-gram language model."
            },
            "slug": "Structured-language-modeling-Chelba-Jelinek",
            "title": {
                "fragments": [],
                "text": "Structured language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "An attempt at using the syntactic structure in natural language for improved language models for speech recognition using an original probabilistic parameterization of a shift-reduce parser."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9353491"
                        ],
                        "name": "Qifeng Zhu",
                        "slug": "Qifeng-Zhu",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Qifeng Zhu",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifeng Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145244442"
                        ],
                        "name": "K. Sonmez",
                        "slug": "K.-Sonmez",
                        "structuredName": {
                            "firstName": "Kemal",
                            "lastName": "Sonmez",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sonmez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739225"
                        ],
                        "name": "S. Sivadas",
                        "slug": "S.-Sivadas",
                        "structuredName": {
                            "firstName": "Sunil",
                            "lastName": "Sivadas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sivadas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732454"
                        ],
                        "name": "T. Shinozaki",
                        "slug": "T.-Shinozaki",
                        "structuredName": {
                            "firstName": "Takahiro",
                            "lastName": "Shinozaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shinozaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066975404"
                        ],
                        "name": "P. Jain",
                        "slug": "P.-Jain",
                        "structuredName": {
                            "firstName": "Pratibha",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745455"
                        ],
                        "name": "D. Ellis",
                        "slug": "D.-Ellis",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ellis",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ellis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862682"
                        ],
                        "name": "G. Doddington",
                        "slug": "G.-Doddington",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Doddington",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Doddington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157307424"
                        ],
                        "name": "B. Chen",
                        "slug": "B.-Chen",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9376398"
                        ],
                        "name": "O. Cretin",
                        "slug": "O.-Cretin",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Cretin",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Cretin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705797"
                        ],
                        "name": "M. Athineos",
                        "slug": "M.-Athineos",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Athineos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Athineos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [ 40 ], [42], [49], [53], [59])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15156045,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d9d2ba2003d7324ae3d5ff7423a13f13efc79ca5",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite successes, there are still significant limitations to speech recognition performance, particularly for conversational speech and/or for speech with significant acoustic degradations from noise or reverberation. For this reason, authors have proposed methods that incorporate different (and larger) analysis windows, which are described in this article. Note in passing that we and many others have already taken advantage of processing techniques that incorporate information over long time ranges, for instance for normalization (by cepstral mean subtraction as stated in B. Atal (1974) or relative spectral analysis (RASTA) based in H. Hermansky and N. Morgan (1994)). They also have proposed features that are based on speech sound class posterior probabilities, which have good properties for both classification and stream combination."
            },
            "slug": "Pushing-the-envelope-aside-[speech-recognition]-Morgan-Zhu",
            "title": {
                "fragments": [],
                "text": "Pushing the envelope - aside [speech recognition]"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Authors have proposed methods that incorporate different analysis windows that incorporate features that are based on speech sound class posterior probabilities, which have good properties for both classification and stream combination."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788590"
                        ],
                        "name": "T. Kitamura",
                        "slug": "T.-Kitamura",
                        "structuredName": {
                            "firstName": "Tadashi",
                            "lastName": "Kitamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kitamura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1650776,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "5707cc05d5ae25e2958afe6a20197bf8c069e7c8",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a Viterbi algorithm to obtain a sub-optimal state sequence for trajectory-HMM, which is derived from HMM with explicit relationship between static and dynamic features. The trajectory-HMM can alleviate some limitations of HMM, which are (i) constant statistics within HMM state and (ii) conditional independence of observations given the state sequence, without increasing the number of model parameters. The proposed algorithm was applied to state-boundary optimization for Viterbi training and N-best rescoring. In a speaker-dependent continuous speech recognition experiment, trajectory-HMM with the proposed algorithm achieved about 14% error reduction over the standard HMM with the conventional Viterbi algorithm."
            },
            "slug": "A-Viterbi-algorithm-for-a-trajectory-model-derived-Zen-Tokuda",
            "title": {
                "fragments": [],
                "text": "A Viterbi algorithm for a trajectory model derived from HMM with explicit relationship between static and dynamic features"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A Viterbi algorithm is introduced to obtain a sub-optimal state sequence for trajectory-HMM, which is derived from HMM with explicit relationship between static and dynamic features, without increasing the number of model parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2969364"
                        ],
                        "name": "M. Akagi",
                        "slug": "M.-Akagi",
                        "structuredName": {
                            "firstName": "Masato",
                            "lastName": "Akagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Akagi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 238179314,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "87729a04b2a625ddb95a81c74c67af9ca1c73c79",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a model of a lower level contextual effect that can cope with coarticulation problems, especially vowel neutralization. The model is constructed to overshoot spectral peak trajectories based on spectral peak interaction, assuming that the lower level contextual effect is represented as the sum of interaction between each spectral peak pair. The interaction function is determined experimentally in order to reduce the distance between a real spectral peak and its target which is a spectral peak mean computed for vowel uttered in isolation. The interaction function thus determined suggests that: (1) there can be a time\u2010frequency lateral inhibition in the auditory system like that on the retina in the visual system, (2) the interaction function is consistent with the results of psychaocoustic experiments concerning the assimilation and/or contrast effect using paired single formant stimuli, and (3) the contextual effect between adjacent phonemes can be represented as the sum of the assimilation and/or contrast effects between each spectral peak pair. Applying the determined interaction function to real speech data to cope with coarticulation problem, spectral peak trajectories overshoot, spectral peaks at the vowel center approach their own targets, and the distance between each vowel category pair increases."
            },
            "slug": "Modeling-of-contextual-effects-based-on-spectral-Akagi",
            "title": {
                "fragments": [],
                "text": "Modeling of contextual effects based on spectral peak interaction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2254677"
                        ],
                        "name": "M. Pitermann",
                        "slug": "M.-Pitermann",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Pitermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pitermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21311200,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "dfa6a3803ac34b91ef1ae75d915f3a7d712bedb0",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Vowel formants play an important role in speech theories and applications; however, the same formant values measured for the steady-state part of a vowel can correspond to different vowel categories. Experimental evidence indicates that dynamic information can also contribute to vowel characterization. Hence, dynamically modeling formant transitions may lead to quantitatively testable predictions in vowel categorization. Because the articulatory strategy used to manage different speaking rates and contrastive stress may depend on speaker and situation, the parameter values of a dynamic formant model may vary with speaking rate and stress. In most experiments speaking rate is rarely controlled, only two or three rates are tested, and most corpora contain just a few repetitions of each item. As a consequence, the dependence of dynamic models on those factors is difficult to gauge. This article presents a study of 2300 [iai] or [i epsilon i] stimuli produced by two speakers at nine or ten speaking rates in a carrier sentence for two contrastive stress patterns. The corpus was perceptually evaluated by naive listeners. Formant frequencies were measured during the steady-state parts of the stimuli, and the formant transitions were dynamically and kinematically modeled. The results indicate that (1) the corpus was characterized by a contextual assimilation instead of a centralization effect; (2) dynamic or kinematic modeling was equivalent as far as the analysis of the model parameters was concerned; (3) the dependence of the model parameter estimates on speaking rate and stress suggests that the formant transitions were sharper for high speaking rate, but no consistent trend was found for contrastive stress; (4) the formant frequencies measured in the steady-state parts of the vowels were sufficient to explain the perceptual results while the dynamic parameters of the models were not."
            },
            "slug": "Effect-of-speaking-rate-and-contrastive-stress-on-Pitermann",
            "title": {
                "fragments": [],
                "text": "Effect of speaking rate and contrastive stress on formant dynamics and vowel perception."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A study of 2300 stimuli produced by two speakers at nine or ten speaking rates in a carrier sentence for two contrastive stress patterns indicates that the corpus was characterized by a contextual assimilation instead of a centralization effect and the formant frequencies measured in the steady-state parts of the vowels were sufficient to explain the perceptual results while the dynamic parameters of the models were not."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380069"
                        ],
                        "name": "B. Atal",
                        "slug": "B.-Atal",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Atal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Atal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29873367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bda2fdab71ac51887bd50f0283cffdb21750445",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for efficient coding of LPC log area parameters. It is now well recognized that sample-by-sample quantization of LPC parameters is not very efficient in minimizing the bit rate needed to code these parameters. Recent methods for reducing the bit rate have used vector and segment quantization methods. Much of the past work in this area has focussed on efficient coding of LPC parameters in the context of vocoders which put a ceiling on achievable speech quality. The results from these studies cannot be directly applied to synthesis of high quality speech. This paper describes a different approach to efficient coding of log area parameters. Our aim is to determine the extent to which the bit rate of LPC parameters can be reduced without sacrificing speech quality. Speech events occur generally at non-uniformly spaced time intervals. Moreover, some speech events are slow while others are fast. Uniform sampling of speech parameters is thus not efficient. We describe a non-uniform sampling and interpolation procedure for efficient coding of log area parameters. A temporal decomposition technique is used to represent the continuous variation of these parameters as a linearly-weighted sum of a number of discrete elementary components. The location and length of each component is automatically adapted to speech events. We find that each elementary component can be coded as a very low information rate signal."
            },
            "slug": "Efficient-coding-of-LPC-parameters-by-temporal-Atal",
            "title": {
                "fragments": [],
                "text": "Efficient coding of LPC parameters by temporal decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The aim is to determine the extent to which the bit rate of LPC parameters can be reduced without sacrificing speech quality."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33848125"
                        ],
                        "name": "J. C. Krause",
                        "slug": "J.-C.-Krause",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Krause",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950297"
                        ],
                        "name": "L. Braida",
                        "slug": "L.-Braida",
                        "structuredName": {
                            "firstName": "Louis",
                            "lastName": "Braida",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Braida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43436453,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d21788326d4dff79c620062e46b06dd5da4d2eda",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentences spoken \"clearly\" are significantly more intelligible than those spoken \"conversationally\" for hearing-impaired listeners in a variety of backgrounds [Picheny et al., J. Speech Hear. Res. 28, 96-103 (1985); Uchanski et al., ibid. 39, 494-509 (1996); Payton et al., J. Acoust. Soc. Am. 95, 1581-1592 (1994)]. While producing clear speech, however, talkers often reduce their speaking rate significantly [Picheny et al., J. Speech Hear. Res. 29, 434-446 (1986); Uchanski et al., ibid. 39, 494-509 (1996)]. Yet speaking slowly is not solely responsible for the intelligibility benefit of clear speech (over conversational speech), since a recent study [Krause and Braida, J. Acoust. Soc. Am. 112, 2165-2172 (2002)] showed that talkers can produce clear speech at normal rates with training. This finding suggests that clear speech has inherent acoustic properties, independent of rate, that contribute to improved intelligibility. Identifying these acoustic properties could lead to improved signal processing schemes for hearing aids. To gain insight into these acoustical properties, conversational and clear speech produced at normal speaking rates were analyzed at three levels of detail (global, phonological, and phonetic). Although results suggest that talkers may have employed different strategies to achieve clear speech at normal rates, two global-level properties were identified that appear likely to be linked to the improvements in intelligibility provided by clear/normal speech: increased energy in the 1000-3000-Hz range of long-term spectra and increased modulation depth of low frequency modulations of the intensity envelope. Other phonological and phonetic differences associated with clear/normal speech include changes in (1) frequency of stop burst releases, (2) VOT of word-initial voiceless stop consonants, and (3) short-term vowel spectra."
            },
            "slug": "Acoustic-properties-of-naturally-produced-clear-at-Krause-Braida",
            "title": {
                "fragments": [],
                "text": "Acoustic properties of naturally produced clear speech at normal speaking rates."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Two global-level properties were identified that appear likely to be linked to the improvements in intelligibility provided by clear/normal speech: increased energy in the 1000-3000-Hz range of long-term spectra and increased modulation depth of low frequency modulations of the intensity envelope."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081421227"
                        ],
                        "name": "Coarticulation \u2022 Suprasegmentals",
                        "slug": "Coarticulation-\u2022-Suprasegmentals",
                        "structuredName": {
                            "firstName": "Coarticulation",
                            "lastName": "Suprasegmentals",
                            "middleNames": [
                                "\u2022"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Coarticulation \u2022 Suprasegmentals"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "However, the essential aspect of the comprehensive framework\u2014the dynamic structure of speech\u2014remains unchanged in the current recognizer implementation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17983056,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "01f67cff10fa261ed69ff31ca71db6d10223e6fc",
            "isKey": false,
            "numCitedBy": 1722,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "teristics of speech. Speech consists of variations in air pressure that result from physical disturbances of air molecules caused by the flow of air out of the lungs. This airflow makes the air molecules alternately crowd together and move apart (oscillate), creating increases and decreases, respectively, in air pressure. The resulting sound wave transmits these changes in pressure from speaker to hearer. Sound waves can be described in terms of physical properties such as cycle, period, frequency, and amplitude. These concepts are most easily illustrated when considering a simple wave corresponding to a pure tone. A cycle is a sequence of one increase and one decrease in air pressure. A period is the amount of time (expressed in seconds or milliseconds) that one cycle takes. Frequency is the number of cycles in one second, expressed in hertz (Hz). An increase in frequency usually results in an increase in perceived pitch. Amplitude refers to the magnitude of vibrations, with larger vibrations resulting in greater peaks of pressure (greater amplitude), which usually result in an increase in perceived loudness. Unlike pure tones, which rarely occur in the environment, speech sounds are complex waves with combinations of different frequencies and amplitudes. However, as first stated by the French mathematician Fourier (1768\u20131830), any complex wave can be described as a combination of simple waves. A complex wave has a regular rate of repetition, known as the fundamental frequency (F0). Changes in F0 give rise to differences in perceived pitch, whereas changes in the number of constituent simple waves and their amplitude relations result in perceived differences in timbre or quality. Fourier\u2019s theorem enables us to describe speech sounds in terms of the frequency and amplitude of each of its constituent simple waves. Such a description is known as the spectrum of a sound. A spectrum is visually displayed as a plot of frequency vs. amplitude, with frequency represented from low to high along the horizontal axis and amplitude from low to high along the vertical axis. The usual energy source for speech is the airstream generated by the lungs. This steady flow of air is converted into brief puffs of air by the vibrating vocal folds, two muscular folds housed in the larynx. The dominant way of conceptualizing the process of speech production is in terms of the source-filter theory, according to which the acoustic characteristics of speech can be understood as a result of a source component and a filter component. The source component is determined by the rate of vocal fold vibration, which in turn is affected by a number of factors, including the rate of airflow and the mass and stiffness of the vocal folds. The rate of vocal fold vibration directly determines the F0 of the waveform. The mean F0 for adult women is approximately 220 Hz, and approximately 130 Hz for adult men. \u201cIn addition to their role as properties of individual speech sounds, F0 and amplitude also signal emphasis, stress, and intonation.\u201d For speech, the source component itself has a complex waveform, and its spectrum will typically show the highest energy at the lowest frequencies and a number of higher frequency components that"
            },
            "slug": "A-Acoustic-Phonetics-Suprasegmentals",
            "title": {
                "fragments": [],
                "text": "A Acoustic Phonetics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32018360"
                        ],
                        "name": "B. Lindblom",
                        "slug": "B.-Lindblom",
                        "structuredName": {
                            "firstName": "Bj\u00f6rn",
                            "lastName": "Lindblom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lindblom"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123566891,
            "fieldsOfStudy": [
                "Physics",
                "Linguistics"
            ],
            "id": "d7cf8fa9630222bb0060ef3737373136815308d7",
            "isKey": false,
            "numCitedBy": 691,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Measurements of formant frequencies and duration are reported for 8 Swedish vowels uttered by a male talker in three consonantal environments under varying timing conditions. An exponential function is used to describe the extent to which formant frequencies in the vowels reach their target values as a function of vowel\u2010segment duration. A target is specified by the asymptotic values of the first two formant frequencies of the vowel and is independent of consonantal context and duration. It is thus an invariant attribute of the vowel. The results suggest an interpretation in terms of a simple dynamic model of vowel articulation."
            },
            "slug": "Spectrographic-Study-of-Vowel-Reduction-Lindblom",
            "title": {
                "fragments": [],
                "text": "Spectrographic Study of Vowel Reduction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748169"
                        ],
                        "name": "Kuansan Wang",
                        "slug": "Kuansan-Wang",
                        "structuredName": {
                            "firstName": "Kuansan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuansan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145320076"
                        ],
                        "name": "W. Chou",
                        "slug": "W.-Chou",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5965277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6aeff187c0b879e6c95fe05c78881a3d0f2cdb2",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech technology and systems in human-machine communication have witnessed a steady and remarkable advancement over the last two decades. Fundamental changes have taken place from theoretical foundations to practical systems, from laboratory prototypes to commercial products, and from proprietary softwares to industrial standards. As the information age continues, research in speech technology is further accelerated by the advent of powerful computing devices, the data-driven pattern recognition methods, and the need to generate machine understandable metadata for Web contents and other information sources."
            },
            "slug": "Speech-technology-and-systems-in-human-machine-the-Deng-Wang",
            "title": {
                "fragments": [],
                "text": "Speech technology and systems in human-machine communication [from the Guest Editors]"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Speech technology and systems in human-machine communication have witnessed a steady and remarkable advancement over the last two decades, with the advent of powerful computing devices, the data-driven pattern recognition methods, and the need to generate machine understandable metadata for Web contents and other information sources."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Process. Mag."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70422141"
                        ],
                        "name": "Elizabeth Shriberg",
                        "slug": "Elizabeth-Shriberg",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Shriberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Shriberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8285852,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a585f634b53be5827aa4448e507654c0124e402a",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Spontaneous conversation is optimized for human-human communication, but differs in some important ways from the types of speech for which human language technology is often developed. This overview describes four fundamental properties of spontaneousspeech that present challenges for spoken language applications because they violate assumptions often applied in automatic processing technology."
            },
            "slug": "Spontaneous-speech:-how-people-really-talk-and-why-Shriberg",
            "title": {
                "fragments": [],
                "text": "Spontaneous speech: how people really talk and why engineers should care"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An overview of four fundamental properties of spontaneous speech that present challenges for spoken language applications because they violate assumptions often applied in automatic processing technology are described."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33340475"
                        ],
                        "name": "A. Oppenheim",
                        "slug": "A.-Oppenheim",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Oppenheim",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oppenheim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109729115"
                        ],
                        "name": "Don H. Johnson",
                        "slug": "Don-H.-Johnson",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Johnson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don H. Johnson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61762779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3cbe0b41cb0787b5a0b031002bc2be6dada0bd7",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In processing continuous-time signals by digitalmeans, it is necessary to represent the signal by a digital sequence. There are many ways other than periodic sampling for obtaining such a sequence. The requirements for such representations and some examples are discussed within the framework of simulating linear time-invariant systems. The representation of digital sequences by other digital sequences is also discussed, with particular emphasis on the use of such representations to implement a nonlinear warping of the digital frequency axis. Some applications and hardware implementation of this digital-frequency warping are described."
            },
            "slug": "Discrete-representation-of-signals-Oppenheim-Johnson",
            "title": {
                "fragments": [],
                "text": "Discrete representation of signals"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The requirements for digital sequences by other digital sequences and the use of such representations to implement a nonlinear warping of the digital frequency axis are discussed within the framework of simulating linear time-invariant systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144531812"
                        ],
                        "name": "Xuedong Huang",
                        "slug": "Xuedong-Huang",
                        "structuredName": {
                            "firstName": "Xuedong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuedong Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 222
                            }
                        ],
                        "text": "Using this framework, many important subjects in speech science and those in speech recognition that were previously studied separately by different communities of speech researchers can now be investigated in a unified fashion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14808146,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "22505ca2210a39b0c96a74b069cd4a583c7985f6",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Although progress has been impressive, there are still several hurdles that speech recognition technology must clear before ubiquitous adoption can be realized. R&D in spontaneous and free-flowing speech style is critical to its success."
            },
            "slug": "Challenges-in-adopting-speech-recognition-Deng-Huang",
            "title": {
                "fragments": [],
                "text": "Challenges in adopting speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "Although progress has been impressive, there are still several hurdles that speech recognition technology must clear before ubiquitous adoption can be realized and research in spontaneous and free-flowing speech style is critical to its success."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44649985,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4153961c9b8807d73a50fead9962b3bd91ea6887",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A table for use in teaching children, particularly those with learning disabilities. The table includes a translucent top upon which a paper worksheet can be placed. An optical projector is mounted below the table top to direct an image onto the translucent table top. The image is seen through the paper worksheet and may be followed or traced by the child to develop psychomotor brain patterns."
            },
            "slug": "Linear-models-for-structure-prediction-Pereira",
            "title": {
                "fragments": [],
                "text": "Linear models for structure prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A table for use in teaching children, particularly those with learning disabilities, that includes a translucent top upon which a paper worksheet can be placed."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109156162"
                        ],
                        "name": "C.-H. Lee",
                        "slug": "C.-H.-Lee",
                        "structuredName": {
                            "firstName": "C.-H.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C.-H. Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208088253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7da5c2ff6cb496eb45a2febbc6eb9a089b4beada",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-knowledge-ignorant-to-knowledge-rich-modeling-Lee",
            "title": {
                "fragments": [],
                "text": "From knowledge-ignorant to knowledge-rich modeling : a new speech research parading for next generation automatic speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "The model expressed in (2) provides clear contrast to the trajectory or trended models where the time-varying acoustic features are approximated as an explicit temporal function of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 63287451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b539c31c30a9c67807c27c5eca3e902ca7154117",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-investigation-of-segmental-hidden-dynamic-models-Bridle",
            "title": {
                "fragments": [],
                "text": "An investigation of segmental hidden dynamic models of speech coarticulation for automatic speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121786"
                        ],
                        "name": "V. Digalakis",
                        "slug": "V.-Digalakis",
                        "structuredName": {
                            "firstName": "Vassilios",
                            "lastName": "Digalakis",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Digalakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803620"
                        ],
                        "name": "J. R. Rohlicek",
                        "slug": "J.-R.-Rohlicek",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Rohlicek",
                            "middleNames": [
                                "Robin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Rohlicek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44668855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "528d976bbe17924874e54dffd8d6fb77f309babb",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A nontraditional approach to the problem of estimating the parameters of a stochastic linear system is presented. The method is based on the expectation-maximization algorithm and can be considered as the continuous analog of the Baum-Welch estimation algorithm for hidden Markov models. The algorithm is used for training the parameters of a dynamical system model that is proposed for better representing the spectral dynamics of speech for recognition. It is assumed that the observed feature vectors of a phone segment are the output of a stochastic linear dynamical system, and it is shown how the evolution of the dynamics as a function of the segment length can be modeled using alternative assumptions. A phoneme classification task using the TIMIT database demonstrates that the approach is the first effective use of an explicit model for statistical dependence between frames of speech. >"
            },
            "slug": "ML-estimation-of-a-stochastic-linear-system-with-EM-Digalakis-Rohlicek",
            "title": {
                "fragments": [],
                "text": "ML estimation of a stochastic linear system with the EM algorithm and its application to speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The nontraditional approach to the problem of estimating the parameters of a stochastic linear system is presented and it is shown how the evolution of the dynamics as a function of the segment length can be modeled using alternative assumptions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 185
                            }
                        ],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech modeling and recognition using a time series model containing trend functions with Markov modulated parameters"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Workshop Automatic Speech Recognition"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "He is currently a member of the Society's Multimedia Signal Processing Technical Committee. He also serves on the editorial board of IEEE Signal Processing Magazine and is the Magazine's Area Editor"
            },
            "venue": {
                "fragments": [],
                "text": "Dr Deng served on the Education Committee and Speech Processing Technical Committees of the IEEE Signal Processing Society\u20132000) and was Associate Editor for IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING He was a Technical Chair of IEEE International Conference on Acoustics, Speech, and Signal Pr"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Many leading researchers in the field understand the fragile nature of the current speech recognition system design, and have advocated that new, serious research is needed to overcome some fundamental limitations of the current speech recognition technology (e.g., [3], [6], [9], [29], [34], [40],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pushing the envelope\u2014Aside"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Process. Mag"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "The model expressed in (2) provides clear contrast to the trajectory or trended models where the time-varying acoustic features are approximated as an explicit temporal function of time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Target-directed mixture linear dynamic models for spontaneous speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models with templates as nonstationary states: An application to speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lecture Notes on Spectrogram Reading"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes on Spectrogram Reading"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In con-\ntrast to the acoustic-dynamic model which represents coarticulation at the surface, observational level, the hidden dynamic model explores a deeper, unobserved (hence \u201chidden\u201d) level of the speech dynamic structure that regulates coarticulation and phonetic reduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Segmental HMMs for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Eurospeech"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 32,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 63,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Structured-speech-modeling-Deng-Yu/62c87f843ae5c1ce7972d7cdcd227e3ec3fe5417?sort=total-citations"
}