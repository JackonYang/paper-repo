{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068068778"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Also, we found that the photometric distortions of Andrew Howard [8] were useful to combat overfitting to the imaging conditions of training data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar approach was used by Andrew Howard [8] in the previous year\u2019s entry, which we empirically verified to perform slightly worse than the proposed scheme."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To complicate matters further, some of the models were mainly trained on smaller relative crops, others on larger ones, inspired by [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 651286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d67175d17c450ab0ac9c256103828f9e9a0acb85",
            "isKey": true,
            "numCitedBy": 362,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner."
            },
            "slug": "Some-Improvements-on-Deep-Convolutional-Neural-Howard",
            "title": {
                "fragments": [],
                "text": "Some Improvements on Deep Convolutional Neural Network Based Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper summarizes the entry in the Imagenet Large Scale Visual Recognition Challenge 2013, which achieved a top 5 classification error rate and achieved over a 20% relative improvement on the previous year's winner."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50196944"
                        ],
                        "name": "Judy Hoffman",
                        "slug": "Judy-Hoffman",
                        "structuredName": {
                            "firstName": "Judy",
                            "lastName": "Hoffman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judy Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152329702"
                        ],
                        "name": "Ning Zhang",
                        "slug": "Ning-Zhang",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368132"
                        ],
                        "name": "Eric Tzeng",
                        "slug": "Eric-Tzeng",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Tzeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Tzeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6161478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8de958fead0d8a9619b55c7299df3257c624a96",
            "isKey": false,
            "numCitedBy": 4234,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
            },
            "slug": "DeCAF:-A-Deep-Convolutional-Activation-Feature-for-Donahue-Jia",
            "title": {
                "fragments": [],
                "text": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "DeCAF, an open-source implementation of deep convolutional activation features, along with all associated network parameters, are released to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726415"
                        ],
                        "name": "Alexander Toshev",
                        "slug": "Alexander-Toshev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Toshev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Toshev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges, on which it significantly outperforms the current state of the art."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2972501,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6",
            "isKey": false,
            "numCitedBy": 963,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations."
            },
            "slug": "Scalable-Object-Detection-Using-Deep-Neural-Erhan-Szegedy",
            "title": {
                "fragments": [],
                "text": "Scalable Object Detection Using Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4071727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1109b663453e78a59e4f66446d71720ac58cec25",
            "isKey": false,
            "numCitedBy": 4352,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
            },
            "slug": "OverFeat:-Integrated-Recognition,-Localization-and-Sermanet-Eigen",
            "title": {
                "fragments": [],
                "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80897,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3960646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
            "isKey": false,
            "numCitedBy": 11803,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."
            },
            "slug": "Visualizing-and-Understanding-Convolutional-Zeiler-Fergus",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large Convolutional Network models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115913164"
                        ],
                        "name": "Min Lin",
                        "slug": "Min-Lin",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35370244"
                        ],
                        "name": "Qiang Chen",
                        "slug": "Qiang-Chen",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In general, one can view the Inception model as a logical culmination of [12] while taking inspiration and guidance from the theoretical work by Arora et al [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Despite being a highly speculative undertaking, modest gains were observed early on when compared with reference networks based on [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al [12] in conjunction with the famous \u201cwe need to go deeper\u201d internet meme [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Thus, we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of 1\u00d71 convolutions in the next layer, as suggested in [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For larger datasets such as Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14], while using dropout [7] to address the problem of overfitting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[12] in order to increase the representational power of neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The use of average pooling before the classifier is based on [12], although our implementation has an additional linear layer."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16636683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
            "isKey": true,
            "numCitedBy": 4208,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
            },
            "slug": "Network-In-Network-Lin-Chen",
            "title": {
                "fragments": [],
                "text": "Network In Network"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "With enhanced local modeling via the micro network, the proposed deep network structure NIN is able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11797475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67dccc9a856b60bdc4d058d83657a089b8ad4486",
            "isKey": false,
            "numCitedBy": 5474,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. \n \nOur contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification."
            },
            "slug": "Two-Stream-Convolutional-Networks-for-Action-in-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Two-Stream Convolutional Networks for Action Recognition in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a two-stream ConvNet architecture which incorporates spatial and temporal networks and demonstrates that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5115386"
                        ],
                        "name": "Yunchao Gong",
                        "slug": "Yunchao-Gong",
                        "structuredName": {
                            "firstName": "Yunchao",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunchao Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39060743"
                        ],
                        "name": "Liwei Wang",
                        "slug": "Liwei-Wang",
                        "structuredName": {
                            "firstName": "Liwei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liwei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37495246"
                        ],
                        "name": "Ruiqi Guo",
                        "slug": "Ruiqi-Guo",
                        "structuredName": {
                            "firstName": "Ruiqi",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruiqi Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1346519,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "a99add9d76d849a8d47b93532703e4ca0f683b92",
            "isKey": false,
            "numCitedBy": 1000,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets."
            },
            "slug": "Multi-scale-Orderless-Pooling-of-Deep-Convolutional-Gong-Wang",
            "title": {
                "fragments": [],
                "text": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simple but effective scheme called multi-scale orderless pooling (MOP-CNN), which extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726415"
                        ],
                        "name": "Alexander Toshev",
                        "slug": "Alexander-Toshev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Toshev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Toshev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8827762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "713f73ce5c3013d9fb796c21b981dc6629af0bd5",
            "isKey": false,
            "numCitedBy": 1181,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC."
            },
            "slug": "Deep-Neural-Networks-for-Object-Detection-Szegedy-Toshev",
            "title": {
                "fragments": [],
                "text": "Deep Neural Networks for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks, and defines a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805076"
                        ],
                        "name": "G. Toderici",
                        "slug": "G.-Toderici",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Toderici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Toderici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152821938"
                        ],
                        "name": "Sanketh Shetty",
                        "slug": "Sanketh-Shetty",
                        "structuredName": {
                            "firstName": "Sanketh",
                            "lastName": "Shetty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanketh Shetty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152456068"
                        ],
                        "name": "Thomas Leung",
                        "slug": "Thomas-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206592218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d4c9c923e9f145d1c01a2de2afc38ec23c44253",
            "isKey": false,
            "numCitedBy": 5148,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%)."
            },
            "slug": "Large-Scale-Video-Classification-with-Convolutional-Karpathy-Toderici",
            "title": {
                "fragments": [],
                "text": "Large-Scale Video Classification with Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work studies multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggests a multiresolution, foveated architecture as a promising way of speeding up the training."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 106
                            }
                        ],
                        "text": "Recently, sparse representation based generative tracking methods have been developed for object tracking [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 39
                            }
                        ],
                        "text": "Practically, for complex real problems [3, 4, 6, 11], deep trees are usually required to fit the training data well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 82
                            }
                        ],
                        "text": "Recent work has focused on motion descriptors that are invariant to camera motion [5, 6, 8, 10, 11, 12, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 34
                            }
                        ],
                        "text": "(c) Joint sparse appearance model [1, 10, 11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 47
                            }
                        ],
                        "text": "Figure 1: Sparse representation based trackers [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 111
                            }
                        ],
                        "text": "Recently, it has proven extremely successful on important applications in data mining [12] and computer vision [6, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 165
                            }
                        ],
                        "text": "In addition, our method also preserves the spatial layout structure among the local patches inside each target candidate, which is ignored by the above three models [1, 2, 3, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[11] Chang Xu, Dacheng Tao, and Chao Xu."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 76
                            }
                        ],
                        "text": "However, most of them concentrate on supervised or semi-supervised learning [7, 11], in which a validation set is required."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[11] Tianzhu Zhang, Bernard Ghanem, Si Liu, and Narendra Ahuja."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1450294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71",
            "isKey": true,
            "numCitedBy": 4491,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
            },
            "slug": "Deep-Inside-Convolutional-Networks:-Visualising-and-Simonyan-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets), and establishes the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145291669"
                        ],
                        "name": "Bolei Zhou",
                        "slug": "Bolei-Zhou",
                        "structuredName": {
                            "firstName": "Bolei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677488"
                        ],
                        "name": "\u00c0. Lapedriza",
                        "slug": "\u00c0.-Lapedriza",
                        "structuredName": {
                            "firstName": "\u00c0gata",
                            "lastName": "Lapedriza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c0. Lapedriza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "a Hamming code [13],[38] as in this work, or product quantization code [31],[17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1849990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9667f8264745b626c6173b1310e2ff0298b09cfc",
            "isKey": false,
            "numCitedBy": 2610,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks."
            },
            "slug": "Learning-Deep-Features-for-Scene-Recognition-using-Zhou-Lapedriza",
            "title": {
                "fragments": [],
                "text": "Learning Deep Features for Scene Recognition using Places Database"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new scene-centric database called Places with over 7 million labeled pictures of scenes is introduced with new methods to compare the density and diversity of image datasets and it is shown that Places is as dense as other scene datasets and has more diversity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "In our case, the word \u201cdeep\u201d is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the \u201cInception module\u201d and also in the more direct sense of increased network depth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215827080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "isKey": false,
            "numCitedBy": 17075,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn."
            },
            "slug": "Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue",
            "title": {
                "fragments": [],
                "text": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279670"
                        ],
                        "name": "Andrea Frome",
                        "slug": "Andrea-Frome",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Frome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Frome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 261138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4aa4069693bee00d1b0759ca3df35e59284e9845",
            "isKey": false,
            "numCitedBy": 1950,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model."
            },
            "slug": "DeViSE:-A-Deep-Visual-Semantic-Embedding-Model-Frome-Corrado",
            "title": {
                "fragments": [],
                "text": "DeViSE: A Deep Visual-Semantic Embedding Model"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text and shows that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082991"
                        ],
                        "name": "Georgia Gkioxari",
                        "slug": "Georgia-Gkioxari",
                        "structuredName": {
                            "firstName": "Georgia",
                            "lastName": "Gkioxari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgia Gkioxari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16991828,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f57137ae926b1a50b9a55457b996bbd1e1635b89",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present convolutional neural networks for the tasks of keypoint (pose) prediction and action classification of people in unconstrained images. Our approach involves training an R-CNN detector with loss functions depending on the task being tackled. We evaluate our method on the challenging PASCAL VOC dataset and compare it to previous leading approaches. Our method gives state-of-the-art results for keypoint and action prediction. Additionally, we introduce a new dataset for action detection, the task of simultaneously localizing people and classifying their actions, and present results using our approach."
            },
            "slug": "R-CNNs-for-Pose-Estimation-and-Action-Detection-Gkioxari-Hariharan",
            "title": {
                "fragments": [],
                "text": "R-CNNs for Pose Estimation and Action Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This work presents convolutional neural networks for the tasks of keypoint (pose) prediction and action classification of people in unconstrained images and gives state-of-the-art results for keypoint and action prediction."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152329702"
                        ],
                        "name": "Ning Zhang",
                        "slug": "Ning-Zhang",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11710343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98bb60748eb8ef7a671cdd22faa87e377fd13060",
            "isKey": false,
            "numCitedBy": 925,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time."
            },
            "slug": "Part-Based-R-CNNs-for-Fine-Grained-Category-Zhang-Donahue",
            "title": {
                "fragments": [],
                "text": "Part-Based R-CNNs for Fine-Grained Category Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a model for fine-grained categorization that overcomes limitations by leveraging deep convolutional features computed on bottom-up region proposals, and learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine- grained category from a pose-normalized representation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9272368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "342786659379879f58bf5c4ff43c84c83a6a7389",
            "isKey": false,
            "numCitedBy": 1062,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top-down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work."
            },
            "slug": "Simultaneous-Detection-and-Segmentation-Hariharan-Arbel\u00e1ez",
            "title": {
                "fragments": [],
                "text": "Simultaneous Detection and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work builds on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN), introducing a novel architecture tailored for SDS, and uses category-specific, top-down figure-ground predictions to refine the bottom-up proposals."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "Figure 3: Precision and success plots of overall performance comparison for the 51 videos in the benchmark [8] (best-viewed on high-resolution display)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "The comparison results on benchmark [8] are shown in Figure 3."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 82
                            }
                        ],
                        "text": "Rather than predicting poses directly based on feature correspondences, we follow [1, 8] in predicting \u201cobject coordinates\u201d (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 189
                            }
                        ],
                        "text": "Here \u03bc \u2208 [0,1] regulates the amount of desired entry-wise sparsity of A with respect to the low-rank prior tr(A) (indeed notice that for \u03bc = 1 we recover the low-rank inducing framework of [2, 8])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] Heng Wang, Alexander Kl\u00e4ser, Cordelia Schmid, and Cheng-Lin Liu."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 7
                            }
                        ],
                        "text": "Unlike [1, 8] that learn a random forest for prediction, we treat the object coordinate hypotheses as unknown (or latent) states and employ the methodology of inference in graphical models in order to rank the set of putative object coordinates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 207
                            }
                        ],
                        "text": "Based on those researches, several papers address the feasibility of detecting mismatched pixels [6] not only to improve the quality of disparity maps [1] but also to leverage mid-level scene representation [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "We set up our baseline using local descriptors from Dense Trajectories (DT) [8], a successful video representation for action recognition in a surveillance setting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 41
                            }
                        ],
                        "text": "This prior was empirically observed (see [2, 8]) to indeed encourage information transfer across tasks; the sparsity term can therefore be interpreted as enforcing such transfer to occur only between tasks that are strongly correlated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12795415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23694b6d61668e62bb11f17c1d75dde3b4951948",
            "isKey": true,
            "numCitedBy": 1613,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to characterize a signal with a gradient vector derived from a generative probability model and to subsequently feed this representation to a discriminative classifier. We propose to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary: a Gaussian mixture model which approximates the distribution of low-level features in images. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance."
            },
            "slug": "Fisher-Kernels-on-Visual-Vocabularies-for-Image-Perronnin-Dance",
            "title": {
                "fragments": [],
                "text": "Fisher Kernels on Visual Vocabularies for Image Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work shows that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms, and proposes to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary: a Gaussian mixture model which approximates the distribution of low-level features in images."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2341378"
                        ],
                        "name": "C. Couprie",
                        "slug": "C.-Couprie",
                        "structuredName": {
                            "firstName": "Camille",
                            "lastName": "Couprie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Couprie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688714"
                        ],
                        "name": "Laurent Najman",
                        "slug": "Laurent-Najman",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Najman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurent Najman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206765110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "237a04dd8291cbdb59b6dc4b53e689af743fe2a3",
            "isKey": false,
            "numCitedBy": 2403,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\u00d7240 image labeling in less than a second, including feature extraction."
            },
            "slug": "Learning-Hierarchical-Features-for-Scene-Labeling-Farabet-Couprie",
            "title": {
                "fragments": [],
                "text": "Learning Hierarchical Features for Scene Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel, alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715051"
                        ],
                        "name": "Misha Denil",
                        "slug": "Misha-Denil",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Denil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Misha Denil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3355894"
                        ],
                        "name": "B. Shakibi",
                        "slug": "B.-Shakibi",
                        "structuredName": {
                            "firstName": "Babak",
                            "lastName": "Shakibi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shakibi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46573521"
                        ],
                        "name": "Laurent Dinh",
                        "slug": "Laurent-Dinh",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Dinh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurent Dinh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 206
                            }
                        ],
                        "text": "We show incremental domain expansion is effective in applying object detectors, trained with only ImageNet, to videos, improving performance by approximately 48% on Activities of Daily Living (ADL) dataset [3] and by 15% on the YouTube Objects dataset [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 71
                            }
                        ],
                        "text": "We first represent the photos using similarity preserving binary codes [1, 3, 5], enabling us to store large number of photos in memory."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "[5] and Michaeli and Irani [3], but significantly outperforms the other state-of-the-art methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] Carlo Tomasi and Roberto Manduchi."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "\u2217Our winning approach at THUMOS14 [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "Additionally, the region proposal step is improved by combining the selective search [6] approach with multi-box [3] predictions for higher ob-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] Hamed Pirsiavash and Deva Ramanan."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Many edge-preserving filters, like bilateral filters [3], guided filters [2], and geodesic filters [1] have been designed to alleviate cross-region mixing problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "We added back 200 region proposals coming from multi-box [3] resulting, in total, in about 60% of the proposals used by [4], while increasing the coverage from 92% to 93%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1639981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8650503ab80ad7299f0845b1843abf3a97f313a",
            "isKey": true,
            "numCitedBy": 992,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy."
            },
            "slug": "Predicting-Parameters-in-Deep-Learning-Denil-Shakibi",
            "title": {
                "fragments": [],
                "text": "Predicting Parameters in Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "It is demonstrated that there is significant redundancy in the parameterization of several deep learning models and not only can the parameter values be predicted, but many of them need not be learned at all."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11683,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706007"
                        ],
                        "name": "Jianchao Yang",
                        "slug": "Jianchao-Yang",
                        "structuredName": {
                            "firstName": "Jianchao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianchao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14013706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c65be1f97642510843667d36e399de58837d3419",
            "isKey": false,
            "numCitedBy": 347,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel supervised hierarchical sparse coding model based on local image descriptors for classification tasks. The supervised dictionary training is performed via back-projection, by minimizing the training error of classifying the image level features, which are extracted by max pooling over the sparse codes within a spatial pyramid. Such a max pooling procedure across multiple spatial scales offer the model translation invariant properties, similar to the Convolutional Neural Network (CNN). Experiments show that our supervised dictionary improves the performance of the proposed model significantly over the unsupervised dictionary, leading to state-of-the-art performance on diverse image databases. Further more, our supervised model targets learning linear features, implying its great potential in handling large scale datasets in real applications."
            },
            "slug": "Supervised-translation-invariant-sparse-coding-Yang-Yu",
            "title": {
                "fragments": [],
                "text": "Supervised translation-invariant sparse coding"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Experiments show that the supervised dictionary improves the performance of the proposed model significantly over the unsupervised dictionary, leading to state-of-the-art performance on diverse image databases and implying its great potential in handling large scale datasets in real applications."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113583"
                        ],
                        "name": "Marko Ristin",
                        "slug": "Marko-Ristin",
                        "structuredName": {
                            "firstName": "Marko",
                            "lastName": "Ristin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marko Ristin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737253"
                        ],
                        "name": "M. Guillaumin",
                        "slug": "M.-Guillaumin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Guillaumin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guillaumin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5252774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24065d385bae5579be07607a1f63eb79cebf8773",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, large image data sets such as \"ImageNet\", \"TinyImages\" or ever-growing social networks like \"Flickr\" have emerged, posing new challenges to image classification that were not apparent in smaller image sets. In particular, the efficient handling of dynamically growing data sets, where not only the amount of training images, but also the number of classes increases over time, is a relatively unexplored problem. To remedy this, we introduce Nearest Class Mean Forests (NCMF), a variant of Random Forests where the decision nodes are based on nearest class mean (NCM) classification. NCMFs not only outperform conventional random forests, but are also well suited for integrating new classes. To this end, we propose and compare several approaches to incorporate data from new classes, so as to seamlessly extend the previously trained forest instead of re-training them from scratch. In our experiments, we show that NCMFs trained on small data sets with 10 classes can be extended to large data sets with 1000 classes without significant loss of accuracy compared to training from scratch on the full data."
            },
            "slug": "Incremental-Learning-of-NCM-Forests-for-Large-Scale-Ristin-Guillaumin",
            "title": {
                "fragments": [],
                "text": "Incremental Learning of NCM Forests for Large-Scale Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work introduces Nearest Class Mean Forests (NCMF), a variant of Random Forests where the decision nodes are based on nearest class mean (NCM) classification, and demonstrates that NCMFs not only outperform conventional random forests, but are also well suited for integrating new classes."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715548"
                        ],
                        "name": "Mark Z. Mao",
                        "slug": "Mark-Z.-Mao",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Mao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Z. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080690"
                        ],
                        "name": "P. Tucker",
                        "slug": "P.-Tucker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tucker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143781496"
                        ],
                        "name": "Ke Yang",
                        "slug": "Ke-Yang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 28
                            }
                        ],
                        "text": "Also we are indebted to the DistBelief [4] team for their support especially to Rajat Monga, Jon Shlens, Alex Krizhevsky, Jeff Dean, Ilya Sutskever and Andrea Frome."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 36
                            }
                        ],
                        "text": "Our networks were trained using the DistBelief [4] distributed machine learning system using modest amount of model and data-parallelism."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "GoogLeNet networks were trained using the DistBelief [4] distributed machine learning system using modest amount of model and data-parallelism."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 372467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "isKey": true,
            "numCitedBy": 3026,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."
            },
            "slug": "Large-Scale-Distributed-Deep-Networks-Dean-Corrado",
            "title": {
                "fragments": [],
                "text": "Large Scale Distributed Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper considers the problem of training a deep network with billions of parameters using tens of thousands of CPU cores and develops two algorithms for large-scale distributed training, Downpour SGD and Sandblaster L-BFGS, which increase the scale and speed of deep network training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144421225"
                        ],
                        "name": "Michael Stark",
                        "slug": "Michael-Stark",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Stark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14700310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c30b9fb837e912ccf3919fdb64e9543fca57799e",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "While knowledge transfer (KT) between object classes has been accepted as a promising route towards scalable recognition, most experimental KT studies are surprisingly limited in the number of object classes considered. To support claims of KT w.r.t. scalability we thus advocate to evaluate KT in a large-scale setting. To this end, we provide an extensive evaluation of three popular approaches to KT on a recently proposed large-scale data set, the ImageNet Large Scale Visual Recognition Competition 2010 data set. In a first setting they are directly compared to one-vs-all classification often neglected in KT papers and in a second setting we evaluate their ability to enable zero-shot learning. While none of the KT methods can improve over one-vs-all classification they prove valuable for zero-shot learning, especially hierarchical and direct similarity based KT. We also propose and describe several extensions of the evaluated approaches that are necessary for this large-scale study."
            },
            "slug": "Evaluating-knowledge-transfer-and-zero-shot-in-a-Rohrbach-Stark",
            "title": {
                "fragments": [],
                "text": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An extensive evaluation of three popular approaches to KT on a recently proposed large-scale data set, the ImageNet Large Scale Visual Recognition Competition 2010 dataSet, finding none of the KT methods can improve over one-vs-all classification but prove valuable for zero-shot learning, especially hierarchical and direct similarity based KT."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759797"
                        ],
                        "name": "Jared Heinly",
                        "slug": "Jared-Heinly",
                        "structuredName": {
                            "firstName": "Jared",
                            "lastName": "Heinly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jared Heinly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46727847"
                        ],
                        "name": "Johannes L. Schonberger",
                        "slug": "Johannes-L.-Schonberger",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Schonberger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johannes L. Schonberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144678099"
                        ],
                        "name": "Enrique Dunn",
                        "slug": "Enrique-Dunn",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Dunn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enrique Dunn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40454588"
                        ],
                        "name": "Jan-Michael Frahm",
                        "slug": "Jan-Michael-Frahm",
                        "structuredName": {
                            "firstName": "Jan-Michael",
                            "lastName": "Frahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan-Michael Frahm"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60368537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7024dbaeb72d25c2722868d27357a206a071a17",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel, large-scale, structure-from-motion framework that advances the state of the art in data scalability from city-scale modeling (millions of images) to world-scale modeling (several tens of millions of images) using just a single computer. The main enabling technology is the use of a streaming-based framework for connected component discovery. Moreover, our system employs an adaptive, online, iconic image clustering approach based on an augmented bag-of-words representation, in order to balance the goals of registration, comprehensiveness, and data compactness. We demonstrate our proposal by operating on a recent publicly available 100 million image crowdsourced photo collection containing images geographically distributed throughout the entire world. Results illustrate that our streaming-based approach does not compromise model completeness, but achieves unprecedented levels of efficiency and scalability."
            },
            "slug": "Reconstructing-the-World*-in-Six-Days-*(As-Captured-Heinly-Schonberger",
            "title": {
                "fragments": [],
                "text": "Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel, large-scale, structure-from-motion framework that advances the state of the art in data scalability from city-scale modeling to world- scale modeling using just a single computer."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2015"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": false,
            "numCitedBy": 19766,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726415"
                        ],
                        "name": "Alexander Toshev",
                        "slug": "Alexander-Toshev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Toshev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Toshev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "f accurate spatial information, the same convolutional network architecture as [9] has also been successfully employed for localization [9, 14], object detection [6,14,18,5] and human pose estimation [19]. Inspired by a neuroscience model of the primate visual cortex, Serre et al. [15] use a series of \ufb01xed Gabor \ufb01lters of different sizes in order to handle multiple scales, similarly to the Inception m"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206592152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a002ce457f7ab3088fbd2691734f1ce79f750c4",
            "isKey": false,
            "numCitedBy": 1910,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regres- sors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formula- tion which capitalizes on recent advances in Deep Learn- ing. We present a detailed empirical analysis with state-of- art or better performance on four academic benchmarks of diverse real-world images."
            },
            "slug": "DeepPose:-Human-Pose-Estimation-via-Deep-Neural-Toshev-Szegedy",
            "title": {
                "fragments": [],
                "text": "DeepPose: Human Pose Estimation via Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The pose estimation is formulated as a DNN-based regression problem towards body joints and a cascade of such DNN regres- sors which results in high precision pose estimates."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152924487"
                        ],
                        "name": "Jiang Wang",
                        "slug": "Jiang-Wang",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691128"
                        ],
                        "name": "Zicheng Liu",
                        "slug": "Zicheng-Liu",
                        "structuredName": {
                            "firstName": "Zicheng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zicheng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50118130"
                        ],
                        "name": "Ying Wu",
                        "slug": "Ying-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34316743"
                        ],
                        "name": "Junsong Yuan",
                        "slug": "Junsong-Yuan",
                        "structuredName": {
                            "firstName": "Junsong",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junsong Yuan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11964979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "321f519e2876e1fa086b36e1b3f91b1efe2ea532",
            "isKey": false,
            "numCitedBy": 1384,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Human action recognition is an important yet challenging task. The recently developed commodity depth sensors open up new possibilities of dealing with this problem but also present some unique challenges. The depth maps captured by the depth cameras are very noisy and the 3D positions of the tracked joints may be completely wrong if serious occlusions occur, which increases the intra-class variations in the actions. In this paper, an actionlet ensemble model is learnt to represent each action and to capture the intra-class variance. In addition, novel features that are suitable for depth data are proposed. They are robust to noise, invariant to translational and temporal misalignments, and capable of characterizing both the human motion and the human-object interactions. The proposed approach is evaluated on two challenging action recognition datasets captured by commodity depth cameras, and another dataset captured by a MoCap system. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms."
            },
            "slug": "Mining-actionlet-ensemble-for-action-recognition-Wang-Liu",
            "title": {
                "fragments": [],
                "text": "Mining actionlet ensemble for action recognition with depth cameras"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An actionlet ensemble model is learnt to represent each action and to capture the intra-class variance, and novel features that are suitable for depth data are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681054"
                        ],
                        "name": "H. J\u00e9gou",
                        "slug": "H.-J\u00e9gou",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "J\u00e9gou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. J\u00e9gou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3271933"
                        ],
                        "name": "M. Douze",
                        "slug": "M.-Douze",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Douze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Douze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144565371"
                        ],
                        "name": "P. P\u00e9rez",
                        "slug": "P.-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P\u00e9rez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "This enables us to easily use a multi-index hash table [4] to index the centers so that the nearest center lookup becomes extremely efficient."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "This can be done by building a multiindex hash table [4] on c j and perform fast lookups for each xi."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 252
                            }
                        ],
                        "text": "We show incremental domain expansion is effective in applying object detectors, trained with only ImageNet, to videos, improving performance by approximately 48% on Activities of Daily Living (ADL) dataset [3] and by 15% on the YouTube Objects dataset [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "The proposed prior is derived from the color-line model [4], a local statistical model which claims that pixel colors x within a local image patch can be well represented by linear combinations of two color centroids:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "We use an in-house implementation of a Krizhevsky style cuda-convnet with dropout [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "The approach taken by GoogLeNet for detection is similar to the RCNN by [4], but is augmented with the Inception model as the region classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 164
                            }
                        ],
                        "text": "Our work is more closely related to the emerging trend of incorporating light transport information, using optical and electronic modifications, to correct for MPI [2, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "We added back 200 region proposals coming from multi-box [3] resulting, in total, in about 60% of the proposals used by [4], while increasing the coverage from 92% to 93%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[4] Mohammad Norouzi, Ali Punjani, and David J."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Given the means are binary, we can directly build a multi-index hash table [4] on the centers, and can efficiently find the exact nearest mean for any binary data point in constant time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1912782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "400e09ceca374f0621335f84a4daf2049d5902be",
            "isKey": true,
            "numCitedBy": 2305,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of image search on a very large scale, where three constraints have to be considered jointly: the accuracy of the search, its efficiency, and the memory usage of the representation. We first propose a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation. We then show how to jointly optimize the dimension reduction and the indexing algorithm, so that it best preserves the quality of vector comparison. The evaluation shows that our approach significantly outperforms the state of the art: the search accuracy is comparable to the bag-of-features approach for an image representation that fits in 20 bytes. Searching a 10 million image dataset takes about 50ms."
            },
            "slug": "Aggregating-local-descriptors-into-a-compact-image-J\u00e9gou-Douze",
            "title": {
                "fragments": [],
                "text": "Aggregating local descriptors into a compact image representation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation, and shows how to jointly optimize the dimension reduction and the indexing algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145563465"
                        ],
                        "name": "Sanjeev Arora",
                        "slug": "Sanjeev-Arora",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Arora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjeev Arora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736564"
                        ],
                        "name": "Aditya Bhaskara",
                        "slug": "Aditya-Bhaskara",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Bhaskara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aditya Bhaskara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144804200"
                        ],
                        "name": "Rong Ge",
                        "slug": "Rong-Ge",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Ge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Ge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1901958"
                        ],
                        "name": "Tengyu Ma",
                        "slug": "Tengyu-Ma",
                        "structuredName": {
                            "firstName": "Tengyu",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tengyu Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 157
                            }
                        ],
                        "text": "In general, one can view the Inception model as a logical culmination of [12] while taking inspiration and guidance from the theoretical work by Arora et al [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "The representation should be kept sparse at most places (as required by the conditions of [2]) and compress the signals only whenever they have to be aggregated en masse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "This suggest future work towards creating sparser and more refined structures in automated ways on the basis of [2], as well as on applying the insights of the Inception architecture to other domains."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 143
                            }
                        ],
                        "text": "Besides mimicking biological systems, this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora et al. [2]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] suggests a layer-by layer construction where one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Arora et al. [2] suggests a layer-by layer construction in which one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 208
                            }
                        ],
                        "text": "The Inception architecture started out as a case study for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by [2] for vision networks and covering the hypothesized outcome by dense, readily available components."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8049057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d6e6adc7a841393fc10b78dc0018e550aff589d",
            "isKey": true,
            "numCitedBy": 288,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an n node multilayer network that has degree at most n\u03b3 for some \u03b3 < 1 and each edge has a random edge weight in [-1, 1]. Our algorithm learns almost all networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. \n \nThe algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural nets with random edge weights."
            },
            "slug": "Provable-Bounds-for-Learning-Some-Deep-Arora-Bhaskara",
            "title": {
                "fragments": [],
                "text": "Provable Bounds for Learning Some Deep Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This work gives algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others, based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699240"
                        ],
                        "name": "Di Wu",
                        "slug": "Di-Wu",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Di Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37006600"
                        ],
                        "name": "Ling Shao",
                        "slug": "Ling-Shao",
                        "structuredName": {
                            "firstName": "Ling",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling Shao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4233420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af431453dd9e8c191971040089c3386b843ce773",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the last few years, with the immense popularity of the Kinect, there has been renewed interest in developing methods for human gesture and action recognition from 3D skeletal data. A number of approaches have been proposed to extract representative features from 3D skeletal data, most commonly hard wired geometric or bio-inspired shape context features. We propose a hierarchial dynamic framework that first extracts high level skeletal joints features and then uses the learned representation for estimating emission probability to infer action sequences. Currently gaussian mixture models are the dominant technique for modeling the emission distribution of hidden Markov models. We show that better action recognition using skeletal features can be achieved by replacing gaussian mixture models by deep neural networks that contain many layers of features to predict probability distributions over states of hidden Markov models. The framework can be easily extended to include a ergodic state to segment and recognize actions simultaneously."
            },
            "slug": "Leveraging-Hierarchical-Parametric-Networks-for-and-Wu-Shao",
            "title": {
                "fragments": [],
                "text": "Leveraging Hierarchical Parametric Networks for Skeletal Joints Based Action Segmentation and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that better action recognition using skeletal features can be achieved by replacing gaussian mixture models by deep neural networks that contain many layers of features to predict probability distributions over states of hidden Markov models."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417546"
                        ],
                        "name": "Simon Hadfield",
                        "slug": "Simon-Hadfield",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Hadfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Hadfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145398628"
                        ],
                        "name": "R. Bowden",
                        "slug": "R.-Bowden",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Bowden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bowden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1377775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dbd7653ede6af15d539f64cb2128752ca029e44",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Action recognition in unconstrained situations is a difficult task, suffering from massive intra-class variations. It is made even more challenging when complex 3D actions are projected down to the image plane, losing a great deal of information. The recent emergence of 3D data, both in broadcast content, and commercial depth sensors, provides the possibility to overcome this issue. This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use of the 3D data, and five new interest point detection strategies are also proposed, that extend to the 3D data. Our evaluation compares all 4 feature descriptors, using 7 different types of interest point, over a variety of threshold levels, for the Hollywood3D dataset. We make the dataset including stereo video, estimated depth maps and all code required to reproduce the benchmark results, available to the wider community."
            },
            "slug": "Hollywood-3D:-Recognizing-Actions-in-3D-Natural-Hadfield-Bowden",
            "title": {
                "fragments": [],
                "text": "Hollywood 3D: Recognizing Actions in 3D Natural Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information, including stereo video, estimated depth maps and all code required to reproduce the benchmark results, available to the wider community."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2118350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a97b5db17acc731ef67321832dbbaf5766153135",
            "isKey": false,
            "numCitedBy": 1748,
            "numCiting": 220,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of the long short-term memory network architecture to multidimensional data, such as images and video sequences."
            },
            "slug": "Supervised-Sequence-Labelling-with-Recurrent-Neural-Graves",
            "title": {
                "fragments": [],
                "text": "Supervised Sequence Labelling with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and an extension of the long short-term memory network architecture to multidimensional data, such as images and video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Studies in Computational Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "9 momentum [17], fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10940950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "isKey": false,
            "numCitedBy": 3555,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. \n \nOur success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods."
            },
            "slug": "On-the-importance-of-initialization-and-momentum-in-Sutskever-Martens",
            "title": {
                "fragments": [],
                "text": "On the importance of initialization and momentum in deep learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs to levels of performance that were previously achievable only with Hessian-Free optimization."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 147
                            }
                        ],
                        "text": "For larger datasets such as Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14], while using dropout [7] to address the problem of overfitting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14832074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1366de5bb112746a555e9c0cd00de3ad8628aea8",
            "isKey": false,
            "numCitedBy": 6189,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
            },
            "slug": "Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by preventing co-adaptation of feature detectors"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2518212"
                        ],
                        "name": "Hakan Bilen",
                        "slug": "Hakan-Bilen",
                        "structuredName": {
                            "firstName": "Hakan",
                            "lastName": "Bilen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hakan Bilen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3048367"
                        ],
                        "name": "M. Pedersoli",
                        "slug": "M.-Pedersoli",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Pedersoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pedersoli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12039270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e533c6488b42b099f320a9921ba26dcde7e97e4f",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on the problem of object detection when the annotation at training time is restricted to presence or absence of object instances at image level. We present a method based on features extracted from a Convolutional Neural Network and latent SVM that can represent and exploit the presence of multiple object instances in an image. Moreover, the detection of the object instances in the image is improved by incorporating in the learning procedure additional constraints that represent domain-specific knowledge such as symmetry and mutual exclusion. We show that the proposed method outperforms the state-of-the-art in weakly-supervised object detection and object classification on the Pascal VOC 2007 dataset."
            },
            "slug": "Weakly-Supervised-Object-Detection-with-Posterior-Bilen-Pedersoli",
            "title": {
                "fragments": [],
                "text": "Weakly Supervised Object Detection with Posterior Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A method based on features extracted from a Convolutional Neural Network and latent SVM that can represent and exploit the presence of multiple object instances in an image that outperforms the state-of-the-art in weakly-supervised object detection and object classification on the Pascal VOC 2007 dataset."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144114624"
                        ],
                        "name": "Juan Xu",
                        "slug": "Juan-Xu",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405907659"
                        ],
                        "name": "Ming Jiang",
                        "slug": "Ming-Jiang",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40440632"
                        ],
                        "name": "Shuo Wang",
                        "slug": "Shuo-Wang",
                        "structuredName": {
                            "firstName": "Shuo",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuo Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744045"
                        ],
                        "name": "M. Kankanhalli",
                        "slug": "M.-Kankanhalli",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kankanhalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kankanhalli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153386875"
                        ],
                        "name": "Qi Zhao",
                        "slug": "Qi-Zhao",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2713780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "709e8791193cc3fe9fef9ad983afcd2891e6c680",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "A large body of previous models to predict where people look in natural scenes focused on pixel-level image attributes. To bridge the semantic gap between the predictive power of computational saliency models and human behavior, we propose a new saliency architecture that incorporates information at three layers: pixel-level image attributes, object-level attributes, and semantic-level attributes. Object- and semantic-level information is frequently ignored, or only a few sample object categories are discussed where scaling to a large number of object categories is not feasible nor neurally plausible. To address this problem, this work constructs a principled vocabulary of basic attributes to describe object- and semantic-level information thus not restricting to a limited number of object categories. We build a new dataset of 700 images with eye-tracking data of 15 viewers and annotation data of 5,551 segmented objects with fine contours and 12 semantic attributes (publicly available with the paper). Experimental results demonstrate the importance of the object- and semantic-level information in the prediction of visual attention."
            },
            "slug": "Predicting-human-gaze-beyond-pixels.-Xu-Jiang",
            "title": {
                "fragments": [],
                "text": "Predicting human gaze beyond pixels."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A principled vocabulary of basic attributes is constructed to describe object- and semantic-level information thus not restricting to a limited number of object categories and experimental results demonstrate the importance of the object-and semantic- level information in the prediction of visual attention."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086328"
                        ],
                        "name": "Christoph Rhemann",
                        "slug": "Christoph-Rhemann",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Rhemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph Rhemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115485"
                        ],
                        "name": "A. Hosni",
                        "slug": "A.-Hosni",
                        "structuredName": {
                            "firstName": "Asmaa",
                            "lastName": "Hosni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hosni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873656"
                        ],
                        "name": "M. Bleyer",
                        "slug": "M.-Bleyer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bleyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bleyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990797"
                        ],
                        "name": "M. Gelautz",
                        "slug": "M.-Gelautz",
                        "structuredName": {
                            "firstName": "Margrit",
                            "lastName": "Gelautz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gelautz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1680724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f908d2fb9cfcaff17030a912e4811fb02aaeec03",
            "isKey": false,
            "numCitedBy": 879,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision tasks can be formulated as labeling problems. The desired solution is often a spatially smooth labeling where label transitions are aligned with color edges of the input image. We show that such solutions can be efficiently achieved by smoothing the label costs with a very fast edge preserving filter. In this paper we propose a generic and simple framework comprising three steps: (i) constructing a cost volume (ii) fast cost volume filtering and (iii) winner-take-all label selection. Our main contribution is to show that with such a simple framework state-of-the-art results can be achieved for several computer vision applications. In particular, we achieve (i) disparity maps in real-time, whose quality exceeds those of all other fast (local) approaches on the Middlebury stereo benchmark, and (ii) optical flow fields with very fine structures as well as large displacements. To demonstrate robustness, the few parameters of our framework are set to nearly identical values for both applications. Also, competitive results for interactive image segmentation are presented. With this work, we hope to inspire other researchers to leverage this framework to other application areas."
            },
            "slug": "Fast-cost-volume-filtering-for-visual-and-beyond-Rhemann-Hosni",
            "title": {
                "fragments": [],
                "text": "Fast cost-volume filtering for visual correspondence and beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a generic and simple framework comprising three steps: constructing a cost volume, fast cost volume filtering and winner-take-all label selection, and achieves state-of-the-art results that achieve disparity maps in real-time, and optical flow fields with very fine structures as well as large displacements."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125402"
                        ],
                        "name": "Francesco Dinuzzo",
                        "slug": "Francesco-Dinuzzo",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Dinuzzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesco Dinuzzo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706780"
                        ],
                        "name": "Cheng Soon Ong",
                        "slug": "Cheng-Soon-Ong",
                        "structuredName": {
                            "firstName": "Cheng Soon",
                            "lastName": "Ong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng Soon Ong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2871555"
                        ],
                        "name": "P. Gehler",
                        "slug": "P.-Gehler",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Gehler",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719301"
                        ],
                        "name": "G. Pillonetto",
                        "slug": "G.-Pillonetto",
                        "structuredName": {
                            "firstName": "Gianluigi",
                            "lastName": "Pillonetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pillonetto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5684889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d68055a44393551b885977deab3479d9b452343",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to learn simultaneously a vector-valued function and a kernel between its components. The obtained kernel can be used both to improve learning performance and to reveal structures in the output space which may be important in their own right. Our method is based on the solution of a suitable regularization problem over a reproducing kernel Hilbert space of vector-valued functions. Although the regularized risk functional is non-convex, we show that it is invex, implying that all local minimizers are global minimizers. We derive a block-wise coordinate descent method that efficiently exploits the structure of the objective functional. Then, we empirically demonstrate that the proposed method can improve classification accuracy. Finally, we provide a visual interpretation of the learned kernel matrix for some well known datasets."
            },
            "slug": "Learning-Output-Kernels-with-Block-Coordinate-Dinuzzo-Ong",
            "title": {
                "fragments": [],
                "text": "Learning Output Kernels with Block Coordinate Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A method to learn simultaneously a vector-valued function and a kernel between its components and a block-wise coordinate descent method that efficiently exploits the structure of the objective functional is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786693"
                        ],
                        "name": "Carl Doersch",
                        "slug": "Carl-Doersch",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Doersch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Doersch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6334137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b222b3b05d8d313420dbde8b163e4336a85dcde9",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical \"visual words\", but lower than full-blown semantic objects. Several approaches [5,6,12,23] have been proposed to discover mid-level visual elements, that are both 1) representative, i.e., frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difficult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm [2, 1, 4, 8]. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset of [5]. We also evaluate our method on the task of scene classification, demonstrating state-of-the-art performance on the MIT Scene-67 dataset."
            },
            "slug": "Mid-level-Visual-Element-Discovery-as-Mode-Seeking-Doersch-Gupta",
            "title": {
                "fragments": [],
                "text": "Mid-level Visual Element Discovery as Discriminative Mode Seeking"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Given a weakly-labeled image collection, this method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels, and proposes the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "In the last three years, mainly due to the advances of deep learning, more concretely convolutional networks [10], the quality of image recognition and object detection has been progressing at a dramatic pace."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7828,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143960345"
                        ],
                        "name": "Alexander Grushin",
                        "slug": "Alexander-Grushin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Grushin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Grushin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49629163"
                        ],
                        "name": "Derek Monner",
                        "slug": "Derek-Monner",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Monner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Monner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742049"
                        ],
                        "name": "J. Reggia",
                        "slug": "J.-Reggia",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Reggia",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reggia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112892256"
                        ],
                        "name": "A. Mishra",
                        "slug": "A.-Mishra",
                        "structuredName": {
                            "firstName": "Ajay",
                            "lastName": "Mishra",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mishra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12903970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "429a2c7fd324722dce57296ec2aead725b0d2104",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The long short-term memory (LSTM) neural network utilizes specialized modulation mechanisms to store information for extended periods of time. It is thus potentially well-suited for complex visual processing, where the current video frame must be considered in the context of past frames. Recent studies have indeed shown that LSTM can effectively recognize and classify human actions (e.g., running, hand waving) in video data; however, these results were achieved under somewhat restricted settings. In this effort, we seek to demonstrate that LSTM's performance remains robust even as experimental conditions deteriorate. Specifically, we show that classification accuracy exhibits graceful degradation when the LSTM network is faced with (a) lower quantities of available training data, (b) tighter deadlines for decision making (i.e., shorter available input data sequences) and (c) poorer video quality (resulting from noise, dropped frames or reduced resolution). We also clearly demonstrate the benefits of memory for video processing, particularly, under high noise or frame drop rates. Our study is thus an initial step towards demonstrating LSTM's potential for robust action recognition in real-world scenarios."
            },
            "slug": "Robust-human-action-recognition-via-long-short-term-Grushin-Monner",
            "title": {
                "fragments": [],
                "text": "Robust human action recognition via long short-term memory"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work shows that classification accuracy exhibits graceful degradation when the LSTM network is faced with lower quantities of available training data, tighter deadlines for decision making and poorer video quality under noise, dropped frames or reduced resolution."
            },
            "venue": {
                "fragments": [],
                "text": "The 2013 International Joint Conference on Neural Networks (IJCNN)"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40417846"
                        ],
                        "name": "Mayank Juneja",
                        "slug": "Mayank-Juneja",
                        "structuredName": {
                            "firstName": "Mayank",
                            "lastName": "Juneja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mayank Juneja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8763431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b730f5dfbd73172a4bba2d00d377a145c046bca",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper, we propose a simple, efficient, and effective method to do so. We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. In this manner, additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previous part-learning methods such as Singh et al. [28]. We also show that a well constructed bag of words or Fisher vector model can substantially outperform the previous state-of-the-art classification performance on this data."
            },
            "slug": "Blocks-That-Shout:-Distinctive-Parts-for-Scene-Juneja-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Blocks That Shout: Distinctive Parts for Scene Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a simple, efficient, and effective method to learn parts incrementally, starting from a single part occurrence with an Exemplar SVM, and can learn parts which are significantly more informative and for a fraction of the cost, compared to previous part-learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10702897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e8ce202fb51cd11d2b926e7ad21e08a58a43903",
            "isKey": false,
            "numCitedBy": 868,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains real time performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets."
            },
            "slug": "Structured-Forests-for-Fast-Edge-Detection-Doll\u00e1r-Zitnick",
            "title": {
                "fragments": [],
                "text": "Structured Forests for Fast Edge Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper forms the problem of predicting local edge masks in a structured learning framework applied to random decision forests and develops a novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747918"
                        ],
                        "name": "S. Bileschi",
                        "slug": "S.-Bileschi",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Bileschi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bileschi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2179592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71e3d9fc53ba14c2feeb7390f0dc99076553b05a",
            "isKey": false,
            "numCitedBy": 1714,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex"
            },
            "slug": "Robust-Object-Recognition-with-Cortex-Like-Serre-Wolf",
            "title": {
                "fragments": [],
                "text": "Robust Object Recognition with Cortex-Like Mechanisms"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123446103"
                        ],
                        "name": "Hilde Kuehne",
                        "slug": "Hilde-Kuehne",
                        "structuredName": {
                            "firstName": "Hilde",
                            "lastName": "Kuehne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hilde Kuehne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676982"
                        ],
                        "name": "Hueihan Jhuang",
                        "slug": "Hueihan-Jhuang",
                        "structuredName": {
                            "firstName": "Hueihan",
                            "lastName": "Jhuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hueihan Jhuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1930964"
                        ],
                        "name": "Est\u00edbaliz Garrote",
                        "slug": "Est\u00edbaliz-Garrote",
                        "structuredName": {
                            "firstName": "Est\u00edbaliz",
                            "lastName": "Garrote",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Est\u00edbaliz Garrote"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206769852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b3b8848a311c501e704c45c6d50430ab7068956",
            "isKey": false,
            "numCitedBy": 2544,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion."
            },
            "slug": "HMDB:-A-large-video-database-for-human-motion-Kuehne-Jhuang",
            "title": {
                "fragments": [],
                "text": "HMDB: A large video database for human motion recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper uses the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube, to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34824003"
                        ],
                        "name": "T. Sharp",
                        "slug": "T.-Sharp",
                        "structuredName": {
                            "firstName": "Toby",
                            "lastName": "Sharp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sharp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078931040"
                        ],
                        "name": "A. Kipman",
                        "slug": "A.-Kipman",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Kipman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kipman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848295"
                        ],
                        "name": "M. Finocchio",
                        "slug": "M.-Finocchio",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Finocchio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Finocchio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40636177"
                        ],
                        "name": "Mat Cook",
                        "slug": "Mat-Cook",
                        "structuredName": {
                            "firstName": "Mat",
                            "lastName": "Cook",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mat Cook"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144564063"
                        ],
                        "name": "R. Moore",
                        "slug": "R.-Moore",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7731948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2915510a39448503ee873f9693cd3808ca74bd81",
            "isKey": false,
            "numCitedBy": 2729,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state of the art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching."
            },
            "slug": "Real-time-human-pose-recognition-in-parts-from-Shotton-Sharp",
            "title": {
                "fragments": [],
                "text": "Real-time human pose recognition in parts from single depth images"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work takes an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem, and generates confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3502855"
                        ],
                        "name": "Marcin Marszalek",
                        "slug": "Marcin-Marszalek",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Marszalek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Marszalek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3261451"
                        ],
                        "name": "Benjamin Rozenfeld",
                        "slug": "Benjamin-Rozenfeld",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Rozenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Rozenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12365014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f86767732f76f478d5845f2e59f99ba106e9265",
            "isKey": false,
            "numCitedBy": 3595,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results."
            },
            "slug": "Learning-realistic-human-actions-from-movies-Laptev-Marszalek",
            "title": {
                "fragments": [],
                "text": "Learning realistic human actions from movies"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new method for video classification that builds upon and extends several recent ideas including local space-time features,space-time pyramids and multi-channel non-linear SVMs is presented and shown to improve state-of-the-art results on the standard KTH action dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "ConvNets have traditionally used random and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and improve learning, yet the trend changed back to full connections with [9] in order to further optimize parallel computation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35241,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145584908"
                        ],
                        "name": "Hui Liang",
                        "slug": "Hui-Liang",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34316743"
                        ],
                        "name": "Junsong Yuan",
                        "slug": "Junsong-Yuan",
                        "structuredName": {
                            "firstName": "Junsong",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junsong Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143725529"
                        ],
                        "name": "D. Thalmann",
                        "slug": "D.-Thalmann",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Thalmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Thalmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12732457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8455f7e992596fbb39212897a9421c9005949f76",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Hand pose tracking and gesture recognition are useful for human-computer interaction, while a major problem is the lack of discriminative features for compact hand representation. We present a robust hand parsing scheme to extract a high-level description of the hand from the depth image. A novel distance-adaptive selection method is proposed to get more discriminative depth-context features. Besides, we propose a Superpixel-Markov Random Field (SMRF) parsing scheme to enforce the spatial smoothness and the label co-occurrence prior to remove the misclassified regions. Compared to pixel-level filtering, the SMRF scheme is more suitable to model the misclassified regions. By fusing the temporal constraints, its performance can be further improved. Overall, the proposed hand parsing scheme is accurate and efficient. The tests on synthesized dataset show it gives much higher accuracy for single-frame parsing and enhanced robustness for continuous sequence parsing compared to benchmarks. The tests on real-world depth images of the hand and human body show the robustness to complex hand configurations of our method and its generalization power to different kinds of articulated objects."
            },
            "slug": "Parsing-the-Hand-in-Depth-Images-Liang-Yuan",
            "title": {
                "fragments": [],
                "text": "Parsing the Hand in Depth Images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A robust hand parsing scheme to extract a high-level description of the hand from the depth image is presented and a Superpixel-Markov Random Field (SMRF) parsing scheme is proposed to enforce the spatial smoothness and the label co-occurrence prior to remove the misclassified regions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3171632"
                        ],
                        "name": "A. Quattoni",
                        "slug": "A.-Quattoni",
                        "structuredName": {
                            "firstName": "Ariadna",
                            "lastName": "Quattoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Quattoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7910040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c82d90336ba365c7914fe4bd6c292a8c6916a801",
            "isKey": false,
            "numCitedBy": 680,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g, bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information. In this paper we propose a prototype based model that can successfully combine both sources of information. To test our approach we created a dataset of 67 indoor scenes categories (the largest available) covering a wide range of domains. The results show that our approach can significantly outperform a state of the art classifier for the task."
            },
            "slug": "Recognizing-indoor-scenes-Quattoni-Torralba",
            "title": {
                "fragments": [],
                "text": "Recognizing indoor scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A prototype based model that can successfully combine local and global discriminative information is proposed that can significantly outperform a state of the art classifier for the indoor scene recognition task."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823362"
                        ],
                        "name": "J. Uijlings",
                        "slug": "J.-Uijlings",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Uijlings",
                            "middleNames": [
                                "R.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uijlings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11442196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37e41557932cc0035eab23fd767bde68f6475c3a",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge."
            },
            "slug": "Segmentation-as-selective-search-for-object-Sande-Uijlings",
            "title": {
                "fragments": [],
                "text": "Segmentation as selective search for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work adapt segmentation as a selective search by reconsidering segmentation to generate many approximate locations over few and precise object delineations because an object whose location is never generated can not be recognised and appearance and immediate nearby context are most effective for object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37108776"
                        ],
                        "name": "Philip Lenz",
                        "slug": "Philip-Lenz",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6724907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42",
            "isKey": false,
            "numCitedBy": 7182,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti."
            },
            "slug": "Are-we-ready-for-autonomous-driving-The-KITTI-suite-Geiger-Lenz",
            "title": {
                "fragments": [],
                "text": "Are we ready for autonomous driving? The KITTI vision benchmark suite"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The autonomous driving platform is used to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection, revealing that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3010882"
                        ],
                        "name": "Johannes L. Sch\u00f6nberger",
                        "slug": "Johannes-L.-Sch\u00f6nberger",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Sch\u00f6nberger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johannes L. Sch\u00f6nberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708577"
                        ],
                        "name": "Filip Radenovi\u0107",
                        "slug": "Filip-Radenovi\u0107",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Radenovi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filip Radenovi\u0107"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40454588"
                        ],
                        "name": "Jan-Michael Frahm",
                        "slug": "Jan-Michael-Frahm",
                        "structuredName": {
                            "firstName": "Jan-Michael",
                            "lastName": "Frahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan-Michael Frahm"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7989386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e82e2b3c4930cfca6572ce04257bf63683141297",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Structure-from-Motion for unordered image collections has significantly advanced in scale over the last decade. This impressive progress can be in part attributed to the introduction of efficient retrieval methods for those systems. While this boosts scalability, it also limits the amount of detail that the large-scale reconstruction systems are able to produce. In this paper, we propose a joint reconstruction and retrieval system that maintains the scalability of large-scale Structure-from-Motion systems while also recovering the often lost ability of reconstructing fine details of the scene. We demonstrate our proposed method on a large-scale dataset of 7.4 million images downloaded from the Internet."
            },
            "slug": "From-single-image-query-to-detailed-3D-Sch\u00f6nberger-Radenovi\u0107",
            "title": {
                "fragments": [],
                "text": "From single image query to detailed 3D reconstruction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a joint reconstruction and retrieval system that maintains the scalability of large-scale Structure-from-Motion systems while also recovering the often lost ability of reconstructing fine details of the scene."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71563118"
                        ],
                        "name": "Jinjun Wang",
                        "slug": "Jinjun-Wang",
                        "structuredName": {
                            "firstName": "Jinjun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinjun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706007"
                        ],
                        "name": "Jianchao Yang",
                        "slug": "Jianchao-Yang",
                        "structuredName": {
                            "firstName": "Jianchao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianchao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39157653"
                        ],
                        "name": "Fengjun Lv",
                        "slug": "Fengjun-Lv",
                        "structuredName": {
                            "firstName": "Fengjun",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengjun Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 82
                            }
                        ],
                        "text": "Recent work has focused on motion descriptors that are invariant to camera motion [5, 6, 8, 10, 11, 12, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Although (1) is a general model that includes as special ca es several popular methods like bag of words (BoW) [36], VLAD [18] and Hamming Embedding (HE) [14], it is clearly motivated by discarding geometric information for efficiency reasons and searching e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6718692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f7713dcc35e7c05becf3be5522f36c9546b0364",
            "isKey": false,
            "numCitedBy": 3240,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least square fitting problem, bearing computational complexity of O(M + K2). Hence even with very large codebooks, our system can still process multiple frames per second. This efficiency significantly adds to the practical values of LLC for real applications."
            },
            "slug": "Locality-constrained-Linear-Coding-for-image-Wang-Yang",
            "title": {
                "fragments": [],
                "text": "Locality-constrained Linear Coding for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM, using the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38916673"
                        ],
                        "name": "B. Yao",
                        "slug": "B.-Yao",
                        "structuredName": {
                            "firstName": "Bangpeng",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195129"
                        ],
                        "name": "Xiaoye Jiang",
                        "slug": "Xiaoye-Jiang",
                        "structuredName": {
                            "firstName": "Xiaoye",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoye Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32165404"
                        ],
                        "name": "A. Lin",
                        "slug": "A.-Lin",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Lin",
                            "middleNames": [
                                "Lai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744254"
                        ],
                        "name": "L. Guibas",
                        "slug": "L.-Guibas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Guibas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Guibas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7455708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e15dc51de6da2bc5cabbb733cf2adf5a2c1f72c",
            "isKey": false,
            "numCitedBy": 546,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we propose to use attributes and parts for recognizing human actions in still images. We define action attributes as the verbs that describe the properties of human actions, while the parts of actions are objects and poselets that are closely related to the actions. We jointly model the attributes and parts by learning a set of sparse bases that are shown to carry much semantic meaning. Then, the attributes and parts of an action image can be reconstructed from sparse coefficients with respect to the learned bases. This dual sparsity provides theoretical guarantee of our bases learning and feature reconstruction approach. On the PASCAL action dataset and a new \u201cStanford 40 Actions\u201d dataset, we show that our method extracts meaningful high-order interactions between attributes and parts in human actions while achieving state-of-the-art classification performance."
            },
            "slug": "Human-action-recognition-by-learning-bases-of-and-Yao-Jiang",
            "title": {
                "fragments": [],
                "text": "Human action recognition by learning bases of action attributes and parts"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work proposes to use attributes and parts for recognizing human actions in still images by learning a set of sparse bases that are shown to carry much semantic meaning, and shows that this dual sparsity provides theoretical guarantee of the bases learning and feature reconstruction approach."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37581938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47980c6e42f1a3381e6c5f3db7230e6a64c40218",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 218,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis targets the detection of humans and other object classes in images and videos. Our focus is on developing robust feature extraction algorithms that encode image regions as highdimensional feature vectors that support high accuracy object/non-object decisions. To test our feature sets we adopt a relatively simple learning framework that uses linear Support Vector Machines to classify each possible image region as an object or as a non-object. The approach is data-driven and purely bottom-up using low-level appearance and motion vectors to detect objects. As a test case we focus on person detection as people are one of the most challenging object classes with many applications, for example in film and video analysis, pedestrian detection for smart cars and video surveillance. Nevertheless we do not make any strong class specific assumptions and the resulting object detection framework also gives state-of-the-art performance for many other classes including cars, motorbikes, cows and sheep. This thesis makes four main contributions. Firstly, we introduce grids of locally normalised Histograms of Oriented Gradients (HOG) as descriptors for object detection in static images. The HOG descriptors are computed over dense and overlapping grids of spatial blocks, with image gradient orientation features extracted at fixed resolution and gathered into a highdimensional feature vector. They are designed to be robust to small changes in image contour locations and directions, and significant changes in image illumination and colour, while remaining highly discriminative for overall visual form. We show that unsmoothed gradients, fine orientation voting, moderately coarse spatial binning, strong normalisation and overlapping blocks are all needed for good performance. Secondly, to detect moving humans in videos, we propose descriptors based on oriented histograms of differential optical flow. These are similar to static HOG descriptors, but instead of image gradients, they are based on local differentials of dense optical flow. They encode the noisy optical flow estimates into robust feature vectors in a manner that is robust to the overall camera motion. Several variants are proposed, some capturing motion boundaries while others encode the relative motions of adjacent image regions. Thirdly, we propose a general method based on kernel density estimation for fusing multiple overlapping detections, that takes into account the number of detections, their confidence scores and the scales of the detections. Lastly, we present work in progress on a parts based approach to person detection that first detects local body parts like heads, torso, and legs and then fuses them to create a global overall person detector."
            },
            "slug": "Finding-People-in-Images-and-Videos-Dalal",
            "title": {
                "fragments": [],
                "text": "Finding People in Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis introduces grids of locally normalised Histograms of Oriented Gradients (HOG) as descriptors for object detection in static images and proposes descriptors based on oriented histograms of differential optical flow to detect moving humans in videos."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144574904"
                        ],
                        "name": "Li Xu",
                        "slug": "Li-Xu",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111073993"
                        ],
                        "name": "Shicheng Zheng",
                        "slug": "Shicheng-Zheng",
                        "structuredName": {
                            "firstName": "Shicheng",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shicheng Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729056"
                        ],
                        "name": "Jiaya Jia",
                        "slug": "Jiaya-Jia",
                        "structuredName": {
                            "firstName": "Jiaya",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaya Jia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1916689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "369e64f4bc471eca37b92c7ef94be904bb9615ba",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We show in this paper that the success of previous maximum a posterior (MAP) based blur removal methods partly stems from their respective intermediate steps, which implicitly or explicitly create an unnatural representation containing salient image structures. We propose a generalized and mathematically sound L0 sparse expression, together with a new effective method, for motion deblurring. Our system does not require extra filtering during optimization and demonstrates fast energy decreasing, making a small number of iterations enough for convergence. It also provides a unified framework for both uniform and non-uniform motion deblurring. We extensively validate our method and show comparison with other approaches with respect to convergence speed, running time, and result quality."
            },
            "slug": "Unnatural-L0-Sparse-Representation-for-Natural-Xu-Zheng",
            "title": {
                "fragments": [],
                "text": "Unnatural L0 Sparse Representation for Natural Image Deblurring"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes a generalized and mathematically sound L0 sparse expression, together with a new effective method, for motion deblurring that does not require extra filtering during optimization and demonstrates fast energy decreasing, making a small number of iterations enough for convergence."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8393812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd1a6669bdfb35c4db0dbd872dc7c58b44fe376a",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman-Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes."
            },
            "slug": "Shared-Segmentation-of-Natural-Scenes-Using-Sudderth-Jordan",
            "title": {
                "fragments": [],
                "text": "Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work develops a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases, and uses Gaussian processes to discover spatially contiguous segments which respect image boundaries."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084916713"
                        ],
                        "name": "Hector Bernal",
                        "slug": "Hector-Bernal",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Bernal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hector Bernal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7086636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffeb684b193afd21aefc5a6b05fb3616cc99418e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In an object recognition scenario with tens of thousands of categories, even a small number of labels per category leads to a very large number of total labels required. We propose a simple method of label sharing between semantically similar categories. We leverage the WordNet hierarchy to define semantic distance between any two categories and use this semantic distance to share labels. Our approach can be used with any classifier. Experimental results on a range of datasets, upto 80 million images and 75,000 categories in size, show that despite the simplicity of the approach, it leads to significant improvements in performance."
            },
            "slug": "Semantic-Label-Sharing-for-Learning-with-Many-Fergus-Bernal",
            "title": {
                "fragments": [],
                "text": "Semantic Label Sharing for Learning with Many Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simple method of label sharing between semantically similar categories is proposed that leverages the WordNet hierarchy to define semantic distance between any two categories and use this semantic distance to share labels."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704494"
                        ],
                        "name": "Jonathan Tompson",
                        "slug": "Jonathan-Tompson",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Tompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Tompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34900570"
                        ],
                        "name": "Murphy Stein",
                        "slug": "Murphy-Stein",
                        "structuredName": {
                            "firstName": "Murphy",
                            "lastName": "Stein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Murphy Stein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2307214"
                        ],
                        "name": "K. Perlin",
                        "slug": "K.-Perlin",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Perlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Perlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5736984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae639e4bdd2e6a11bc44ff7f1ae53cd25462042b",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel method for real-time continuous pose recovery of markerless complex articulable objects from a single depth image. Our method consists of the following stages: a randomized decision forest classifier for image segmentation, a robust method for labeled dataset generation, a convolutional network for dense feature extraction, and finally an inverse kinematics stage for stable real-time pose recovery. As one possible application of this pipeline, we show state-of-the-art results for real-time puppeteering of a skinned hand-model."
            },
            "slug": "Real-Time-Continuous-Pose-Recovery-of-Human-Hands-Tompson-Stein",
            "title": {
                "fragments": [],
                "text": "Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A novel method for real-time continuous pose recovery of markerless complex articulable objects from a single depth image using a randomized decision forest classifier for image segmentation, a robust method for labeled dataset generation, a convolutional network for dense feature extraction, and finally an inverse kinematics stage for stable real- time pose recovery."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276694"
                        ],
                        "name": "M. Havlena",
                        "slug": "M.-Havlena",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Havlena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Havlena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144810819"
                        ],
                        "name": "K. Schindler",
                        "slug": "K.-Schindler",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Schindler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schindler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15285158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4415e7ee6770388ef367ebb8beb6de3d7a245b4",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature matching between pairs of images is a main bottleneck of structure-from-motion computation from large, unordered image sets. We propose an efficient way to establish point correspondences between all pairs of images in a dataset, without having to test each individual pair. The principal message of this paper is that, given a sufficiently large visual vocabulary, feature matching can be cast as image indexing, subject to the additional constraints that index words must be rare in the database and unique in each image. We demonstrate that the proposed matching method, in conjunction with a standard inverted file, is 2-3 orders of magnitude faster than conventional pairwise matching. The proposed vocabulary-based matching has been integrated into a standard SfM pipeline, and delivers results similar to those of the conventional method in much less time."
            },
            "slug": "VocMatch:-Efficient-Multiview-Correspondence-for-Havlena-Schindler",
            "title": {
                "fragments": [],
                "text": "VocMatch: Efficient Multiview Correspondence for Structure from Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The principal message of this paper is that, given a sufficiently large visual vocabulary, feature matching can be cast as image indexing, subject to the additional constraints that index words must be rare in the database and unique in each image."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790643"
                        ],
                        "name": "S. Schulter",
                        "slug": "S.-Schulter",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Schulter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schulter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202367"
                        ],
                        "name": "Paul Wohlhart",
                        "slug": "Paul-Wohlhart",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Wohlhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Wohlhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695579"
                        ],
                        "name": "C. Leistner",
                        "slug": "C.-Leistner",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Leistner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leistner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741702"
                        ],
                        "name": "Amir Saffari",
                        "slug": "Amir-Saffari",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Saffari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amir Saffari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791182"
                        ],
                        "name": "P. Roth",
                        "slug": "P.-Roth",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Roth",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6557162,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffc1e4ba9fb5b8535a0193d90f1bc253c4dae540",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits of common Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth."
            },
            "slug": "Alternating-Decision-Forests-Schulter-Wohlhart",
            "title": {
                "fragments": [],
                "text": "Alternating Decision Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem, and derives the new classifier and gives a discussion and evaluation on standard machine learning data sets."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1041733,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fbfd6a9daa73756196ec663d65019a7a9b58600",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the term cosegmentation which denotes the task of segmenting simultaneously the common parts of an image pair. A generative model for cosegmentation is presented. Inference in the model leads to minimizing an energy with an MRF term encoding spatial coherency and a global constraint which attempts to match the appearance histograms of the common parts. This energy has not been proposed previously and its optimization is challenging and NP-hard. For this problem a novel optimization scheme which we call trust region graph cuts is presented. We demonstrate that this framework has the potential to improve a wide range of research: Object driven image retrieval, video tracking and segmentation, and interactive image editing. The power of the framework lies in its generality, the common part can be a rigid/non-rigid object (or scene), observed from different viewpoints or even similar objects of the same class."
            },
            "slug": "Cosegmentation-of-Image-Pairs-by-Histogram-Matching-Rother-Minka",
            "title": {
                "fragments": [],
                "text": "Cosegmentation of Image Pairs by Histogram Matching - Incorporating a Global Constraint into MRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is demonstrated that this generative model for cosegmentation has the potential to improve a wide range of research: Object driven image retrieval, video tracking and segmentation, and interactive image editing."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145371957"
                        ],
                        "name": "Chang Xu",
                        "slug": "Chang-Xu",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143719920"
                        ],
                        "name": "D. Tao",
                        "slug": "D.-Tao",
                        "structuredName": {
                            "firstName": "Dacheng",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31796215"
                        ],
                        "name": "Chao Xu",
                        "slug": "Chao-Xu",
                        "structuredName": {
                            "firstName": "Chao",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao Xu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2054489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43996253f97d7023af1b1869a7ec2e6376ca1986",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we extend the theory of the information bottleneck (IB) to learning from examples represented by multi-view features. We formulate the problem as one of encoding a communication system with multiple senders, each of which represents one view of the data. Based on the precise components filtered out from multiple information sources through a \u201cbottleneck\u201d, a margin maximization approach is then used to strengthen the discrimination of the encoder by improving the code distance within the frame of coding theory. The resulting algorithm therefore inherits all the merits of the IB principle and coding theory. It has two distinct advantages over existing algorithms, namely, that our method finds a tradeoff between the accuracy and complexity of the multi-view model, and that the encoded multi-view data retains sufficient discrimination for classification. We also derive the robustness and generalization error bound of the proposed algorithm, and reveal the specific properties of multi-view learning. First, the complementarity of multi-view features guarantees the robustness of the algorithm. Second, the consensus of multi-view features reduces the empirical Rademacher complexity of the objective function, enhances the accuracy of the solution, and improves the generalization error bound of the algorithm. The resulting objective function is solved efficiently using the alternating direction method. Experimental results on annotation, classification and recognition tasks demonstrate that the proposed algorithm is promising for practical applications."
            },
            "slug": "Large-Margin-Multi-ViewInformation-Bottleneck-Xu-Tao",
            "title": {
                "fragments": [],
                "text": "Large-Margin Multi-ViewInformation Bottleneck"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper forms the problem as one of encoding a communication system with multiple senders, each of which represents one view of the data, and derives the robustness and generalization error bound of the proposed algorithm, and reveals the specific properties of multi-view learning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109615009"
                        ],
                        "name": "Xi Zhou",
                        "slug": "Xi-Zhou",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49104973"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7405065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e65c9f0a64b6a4333b12e2adc3861ad75aca83b",
            "isKey": false,
            "numCitedBy": 555,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new framework for image classification using local visual descriptors. The pipeline first performs a non-linear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model. For all the three steps we suggest novel solutions which make our approach appealing in theory, more scalable in computation, and transparent in classification. Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks."
            },
            "slug": "Image-Classification-Using-Super-Vector-Coding-of-Zhou-Yu",
            "title": {
                "fragments": [],
                "text": "Image Classification Using Super-Vector Coding of Local Image Descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper introduces a new framework for image classification using local visual descriptors that first performs a non-linear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30385154"
                        ],
                        "name": "A. Spyropoulos",
                        "slug": "A.-Spyropoulos",
                        "structuredName": {
                            "firstName": "Aristotle",
                            "lastName": "Spyropoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Spyropoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505902"
                        ],
                        "name": "N. Komodakis",
                        "slug": "N.-Komodakis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Komodakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Komodakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706145"
                        ],
                        "name": "Philippos Mordohai",
                        "slug": "Philippos-Mordohai",
                        "structuredName": {
                            "firstName": "Philippos",
                            "lastName": "Mordohai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippos Mordohai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 218500984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "248d372355bb752ad6c05f4e84ee44aaeb47b68f",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "While machine learning has been instrumental to the ongoing progress in most areas of computer vision, it has not been applied to the problem of stereo matching with similar frequency or success. We present a supervised learning approach for predicting the correctness of stereo matches based on a random forest and a set of features that capture various forms of information about each pixel. We show highly competitive results in predicting the correctness of matches and in confidence estimation, which allows us to rank pixels according to the reliability of their assigned disparities. Moreover, we show how these confidence values can be used to improve the accuracy of disparity maps by integrating them with an MRF-based stereo algorithm. This is an important distinction from current literature that has mainly focused on sparsification by removing potentially erroneous disparities to generate quasi-dense disparity maps."
            },
            "slug": "Learning-to-Detect-Ground-Control-Points-for-the-of-Spyropoulos-Komodakis",
            "title": {
                "fragments": [],
                "text": "Learning to Detect Ground Control Points for Improving the Accuracy of Stereo Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work presents a supervised learning approach for predicting the correctness of stereo matches based on a random forest and a set of features that capture various forms of information about each pixel, and shows highly competitive results in predicting the Correctness of matches and in confidence estimation."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 918485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d905b121e3e1f27d1f72195e27b7c8ac1a4386",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Bourdev and Malik (ICCV 09) introduced a new notion of parts, poselets, constructed to be tightly clustered both in the configuration space of keypoints, as well as in the appearance space of image patches. In this paper we develop a new algorithm for detecting people using poselets. Unlike that work which used 3D annotations of keypoints, we use only 2D annotations which are much easier for naive human annotators. The main algorithmic contribution is in how we use the pattern of poselet activations. Individual poselet activations are noisy, but considering the spatial context of each can provide vital disambiguating information, just as object detection can be improved by considering the detection scores of nearby objects in the scene. This can be done by training a two-layer feed-forward network with weights set using a max margin technique. The refined poselet activations are then clustered into mutually consistent hypotheses where consistency is based on empirically determined spatial keypoint distributions. Finally, bounding boxes are predicted for each person hypothesis and shape masks are aligned to edges in the image to provide a segmentation. To the best of our knowledge, the resulting system is the current best performer on the task of people detection and segmentation with an average precision of 47.8% and 40.5% respectively on PASCAL VOC 2009."
            },
            "slug": "Detecting-People-Using-Mutually-Consistent-Poselet-Bourdev-Maji",
            "title": {
                "fragments": [],
                "text": "Detecting People Using Mutually Consistent Poselet Activations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new algorithm for detecting people using poselets is developed which uses only 2D annotations which are much easier for naive human annotators and is the current best performer on the task of people detection and segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789486"
                        ],
                        "name": "G. Finlayson",
                        "slug": "G.-Finlayson",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Finlayson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Finlayson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17682180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fed350bc7c6ec7d402850c7a06b81aa0907f06d0",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Image colors are biased by the color of the prevailing illumination. As such the color at pixel cannot always be used directly in solving vision tasks from recognition, to tracking to general scene understanding. Illuminant estimation algorithms attempt to infer the color of the light incident in a scene and then a color cast removal step discounts the color bias due to illumination. However, despite sustained research since almost the inception of computer vision, progress has been modest. The best algorithms - now often built on top of expensive feature extraction and machine learning - are only about twice as good as the simplest approaches. This paper, in effect, will show how simple moment based algorithms - such as Gray-World - can, with the addition of a simple correction step, deliver much improved illuminant estimation performance. The corrected Gray-World algorithm maps the mean image color using a fixed (per camera) 3x3 matrix transform. More generally, our moment approach employs 1st, 2nd and higher order moments - of colors or features such as color derivatives - and these again are linearly corrected to give an illuminant estimate. The question of how to correct the moments is an important one yet we will show a simple alternating least-squares training procedure suffices. Remarkably, across the major datasets - evaluated using a 3-fold cross validation procedure - our simple corrected moment approach always delivers the best results (and the performance increment is often large compared with the prior art). Significantly, outlier performance was found to be much improved."
            },
            "slug": "Corrected-Moment-Illuminant-Estimation-Finlayson",
            "title": {
                "fragments": [],
                "text": "Corrected-Moment Illuminant Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper will show how simple moment based algorithms - such as Gray-World - can, with the addition of a simple correction step, deliver much improved illuminant estimation performance."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3860190"
                        ],
                        "name": "Cem Keskin",
                        "slug": "Cem-Keskin",
                        "structuredName": {
                            "firstName": "Cem",
                            "lastName": "Keskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cem Keskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460659"
                        ],
                        "name": "Mustafa Furkan K\u0131ra\u00e7",
                        "slug": "Mustafa-Furkan-K\u0131ra\u00e7",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "K\u0131ra\u00e7",
                            "middleNames": [
                                "Furkan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mustafa Furkan K\u0131ra\u00e7"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3348842"
                        ],
                        "name": "Yunus Emre Kara",
                        "slug": "Yunus-Emre-Kara",
                        "structuredName": {
                            "firstName": "Yunus",
                            "lastName": "Kara",
                            "middleNames": [
                                "Emre"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunus Emre Kara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694599"
                        ],
                        "name": "L. Akarun",
                        "slug": "L.-Akarun",
                        "structuredName": {
                            "firstName": "Lale",
                            "lastName": "Akarun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Akarun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44680990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bacd80a1b178a44666e712daf7923fd9a0bede2",
            "isKey": false,
            "numCitedBy": 302,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Vision based articulated hand pose estimation and hand shape classification are challenging problems. This paper proposes novel algorithms to perform these tasks using depth sensors. In particular, we introduce a novel randomized decision forest (RDF) based hand shape classifier, and use it in a novel multi---layered RDF framework for articulated hand pose estimation. This classifier assigns the input depth pixels to hand shape classes, and directs them to the corresponding hand pose estimators trained specifically for that hand shape. We introduce two novel types of multi---layered RDFs: Global Expert Network (GEN) and Local Expert Network (LEN), which achieve significantly better hand pose estimates than a single---layered skeleton estimator and generalize better to previously unseen hand poses. The novel hand shape classifier is also shown to be accurate and fast. The methods run in real---time on the CPU, and can be ported to the GPU for further increase in speed."
            },
            "slug": "Hand-Pose-Estimation-and-Hand-Shape-Classification-Keskin-K\u0131ra\u00e7",
            "title": {
                "fragments": [],
                "text": "Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Two novel types of multi---layered RDFs are introduced: Global Expert Network (GEN) and Local Expert network (LEN), which achieve significantly better hand pose estimates than a single---layering skeleton estimator and generalize better to previously unseen hand poses."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3366378"
                        ],
                        "name": "Stefan Hinterstoi\u00dfer",
                        "slug": "Stefan-Hinterstoi\u00dfer",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Hinterstoi\u00dfer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Hinterstoi\u00dfer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819129676"
                        ],
                        "name": "Stefan Holzer",
                        "slug": "Stefan-Holzer",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Holzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Holzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782397"
                        ],
                        "name": "Cedric Cagniart",
                        "slug": "Cedric-Cagniart",
                        "structuredName": {
                            "firstName": "Cedric",
                            "lastName": "Cagniart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cedric Cagniart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46505857"
                        ],
                        "name": "Slobodan Ilic",
                        "slug": "Slobodan-Ilic",
                        "structuredName": {
                            "firstName": "Slobodan",
                            "lastName": "Ilic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slobodan Ilic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70162540"
                        ],
                        "name": "K. Konolige",
                        "slug": "K.-Konolige",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Konolige",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Konolige"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145587210"
                        ],
                        "name": "Nassir Navab",
                        "slug": "Nassir-Navab",
                        "structuredName": {
                            "firstName": "Nassir",
                            "lastName": "Navab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nassir Navab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689738"
                        ],
                        "name": "V. Lepetit",
                        "slug": "V.-Lepetit",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Lepetit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lepetit"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13454288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "472df520353b3715ee8e5234db01d0f551e6d9a2",
            "isKey": false,
            "numCitedBy": 435,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for detecting 3D objects using multi-modalities. While it is generic, we demonstrate it on the combination of an image and a dense depth map which give complementary object information. It works in real-time, under heavy clutter, does not require a time consuming training stage, and can handle untextured objects. It is based on an efficient representation of templates that capture the different modalities, and we show in many experiments on commodity hardware that our approach significantly outperforms state-of-the-art methods on single modalities."
            },
            "slug": "Multimodal-templates-for-real-time-detection-of-in-Hinterstoi\u00dfer-Holzer",
            "title": {
                "fragments": [],
                "text": "Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work presents a method for detecting 3D objects using multi-modalities based on an efficient representation of templates that capture the different modalities, and shows in many experiments on commodity hardware that it significantly outperforms state-of-the-art methods on single modalities."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40406620"
                        ],
                        "name": "R. Haeusler",
                        "slug": "R.-Haeusler",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Haeusler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haeusler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066327290"
                        ],
                        "name": "Rahul Nair",
                        "slug": "Rahul-Nair",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Nair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Nair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793546"
                        ],
                        "name": "D. Kondermann",
                        "slug": "D.-Kondermann",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kondermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kondermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10421625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09c2e0d4aabe137789e4da403b712fabc9b20c5b",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawn from the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census data term, which is the best performing real-time capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset."
            },
            "slug": "Ensemble-Learning-for-Confidence-Measures-in-Stereo-Haeusler-Nair",
            "title": {
                "fragments": [],
                "text": "Ensemble Learning for Confidence Measures in Stereo Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work applies the random decision forest framework to a large set of diverse stereo confidence measures and obtains consistently improved area under curve values of sparsification measures in comparison to best performing single stereoconfidence measures where numbers of stereo errors are large."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152329702"
                        ],
                        "name": "Ning Zhang",
                        "slug": "Ning-Zhang",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49791682"
                        ],
                        "name": "Ryan Farrell",
                        "slug": "Ryan-Farrell",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Farrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Farrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346186"
                        ],
                        "name": "Forrest N. Iandola",
                        "slug": "Forrest-N.-Iandola",
                        "structuredName": {
                            "firstName": "Forrest",
                            "lastName": "Iandola",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Forrest N. Iandola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8657174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19c9ac899d5c1a008eaee887556bc1b61ff8132e",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements of our approach over state-of-art algorithms."
            },
            "slug": "Deformable-Part-Descriptors-for-Fine-Grained-and-Zhang-Farrell",
            "title": {
                "fragments": [],
                "text": "Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models based on strongly-supervised DPM parts, which enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760861"
                        ],
                        "name": "N. Razavi",
                        "slug": "N.-Razavi",
                        "structuredName": {
                            "firstName": "Nima",
                            "lastName": "Razavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Razavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15303062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8339da16dced3343173fc71373012d73c9f59f8d",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Scalability of object detectors with respect to the number of classes is a very important issue for applications where many object classes need to be detected. While combining single-class detectors yields a linear complexity for testing, multi-class detectors that localize all objects at once come often at the cost of a reduced detection accuracy. In this work, we present a scalable multi-class detection algorithm which scales sublinearly with the number of classes without compromising accuracy. To this end, a shared discriminative codebook of feature appearances is jointly trained for all classes and detection is also performed for all classes jointly. Based on the learned sharing distributions of features among classes, we build a taxonomy of object classes. The taxonomy is then exploited to further reduce the cost of multi-class object detection. Our method has linear training and sublinear detection complexity in the number of classes. We have evaluated our method on the challenging PASCAL VOC'06 and PASCAL VOC'07 datasets and show that scaling the system does not lead to a loss in accuracy."
            },
            "slug": "Scalable-multi-class-object-detection-Razavi-Gall",
            "title": {
                "fragments": [],
                "text": "Scalable multi-class object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work presents a scalable multi-class detection algorithm which scales sublinearly with the number of classes without compromising accuracy, and builds a taxonomy of object classes which is exploited to further reduce the cost of multi- class object detection."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2451341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e083dc8aeb7983a5cdff146985363d38caf0886",
            "isKey": false,
            "numCitedBy": 609,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases, we help identify future research directions for the field."
            },
            "slug": "Pedestrian-detection:-A-benchmark-Doll\u00e1r-Wojek",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection: A benchmark"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Caltech Pedestrian Dataset is introduced, which is two orders of magnitude larger than existing datasets and proposes improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38101706"
                        ],
                        "name": "Xiaodong Yang",
                        "slug": "Xiaodong-Yang",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 574767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae76644810fa6ed28d2a0765f444fa0c950bf51a",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new framework for human activity recognition from video sequences captured by a depth camera. We cluster hypersurface normals in a depth sequence to form the polynormal which is used to jointly characterize the local motion and shape information. In order to globally capture the spatial and temporal orders, an adaptive spatio-temporal pyramid is introduced to subdivide a depth video into a set of space-time grids. We then propose a novel scheme of aggregating the low-level polynormals into the super normal vector (SNV) which can be seen as a simplified version of the Fisher kernel representation. In the extensive experiments, we achieve classification results superior to all previous published results on the four public benchmark datasets, i.e., MSRAction3D, MSRDailyActivity3D, MSRGesture3D, and MSRActionPairs3D."
            },
            "slug": "Super-Normal-Vector-for-Activity-Recognition-Using-Yang-Tian",
            "title": {
                "fragments": [],
                "text": "Super Normal Vector for Activity Recognition Using Depth Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new framework for human activity recognition from video sequences captured by a depth camera is presented and a novel scheme of aggregating the low-level polynormals into the super normal vector (SNV) is proposed which can be seen as a simplified version of the Fisher kernel representation."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232265"
                        ],
                        "name": "Deqing Sun",
                        "slug": "Deqing-Sun",
                        "structuredName": {
                            "firstName": "Deqing",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deqing Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49820715"
                        ],
                        "name": "Jonas Wulff",
                        "slug": "Jonas-Wulff",
                        "structuredName": {
                            "firstName": "Jonas",
                            "lastName": "Wulff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonas Wulff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758236"
                        ],
                        "name": "H. Pfister",
                        "slug": "H.-Pfister",
                        "structuredName": {
                            "firstName": "Hanspeter",
                            "lastName": "Pfister",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pfister"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2624883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d350be53eff985e199c596f3b92bca817e95c778",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Layered models allow scene segmentation and motion estimation to be formulated together and to inform one another. Traditional layered motion methods, however, employ fairly weak models of scene structure, relying on locally connected Ising/Potts models which have limited ability to capture long-range correlations in natural scenes. To address this, we formulate a fully-connected layered model that enables global reasoning about the complicated segmentations of real objects. Optimization with fully-connected graphical models is challenging, and our inference algorithm leverages recent work on efficient mean field updates for fully-connected conditional random fields. These methods can be implemented efficiently using high-dimensional Gaussian filtering. We combine these ideas with a layered flow model, and find that the long-range connections greatly improve segmentation into figure-ground layers when compared with locally connected MRF models. Experiments on several benchmark datasets show that the method can recover fine structures and large occlusion regions, with good flow accuracy and much lower computational cost than previous locally-connected layered models."
            },
            "slug": "A-Fully-Connected-Layered-Model-of-Foreground-and-Sun-Wulff",
            "title": {
                "fragments": [],
                "text": "A Fully-Connected Layered Model of Foreground and Background Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work forms a fully-connected layered model that enables global reasoning about the complicated segmentations of real objects, and combines these ideas with a layered flow model, and finds that the long-range connections greatly improve segmentation into figure-ground layers when compared with locally connected MRF models."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40245930"
                        ],
                        "name": "Danhang Tang",
                        "slug": "Danhang-Tang",
                        "structuredName": {
                            "firstName": "Danhang",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danhang Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1999236"
                        ],
                        "name": "H. Chang",
                        "slug": "H.-Chang",
                        "structuredName": {
                            "firstName": "Hyung",
                            "lastName": "Chang",
                            "middleNames": [
                                "Jin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41203992"
                        ],
                        "name": "Alykhan Tejani",
                        "slug": "Alykhan-Tejani",
                        "structuredName": {
                            "firstName": "Alykhan",
                            "lastName": "Tejani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alykhan Tejani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143617697"
                        ],
                        "name": "Tae-Kyun Kim",
                        "slug": "Tae-Kyun-Kim",
                        "structuredName": {
                            "firstName": "Tae-Kyun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tae-Kyun Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4632519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db0750837ca4e905ffb419f98f893e8ddfaa2da2",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present the Latent Regression Forest (LRF), a novel framework for real-time, 3D hand pose estimation from a single depth image. In contrast to prior forest-based methods, which take dense pixels as input, classify them independently and then estimate joint positions afterwards, our method can be considered as a structured coarse-to-fine search, starting from the centre of mass of a point cloud until locating all the skeletal joints. The searching process is guided by a learnt Latent Tree Model which reflects the hierarchical topology of the hand. Our main contributions can be summarised as follows: (i) Learning the topology of the hand in an unsupervised, data-driven manner. (ii) A new forest-based, discriminative framework for structured search in images, as well as an error regression step to avoid error accumulation. (iii) A new multi-view hand pose dataset containing 180K annotated images from 10 different subjects. Our experiments show that the LRF out-performs state-of-the-art methods in both accuracy and efficiency."
            },
            "slug": "Latent-Regression-Forest:-Structured-Estimation-of-Tang-Chang",
            "title": {
                "fragments": [],
                "text": "Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The Latent Regression Forest is presented, a novel framework for real-time, 3D hand pose estimation from a single depth image and shows that the LRF out-performs state-of-the-art methods in both accuracy and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872774"
                        ],
                        "name": "W. Dong",
                        "slug": "W.-Dong",
                        "structuredName": {
                            "firstName": "Weisheng",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143713952"
                        ],
                        "name": "Guangming Shi",
                        "slug": "Guangming-Shi",
                        "structuredName": {
                            "firstName": "Guangming",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangming Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153897993"
                        ],
                        "name": "Xin Li",
                        "slug": "Xin-Li",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6991576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9ecd4382cdb12bc6f26c7e6915ca849827a84b1",
            "isKey": false,
            "numCitedBy": 535,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": "Simultaneous sparse coding (SSC) or nonlocal image representation has shown great potential in various low-level vision tasks, leading to several state-of-the-art image restoration techniques, including BM3D and LSSC. However, it still lacks a physically plausible explanation about why SSC is a better model than conventional sparse coding for the class of natural images. Meanwhile, the problem of sparsity optimization, especially when tangled with dictionary learning, is computationally difficult to solve. In this paper, we take a low-rank approach toward SSC and provide a conceptually simple interpretation from a bilateral variance estimation perspective, namely that singular-value decomposition of similar packed patches can be viewed as pooling both local and nonlocal information for estimating signal variances. Such perspective inspires us to develop a new class of image restoration algorithms called spatially adaptive iterative singular-value thresholding (SAIST). For noise data, SAIST generalizes the celebrated BayesShrink from local to nonlocal models; for incomplete data, SAIST extends previous deterministic annealing-based solution to sparsity optimization through incorporating the idea of dictionary learning. In addition to conceptual simplicity and computational efficiency, SAIST has achieved highly competent (often better) objective performance compared to several state-of-the-art methods in image denoising and completion experiments. Our subjective quality results compare favorably with those obtained by existing techniques, especially at high noise levels and with a large amount of missing data."
            },
            "slug": "Nonlocal-Image-Restoration-With-Bilateral-Variance-Dong-Shi",
            "title": {
                "fragments": [],
                "text": "Nonlocal Image Restoration With Bilateral Variance Estimation: A Low-Rank Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper takes a low-rank approach toward SSC and provides a conceptually simple interpretation from a bilateral variance estimation perspective, namely that singular-value decomposition of similar packed patches can be viewed as pooling both local and nonlocal information for estimating signal variances."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27367,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161037"
                        ],
                        "name": "Lingqiao Liu",
                        "slug": "Lingqiao-Liu",
                        "structuredName": {
                            "firstName": "Lingqiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingqiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36547165"
                        ],
                        "name": "Lei Wang",
                        "slug": "Lei-Wang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5546141"
                        ],
                        "name": "A. V. Hengel",
                        "slug": "A.-V.-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. Hengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722340"
                        ],
                        "name": "Chao Ching Wang",
                        "slug": "Chao-Ching-Wang",
                        "structuredName": {
                            "firstName": "Chao",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ching"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao Ching Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2054956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f2f2c895f0141cc770dd72a28c34a51cb772850",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians. \n \nIn this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems."
            },
            "slug": "Encoding-High-Dimensional-Local-Features-by-Sparse-Liu-Shen",
            "title": {
                "fragments": [],
                "text": "Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace, termed Sparse Coding based Fisher Vector Coding (SCFVC), which significantly outperforms the traditional GMM based Fisher vector encoding and achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 82
                            }
                        ],
                        "text": "Recent work has focused on motion descriptors that are invariant to camera motion [5, 6, 8, 10, 11, 12, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 56
                            }
                        ],
                        "text": "This paradigm has been applied for hand pose estimation [2, 4, 7, 13] but is less successful than for body pose."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "a Hamming code [13],[38] as in this work, or product quantization code [31],[17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 113
                            }
                        ],
                        "text": "While we use the similar pixel difference features as in cascaded pose regression [1, 3] and many previous works [2, 4, 6, 7, 13], we show that the parameterization of pixel indexing is the key to achieve certain geometrical invariance and analyze the invariance properties of previous features [6, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": true,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2405613"
                        ],
                        "name": "Omar Oreifej",
                        "slug": "Omar-Oreifej",
                        "structuredName": {
                            "firstName": "Omar",
                            "lastName": "Oreifej",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omar Oreifej"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691128"
                        ],
                        "name": "Zicheng Liu",
                        "slug": "Zicheng-Liu",
                        "structuredName": {
                            "firstName": "Zicheng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zicheng Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6482700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bbe9a802c83f52f365e6cef799df9dc19f9dbdb",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently, thus, they often fail to capture the complex joint shape-motion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks."
            },
            "slug": "HON4D:-Histogram-of-Oriented-4D-Normals-for-from-Oreifej-Liu",
            "title": {
                "fragments": [],
                "text": "HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A new descriptor for activity recognition from videos acquired by a depth sensor is presented that better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145404094"
                        ],
                        "name": "Lingyun Zhang",
                        "slug": "Lingyun-Zhang",
                        "structuredName": {
                            "firstName": "Lingyun",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingyun Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49488601"
                        ],
                        "name": "Matthew H Tong",
                        "slug": "Matthew-H-Tong",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Tong",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew H Tong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34749896"
                        ],
                        "name": "Tim K. Marks",
                        "slug": "Tim-K.-Marks",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Marks",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim K. Marks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2207531"
                        ],
                        "name": "Honghao Shan",
                        "slug": "Honghao-Shan",
                        "structuredName": {
                            "firstName": "Honghao",
                            "lastName": "Shan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honghao Shan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524582"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16329198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e80c48441351fc1d928524710b4500a0de8315bb",
            "isKey": false,
            "numCitedBy": 1211,
            "numCiting": 184,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a definition of saliency by considering what the visual system is trying to optimize when directing attention. The resulting model is a Bayesian framework from which bottom-up saliency emerges naturally as the self-information of visual features, and overall saliency (incorporating top-down information with bottom-up saliency) emerges as the pointwise mutual information between the features and the target when searching for a target. An implementation of our framework demonstrates that our model's bottom-up saliency maps perform as well as or better than existing algorithms in predicting people's fixations in free viewing. Unlike existing saliency measures, which depend on the statistics of the particular image being viewed, our measure of saliency is derived from natural image statistics, obtained in advance from a collection of natural images. For this reason, we call our model SUN (Saliency Using Natural statistics). A measure of saliency based on natural image statistics, rather than based on a single test image, provides a straightforward explanation for many search asymmetries observed in humans; the statistics of a single test image lead to predictions that are not consistent with these asymmetries. In our model, saliency is computed locally, which is consistent with the neuroanatomy of the early visual system and results in an efficient algorithm with few free parameters."
            },
            "slug": "SUN:-A-Bayesian-framework-for-saliency-using-Zhang-Tong",
            "title": {
                "fragments": [],
                "text": "SUN: A Bayesian framework for saliency using natural statistics."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "In the model, saliency is computed locally, which is consistent with the neuroanatomy of the early visual system and results in an efficient algorithm with few free parameters, which provides a straightforward explanation for many search asymmetries observed in humans."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of vision"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045340"
                        ],
                        "name": "Tomasz Malisiewicz",
                        "slug": "Tomasz-Malisiewicz",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Malisiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Malisiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13898083,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "39689bd427f5668a8fbb3113019c2b8393f0b1a7",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to find visually similar images even if they appear quite different at the raw pixel level. This task is particularly important for matching images across visual domains, such as photos taken over different seasons or lighting conditions, paintings, hand-drawn sketches, etc. We propose a surprisingly simple method that estimates the relative importance of different features in a query image based on the notion of \"data-driven uniqueness\". We employ standard tools from discriminative object detection in a novel way, yielding a generic approach that does not depend on a particular image representation or a specific visual domain. Our approach shows good performance on a number of difficult cross-domain visual tasks e.g., matching paintings or sketches to real photographs. The method also allows us to demonstrate novel applications such as Internet re-photography, and painting2gps. While at present the technique is too computationally intensive to be practical for interactive image retrieval, we hope that some of the ideas will eventually become applicable to that domain as well."
            },
            "slug": "Data-driven-visual-similarity-for-cross-domain-Shrivastava-Malisiewicz",
            "title": {
                "fragments": [],
                "text": "Data-driven visual similarity for cross-domain image matching"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A surprisingly simple method that estimates the relative importance of different features in a query image based on the notion of \"data-driven uniqueness\" is proposed, yielding a generic approach that does not depend on a particular image representation or a specific visual domain."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14555334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "711a22f55111db6f5599076dcdd791a94a5e9368",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Compositional models provide an elegant formalism for representing the visual appearance of highly variable objects. While such models are appealing from a theoretical point of view, it has been difficult to demonstrate that they lead to performance advantages on challenging datasets. Here we develop a grammar model for person detection and show that it outperforms previous high-performance systems on the PASCAL benchmark. Our model represents people using a hierarchy of deformable parts, variable structure and an explicit model of occlusion for partially visible objects. To train the model, we introduce a new discriminative framework for learning structured prediction models from weakly-labeled data."
            },
            "slug": "Object-Detection-with-Grammar-Models-Girshick-Felzenszwalb",
            "title": {
                "fragments": [],
                "text": "Object Detection with Grammar Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A grammar model for person detection is developed and it outperforms previous high-performance systems on the PASCAL benchmark and introduces a new discriminative framework for learning structured prediction models from weakly-labeled data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3203657"
                        ],
                        "name": "Andrew Delong",
                        "slug": "Andrew-Delong",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Delong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Delong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319877"
                        ],
                        "name": "A. Osokin",
                        "slug": "A.-Osokin",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Osokin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Osokin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2021123"
                        ],
                        "name": "Hossam N. Isack",
                        "slug": "Hossam-N.-Isack",
                        "structuredName": {
                            "firstName": "Hossam",
                            "lastName": "Isack",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hossam N. Isack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1219701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce51aa425cccb509830f5e5882230ff976344cc5",
            "isKey": false,
            "numCitedBy": 546,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "The \u03b1-expansion algorithm has had a significant impact in computer vision due to its generality, effectiveness, and speed. It is commonly used to minimize energies that involve unary, pairwise, and specialized higher-order terms. Our main algorithmic contribution is an extension of \u03b1-expansion that also optimizes \u201clabel costs\u201d with well-characterized optimality bounds. Label costs penalize a solution based on the set of labels that appear in it, for example by simply penalizing the number of labels in the solution.Our energy has a natural interpretation as minimizing description length (MDL) and sheds light on classical algorithms like K-means and expectation-maximization (EM). Label costs are useful for multi-model fitting and we demonstrate several such applications: homography detection, motion segmentation, image segmentation, and compression. Our C++ and MATLAB code is publicly available http://vision.csd.uwo.ca/code/."
            },
            "slug": "Fast-Approximate-Energy-Minimization-with-Label-Delong-Osokin",
            "title": {
                "fragments": [],
                "text": "Fast Approximate Energy Minimization with Label Costs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The main algorithmic contribution is an extension of \u03b1-expansion that also optimizes \u201clabel costs\u201d with well-characterized optimality bounds, which has a natural interpretation as minimizing description length (MDL) and sheds light on classical algorithms like K-means and expectation-maximization (EM)."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113225771"
                        ],
                        "name": "Tian Lan",
                        "slug": "Tian-Lan",
                        "structuredName": {
                            "firstName": "Tian",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tian Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46396571"
                        ],
                        "name": "Yang Wang",
                        "slug": "Yang-Wang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 78173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40da1560afbf65bb1d66e75a33dfe617e0dc4a2e",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we develop an algorithm for action recognition and localization in videos. The algorithm uses a figure-centric visual word representation. Different from previous approaches it does not require reliable human detection and tracking as input. Instead, the person location is treated as a latent variable that is inferred simultaneously with action recognition. A spatial model for an action is learned in a discriminative fashion under a figure-centric representation. Temporal smoothness over video sequences is also enforced. We present results on the UCF-Sports dataset, verifying the effectiveness of our model in situations where detection and tracking of individuals is challenging."
            },
            "slug": "Discriminative-figure-centric-models-for-joint-and-Lan-Wang",
            "title": {
                "fragments": [],
                "text": "Discriminative figure-centric models for joint action localization and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper develops an algorithm for action recognition and localization in videos that does not require reliable human detection and tracking as input and uses a figure-centric visual word representation."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47839689"
                        ],
                        "name": "Lu Xia",
                        "slug": "Lu-Xia",
                        "structuredName": {
                            "firstName": "Lu",
                            "lastName": "Xia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lu Xia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705627"
                        ],
                        "name": "J. Aggarwal",
                        "slug": "J.-Aggarwal",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Aggarwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aggarwal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 761694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "890b1f7946ada41e03dcc2807e74d64372716496",
            "isKey": false,
            "numCitedBy": 432,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPs from depth videos (called DSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms state-of-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications."
            },
            "slug": "Spatio-temporal-Depth-Cuboid-Similarity-Feature-for-Xia-Aggarwal",
            "title": {
                "fragments": [],
                "text": "Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A filtering method to extract STIPs from depth videos (called DSTIP) that effectively suppress the noisy measurements is presented and a novel depth cuboid similarity feature (DCSF) is built to describe the local 3D depth cuboids around the DSTips with an adaptable supporting size."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144908136"
                        ],
                        "name": "Sameer Agarwal",
                        "slug": "Sameer-Agarwal",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798912"
                        ],
                        "name": "Yasutaka Furukawa",
                        "slug": "Yasutaka-Furukawa",
                        "structuredName": {
                            "firstName": "Yasutaka",
                            "lastName": "Furukawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasutaka Furukawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35577716"
                        ],
                        "name": "Ian Simon",
                        "slug": "Ian-Simon",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800609"
                        ],
                        "name": "B. Curless",
                        "slug": "B.-Curless",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Curless",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Curless"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679223"
                        ],
                        "name": "S. Seitz",
                        "slug": "S.-Seitz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Seitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7448214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9922e9f7c87b859df2e8af355f03744d33023f0e",
            "isKey": false,
            "numCitedBy": 1077,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system that can match and reconstruct 3D scenes from extremely large collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo sharing sites. Our system uses a collection of novel parallel distributed matching and reconstruction algorithms, designed to maximize parallelism at each stage in the pipeline and minimize serialization bottlenecks. It is designed to scale gracefully with both the size of the problem and the amount of available computation. We have experimented with a variety of alternative algorithms at each stage of the pipeline and report on which ones work best in a parallel computing environment. Our experimental results demonstrate that it is now possible to reconstruct cities consisting of 150K images in less than a day on a cluster with 500 compute cores."
            },
            "slug": "Building-Rome-in-a-day-Agarwal-Furukawa",
            "title": {
                "fragments": [],
                "text": "Building Rome in a day"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A system that can match and reconstruct 3D scenes from extremely large collections of photographs such as those found by searching for a given city on Internet photo sharing sites and is designed to scale gracefully with both the size of the problem and the amount of available computation."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790643"
                        ],
                        "name": "S. Schulter",
                        "slug": "S.-Schulter",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Schulter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schulter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695579"
                        ],
                        "name": "C. Leistner",
                        "slug": "C.-Leistner",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Leistner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leistner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202367"
                        ],
                        "name": "Paul Wohlhart",
                        "slug": "Paul-Wohlhart",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Wohlhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Wohlhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791182"
                        ],
                        "name": "P. Roth",
                        "slug": "P.-Roth",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Roth",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7588014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33594e1bfe93fc6c74c2bcfc1bc39c524fa9e2ca",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Alternating Regression Forests (ARFs), a novel regression algorithm that learns a Random Forest by optimizing a global loss function over all trees. This interrelates the information of single trees during the training phase and results in more accurate predictions. ARFs can minimize any differentiable regression loss without sacrificing the appealing properties of Random Forests, like low computational complexity during both, training and testing. Inspired by recent developments for classification [19], we derive a new algorithm capable of dealing with different regression loss functions, discuss its properties and investigate the relations to other methods like Boosted Trees. We evaluate ARFs on standard machine learning benchmarks, where we observe better generalization power compared to both standard Random Forests and Boosted Trees. Moreover, we apply the proposed regressor to two computer vision applications: object detection and head pose estimation from depth images. ARFs outperform the Random Forest baselines in both tasks, illustrating the importance of optimizing a common loss function for all trees."
            },
            "slug": "Alternating-Regression-Forests-for-Object-Detection-Schulter-Leistner",
            "title": {
                "fragments": [],
                "text": "Alternating Regression Forests for Object Detection and Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Alternative Regression Forests are presented, a novel regression algorithm that learns a Random Forest by optimizing a global loss function over all trees and outperform the Random Forest baselines in both tasks, illustrating the importance of optimizing a common loss function for all trees."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2694595"
                        ],
                        "name": "Martin Solli",
                        "slug": "Martin-Solli",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Solli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Solli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143757538"
                        ],
                        "name": "R. Lenz",
                        "slug": "R.-Lenz",
                        "structuredName": {
                            "firstName": "Reiner",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lenz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14891762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bb83b00e7b8eb27ad04d4bb80499e91fc471a07",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce two large databases consisting of 750 000 and 1.2 million thumbnail-sized images, labeled with emotion-related keywords. The smaller database consists of images from Matton Images, an image provider. The larger database consists of web images that were indexed by the crawler of the image search engine Picsearch. The images in the Picsearch database belong to one of 98 emotion related categories and contain meta-data in the form of secondary keywords, the originating website and some view statistics. We use two psycho-physics related feature vectors based on the emotional impact of color combinations, the standard RGB-histogram and two SIFT-related descriptors to characterize the visual properties of the images. These features are then used in two-class classification experiments to explore the discrimination properties of emotion-related categories. The clustering software and the classifiers are available in the public domain, and the same standard configurations are used in all experiments. Our findings show that for emotional categories, descriptors based on global image statistics (global histograms) perform better than local image descriptors (bag-of-words models). This indicates that content-based indexing and retrieval using emotion-based approaches are fundamentally different from the dominant object-recognition based approaches for which SIFT-related features are the standard descriptors."
            },
            "slug": "Emotion-related-structures-in-large-image-databases-Solli-Lenz",
            "title": {
                "fragments": [],
                "text": "Emotion related structures in large image databases"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This work introduces two large databases consisting of 750 000 and 1.2 million thumbnail-sized images, labeled with emotion-related keywords, and shows that for emotional categories, descriptors based on global image statistics (global histograms) perform better than local image descriptors (bag-of-words models)."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740145"
                        ],
                        "name": "V. Lempitsky",
                        "slug": "V.-Lempitsky",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lempitsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lempitsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1532594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f281b87ef58c0dd244ec4743fb1f899b4948308",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for the detection of instances of an object class, such as cars or pedestrians, in natural images. Similarly to some previous works, this is accomplished via generalized Hough transform, where the detections of individual object parts cast probabilistic votes for possible locations of the centroid of the whole object; the detection hypotheses then correspond to the maxima of the Hough image that accumulates the votes from all parts. However, whereas the previous methods detect object parts using generative codebooks of part appearances, we take a more discriminative approach to object part detection. Towards this end, we train a class-specific Hough forest, which is a random forest that directly maps the image patch appearance to the probabilistic vote about the possible location of the object centroid. We demonstrate that Hough forests improve the results of the Hough-transform object detection significantly and achieve state-of-the-art performance for several classes and datasets."
            },
            "slug": "Class-specific-Hough-forests-for-object-detection-Gall-Lempitsky",
            "title": {
                "fragments": [],
                "text": "Class-specific Hough forests for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that Hough forests improve the results of the Hough-transform object detection significantly and achieve state-of-the-art performance for several classes and datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331321"
                        ],
                        "name": "Eric Brachmann",
                        "slug": "Eric-Brachmann",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brachmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Brachmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2903785"
                        ],
                        "name": "Alexander Krull",
                        "slug": "Alexander-Krull",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Krull",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Krull"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066458600"
                        ],
                        "name": "Frank Michel",
                        "slug": "Frank-Michel",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Michel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Michel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952883"
                        ],
                        "name": "S. Gumhold",
                        "slug": "S.-Gumhold",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Gumhold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gumhold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16080844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c740bb2a94ad3942e3a6ee329724cb3905e619",
            "isKey": false,
            "numCitedBy": 480,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This work addresses the problem of estimating the 6D Pose of specific objects from a single RGB-D image. We present a flexible approach that can deal with generic objects, both textured and texture-less. The key new concept is a learned, intermediate representation in form of a dense 3D object coordinate labelling paired with a dense class labelling. We are able to show that for a common dataset with texture-less objects, where template-based techniques are suitable and state of the art, our approach is slightly superior in terms of accuracy. We also demonstrate the benefits of our approach, compared to template-based techniques, in terms of robustness with respect to varying lighting conditions. Towards this end, we contribute a new ground truth dataset with 10k images of 20 objects captured each under three different lighting conditions. We demonstrate that our approach scales well with the number of objects and has capabilities to run fast."
            },
            "slug": "Learning-6D-Object-Pose-Estimation-Using-3D-Object-Brachmann-Krull",
            "title": {
                "fragments": [],
                "text": "Learning 6D Object Pose Estimation Using 3D Object Coordinates"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work addresses the problem of estimating the 6D Pose of specific objects from a single RGB-D image by presenting a learned, intermediate representation in form of a dense 3D object coordinate labelling paired with a dense class labelling."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787725"
                        ],
                        "name": "Fabio Galasso",
                        "slug": "Fabio-Galasso",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Galasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Galasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40031905"
                        ],
                        "name": "N. Nagaraja",
                        "slug": "N.-Nagaraja",
                        "structuredName": {
                            "firstName": "Naveen",
                            "lastName": "Nagaraja",
                            "middleNames": [
                                "Shankar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nagaraja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704621"
                        ],
                        "name": "Tatiana Jimenez Cardenas",
                        "slug": "Tatiana-Jimenez-Cardenas",
                        "structuredName": {
                            "firstName": "Tatiana",
                            "lastName": "Cardenas",
                            "middleNames": [
                                "Jimenez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tatiana Jimenez Cardenas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8786368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4194159e8cb641b2df7e995794e3af2a8a03ab23",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Video segmentation research is currently limited by the lack of a benchmark dataset that covers the large variety of sub problems appearing in video segmentation and that is large enough to avoid over fitting. Consequently, there is little analysis of video segmentation which generalizes across subtasks, and it is not yet clear which and how video segmentation should leverage the information from the still-frames, as previously studied in image segmentation, alongside video specific information, such as temporal volume, motion and occlusion. In this work we provide such an analysis based on annotations of a large video dataset, where each video is manually segmented by multiple persons. Moreover, we introduce a new volume-based metric that includes the important aspect of temporal consistency, that can deal with segmentation hierarchies, and that reflects the tradeoff between over-segmentation and segmentation accuracy."
            },
            "slug": "A-Unified-Video-Segmentation-Benchmark:-Annotation,-Galasso-Nagaraja",
            "title": {
                "fragments": [],
                "text": "A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces a new volume-based metric that includes the important aspect of temporal consistency, that can deal with segmentation hierarchies, and that reflects the tradeoff between over-segmentation and segmentation accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18650202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15d53db3001fb150ab2e17e02730379b429b95ed",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient detection of objects in images is complicated by variations of object appearance due to intra-class object differences, articulation, lighting, occlusions, and aspect variations. To reduce the search required for detection, we employ the bottom-up approach where we find candidate image features and associate some of them with parts of the object model. We represent objects as collections of local features, and would like to allow any of them to be absent, with only a small subset sufficient for detection;furthermore, our model should allow efficient correspondence search. We propose a model, Mixture of Trees, that achieves these goals. With a mixture of trees, we can model the individual appearances of the features, relationships among them, and the aspect, and handle occlusions. Independences captured in the model make efficient inference possible. In our earlier work, we have shown that mixtures of trees can be used to model objects with a natural tree structure, in the context of human tracking. Now we show that a natural tree structure is not required, and use a mixture of trees for both frontal and view-invariant face detection. We also show that by modeling faces as collections of features we can establish an intrinsic coordinate frame for a face, and estimate the out-of-plane rotation of a face."
            },
            "slug": "Mixtures-of-trees-for-object-recognition-Ioffe-Forsyth",
            "title": {
                "fragments": [],
                "text": "Mixtures of trees for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that a natural tree structure is not required, and a mixture of trees is used for both frontal and view-invariant face detection, and by modeling faces as collections of features the authors can establish an intrinsic coordinate frame for a face, and estimate the out-of-plane rotation of a face."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758039"
                        ],
                        "name": "T. Pajdla",
                        "slug": "T.-Pajdla",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Pajdla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pajdla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118206472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a72afeb25797b36d110df9615b8e85402c80202",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Coarse-to-Fine Auto-Encoder Networks (CFAN) for Real-Time Face Alignment.- From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices.- Pose Machines: Articulated Pose Estimation via Inference Machines.- Piecewise-Planar StereoScan: Structure and Motion from Plane Primitives.- Nonrigid Surface Registration and Completion from RGBD Images.- Unsupervised Dense Object Discovery, Detection, Tracking and Reconstruction.- Know Your Limits: Accuracy of Long Range Stereoscopic Object Measurements in Practice.- As-Rigid-As-Possible Stereo under Second Order Smoothness Priors.- Real-Time Minimization of the Piecewise Smooth Mumford-Shah Functional.- A MAP-Estimation Framework for Blind Deblurring Using High-Level Edge Priors.- Efficient Color Constancy with Local Surface Reflectance Statistics.- A Contrast Enhancement Framework with JPEG Artifacts Suppression.- Radial Bright Channel Prior for Single Image Vignetting Correction.- Tubular Structure Filtering by Ranking Orientation Responses of Path Operators.- Optimization-Based Artifact Correction for Electron Microscopy Image Stacks.- Metric-Based Pairwise and Multiple Image Registration.- Canonical Correlation Analysis on Riemannian Manifolds and Its Applications.- Scalable 6-DOF Localization on Mobile Devices.- On Mean Pose and Variability of 3D Deformable Models.- Hybrid Stochastic / Deterministic Optimization for Tracking Sports Players and Pedestrians.- What Do I See? Modeling Human Visual Perception for Multi-person Tracking.- Consistent Re-identification in a Camera Network.- Surface Normal Deconvolution: Photometric Stereo for Optically Thick Translucent Objects.- Intrinsic Video.- Robust and Accurate Non-parametric Estimation of Reflectance Using Basis Decomposition and Correction Functions.- Intrinsic Textures for Relightable Free-Viewpoint Video.- Reasoning about Object Affordances in a Knowledge Base Representation.- Binary Codes Embedding for Fast Image Tagging with Incomplete Labels.- Recognizing Products: A Per-exemplar Multi-label Image Classification Approach.- Part-Pair Representation for Part Localization.- Weakly Supervised Learning of Objects, Attributes and Their Associations.- Interestingness Prediction by Robust Learning to Rank.- Pairwise Probabilistic Voting: Fast Place Recognition without RANSAC.- Robust Instance Recognition in Presence of Occlusion and Clutter.- Learning 6D Object Pose Estimation Using 3D Object Coordinates.- Growing Regression Forests by Classification: Applications to Object Pose Estimation.- Stacked Deformable Part Model with Shape Regression for Object Part Localization.- Transductive Multi-view Embedding for Zero-Shot Recognition and Annotation.- Self-explanatory Sparse Representation for Image Classification.- Efficient k-Support Matrix Pursuit.- Geodesic Regression on the Grassmannian.- Model Selection by Linear Programming.- Perceptually Inspired Layout-Aware Losses for Image Segmentation.- Large Margin Local Metric Learning.- Movement Pattern Histogram for Action Recognition and Retrieval.- Pose Filter Based Hidden-CRF Models for Activity Detection.- Action Recognition Using Super Sparse Coding Vector with Spatio-temporal Awareness.- HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for Action Recognition.- Natural Action Recognition Using Invariant 3D Motion Encoding.- Detecting Social Actions of Fruit Flies.- Progressive Mode-Seeking on Graphs for Sparse Feature Matching.- Globally Optimal Inlier Set Maximization with Unknown Rotation and Focal Length.- Match Selection and Refinement for Highly Accurate Two-View Structure from Motion.- LSD-SLAM: Large-Scale Direct Monocular SLAM."
            },
            "slug": "Computer-vision-ECCV-2014-:-13th-European-Zurich,-:-Fleet-Pajdla",
            "title": {
                "fragments": [],
                "text": "Computer vision -- ECCV 2014 : 13th European conference Zurich, Switzerland, September 6-12, 2014 : proceedings"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Coarse-to-Fine Auto-Encoder Networks (CFAN) for Real-Time Face Alignment and more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": false,
            "numCitedBy": 9371,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2944502"
                        ],
                        "name": "Daniel Zoran",
                        "slug": "Daniel-Zoran",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Zoran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Zoran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1726588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "529403ab43b381b942b67751862b614cbd94341b",
            "isKey": false,
            "numCitedBy": 1222,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning good image priors is of utmost importance for the study of vision, computer vision and image processing applications. Learning priors and optimizing over whole images can lead to tremendous computational challenges. In contrast, when we work with small image patches, it is possible to learn priors and perform patch restoration very efficiently. This raises three questions - do priors that give high likelihood to the data also lead to good performance in restoration? Can we use such patch based priors to restore a full image? Can we learn better patch priors? In this work we answer these questions. We compare the likelihood of several patch models and show that priors that give high likelihood to data perform better in patch restoration. Motivated by this result, we propose a generic framework which allows for whole image restoration using any patch based prior for which a MAP (or approximate MAP) estimate can be calculated. We show how to derive an appropriate cost function, how to optimize it and how to use it to restore whole images. Finally, we present a generic, surprisingly simple Gaussian Mixture prior, learned from a set of natural images. When used with the proposed framework, this Gaussian Mixture Model outperforms all other generic prior methods for image denoising, deblurring and inpainting."
            },
            "slug": "From-learning-models-of-natural-image-patches-to-Zoran-Weiss",
            "title": {
                "fragments": [],
                "text": "From learning models of natural image patches to whole image restoration"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A generic framework which allows for whole image restoration using any patch based prior for which a MAP (or approximate MAP) estimate can be calculated is proposed and a generic, surprisingly simple Gaussian Mixture prior is presented, learned from a set of natural images."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787725"
                        ],
                        "name": "Fabio Galasso",
                        "slug": "Fabio-Galasso",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Galasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Galasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9416731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "689dd0f68b81bf15b88775c3c6b59af02436c299",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to its importance, video segmentation has regained interest recently. However, there is no common agreement about the necessary ingredients for best performance. This work contributes a thorough analysis of various within- and between-frame affinities suitable for video segmentation. Our results show that a frame-based superpixel segmentation combined with a few motion and appearance-based affinities are sufficient to obtain good video segmentation performance. A second contribution of the paper is the extension of [1] to include motion-cues, which makes the algorithm globally aware of motion, thus improving its performance for video sequences. Finally, we contribute an extension of an established image segmentation benchmark [1] to videos, allowing coarse-to-fine video segmentations and multiple human annotations. Our results are tested on BMDS [2], and compared to existing methods."
            },
            "slug": "Video-Segmentation-with-Superpixels-Galasso-Cipolla",
            "title": {
                "fragments": [],
                "text": "Video Segmentation with Superpixels"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results show that a frame-based superpixel segmentation combined with a few motion and appearance-based affinities are sufficient to obtain good video segmentation performance."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2710964"
                        ],
                        "name": "Atmane Khellal",
                        "slug": "Atmane-Khellal",
                        "structuredName": {
                            "firstName": "Atmane",
                            "lastName": "Khellal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Atmane Khellal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743928"
                        ],
                        "name": "Hongbin Ma",
                        "slug": "Hongbin-Ma",
                        "structuredName": {
                            "firstName": "Hongbin",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongbin Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2298179"
                        ],
                        "name": "Qing Fei",
                        "slug": "Qing-Fei",
                        "structuredName": {
                            "firstName": "Qing",
                            "lastName": "Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qing Fei"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9739762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba521c0f50bc5459f210339bf527605982c68436",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a new approach of learning features based on convolutional neural networks for pedestrian detection in far infrared images is presented. Unlike traditional recognition systems which use hand-designed features like SIFT or HOG, our convolutional networks architecture learns new features and representations more appropriate to the classification task in infrared images. Another pedestrian detector based on logistic regression is designed and compared to convolutional networks based classifier. Our system built over non-visible range sensor may have an important role in next generation robotics, especially in perception, advanced driver assistant systems ADAS and intelligent surveillance systems."
            },
            "slug": "Pedestrian-Classification-and-Detection-in-Far-Khellal-Ma",
            "title": {
                "fragments": [],
                "text": "Pedestrian Classification and Detection in Far Infrared Images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A convolutional networks architecture based pedestrian detector built over non-visible range sensor may have an important role in next generation robotics, especially in perception, advanced driver assistant systems ADAS and intelligent surveillance systems."
            },
            "venue": {
                "fragments": [],
                "text": "ICIRA"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5959482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "330da625c15427c6e42ccfa3b747fb29e5835bf0",
            "isKey": false,
            "numCitedBy": 21868,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            },
            "slug": "Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen",
            "title": {
                "fragments": [],
                "text": "Efficient Estimation of Word Representations in Vector Space"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Two novel model architectures for computing continuous vector representations of words from very large data sets are proposed and it is shown that these vectors provide state-of-the-art performance on the authors' test set for measuring syntactic and semantic word similarities."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31426451"
                        ],
                        "name": "G. Lefebvre",
                        "slug": "G.-Lefebvre",
                        "structuredName": {
                            "firstName": "Gr\u00e9goire",
                            "lastName": "Lefebvre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lefebvre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30657911"
                        ],
                        "name": "Samuel Berlemont",
                        "slug": "Samuel-Berlemont",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Berlemont",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Berlemont"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2408697"
                        ],
                        "name": "F. Mamalet",
                        "slug": "F.-Mamalet",
                        "structuredName": {
                            "firstName": "Franck",
                            "lastName": "Mamalet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mamalet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723337"
                        ],
                        "name": "Christophe Garcia",
                        "slug": "Christophe-Garcia",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christophe Garcia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 216125258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ffd8d720e2b239691a8446efe5107715b32714b",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new robust method for inertial MEM (MicroElectroMechanical systems) 3D gesture recognition. The linear acceleration and the angular velocity, respectively provided by the accelerometer and the gyrometer, are sampled in time resulting in 6D values at each time step which are used as inputs for the gesture recognition system. We propose to build a system based on Bidirectional Long Short-Term Memory Recurrent Neural Networks (BLSTM-RNN) for gesture classification from raw MEM data. We also compare this system to a geometric approach using DTW (Dynamic Time Warping) and a statistical method based on HMM (Hidden Markov Model) from filtered and denoised MEM data. Experimental results on 22 individuals producing 14 gestures in the air show that the proposed approach outperforms classical classification methods with a classification mean rate of 95.57% and a standard deviation of 0.50 for 616 test gestures. Furthermore, these experiments underline that combining accelerometer and gyrometer information gives better results that using a single inertial description."
            },
            "slug": "BLSTM-RNN-Based-3D-Gesture-Classification-Lefebvre-Berlemont",
            "title": {
                "fragments": [],
                "text": "BLSTM-RNN Based 3D Gesture Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new robust method for inertial MEM (MicroElectroMechanical systems) 3D gesture recognition based on Bidirectional Long Short-Term Memory Recurrent Neural Networks (BLSTM-RNN) for gesture classification from raw MEM data is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34824003"
                        ],
                        "name": "T. Sharp",
                        "slug": "T.-Sharp",
                        "structuredName": {
                            "firstName": "Toby",
                            "lastName": "Sharp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sharp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40636177"
                        ],
                        "name": "Mat Cook",
                        "slug": "Mat-Cook",
                        "structuredName": {
                            "firstName": "Mat",
                            "lastName": "Cook",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mat Cook"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848295"
                        ],
                        "name": "M. Finocchio",
                        "slug": "M.-Finocchio",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Finocchio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Finocchio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144564063"
                        ],
                        "name": "R. Moore",
                        "slug": "R.-Moore",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078931040"
                        ],
                        "name": "A. Kipman",
                        "slug": "A.-Kipman",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Kipman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kipman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8886546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1113b4fcf644616e2587eacead2bca4b794ac47d",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe two new approaches to human pose estimation. Both can quickly and accurately predict the 3D positions of body joints from a single depth image without using any temporal information. The key to both approaches is the use of a large, realistic, and highly varied synthetic set of training images. This allows us to learn models that are largely invariant to factors such as pose, body shape, field-of-view cropping, and clothing. Our first approach employs an intermediate body parts representation, designed so that an accurate per-pixel classification of the parts will localize the joints of the body. The second approach instead directly regresses the positions of body joints. By using simple depth pixel comparison features and parallelizable decision forests, both approaches can run super-real time on consumer hardware. Our evaluation investigates many aspects of our methods, and compares the approaches to each other and to the state of the art. Results on silhouettes suggest broader applicability to other imaging modalities."
            },
            "slug": "Efficient-Human-Pose-Estimation-from-Single-Depth-Shotton-Girshick",
            "title": {
                "fragments": [],
                "text": "Efficient Human Pose Estimation from Single Depth Images"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Two new approaches to human pose estimation are described, both of which can quickly and accurately predict the 3D positions of body joints from a single depth image without using any temporal information."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232265"
                        ],
                        "name": "Deqing Sun",
                        "slug": "Deqing-Sun",
                        "structuredName": {
                            "firstName": "Deqing",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deqing Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206591220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a87428c6b2205240485ee6bb9cfb00fd9ed359c",
            "isKey": false,
            "numCitedBy": 1397,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that \u201cclassical\u201d flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. Moreover, we find that while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions. To understand the principles behind this phenomenon, we derive a new objective that formalizes the median filtering heuristic. This objective includes a nonlocal term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that ranks at the top of the Middlebury benchmark."
            },
            "slug": "Secrets-of-optical-flow-estimation-and-their-Sun-Roth",
            "title": {
                "fragments": [],
                "text": "Secrets of optical flow estimation and their principles"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is discovered that \u201cclassical\u201d flow formulations perform surprisingly well when combined with modern optimization and implementation techniques, and while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144910249"
                        ],
                        "name": "Chi Xu",
                        "slug": "Chi-Xu",
                        "structuredName": {
                            "firstName": "Chi",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49280434"
                        ],
                        "name": "Li Cheng",
                        "slug": "Li-Cheng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Cheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2018737,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94fdacd117c545300438ce39bbc684833f9e36e7",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We tackle the practical problem of hand pose estimation from a single noisy depth image. A dedicated three-step pipeline is proposed: Initial estimation step provides an initial estimation of the hand in-plane orientation and 3D location, Candidate generation step produces a set of 3D pose candidate from the Hough voting space with the help of the rotational invariant depth features, Verification step delivers the final 3D hand pose as the solution to an optimization problem. We analyze the depth noises, and suggest tips to minimize their negative impacts on the overall performance. Our approach is able to work with Kinect-type noisy depth images, and reliably produces pose estimations of general motions efficiently (12 frames per second). Extensive experiments are conducted to qualitatively and quantitatively evaluate the performance with respect to the state-of-the-art methods that have access to additional RGB images. Our approach is shown to deliver on par or even better results."
            },
            "slug": "Efficient-Hand-Pose-Estimation-from-a-Single-Depth-Xu-Cheng",
            "title": {
                "fragments": [],
                "text": "Efficient Hand Pose Estimation from a Single Depth Image"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work tackles the practical problem of hand pose estimation from a single noisy depth image, and proposes a dedicated three-step pipeline that is able to work with Kinect-type noisy depth images, and reliably produces pose estimations of general motions efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232265"
                        ],
                        "name": "Deqing Sun",
                        "slug": "Deqing-Sun",
                        "structuredName": {
                            "firstName": "Deqing",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deqing Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6058145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76abc5301bacc7bedf690f067a98200bf5095402",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Layered models provide a compelling approach for estimating image motion and segmenting moving scenes. Previous methods, however, have failed to capture the structure of complex scenes, provide precise object boundaries, effectively estimate the number of layers in a scene, or robustly determine the depth order of the layers. Furthermore, previous methods have focused on optical flow between pairs of frames rather than longer sequences. We show that image sequences with more frames are needed to resolve ambiguities in depth ordering at occlusion boundaries; temporal layer constancy makes this feasible. Our generative model of image sequences is rich but difficult to optimize with traditional gradient descent methods. We propose a novel discrete approximation of the continuous objective in terms of a sequence of depth-ordered MRFs and extend graph-cut optimization methods with new \u201cmoves\u201d that make joint layer segmentation and motion estimation feasible. Our optimizer, which mixes discrete and continuous optimization, automatically determines the number of layers and reasons about their depth ordering. We demonstrate the value of layered models, our optimization strategy, and the use of more than two frames on both the Middlebury optical flow benchmark and the MIT layer segmentation benchmark."
            },
            "slug": "Layered-segmentation-and-optical-flow-estimation-Sun-Sudderth",
            "title": {
                "fragments": [],
                "text": "Layered segmentation and optical flow estimation over time"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that image sequences with more frames are needed to resolve ambiguities in depth ordering at occlusion boundaries; temporal layer constancy makes this feasible and the optimizer, which mixes discrete and continuous optimization, automatically determines the number of layers and reasons about their depth ordering."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8193421"
                        ],
                        "name": "Yicong Tian",
                        "slug": "Yicong-Tian",
                        "structuredName": {
                            "firstName": "Yicong",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yicong Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 622833,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "eea7842025dad6b4cf53445c161538536020b412",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D sub volumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions."
            },
            "slug": "Spatiotemporal-Deformable-Part-Models-for-Action-Tian-Sukthankar",
            "title": {
                "fragments": [],
                "text": "Spatiotemporal Deformable Part Models for Action Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video by focusing on the most distinctive parts of each action."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37108776"
                        ],
                        "name": "Philip Lenz",
                        "slug": "Philip-Lenz",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760556"
                        ],
                        "name": "C. Stiller",
                        "slug": "C.-Stiller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Stiller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9455111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79b949d9b35c3f51dd20fb5c746cc81fc87147eb",
            "isKey": false,
            "numCitedBy": 4616,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10\u2013100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide."
            },
            "slug": "Vision-meets-robotics:-The-KITTI-dataset-Geiger-Lenz",
            "title": {
                "fragments": [],
                "text": "Vision meets robotics: The KITTI dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research, using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras and a high-precision GPS/IMU inertial navigation system."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080735"
                        ],
                        "name": "Yihang Bo",
                        "slug": "Yihang-Bo",
                        "structuredName": {
                            "firstName": "Yihang",
                            "lastName": "Bo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihang Bo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2818836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21fa37258834c2e3f075a8465d8de1c178cdaaf5",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a simple model for parsing pedestrians based on shape. Our model assembles candidate parts from an oversegmentation of the image and matches them to a library of exemplars. Our matching uses a hierarchical decomposition into a variable number of parts and computes scores on partial matchings in order to prune the search space of candidate segment. Simple constraints enforce consistent layout of parts. Because our model is shape-based, it generalizes well. We use exemplars from a controlled dataset of poses but achieve good test performance on unconstrained images of pedestrians in street scenes. We demonstrate results of parsing detections returned from a standard scanning-window pedestrian detector and use the resulting parse to perform viewpoint prediction and detection re-scoring."
            },
            "slug": "Shape-based-pedestrian-parsing-Bo-Fowlkes",
            "title": {
                "fragments": [],
                "text": "Shape-based pedestrian parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A simple model for parsing pedestrians based on shape that assembles candidate parts from an oversegmentation of the image and matches them to a library of exemplars to perform viewpoint prediction and detection re-scoring."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109120086"
                        ],
                        "name": "Limin Wang",
                        "slug": "Limin-Wang",
                        "structuredName": {
                            "firstName": "Limin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Limin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143970608"
                        ],
                        "name": "Y. Qiao",
                        "slug": "Y.-Qiao",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Qiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15068174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9c9b8194ac81f97bfedb7d15124e7b80c3c3d68",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Action detection is of great importance in understanding human motion from video. Compared with action recognition, it not only recognizes action type, but also localizes its spatiotemporal extent. This paper presents a relational model for action detection, which first decomposes human action into temporal \u201ckey poses\u201d and then further into spatial \u201caction parts\u201d. Specifically, we start by clustering cuboids around each human joint into dynamic-poselets using a new descriptor. The cuboids from the same cluster share consistent geometric and dynamic structure, and each cluster acts as a mixture of body parts. We then propose a sequential skeleton model to capture the relations among dynamic-poselets. This model unifies the tasks of learning the composites of mixture dynamic-poselets, the spatiotemporal structures of action parts, and the local model for each action part in a single framework. Our model not only allows to localize the action in a video stream, but also enables a detailed pose estimation of an actor. We formulate the model learning problem in a structured SVM framework and speed up model inference by dynamic programming. We conduct experiments on three challenging action detection datasets: the MSR-II dataset, the UCF Sports dataset, and the JHMDB dataset. The results show that our method achieves superior performance to the state-of-the-art methods on these datasets."
            },
            "slug": "Video-Action-Detection-with-Relational-Wang-Qiao",
            "title": {
                "fragments": [],
                "text": "Video Action Detection with Relational Dynamic-Poselets"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A relational model for action detection, which first decomposes human action into temporal \u201ckey poses\u201d and then further into spatial \u201caction parts\u201d, which not only allows to localize the action in a video stream, but also enables a detailed pose estimation of an actor."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365442"
                        ],
                        "name": "B. Alexe",
                        "slug": "B.-Alexe",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Alexe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alexe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9671213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da0ce73dd003048075fec44ae0dbaea6397eeb5b",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning a new object class from cluttered training images is very challenging when the location of object instances is unknown, i.e. in a weakly supervised setting. Many previous works require objects covering a large portion of the images. We present a novel approach that can cope with extensive clutter as well as large scale and appearance variations between object instances. To make this possible we exploit generic knowledge learned beforehand from images of other classes for which location annotation is available. Generic knowledge facilitates learning any new class from weakly supervised images, because it reduces the uncertainty in the location of its object instances. We propose a conditional random field that starts from generic knowledge and then progressively adapts to the new class. Our approach simultaneously localizes object instances while learning an appearance model specific for the class. We demonstrate this on several datasets, including the very challenging Pascal VOC 2007. Furthermore, our method allows training any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations."
            },
            "slug": "Weakly-Supervised-Localization-and-Learning-with-Deselaers-Alexe",
            "title": {
                "fragments": [],
                "text": "Weakly Supervised Localization and Learning with Generic Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A conditional random field that starts from generic knowledge and then progressively adapts to the new class is proposed that allows training any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144574904"
                        ],
                        "name": "Li Xu",
                        "slug": "Li-Xu",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729056"
                        ],
                        "name": "Jiaya Jia",
                        "slug": "Jiaya-Jia",
                        "structuredName": {
                            "firstName": "Jiaya",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaya Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774618"
                        ],
                        "name": "Y. Matsushita",
                        "slug": "Y.-Matsushita",
                        "structuredName": {
                            "firstName": "Yasuyuki",
                            "lastName": "Matsushita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Matsushita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2673457,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "66f9ee80721a20b1d191e06c5f5ab42e542a1241",
            "isKey": false,
            "numCitedBy": 535,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the cause of a severe optical flow estimation problem that fine motion structures cannot always be correctly reconstructed in the commonly employed multi-scale variational framework. Our major finding is that significant and abrupt displacement transition wrecks small-scale motion structures in the coarse-to-fine refinement. A novel optical flow estimation method is proposed in this paper to address this issue, which reduces the reliance of the flow estimates on their initial values propagated from the coarser level and enables recovering many motion details in each scale. The contribution of this paper also includes adaption of the objective function and development of a new optimization procedure. The effectiveness of our method is borne out by experiments for both large- and small-displacement optical flow estimation."
            },
            "slug": "Motion-detail-preserving-optical-flow-estimation-Xu-Jia",
            "title": {
                "fragments": [],
                "text": "Motion Detail Preserving Optical Flow Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel optical flow estimation method is proposed, which reduces the reliance of the flow estimates on their initial values propagated from the coarser level and enables recovering many motion details in each scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709824"
                        ],
                        "name": "Ben Glocker",
                        "slug": "Ben-Glocker",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Glocker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Glocker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713941"
                        ],
                        "name": "C. Zach",
                        "slug": "C.-Zach",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Zach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Zach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79406746"
                        ],
                        "name": "S. Izadi",
                        "slug": "S.-Izadi",
                        "structuredName": {
                            "firstName": "Shahram",
                            "lastName": "Izadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Izadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8632684,
            "fieldsOfStudy": [
                "Environmental Science",
                "Computer Science"
            ],
            "id": "00f2a152454db273b0d6831b6550beb37a135890",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines."
            },
            "slug": "Scene-Coordinate-Regression-Forests-for-Camera-in-Shotton-Glocker",
            "title": {
                "fragments": [],
                "text": "Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work addresses the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image, and employs a regression forest that is capable of inferting an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704531"
                        ],
                        "name": "Novi Quadrianto",
                        "slug": "Novi-Quadrianto",
                        "structuredName": {
                            "firstName": "Novi",
                            "lastName": "Quadrianto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Novi Quadrianto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1740642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0d584423cd4afc3c1b861b7c62bb5da48021d66",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of metric learning for multi-view data, namely the construction of embedding projections from data in different representations into a shared feature space, such that the Euclidean distance in this space provides a meaningful within-view as well as between-view similarity. Our motivation stems from the problem of cross-media retrieval tasks, where the availability of a joint Euclidean distance function is a prerequisite to allow fast, in particular hashing-based, nearest neighbor queries. \n \nWe formulate an objective function that expresses the intuitive concept that matching samples are mapped closely together in the output space, whereas non-matching samples are pushed apart, no matter in which view they are available. The resulting optimization problem is not convex, but it can be decomposed explicitly into a convex and a concave part, thereby allowing efficient optimization using the convex-concave procedure. Experiments on an image retrieval task show that nearest-neighbor based cross-view retrieval is indeed possible, and the proposed technique improves the retrieval accuracy over baseline techniques."
            },
            "slug": "Learning-Multi-View-Neighborhood-Preserving-Quadrianto-Lampert",
            "title": {
                "fragments": [],
                "text": "Learning Multi-View Neighborhood Preserving Projections"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work addresses the problem of metric learning for multi-view data, namely the construction of embedding projections from data in different representations into a shared feature space, such that the Euclidean distance in this space provides a meaningful within-view as well as between-view similarity."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48095899"
                        ],
                        "name": "G. Fanelli",
                        "slug": "G.-Fanelli",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Fanelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fanelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 840914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43dfc9392d2aba128a40f458581a294bcaac497e",
            "isKey": false,
            "numCitedBy": 464,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Fast and reliable algorithms for estimating the head pose are essential for many applications and higher-level face analysis tasks. We address the problem of head pose estimation from depth data, which can be captured using the ever more affordable 3D sensing technologies available today. To achieve robustness, we formulate pose estimation as a regression problem. While detecting specific face parts like the nose is sensitive to occlusions, learning the regression on rather generic surface patches requires enormous amount of training data in order to achieve accurate estimates. We propose to use random regression forests for the task at hand, given their capability to handle large training datasets. Moreover, we synthesize a great amount of annotated training data using a statistical model of the human face. In our experiments, we show that our approach can handle real data presenting large pose changes, partial occlusions, and facial expressions, even though it is trained only on synthetic neutral face data. We have thoroughly evaluated our system on a publicly available database on which we achieve state-of-the-art performance without having to resort to the graphics card."
            },
            "slug": "Real-time-head-pose-estimation-with-random-forests-Fanelli-Gall",
            "title": {
                "fragments": [],
                "text": "Real time head pose estimation with random regression forests"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work addresses the problem of head pose estimation from depth data, which can be captured using the ever more affordable 3D sensing technologies available today, by proposing to use random regression forests for the task at hand."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40521794"
                        ],
                        "name": "D. Pfeiffer",
                        "slug": "D.-Pfeiffer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pfeiffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pfeiffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762392"
                        ],
                        "name": "S. Gehrig",
                        "slug": "S.-Gehrig",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Gehrig",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gehrig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763389"
                        ],
                        "name": "Nicolai Schneider",
                        "slug": "Nicolai-Schneider",
                        "structuredName": {
                            "firstName": "Nicolai",
                            "lastName": "Schneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolai Schneider"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9572862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6052969b1220a56f73a304a158e7cddfe5612c8",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. This is straightforward but it does not take full advantage of the available information. In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World, a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a posteriori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base of which parts are made publicly available. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level."
            },
            "slug": "Exploiting-the-Power-of-Stereo-Confidences-Pfeiffer-Gehrig",
            "title": {
                "fragments": [],
                "text": "Exploiting the Power of Stereo Confidences"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper makes full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner and shows that using stereoconfidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333123"
                        ],
                        "name": "Abhishek Kumar",
                        "slug": "Abhishek-Kumar",
                        "structuredName": {
                            "firstName": "Abhishek",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhishek Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6423592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "917e01d4be4a3417b2941ecd548186a6bf868358",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a spectral clustering algorithm for the multi-view setting where we have access to multiple views of the data, each of which can be independently used for clustering. Our spectral clustering algorithm has a flavor of co-training, which is already a widely used idea in semi-supervised learning. We work on the assumption that the true underlying clustering would assign a point to the same cluster irrespective of the view. Hence, we constrain our approach to only search for the clusterings that agree across the views. Our algorithm does not have any hyperparameters to set, which is a major advantage in unsupervised learning. We empirically compare with a number of baseline methods on synthetic and real-world datasets to show the efficacy of the proposed algorithm."
            },
            "slug": "A-Co-training-Approach-for-Multi-view-Spectral-Kumar-Daum\u00e9",
            "title": {
                "fragments": [],
                "text": "A Co-training Approach for Multi-view Spectral Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A spectral clustering algorithm for the multi-view setting where the authors have access to multiple views of the data, each of which can be independently used for clustering, which has a flavor of co-training."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872774"
                        ],
                        "name": "W. Dong",
                        "slug": "W.-Dong",
                        "structuredName": {
                            "firstName": "Weisheng",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152830307"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143713952"
                        ],
                        "name": "Guangming Shi",
                        "slug": "Guangming-Shi",
                        "structuredName": {
                            "firstName": "Guangming",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangming Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39529395"
                        ],
                        "name": "Xiaolin Wu",
                        "slug": "Xiaolin-Wu",
                        "structuredName": {
                            "firstName": "Xiaolin",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaolin Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14877173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80167e8e03cdb830948b23af2a3bb2d3131dfdfb",
            "isKey": false,
            "numCitedBy": 1185,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "As a powerful statistical image modeling technique, sparse representation has been successfully used in various image restoration applications. The success of sparse representation owes to the development of the l1-norm optimization techniques and the fact that natural images are intrinsically sparse in some domains. The image restoration quality largely depends on whether the employed sparse domain can represent well the underlying image. Considering that the contents can vary significantly across different images or different patches in a single image, we propose to learn various sets of bases from a precollected dataset of example image patches, and then, for a given patch to be processed, one set of bases are adaptively selected to characterize the local sparse domain. We further introduce two adaptive regularization terms into the sparse representation framework. First, a set of autoregressive (AR) models are learned from the dataset of example image patches. The best fitted AR models to a given patch are adaptively selected to regularize the image local structures. Second, the image nonlocal self-similarity is introduced as another regularization term. In addition, the sparsity regularization parameter is adaptively estimated for better image restoration performance. Extensive experiments on image deblurring and super-resolution validate that by using adaptive sparse domain selection and adaptive regularization, the proposed method achieves much better results than many state-of-the-art algorithms in terms of both PSNR and visual perception."
            },
            "slug": "Image-Deblurring-and-Super-Resolution-by-Adaptive-Dong-Zhang",
            "title": {
                "fragments": [],
                "text": "Image Deblurring and Super-Resolution by Adaptive Sparse Domain Selection and Adaptive Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Extensive experiments on image deblurring and super-resolution validate that by using adaptive sparse domain selection and adaptive regularization, the proposed method achieves much better results than many state-of-the-art algorithms in terms of both PSNR and visual perception."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50033175"
                        ],
                        "name": "Andreas Argyriou",
                        "slug": "Andreas-Argyriou",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Argyriou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Argyriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6617228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e7b0395d7b34e9d34cca779afd0c10da6e135b5",
            "isKey": false,
            "numCitedBy": 1446,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe present a method for learning sparse representations shared across multiple tasks. This method is a generalization of the well-known single-task 1-norm regularization. It is based on a novel non-convex regularizer which controls the number of learned features common across the tasks. We prove that the method is equivalent to solving a convex optimization problem for which there is an iterative algorithm which converges to an optimal solution. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the former step it learns task-specific functions and in the latter step it learns common-across-tasks sparse representations for these functions. We also provide an extension of the algorithm which learns sparse nonlinear representations using kernels. We report experiments on simulated and real data sets which demonstrate that the proposed method can both improve the performance relative to learning each task independently and lead to a few learned features common across related tasks. Our algorithm can also be used, as a special case, to simply select\u2014not learn\u2014a few common variables across the tasks.\n"
            },
            "slug": "Convex-multi-task-feature-learning-Argyriou-Evgeniou",
            "title": {
                "fragments": [],
                "text": "Convex multi-task feature learning"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is proved that the method for learning sparse representations shared across multiple tasks is equivalent to solving a convex optimization problem for which there is an iterative algorithm which converges to an optimal solution."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40245930"
                        ],
                        "name": "Danhang Tang",
                        "slug": "Danhang-Tang",
                        "structuredName": {
                            "firstName": "Danhang",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danhang Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148786644"
                        ],
                        "name": "Tsz-Ho Yu",
                        "slug": "Tsz-Ho-Yu",
                        "structuredName": {
                            "firstName": "Tsz-Ho",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsz-Ho Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143617697"
                        ],
                        "name": "Tae-Kyun Kim",
                        "slug": "Tae-Kyun-Kim",
                        "structuredName": {
                            "firstName": "Tae-Kyun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tae-Kyun Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4663640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "780eeaa7605562cb2363e051864867958db6e876",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the first semi-supervised transductive algorithm for real-time articulated hand pose estimation. Noisy data and occlusions are the major challenges of articulated hand pose estimation. In addition, the discrepancies among realistic and synthetic pose data undermine the performances of existing approaches that use synthetic data extensively in training. We therefore propose the Semi-supervised Transductive Regression (STR) forest which learns the relationship between a small, sparsely labelled realistic dataset and a large synthetic dataset. We also design a novel data-driven, pseudo-kinematic technique to refine noisy or occluded joints. Our contributions include: (i) capturing the benefits of both realistic and synthetic data via transductive learning, (ii) showing accuracies can be improved by considering unlabelled data, and (iii) introducing a pseudo-kinematic technique to refine articulations efficiently. Experimental results show not only the promising performance of our method with respect to noise and occlusions, but also its superiority over state-of-the-arts in accuracy, robustness and speed."
            },
            "slug": "Real-Time-Articulated-Hand-Pose-Estimation-Using-Tang-Yu",
            "title": {
                "fragments": [],
                "text": "Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Semi-supervised Transductive Regression (STR) forest is proposed which learns the relationship between a small, sparsely labelled realistic dataset and a large synthetic dataset, and a novel data-driven, pseudo-kinematic technique to refine noisy or occluded joints."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266088"
                        ],
                        "name": "T. Leung",
                        "slug": "T.-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "After estimating blur kernels, we apply the non-blind deconvolution method of Zoran and Weiss [6] to recover the whole latent images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 106
                            }
                        ],
                        "text": "Recently, sparse representation based generative tracking methods have been developed for object tracking [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "We start from a large-margin embedding (LME) classification model [6], which we adapt to a detection task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 156
                            }
                        ],
                        "text": "Figure 3: Comparison of sparsification curves for various confidence measures including learning based approaches [1, 3] and individual confidence measures [3, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 217
                            }
                        ],
                        "text": "Indeed, when knowledge about task relatedness is available, it can be profitably incorporated in multi-task learning approaches for example by designing suitable embedding/coding schemes, kernels or regularizers, see [4, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 300
                            }
                        ],
                        "text": "Both of these assumptions are valid for faces: first-order spherical harmonic functions are able to describe up to 95% of facial illumination variance [4] and the low-dimensional properties of faces has been exploited in works such as 3D Morphable Models (3DMM)[2] and Active Appearance Models (AAM) [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Based on those researches, several papers address the feasibility of detecting mismatched pixels [6] not only to improve the quality of disparity maps [1] but also to leverage mid-level scene representation [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "While recent optical flow methods such as [6] are highly accurate, for many applications speed is often as important."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 35
                            }
                        ],
                        "text": "(a) Global sparse appearance model [3, 4, 6, 7, 9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Additionally, the region proposal step is improved by combining the selective search [6] approach with multi-box [3] predictions for higher ob-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14915716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90d6e7f2202f754d8588f9536e3f5b4a24701f24",
            "isKey": true,
            "numCitedBy": 1713,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions.Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions. We also illustrate how the 3D texton model can be used to predict the appearance of materials under novel conditions."
            },
            "slug": "Representing-and-Recognizing-the-Visual-Appearance-Leung-Malik",
            "title": {
                "fragments": [],
                "text": "Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A unified model to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions is provided."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39678486"
                        ],
                        "name": "Neel Joshi",
                        "slug": "Neel-Joshi",
                        "structuredName": {
                            "firstName": "Neel",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neel Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 322744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17eb16501cbff72beaa637b9226a8472d0b3de81",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Image blur and noise are difficult to avoid in many situations and can often ruin a photograph. We present a novel image deconvolution algorithm that deblurs and denoises an image given a known shift-invariant blur kernel. Our algorithm uses local color statistics derived from the image as a constraint in a unified framework that can be used for deblurring, denoising, and upsampling. A pixel's color is required to be a linear combination of the two most prevalent colors within a neighborhood of the pixel. This two-color prior has two major benefits: it is tuned to the content of the particular image and it serves to decouple edge sharpness from edge strength. Our unified algorithm for deblurring and denoising out-performs previous methods that are specialized for these individual applications. We demonstrate this with both qualitative results and extensive quantitative comparisons that show that we can out-perform previous methods by approximately 1 to 3 DB."
            },
            "slug": "Image-deblurring-and-denoising-using-color-priors-Joshi-Zitnick",
            "title": {
                "fragments": [],
                "text": "Image deblurring and denoising using color priors"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work presents a novel image deconvolution algorithm that deblurs and denoises an image given a known shift-invariant blur kernel in a unified framework that can be used for deblurring, denoising, and upsampling."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145371957"
                        ],
                        "name": "Chang Xu",
                        "slug": "Chang-Xu",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143719920"
                        ],
                        "name": "D. Tao",
                        "slug": "D.-Tao",
                        "structuredName": {
                            "firstName": "Dacheng",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31796215"
                        ],
                        "name": "Chao Xu",
                        "slug": "Chao-Xu",
                        "structuredName": {
                            "firstName": "Chao",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17549749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "032d67d27ecacbf6c5b82eb67e5d02d81fb43a7a",
            "isKey": false,
            "numCitedBy": 867,
            "numCiting": 146,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning."
            },
            "slug": "A-Survey-on-Multi-view-Learning-Xu-Tao",
            "title": {
                "fragments": [],
                "text": "A Survey on Multi-view Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "By exploring the consistency and complementary properties of different views, multi-View learning is rendered more effective, more promising, and has better generalization ability than single-view learning."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787725"
                        ],
                        "name": "Fabio Galasso",
                        "slug": "Fabio-Galasso",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Galasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Galasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316866"
                        ],
                        "name": "Margret Keuper",
                        "slug": "Margret-Keuper",
                        "structuredName": {
                            "firstName": "Margret",
                            "lastName": "Keuper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margret Keuper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11382182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8336dc3434a30dd677e3a2c95566739d7947b06a",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational and memory costs restrict spectral techniques to rather small graphs, which is a serious limitation especially in video segmentation. In this paper, we propose the use of a reduced graph based on superpixels. In contrast to previous work, the reduced graph is reweighted such that the resulting segmentation is equivalent, under certain assumptions, to that of the full graph. We consider equivalence in terms of the normalized cut and of its spectral clustering relaxation. The proposed method reduces runtime and memory consumption and yields on par results in image and video segmentation. Further, it enables an efficient data representation and update for a new streaming video segmentation approach that also achieves state-of-the-art performance."
            },
            "slug": "Spectral-Graph-Reduction-for-Efficient-Image-and-Galasso-Keuper",
            "title": {
                "fragments": [],
                "text": "Spectral Graph Reduction for Efficient Image and Streaming Video Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed reduced graph is reweighted such that the resulting segmentation is equivalent, under certain assumptions, to that of the full graph, and reduces runtime and memory consumption and yields on par results in image and video segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46506697"
                        ],
                        "name": "Heng Wang",
                        "slug": "Heng-Wang",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 753512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d721f4d64b8e722222c876f0a0f226ed49476347",
            "isKey": false,
            "numCitedBy": 2828,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art."
            },
            "slug": "Action-Recognition-with-Improved-Trajectories-Wang-Schmid",
            "title": {
                "fragments": [],
                "text": "Action Recognition with Improved Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets are improved by taking into account camera motion to correct them."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1641711590"
                        ],
                        "name": "Xu Jia",
                        "slug": "Xu-Jia",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153176123"
                        ],
                        "name": "Huchuan Lu",
                        "slug": "Huchuan-Lu",
                        "structuredName": {
                            "firstName": "Huchuan",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huchuan Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1315786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8398d4fa0c4722bd6f0eb0d374178c5a35cd3f3",
            "isKey": false,
            "numCitedBy": 1260,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse representation has been applied to visual tracking by finding the best candidate with minimal reconstruction error using target templates. However most sparse representation based trackers only consider the holistic representation and do not make full use of the sparse coefficients to discriminate between the target and the background, and hence may fail with more possibility when there is similar object or occlusion in the scene. In this paper we develop a simple yet robust tracking method based on the structural local sparse appearance model. This representation exploits both partial information and spatial information of the target based on a novel alignment-pooling method. The similarity obtained by pooling across the local patches helps not only locate the target more accurately but also handle occlusion. In addition, we employ a template update strategy which combines incremental subspace learning and sparse representation. This strategy adapts the template to the appearance change of the target with less possibility of drifting and reduces the influence of the occluded target template as well. Both qualitative and quantitative evaluations on challenging benchmark image sequences demonstrate that the proposed tracking algorithm performs favorably against several state-of-the-art methods."
            },
            "slug": "Visual-tracking-via-adaptive-structural-local-model-Jia-Lu",
            "title": {
                "fragments": [],
                "text": "Visual tracking via adaptive structural local sparse appearance model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simple yet robust tracking method based on the structural local sparse appearance model which exploits both partial information and spatial information of the target based on a novel alignment-pooling method and employs a template update strategy which combines incremental subspace learning and sparse representation."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232265"
                        ],
                        "name": "Deqing Sun",
                        "slug": "Deqing-Sun",
                        "structuredName": {
                            "firstName": "Deqing",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deqing Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1565859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2362cf37d9693cbc1d2f7ac50b44fca4b28d6cd",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions."
            },
            "slug": "Layered-image-motion-with-explicit-occlusions,-and-Sun-Sudderth",
            "title": {
                "fragments": [],
                "text": "Layered image motion with explicit occlusions, temporal consistency, and depth ordering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches and achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477518"
                        ],
                        "name": "Grigorios Tzortzis",
                        "slug": "Grigorios-Tzortzis",
                        "structuredName": {
                            "firstName": "Grigorios",
                            "lastName": "Tzortzis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grigorios Tzortzis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717776"
                        ],
                        "name": "A. Likas",
                        "slug": "A.-Likas",
                        "structuredName": {
                            "firstName": "Aristidis",
                            "lastName": "Likas",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Likas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10797723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec14347ffaba614d2a3fa5e1f2bf734770eedd3",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Exploiting multiple representations, or views, for the same set of instances within a clustering framework is a popular practice for boosting clustering accuracy. However, some of the available sources may be misleading (due to noise, errors in measurement etc.) in revealing the true structure of the data, thus, their inclusion in the clustering process may have negative influence. This aspect seems to be overlooked in the multi-view literature where all representations are equally considered. In this work, views are expressed in terms of given kernel matrices and a weighted combination of the kernels is learned in parallel to the partitioning. Weights assigned to kernels are indicative of the quality of the corresponding views' information. Additionally, the combination scheme incorporates a parameter that controls the admissible sparsity of the weights to avoid extremes and tailor them to the data. Two efficient iterative algorithms are proposed that alternate between updating the view weights and recomputing the clusters to optimize the intra-cluster variance from different perspectives. The conducted experiments reveal the effectiveness of our methodology compared to other multi-view methods."
            },
            "slug": "Kernel-Based-Weighted-Multi-view-Clustering-Tzortzis-Likas",
            "title": {
                "fragments": [],
                "text": "Kernel-Based Weighted Multi-view Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work exploits multiple representations for the same set of instances within a clustering framework in terms of given kernel matrices and a weighted combination of the kernels is learned in parallel to the partitioning."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE 12th International Conference on Data Mining"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066445700"
                        ],
                        "name": "Jan Knopp",
                        "slug": "Jan-Knopp",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Knopp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Knopp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4373221"
                        ],
                        "name": "Mukta Prasad",
                        "slug": "Mukta-Prasad",
                        "structuredName": {
                            "firstName": "Mukta",
                            "lastName": "Prasad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mukta Prasad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072996455"
                        ],
                        "name": "G. Willems",
                        "slug": "G.-Willems",
                        "structuredName": {
                            "firstName": "Geert",
                            "lastName": "Willems",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Willems"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732855"
                        ],
                        "name": "R. Timofte",
                        "slug": "R.-Timofte",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Timofte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Timofte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1722114,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1551ab876f75b1d11eab4a7ebd21c95ac771da43",
            "isKey": false,
            "numCitedBy": 357,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Most methods for the recognition of shape classes from 3D datasets focus on classifying clean, often manually generated models. However, 3D shapes obtained through acquisition techniques such as Structure-from-Motion or LIDAR scanning are noisy, clutter and holes. In that case global shape features--still dominating the 3D shape class recognition literature--are less appropriate. Inspired by 2D methods, recently researchers have started to work with local features. In keeping with this strand, we propose a new robust 3D shape classification method. It contains two main contributions. First, we extend a robust 2D feature descriptor, SURF, to be used in the context of 3D shapes. Second, we show how 3D shape class recognition can be improved by probabilistic Hough transform based methods, already popular in 2D. Through our experiments on partial shape retrieval, we show the power of the proposed 3D features. Their combination with the Hough transform yields superior results for class recognition on standard datasets. The potential for the applicability of such a method in classifying 3D obtained from Structure-from-Motion methods is promising, as we show in some initial experiments."
            },
            "slug": "Hough-Transform-and-3D-SURF-for-Robust-Three-Knopp-Prasad",
            "title": {
                "fragments": [],
                "text": "Hough Transform and 3D SURF for Robust Three Dimensional Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new robust 3D shape classification method is proposed, which extends a robust 2D feature descriptor, SURF, to be used in the context of 3D shapes and shows how3D shape class recognition can be improved by probabilistic Hough transform based methods, already popular in 2D."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403990822"
                        ],
                        "name": "F. Couzinie-Devy",
                        "slug": "F.-Couzinie-Devy",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Couzinie-Devy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Couzinie-Devy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152146009"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1307614,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e6f732b7668fde550277123564670197d307d715",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multi-label energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa's method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti~et al."
            },
            "slug": "Learning-to-Estimate-and-Remove-Non-uniform-Image-Couzinie-Devy-Sun",
            "title": {
                "fragments": [],
                "text": "Learning to Estimate and Remove Non-uniform Image Blur"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear motion using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880407"
                        ],
                        "name": "T. Michaeli",
                        "slug": "T.-Michaeli",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Michaeli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Michaeli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16638962,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "24e1ea4d01f7bd8bf3e4bebdadbd158f27eec5a3",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrence of small image patches across different scales of a natural image has been previously used for solving ill-posed problems (e.g. super- resolution from a single image). In this paper we show how this multi-scale property can also be used for \u201cblind-deblurring\u201d, namely, removal of an unknown blur from a blurry image. While patches repeat \u2018as is\u2019 across scales in a sharp natural image, this cross-scale recurrence significantly diminishes in blurry images. We exploit these deviations from ideal patch recurrence as a cue for recovering the underlying (unknown) blur kernel. More specifically, we look for the blur kernel k, such that if its effect is \u201cundone\u201d (if the blurry image is deconvolved with k), the patch similarity across scales of the image will be maximized. We report extensive experimental evaluations, which indicate that our approach compares favorably to state-of-the-art blind deblurring methods, and in particular, is more robust than them."
            },
            "slug": "Blind-Deblurring-Using-Internal-Patch-Recurrence-Michaeli-Irani",
            "title": {
                "fragments": [],
                "text": "Blind Deblurring Using Internal Patch Recurrence"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper exploits deviations from ideal patch recurrence as a cue for recovering the underlying (unknown) blur kernel k, such that if its effect is \u201cundone\u201d (if the blurry image is deconvolved with k), the patch similarity across scales of the image will be maximized."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40454588"
                        ],
                        "name": "Jan-Michael Frahm",
                        "slug": "Jan-Michael-Frahm",
                        "structuredName": {
                            "firstName": "Jan-Michael",
                            "lastName": "Frahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan-Michael Frahm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2322639"
                        ],
                        "name": "P. Georgel",
                        "slug": "P.-Georgel",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Georgel",
                            "middleNames": [
                                "Fite"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Georgel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144866265"
                        ],
                        "name": "D. Gallup",
                        "slug": "D.-Gallup",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gallup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gallup"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152656752"
                        ],
                        "name": "Tim Johnson",
                        "slug": "Tim-Johnson",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47364998"
                        ],
                        "name": "R. Raguram",
                        "slug": "R.-Raguram",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Raguram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raguram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38955885"
                        ],
                        "name": "Changchang Wu",
                        "slug": "Changchang-Wu",
                        "structuredName": {
                            "firstName": "Changchang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changchang Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2969092"
                        ],
                        "name": "Yi-Hung Jen",
                        "slug": "Yi-Hung-Jen",
                        "structuredName": {
                            "firstName": "Yi-Hung",
                            "lastName": "Jen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi-Hung Jen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144678099"
                        ],
                        "name": "Enrique Dunn",
                        "slug": "Enrique-Dunn",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Dunn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enrique Dunn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2628245"
                        ],
                        "name": "Brian Clipp",
                        "slug": "Brian-Clipp",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Clipp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Clipp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2333407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7db8add47367430b35ff3c5373f8d00450f13348",
            "isKey": false,
            "numCitedBy": 614,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces an approach for dense 3D reconstruction from unregistered Internet-scale photo collections with about 3 million images within the span of a day on a single PC (\"cloudless\"). Our method advances image clustering, stereo, stereo fusion and structure from motion to achieve high computational performance. We leverage geometric and appearance constraints to obtain a highly parallel implementation on modern graphics processors and multi-core architectures. This leads to two orders of magnitude higher performance on an order of magnitude larger dataset than competing state-of-the-art approaches."
            },
            "slug": "Building-Rome-on-a-Cloudless-Day-Frahm-Georgel",
            "title": {
                "fragments": [],
                "text": "Building Rome on a Cloudless Day"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper introduces an approach for dense 3D reconstruction from unregistered Internet-scale photo collections with about 3 million images within the span of a day on a single PC (\"cloudless\"), leveraging geometric and appearance constraints to obtain a highly parallel implementation on modern graphics processors and multi-core architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38551815"
                        ],
                        "name": "Anat Levin",
                        "slug": "Anat-Levin",
                        "structuredName": {
                            "firstName": "Anat",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anat Levin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11951016,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c434bd9db63fca8c1fa3ff28c2d3962441b77f89",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. \n \nOur approach relies on the observation that the statistics of derivative filters in images are significantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box filter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture."
            },
            "slug": "Blind-Motion-Deblurring-Using-Image-Statistics-Levin",
            "title": {
                "fragments": [],
                "text": "Blind Motion Deblurring Using Image Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work addresses the problem of blind motion deblurring from a single image, caused by a few moving objects, and relies on the observation that the statistics of derivative filters in images are significantly changed by blur."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731122"
                        ],
                        "name": "Qi Zhang",
                        "slug": "Qi-Zhang",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029246"
                        ],
                        "name": "Xiaoyong Shen",
                        "slug": "Xiaoyong-Shen",
                        "structuredName": {
                            "firstName": "Xiaoyong",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyong Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144574904"
                        ],
                        "name": "Li Xu",
                        "slug": "Li-Xu",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729056"
                        ],
                        "name": "Jiaya Jia",
                        "slug": "Jiaya-Jia",
                        "structuredName": {
                            "firstName": "Jiaya",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaya Jia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13443906,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7daebb130b491b1ae8afff0e4e9b5f5302e2d996",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Images contain many levels of important structures and edges. Compared to masses of research to make filters edge preserving, finding scale-aware local operations was seldom addressed in a practical way, albeit similarly vital in image processing and computer vision. We propose a new framework to filter images with the complete control of detail smoothing under a scale measure. It is based on a rolling guidance implemented in an iterative manner that converges quickly. Our method is simple in implementation, easy to understand, fully extensible to accommodate various data operations, and fast to produce results. Our implementation achieves realtime performance and produces artifact-free results in separating different scale structures. This filter also introduces several inspiring properties different from previous edge-preserving ones."
            },
            "slug": "Rolling-Guidance-Filter-Zhang-Shen",
            "title": {
                "fragments": [],
                "text": "Rolling Guidance Filter"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new framework to filter images with the complete control of detail smoothing under a scale measure is proposed, based on a rolling guidance implemented in an iterative manner that converges quickly and achieves realtime performance and produces artifact-free results."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110687280"
                        ],
                        "name": "Libin Sun",
                        "slug": "Libin-Sun",
                        "structuredName": {
                            "firstName": "Libin",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Libin Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7942592"
                        ],
                        "name": "Sunghyun Cho",
                        "slug": "Sunghyun-Cho",
                        "structuredName": {
                            "firstName": "Sunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109763110"
                        ],
                        "name": "Jue Wang",
                        "slug": "Jue-Wang",
                        "structuredName": {
                            "firstName": "Jue",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jue Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 382798,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "de734cdfb68f4bcd91dcba4f243ced796c093394",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Blind image deconvolution, i.e., estimating a blur kernel k and a latent image x from an input blurred image y, is a severely ill-posed problem. In this paper we introduce a new patch-based strategy for kernel estimation in blind deconvolution. Our approach estimates a \u201ctrusted\u201d subset of x by imposing a patch prior specifically tailored towards modeling the appearance of image edge and corner primitives. To choose proper patch priors we examine both statistical priors learned from a natural image dataset and a simple patch prior from synthetic structures. Based on the patch priors, we iteratively recover the partial latent image x and the blur kernel k. A comprehensive evaluation shows that our approach achieves state-of-the-art results for uniformly blurred images."
            },
            "slug": "Edge-based-blur-kernel-estimation-using-patch-Sun-Cho",
            "title": {
                "fragments": [],
                "text": "Edge-based blur kernel estimation using patch priors"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new patch-based strategy for kernel estimation in blind deconvolution estimates a \u201ctrusted\u201d subset of x by imposing a patch prior specifically tailored towards modeling the appearance of image edge and corner primitives."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Computational Photography (ICCP)"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110239969"
                        ],
                        "name": "John Y. A. Wang",
                        "slug": "John-Y.-A.-Wang",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wang",
                            "middleNames": [
                                "Y.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Y. A. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5498425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "088898bc4caf25d179533afe0335aeb52dd6f723",
            "isKey": false,
            "numCitedBy": 1327,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a system for representing moving images with sets of overlapping layers. Each layer contains an intensity map that defines the additive values of each pixel, along with an alpha map that serves as a mask indicating the transparency. The layers are ordered in depth and they occlude each other in accord with the rules of compositing. Velocity maps define how the layers are to be warped over time. The layered representation is more flexible than standard image transforms and can capture many important properties of natural image sequences. We describe some methods for decomposing image sequences into layers using motion analysis, and we discuss how the representation may be used for image coding and other applications."
            },
            "slug": "Representing-moving-images-with-layers-Wang-Adelson",
            "title": {
                "fragments": [],
                "text": "Representing moving images with layers"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A system for representing moving images with sets of overlapping layers that is more flexible than standard image transforms and can capture many important properties of natural image sequences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40404576"
                        ],
                        "name": "S. Amin",
                        "slug": "S.-Amin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Amin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906895"
                        ],
                        "name": "M. Andriluka",
                        "slug": "M.-Andriluka",
                        "structuredName": {
                            "firstName": "Mykhaylo",
                            "lastName": "Andriluka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andriluka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9349950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49435aab7cdf259335725acc96691f755e436f55",
            "isKey": false,
            "numCitedBy": 479,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "While activity recognition is a current focus of research the challenging problem of fine-grained activity recognition is largely overlooked. We thus propose a novel database of 65 cooking activities, continuously recorded in a realistic setting. Activities are distinguished by fine-grained body motions that have low inter-class variability and high intra-class variability due to diverse subjects and ingredients. We benchmark two approaches on our dataset, one based on articulated pose tracks and the second using holistic video features. While the holistic approach outperforms the pose-based approach, our evaluation suggests that fine-grained activities are more difficult to detect and the body model can help in those cases. Providing high-resolution videos as well as an intermediate pose representation we hope to foster research in fine-grained activity recognition."
            },
            "slug": "A-database-for-fine-grained-activity-detection-of-Rohrbach-Amin",
            "title": {
                "fragments": [],
                "text": "A database for fine grained activity detection of cooking activities"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel database of 65 cooking activities, continuously recorded in a realistic setting, is proposed, suggesting that fine-grained activities are more difficult to detect and the body model can help in those cases."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47300766"
                        ],
                        "name": "Xudong Cao",
                        "slug": "Xudong-Cao",
                        "structuredName": {
                            "firstName": "Xudong",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xudong Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732264"
                        ],
                        "name": "Yichen Wei",
                        "slug": "Yichen-Wei",
                        "structuredName": {
                            "firstName": "Yichen",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichen Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716835"
                        ],
                        "name": "Fang Wen",
                        "slug": "Fang-Wen",
                        "structuredName": {
                            "firstName": "Fang",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fang Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3351622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "489fc2a88f0257b577c3fecb5e4a59451c97dbfe",
            "isKey": false,
            "numCitedBy": 1082,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a very efficient, highly accurate, \u201cExplicit Shape Regression\u201d approach for face alignment. Unlike previous regression-based approaches, we directly learn a vectorial regression function to infer the whole facial shape (a set of facial landmarks) from the image and explicitly minimize the alignment errors over the training data. The inherent shape constraint is naturally encoded into the regressor in a cascaded learning framework and applied from coarse to fine during the test, without using a fixed parametric shape model as in most previous methods. To make the regression more effective and efficient, we design a two-level boosted regression, shape indexed features and a correlation-based feature selection method. This combination enables us to learn accurate models from large training data in a short time (20 min for 2,000 training images), and run regression extremely fast in test (15 ms for a 87 landmarks shape). Experiments on challenging data show that our approach significantly outperforms the state-of-the-art in terms of both accuracy and efficiency."
            },
            "slug": "Face-Alignment-by-Explicit-Shape-Regression-Cao-Wei",
            "title": {
                "fragments": [],
                "text": "Face Alignment by Explicit Shape Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A very efficient, highly accurate, \u201cExplicit Shape Regression\u201d approach for face alignment that significantly outperforms the state-of-the-art in terms of both accuracy and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111251935"
                        ],
                        "name": "Tae Hyun Kim",
                        "slug": "Tae-Hyun-Kim",
                        "structuredName": {
                            "firstName": "Tae",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tae Hyun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135837"
                        ],
                        "name": "Kyoung Mu Lee",
                        "slug": "Kyoung-Mu-Lee",
                        "structuredName": {
                            "firstName": "Kyoung",
                            "lastName": "Lee",
                            "middleNames": [
                                "Mu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyoung Mu Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15749523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc46242c10643235b734cf856aea61ed4ba9d948",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Most state-of-the-art dynamic scene deblurring methods based on accurate motion segmentation assume that motion blur is small or that the specific type of motion causing the blur is known. In this paper, we study a motion segmentation-free dynamic scene deblurring method, which is unlike other conventional methods. When the motion can be approximated to linear motion that is locally (pixel-wise) varying, we can handle various types of blur caused by camera shake, including out-of-plane motion, depth variation, radial distortion, and so on. Thus, we propose a new energy model simultaneously estimating motion flow and the latent image based on robust total variation (TV)-L1 model. This approach is necessary to handle abrupt changes in motion without segmentation. Furthermore, we address the problem of the traditional coarse-to-fine deblurring framework, which gives rise to artifacts when restoring small structures with distinct motion. We thus propose a novel kernel re-initialization method which reduces the error of motion flow propagated from a coarser level. Moreover, a highly effective convex optimization-based solution mitigating the computational difficulties of the TV-L1 model is established. Comparative experimental results on challenging real blurry images demonstrate the efficiency of the proposed method."
            },
            "slug": "Segmentation-Free-Dynamic-Scene-Deblurring-Kim-Lee",
            "title": {
                "fragments": [],
                "text": "Segmentation-Free Dynamic Scene Deblurring"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a new energy model simultaneously estimating motion flow and the latent image based on robust total variation (TV)-L1 model, and addresses the problem of the traditional coarse-to-fine deblurring framework, which gives rise to artifacts when restoring small structures with distinct motion."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3203657"
                        ],
                        "name": "Andrew Delong",
                        "slug": "Andrew-Delong",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Delong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Delong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089071"
                        ],
                        "name": "Lena Gorelick",
                        "slug": "Lena-Gorelick",
                        "structuredName": {
                            "firstName": "Lena",
                            "lastName": "Gorelick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lena Gorelick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 440644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43ae07ae86545a586e5fee60f760120e0f1c32fb",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision is full of problems elegantly expressed in terms of energy minimization. We characterize a class of energies with hierarchical costs and propose a novel hierarchical fusion algorithm. Hierarchical costs are natural for modeling an array of difficult problems. For example, in semantic segmentation one could rule out unlikely object combinations via hierarchical context. In geometric model estimation, one could penalize the number of unique model families in a solution, not just the number of models\u2014a kind of hierarchical MDL criterion. Hierarchical fusion uses the well-known \u03b1-expansion algorithm as a subroutine, and offers a much better approximation bound in important cases."
            },
            "slug": "Minimizing-Energies-with-Hierarchical-Costs-Delong-Gorelick",
            "title": {
                "fragments": [],
                "text": "Minimizing Energies with Hierarchical Costs"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A class of energies with hierarchical costs is characterized and a novel hierarchical fusion algorithm is proposed, which uses the well-known \u03b1-expansion algorithm as a subroutine, and offers a much better approximation bound in important cases."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354749"
                        ],
                        "name": "Yu-Yin Sun",
                        "slug": "Yu-Yin-Sun",
                        "structuredName": {
                            "firstName": "Yu-Yin",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Yin Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110463675"
                        ],
                        "name": "Yu-Feng Li",
                        "slug": "Yu-Feng-Li",
                        "structuredName": {
                            "firstName": "Yu-Feng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Feng Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2330026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49c67ea9313bd859b201603857c45e3597c77b4e",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous studies on multi-instance learning typically treated instances in the bags as independently and identically distributed. The instances in a bag, however, are rarely independent in real tasks, and a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits relations among instances. In this paper, we propose two simple yet effective methods. In the first method, we explicitly map every bag to an undirected graph and design a graph kernel for distinguishing the positive and negative bags. In the second method, we implicitly construct graphs by deriving affinity matrices and propose an efficient graph kernel considering the clique information. The effectiveness of the proposed methods are validated by experiments."
            },
            "slug": "Multi-instance-learning-by-treating-instances-as-Zhou-Sun",
            "title": {
                "fragments": [],
                "text": "Multi-instance learning by treating instances as non-I.I.D. samples"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper explicitly map every bag to an undirected graph and design a graph kernel for distinguishing the positive and negative bags and implicitly construct graphs by deriving affinity matrices and propose an efficient graph kernel considering the clique information."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2133169693"
                        ],
                        "name": "Xiaohui Wang",
                        "slug": "Xiaohui-Wang",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144202060"
                        ],
                        "name": "Jia Jia",
                        "slug": "Jia-Jia",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111611984"
                        ],
                        "name": "Jiaming Yin",
                        "slug": "Jiaming-Yin",
                        "structuredName": {
                            "firstName": "Jiaming",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaming Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7239047"
                        ],
                        "name": "Lianhong Cai",
                        "slug": "Lianhong-Cai",
                        "structuredName": {
                            "firstName": "Lianhong",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianhong Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18093536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f09e8b31a6e9e815237b89867c3f758d19977158",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Images can not only display contents themselves, but also convey emotions, e.g., excitement, sadness. Affective image classification is useful and hot in many fields such as computer vision and multimedia. Current researches usually consider the relationship model between images and emotions as a black box. They extract the traditional discursive visual features such as SIFT and wavelet textures, and use them directly upon various classification algorithms. However, these visual features are not interpretable, and people cannot know why such a set of features induce a particular emotion. And due to the highly subjective nature of images, the classification accuracies on these visual features are not satisfactory for a long time. We propose the interpretable aesthetic features to describe images inspired by art theories, which are intuitive, discriminative and easily understandable. Affective image classification based on these features can achieve higher accuracy, compared with the state-of-the-art. Specifically, the features can also intuitively explain why an image tends to convey a certain emotion. We also develop an emotion guided image gallery to demonstrate the proposed feature collection."
            },
            "slug": "Interpretable-aesthetic-features-for-affective-Wang-Jia",
            "title": {
                "fragments": [],
                "text": "Interpretable aesthetic features for affective image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes the interpretable aesthetic features to describe images inspired by art theories, which are intuitive, discriminative and easily understandable, and can achieve higher accuracy, compared with the state-of-the-art."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Image Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144491954"
                        ],
                        "name": "M. \u00c1lvarez",
                        "slug": "M.-\u00c1lvarez",
                        "structuredName": {
                            "firstName": "Mauricio",
                            "lastName": "\u00c1lvarez",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. \u00c1lvarez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690976"
                        ],
                        "name": "L. Rosasco",
                        "slug": "L.-Rosasco",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Rosasco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rosasco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 456491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93aa063d330ed7509f55b1297138cf20300fb969",
            "isKey": false,
            "numCitedBy": 567,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods are among the most popular techniques in machine learning. From a regularization perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a probabilistic perspective they are the key in the context of Gaussian processes, where the kernel function is known as the covariance function. Traditionally, kernel methods have been used in supervised learning problems with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partially by frameworks like multitask learning. In this monograph, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods."
            },
            "slug": "Kernels-for-Vector-Valued-Functions:-a-Review-\u00c1lvarez-Rosasco",
            "title": {
                "fragments": [],
                "text": "Kernels for Vector-Valued Functions: a Review"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This monograph reviews different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7051002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1abfc1c96fe3f2dd2b9282835dea1fd6906fedb0",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides a foundation for multi-task learning using reproducing kernel Hilbert spaces of vector-valued functions. In this setting, the kernel is a matrix-valued function. Some explicit examples will be described which go beyond our earlier results in [7]. In particular, we characterize classes of matrix- valued kernels which are linear and are of the dot product or the translation invariant type. We discuss how these kernels can be used to model relations between the tasks and present linear multi-task learning algorithms. Finally, we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation."
            },
            "slug": "Kernels-for-Multi--task-Learning-Micchelli-Pontil",
            "title": {
                "fragments": [],
                "text": "Kernels for Multi--task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper provides a foundation for multi-task learning using reproducing kernel Hilbert spaces of vector-valued functions using classes of matrix- valued kernels which are linear and are of the dot product or the translation invariant type."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720010"
                        ],
                        "name": "Yanshan Xiao",
                        "slug": "Yanshan-Xiao",
                        "structuredName": {
                            "firstName": "Yanshan",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanshan Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330453"
                        ],
                        "name": "Bo Liu",
                        "slug": "Bo-Liu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148761004"
                        ],
                        "name": "Longbing Cao",
                        "slug": "Longbing-Cao",
                        "structuredName": {
                            "firstName": "Longbing",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longbing Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965064"
                        ],
                        "name": "Jie Yin",
                        "slug": "Jie-Yin",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748808"
                        ],
                        "name": "Xindong Wu",
                        "slug": "Xindong-Wu",
                        "structuredName": {
                            "firstName": "Xindong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xindong Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15290986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ab86f25753d4a9cf55d913c2955b539ee552cd5",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiple instance learning (MIL) is a generalization of supervised learning which attempts to learn useful information from bags of instances. In MIL, the true labels of the instances in positive bags are not always available for training. This leads to a critical challenge, namely, handling the ambiguity of instance labels in positive bags. To address this issue, this paper proposes a novel MIL method named SMILE (Similarity-based Multiple Instance LEarning). It introduces a similarity weight to each instance in positive bag, which represents the instance similarity towards the positive and negative classes. The instances in positive bags, together with their similarity weights, are thereafter incorporated into the learning phase to build an extended SVM-based predictive classifier. Experiments on three real-world datasets consisting of 12 subsets show that SMILE achieves markedly better classification accuracy than state-of-the-art MIL methods."
            },
            "slug": "SMILE:-A-Similarity-Based-Approach-for-Multiple-Xiao-Liu",
            "title": {
                "fragments": [],
                "text": "SMILE: A Similarity-Based Approach for Multiple Instance Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A novel MIL method named SMILE (Similarity-based Multiple Instance LEarning), which introduces a similarity weight to each instance in positive bag, which represents the instance similarity towards the positive and negative classes."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Data Mining"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38700588"
                        ],
                        "name": "Wei Wang",
                        "slug": "Wei-Wang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17037938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32a4ce65df5321ef2e2ebad6dda990f14fbf0a23",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Co-training is a semi-supervised learning paradigm which trains two learners respectively from two different views and lets the learners label some unlabeled examples for each other. In this paper, we present a new PAC analysis on co-training style algorithms. We show that the co-training process can succeed even without two views, given that the two learners have large difference, which explains the success of some co-training style algorithms that do not require two views. Moreover, we theoretically explain that why the co-training process could not improve the performance further after a number of rounds, and present a rough estimation on the appropriate round to terminate co-training to avoid some wasteful learning rounds."
            },
            "slug": "Analyzing-Co-training-Style-Algorithms-Wang-Zhou",
            "title": {
                "fragments": [],
                "text": "Analyzing Co-training Style Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the co- training process can succeed even without two views, given that the two learners have large difference, which explains the success of some co-training style algorithms that do not require two views."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145462236"
                        ],
                        "name": "Laurent Jacob",
                        "slug": "Laurent-Jacob",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Jacob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurent Jacob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 488434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9491bcc1e54b52bea617283f7f716cf009068bce",
            "isKey": false,
            "numCitedBy": 450,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non-convex methods dedicated to the same problem."
            },
            "slug": "Clustered-Multi-Task-Learning:-A-Convex-Formulation-Jacob-Bach",
            "title": {
                "fragments": [],
                "text": "Clustered Multi-Task Learning: A Convex Formulation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new spectral norm is designed that encodes this a priori assumption that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors, resulting in a new convex optimization formulation for multi-task learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2731900"
                        ],
                        "name": "Oliver J. Woodford",
                        "slug": "Oliver-J.-Woodford",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Woodford",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oliver J. Woodford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34257283"
                        ],
                        "name": "Minh-Tri Pham",
                        "slug": "Minh-Tri-Pham",
                        "structuredName": {
                            "firstName": "Minh-Tri",
                            "lastName": "Pham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minh-Tri Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801052"
                        ],
                        "name": "A. Maki",
                        "slug": "A.-Maki",
                        "structuredName": {
                            "firstName": "Atsuto",
                            "lastName": "Maki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Maki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3054009"
                        ],
                        "name": "Frank Perbet",
                        "slug": "Frank-Perbet",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Perbet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Perbet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746940"
                        ],
                        "name": "B. Stenger",
                        "slug": "B.-Stenger",
                        "structuredName": {
                            "firstName": "Bj\u00f6rn",
                            "lastName": "Stenger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Stenger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2575522,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b43ac3705c7cabdb21b612f54dd0b69f2ff938df",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In applying the Hough transform to the problem of 3D shape recognition and registration, we develop two new and powerful improvements to this popular inference method. The first, intrinsic Hough, solves the problem of exponential memory requirements of the standard Hough transform by exploiting the sparsity of the Hough space. The second, minimum-entropy Hough, explains away incorrect votes, substantially reducing the number of modes in the posterior distribution of class and pose, and improving precision. Our experiments demonstrate that these contributions make the Hough transform not only tractable but also highly accurate for our example application. Both contributions can be applied to other tasks that already use the standard Hough transform."
            },
            "slug": "Demisting-the-Hough-Transform-for-3D-Shape-and-Woodford-Pham",
            "title": {
                "fragments": [],
                "text": "Demisting the Hough Transform for 3D Shape Recognition and Registration"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Two new and powerful improvements to this popular inference method, intrinsic and minimum-entropy Hough, are developed, which solve the problem of exponential memory requirements of the standard Hough transform by exploiting the sparsity of the Hough space."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679223"
                        ],
                        "name": "S. Seitz",
                        "slug": "S.-Seitz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Seitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5891646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e02368354bd335003ed00f2bc8670deac1d70d3f",
            "isKey": false,
            "numCitedBy": 302,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of efficient structure from motion for large, unordered, highly redundant, and irregularly sampled photo collections, such as those found on Internet photo-sharing sites. Our approach computes a small skeletal subset of images, reconstructs the skeletal set, and adds the remaining images using pose estimation. Our technique drastically reduces the number of parameters that are considered, resulting in dramatic speedups, while provably approximating the covariance of the full set of parameters. To compute a skeletal image set, we first estimate the accuracy of two-frame reconstructions between pairs of overlapping images, then use a graph algorithm to select a subset of images that, when reconstructed, approximates the accuracy of the full set. A final bundle adjustment can then optionally be used to restore any loss of accuracy."
            },
            "slug": "Skeletal-graphs-for-efficient-structure-from-motion-Snavely-Seitz",
            "title": {
                "fragments": [],
                "text": "Skeletal graphs for efficient structure from motion"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work computes a small skeletal subset of images, reconstructs the skeletal set, and adds the remaining images using pose estimation, resulting in dramatic speedups, while provably approximating the covariance of the full set of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17055992,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7",
            "isKey": false,
            "numCitedBy": 12429,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and interconnexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours. In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Daniel & Whitteiidge (1959) have recently extended this work in the primate. Most of our present knowledge of retinotopic projections, binocular overlap, and the second visual area is based on these investigations. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to"
            },
            "slug": "Receptive-fields,-binocular-interaction-and-in-the-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This method is used to examine receptive fields of a more complex type and to make additional observations on binocular interaction and this approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145327993"
                        ],
                        "name": "A. Khoreva",
                        "slug": "A.-Khoreva",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Khoreva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khoreva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787725"
                        ],
                        "name": "Fabio Galasso",
                        "slug": "Fabio-Galasso",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Galasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Galasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7619088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a569c1d6e0d9efda1bd838f239da71fa3e042884",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years it has been shown that clustering and segmentation methods can greatly benefit from the integration of prior information in terms of must-link constraints. Very recently the use of such constraints has been integrated in a rigorous manner also in graph-based methods such as normalized cut. On the other hand spectral clustering as relaxation of the normalized cut has been shown to be among the best methods for video segmentation. In this paper we merge these two developments and propose to learn must-link constraints for video segmentation with spectral clustering. We show that the integration of learned must-link constraints not only improves the segmentation result but also significantly reduces the required runtime, making the use of costly spectral methods possible for today\u2019s high quality video."
            },
            "slug": "Learning-Must-Link-Constraints-for-Video-Based-on-Khoreva-Galasso",
            "title": {
                "fragments": [],
                "text": "Learning Must-Link Constraints for Video Segmentation Based on Spectral Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the integration of learned must-link constraints not only improves the segmentation result but also significantly reduces the required runtime, making the use of costly spectral methods possible for today\u2019s high quality video."
            },
            "venue": {
                "fragments": [],
                "text": "GCPR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] Antanas Verikas, Adas Gelzinis, and Marija Bacauskiene."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 82
                            }
                        ],
                        "text": "Recent work has focused on motion descriptors that are invariant to camera motion [5, 6, 8, 10, 11, 12, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] Jonathan Tompson, Murphy Stein, Yann LeCun, and Ken Perlin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Regression based approaches directly estimate the hand pose from the depth image, using latent regression forest [11] or deep convolutional neutral networks [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Recently, it has proven extremely successful on important applications in data mining [12] and computer vision [6, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14457153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642e328cae81c5adb30069b680cf60ba6b475153",
            "isKey": true,
            "numCitedBy": 6760,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films."
            },
            "slug": "Video-Google:-a-text-retrieval-approach-to-object-Sivic-Zisserman",
            "title": {
                "fragments": [],
                "text": "Video Google: a text retrieval approach to object matching in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video, represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146267437"
                        ],
                        "name": "Zheng Wu",
                        "slug": "Zheng-Wu",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726218"
                        ],
                        "name": "Nathan W. Fuller",
                        "slug": "Nathan-W.-Fuller",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Fuller",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan W. Fuller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4731761"
                        ],
                        "name": "Diane H. Theriault",
                        "slug": "Diane-H.-Theriault",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Theriault",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diane H. Theriault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723703"
                        ],
                        "name": "Margrit Betke",
                        "slug": "Margrit-Betke",
                        "structuredName": {
                            "firstName": "Margrit",
                            "lastName": "Betke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margrit Betke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11518157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c76de57b8b1c9e63b1883cbdea9ec8e68ddf493",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We hereby publish a new thermal infrared video benchmark, called TIV, for various visual analysis tasks, which include single object tracking in clutter, multi-object tracking in single or multiple views, analyzing motion patterns of large groups, and censusing wild animals in flight. Our data describe real world scenarios, such as bats emerging from their caves in large numbers, a crowded street view during a marathon competition, and students walking through an atrium during class break. We also introduce baseline methods and evaluation protocols for these tasks. Our TIV benchmark enriches and diversifies video data sets available to the research community with thermal infrared footage, which poses new and challenging video analysis problems. We hope the TIV benchmark will help the community to better understand these interesting problems, generate new ideas, and value it as a testbed to compare solutions."
            },
            "slug": "A-Thermal-Infrared-Video-Benchmark-for-Visual-Wu-Fuller",
            "title": {
                "fragments": [],
                "text": "A Thermal Infrared Video Benchmark for Visual Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This work publishes a new thermal infrared video benchmark, called TIV, for various visual analysis tasks, which include single object tracking in clutter, multi-object tracking in single or multiple views, analyzing motion patterns of large groups, and censusing wild animals in flight."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2930640"
                        ],
                        "name": "P. Welinder",
                        "slug": "P.-Welinder",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Welinder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Welinder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2165997,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e654ba9b84d4bc2d723335548c7a49c71b0b06f8",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a fast and accurate algorithm for computing the 2D pose of objects in images called cascaded pose regression (CPR). CPR progressively refines a loosely specified initial guess, where each refinement is carried out by a different regressor. Each regressor performs simple image measurements that are dependent on the output of the previous regressors; the entire system is automatically learned from human annotated training examples. CPR is not restricted to rigid transformations: \u2018pose\u2019 is any parameterized variation of the object's appearance such as the degrees of freedom of deformable and articulated objects. We compare CPR against both standard regression techniques and human performance (computed from redundant human annotations). Experiments on three diverse datasets (mice, faces, fish) suggest CPR is fast (2\u20133ms per pose estimate), accurate (approaching human performance), and easy to train from small amounts of labeled data."
            },
            "slug": "Cascaded-pose-regression-Doll\u00e1r-Welinder",
            "title": {
                "fragments": [],
                "text": "Cascaded pose regression"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments suggest CPR is fast (2\u20133ms per pose estimate), accurate (approaching human performance), and easy to train from small amounts of labeled data."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8616813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0bcc580a1e9e32b3329363bab43331ed9c5a7d4",
            "isKey": false,
            "numCitedBy": 1302,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for discrete energy minimization are of fundamental importance in computer vision. In this paper, we focus on the recent technique proposed by Wainwright et al. (Nov. 2005)- tree-reweighted max-product message passing (TRW). It was inspired by the problem of maximizing a lower bound on the energy. However, the algorithm is not guaranteed to increase this bound - it may actually go down. In addition, TRW does not always converge. We develop a modification of this algorithm which we call sequential tree-reweighted message passing. Its main property is that the bound is guaranteed not to decrease. We also give a weak tree agreement condition which characterizes local maxima of the bound with respect to TRW algorithms. We prove that our algorithm has a limit point that achieves weak tree agreement. Finally, we show that, our algorithm requires half as much memory as traditional message passing approaches. Experimental results demonstrate that on certain synthetic and real problems, our algorithm outperforms both the ordinary belief propagation and tree-reweighted algorithm in (M. J. Wainwright, et al., Nov. 2005). In addition, on stereo problems with Potts interactions, we obtain a lower energy than graph cuts"
            },
            "slug": "Convergent-Tree-Reweighted-Message-Passing-for-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Convergent Tree-Reweighted Message Passing for Energy Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper develops a modification of the recent technique proposed by Wainwright et al. (Nov. 2005), called sequential tree-reweighted message passing, which outperforms both the ordinary belief propagation and tree- reweighted algorithm in both synthetic and real problems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": false,
            "numCitedBy": 5468,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 47325215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78947497cbbffc691aac3f590d972130259af9ce",
            "isKey": false,
            "numCitedBy": 5018,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner."
            },
            "slug": "Distance-Metric-Learning-for-Large-Margin-Nearest-Weinberger-Saul",
            "title": {
                "fragments": [],
                "text": "Distance Metric Learning for Large Margin Nearest Neighbor Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper shows how to learn a Mahalanobis distance metric for kNN classification from labeled examples in a globally integrated manner and finds that metrics trained in this way lead to significant improvements in kNN Classification."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4129821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffd97ed7408fa75b86d4c8e4280f8d730ffa2cf0",
            "isKey": false,
            "numCitedBy": 1304,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied."
            },
            "slug": "Large-Displacement-Optical-Flow:-Descriptor-in-Brox-Malik",
            "title": {
                "fragments": [],
                "text": "Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A way to approach the problem of dense optical flow estimation by integrating rich descriptors into the variational optical flow setting, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139731"
                        ],
                        "name": "J. Machajdik",
                        "slug": "J.-Machajdik",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Machajdik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Machajdik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699657"
                        ],
                        "name": "A. Hanbury",
                        "slug": "A.-Hanbury",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Hanbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3354049,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "2c411a12f33f15451e1659a3435391962c0cc144",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Images can affect people on an emotional level. Since the emotions that arise in the viewer of an image are highly subjective, they are rarely indexed. However there are situations when it would be helpful if images could be retrieved based on their emotional content. We investigate and develop methods to extract and combine low-level features that represent the emotional content of an image, and use these for image emotion classification. Specifically, we exploit theoretical and empirical concepts from psychology and art theory to extract image features that are specific to the domain of artworks with emotional expression. For testing and training, we use three data sets: the International Affective Picture System (IAPS); a set of artistic photography from a photo sharing site (to investigate whether the conscious use of colors and textures displayed by the artists improves the classification); and a set of peer rated abstract paintings to investigate the influence of the features and ratings on pictures without contextual content. Improved classification results are obtained on the International Affective Picture System (IAPS), compared to state of the art work."
            },
            "slug": "Affective-image-classification-using-features-by-Machajdik-Hanbury",
            "title": {
                "fragments": [],
                "text": "Affective image classification using features inspired by psychology and art theory"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work investigates and develops methods to extract and combine low-level features that represent the emotional content of an image, and uses these for image emotion classification."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2731900"
                        ],
                        "name": "Oliver J. Woodford",
                        "slug": "Oliver-J.-Woodford",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Woodford",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oliver J. Woodford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2818045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e7f51853229843c0056681f97754869032348ee",
            "isKey": false,
            "numCitedBy": 353,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Second-order priors on the smoothness of 3D surfaces are a better model of typical scenes than first-order priors. However, stereo reconstruction using global inference algorithms, such as graph-cuts, has not been able to incorporate second-order priors because the triple cliques needed to express them yield intractable (non-submodular) optimization problems. This paper shows that inference with triple cliques can be effectively optimized. Our optimization strategy is a development of recent extensions to a-expansion, based on the \"QPBO\" algorithm [5, 14, 26]. The strategy is to repeatedly merge proposal depth maps using a novel extension of QPBO. Proposal depth maps can come from any source, for example fronto-parallel planes as in a-expansion, or indeed any existing stereo algorithm, with arbitrary parameter settings. Experimental results demonstrate the usefulness of the second-order prior and the efficacy of our optimization framework. An implementation of our stereo framework is available online [34]."
            },
            "slug": "Global-stereo-reconstruction-under-second-order-Woodford-Torr",
            "title": {
                "fragments": [],
                "text": "Global stereo reconstruction under second order smoothness priors"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper shows that inference with triple cliques can be effectively optimized, and demonstrates the usefulness of the second-order prior and the efficacy of the optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747500"
                        ],
                        "name": "A. Mian",
                        "slug": "A.-Mian",
                        "structuredName": {
                            "firstName": "Ajmal",
                            "lastName": "Mian",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698675"
                        ],
                        "name": "Bennamoun",
                        "slug": "Bennamoun",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Bennamoun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bennamoun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689848"
                        ],
                        "name": "R. Owens",
                        "slug": "R.-Owens",
                        "structuredName": {
                            "firstName": "Robyn",
                            "lastName": "Owens",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Owens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10978994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac4ca15a40799b2159c918fcf3b6d1794efd6ca2",
            "isKey": false,
            "numCitedBy": 473,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Viewpoint independent recognition of free-form objects and their segmentation in the presence of clutter and occlusions is a challenging task. We present a novel 3D model-based algorithm which performs this task automatically and efficiently. A 3D model of an object is automatically constructed offline from its multiple unordered range images (views). These views are converted into multidimensional table representations (which we refer to as tensors). Correspondences are automatically established between these views by simultaneously matching the tensors of a view with those of the remaining views using a hash table-based voting scheme. This results in a graph of relative transformations used to register the views before they are integrated into a seamless 3D model. These models and their tensor representations constitute the model library. During online recognition, a tensor from the scene is simultaneously matched with those in the library by casting votes. Similarity measures are calculated for the model tensors which receive the most votes. The model with the highest similarity is transformed to the scene and, if it aligns accurately with an object in the scene, that object is declared as recognized and is segmented. This process is repeated until the scene is completely segmented. Experiments were performed on real and synthetic data comprised of 55 models and 610 scenes and an overall recognition rate of 95 percent was achieved. Comparison with the spin images revealed that our algorithm is superior in terms of recognition rate and efficiency"
            },
            "slug": "Three-Dimensional-Model-Based-Object-Recognition-in-Mian-Bennamoun",
            "title": {
                "fragments": [],
                "text": "Three-Dimensional Model-Based Object Recognition and Segmentation in Cluttered Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel 3D model-based algorithm is presented which performs viewpoint independent recognition of free-form objects and their segmentation in the presence of clutter and occlusions automatically and efficiently and is superior in terms of recognition rate and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037259"
                        ],
                        "name": "Yinqiang Zheng",
                        "slug": "Yinqiang-Zheng",
                        "structuredName": {
                            "firstName": "Yinqiang",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinqiang Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990768"
                        ],
                        "name": "Guangcan Liu",
                        "slug": "Guangcan-Liu",
                        "structuredName": {
                            "firstName": "Guangcan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangcan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021175"
                        ],
                        "name": "S. Sugimoto",
                        "slug": "S.-Sugimoto",
                        "structuredName": {
                            "firstName": "Shigeki",
                            "lastName": "Sugimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sugimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742442"
                        ],
                        "name": "M. Okutomi",
                        "slug": "M.-Okutomi",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Okutomi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Okutomi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206591718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4006448d5b8ca4d9ff9ce138e9d756517e14ec35",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A great variety of computer vision tasks, such as rigid/nonrigid structure from motion and photometric stereo, can be unified into the problem of approximating a low-rank data matrix in the presence of missing data and outliers. To improve robustness, the L1-norm measurement has long been recommended. Unfortunately, existing methods usually fail to minimize the L1-based nonconvex objective function sufficiently. In this work, we propose to add a convex trace-norm regularization term to improve convergence, without introducing too much heterogenous information. We also customize a scalable first-order optimization algorithm to solve the regularized formulation on the basis of the augmented Lagrange multiplier (ALM) method. Extensive experimental results verify that our regularized formulation is reasonable, and the solving algorithm is very efficient, insensitive to initialization and robust to high percentage of missing data and/or outliers1."
            },
            "slug": "Practical-low-rank-matrix-approximation-under-Zheng-Liu",
            "title": {
                "fragments": [],
                "text": "Practical low-rank matrix approximation under robust L1-norm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes to add a convex trace-norm regularization term to improve convergence, without introducing too much heterogenous information, and customize a scalable first-order optimization algorithm to solve the regularized formulation on the basis of the augmented Lagrange multiplier (ALM) method."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46506697"
                        ],
                        "name": "Heng Wang",
                        "slug": "Heng-Wang",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909350"
                        ],
                        "name": "Alexander Kl\u00e4ser",
                        "slug": "Alexander-Kl\u00e4ser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kl\u00e4ser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Kl\u00e4ser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13537104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3afbb0e64fcb70496b44b30b76fac9456cc51e34",
            "isKey": false,
            "numCitedBy": 2229,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature trajectories have shown to be efficient for representing videos. Typically, they are extracted using the KLT tracker or matching SIFT descriptors between frames. However, the quality as well as quantity of these trajectories is often not sufficient. Inspired by the recent success of dense sampling in image classification, we propose an approach to describe videos by dense trajectories. We sample dense points from each frame and track them based on displacement information from a dense optical flow field. Given a state-of-the-art optical flow algorithm, our trajectories are robust to fast irregular motions as well as shot boundaries. Additionally, dense trajectories cover the motion information in videos well. We, also, investigate how to design descriptors to encode the trajectory information. We introduce a novel descriptor based on motion boundary histograms, which is robust to camera motion. This descriptor consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos. We evaluate our video description in the context of action classification with a bag-of-features approach. Experimental results show a significant improvement over the state of the art on four datasets of varying difficulty, i.e. KTH, YouTube, Hollywood2 and UCF sports."
            },
            "slug": "Action-recognition-by-dense-trajectories-Wang-Kl\u00e4ser",
            "title": {
                "fragments": [],
                "text": "Action recognition by dense trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces a novel descriptor based on motion boundary histograms, which is robust to camera motion and consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9200530"
                        ],
                        "name": "Juan Carlos Niebles",
                        "slug": "Juan-Carlos-Niebles",
                        "structuredName": {
                            "firstName": "Juan Carlos",
                            "lastName": "Niebles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Carlos Niebles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109675919"
                        ],
                        "name": "Chih-Wei Chen",
                        "slug": "Chih-Wei-Chen",
                        "structuredName": {
                            "firstName": "Chih-Wei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Wei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14779543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "994a7b903b937f8b177c035db86852091fd26aa7",
            "isKey": false,
            "numCitedBy": 745,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Much recent research in human activity recognition has focused on the problem of recognizing simple repetitive (walking, running, waving) and punctual actions (sitting up, opening a door, hugging). However, many interesting human activities are characterized by a complex temporal composition of simple actions. Automatic recognition of such complex actions can benefit from a good understanding of the temporal structures. We present in this paper a framework for modeling motion by exploiting the temporal structure of the human activities. In our framework, we represent activities as temporal compositions of motion segments. We train a discriminative model that encodes a temporal decomposition of video sequences, and appearance models for each motion segment. In recognition, a query video is matched to the model according to the learned appearances and motion segment decomposition. Classification is made based on the quality of matching between the motion segment classifiers and the temporal segments in the query sequence. To validate our approach, we introduce a new dataset of complex Olympic Sports activities. We show that our algorithm performs better than other state of the art methods."
            },
            "slug": "Modeling-Temporal-Structure-of-Decomposable-Motion-Niebles-Chen",
            "title": {
                "fragments": [],
                "text": "Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A framework for modeling motion by exploiting the temporal structure of the human activities, which represents activities as temporal compositions of motion segments, and shows that the algorithm performs better than other state of the art methods."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16242306,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "03ce1779bff26648490516eb4c9a13feaaf61f06",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In the general case, a trilinear relationship between three perspective views is shown to exist. The trilinearity result is shown to be of much practical use in visual recognition by alignment-yielding a direct reprojection method that cuts through the computations of camera transformation, scene structure and epipolar geometry. Moreover, the direct method is linear and sets a new lower theoretical bound on the minimal number of points that are required for a linear solution for the task of reprojection. The proof of the central result may be of further interest as it demonstrates certain regularities across homographics of the plane and introduces new view invariants. Experiments on simulated and real image data were conducted, including a comparative analysis with epipolar intersection and the linear combination methods, with results indicating a greater degree of robustness in practice and a higher level of performance in reprojection tasks. >"
            },
            "slug": "Algebraic-Functions-For-Recognition-Shashua",
            "title": {
                "fragments": [],
                "text": "Algebraic Functions For Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The trilinearity result is shown to be of much practical use in visual recognition by alignment-yielding a direct reprojection method that cuts through the computations of camera transformation, scene structure and epipolar geometry."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 195859047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2f78c2b2b325d72f359d4c797c9aab6a8e60942",
            "isKey": false,
            "numCitedBy": 2998,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms."
            },
            "slug": "A-Taxonomy-and-Evaluation-of-Dense-Two-Frame-Stereo-Scharstein-Szeliski",
            "title": {
                "fragments": [],
                "text": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper has designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3177797"
                        ],
                        "name": "A. Borji",
                        "slug": "A.-Borji",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Borji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Borji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1975008"
                        ],
                        "name": "Andreas Lennartz",
                        "slug": "Andreas-Lennartz",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lennartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Lennartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739588"
                        ],
                        "name": "M. Pomplun",
                        "slug": "M.-Pomplun",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Pomplun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pomplun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] Know your meme: We need to go deeper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 199
                            }
                        ],
                        "text": "While most domain adaptation techniques focus on applications where both training and test instances are images, a few address the problem in the context of image-to-video object detector adaptation [1, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 71
                            }
                        ],
                        "text": "We first represent the photos using similarity preserving binary codes [1, 3, 5], enabling us to store large number of photos in memory."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Our study is inspired by the example set by Deng et al.[1] for image categorization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] Adrien Gaidon, Gloria Zen, and Jose A."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "Many edge-preserving filters, like bilateral filters [3], guided filters [2], and geodesic filters [1] have been designed to alleviate cross-region mixing problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 238
                            }
                        ],
                        "text": "We propose an efficient deep neural network architecture for computer vision, codenamed \u201cInception\u201d, which derives its name from the \u201cNetwork in network\u201d paper by Lin et al [5] in conjunction with the \u201cwe need to go deeper\u201d internet meme [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] Jacopo Grazzini and Pierre Soille."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Iterative Quantization (ITQ) [1] is an effective approach to decrease the quantization distortion by applying an orthogonal rotation on the transformed data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16073973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d25974eb72fe9ba020a40064cd7b68df137c0358",
            "isKey": true,
            "numCitedBy": 41,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-do-eyes-reveal-about-the-mind:-Algorithmic-of-Borji-Lennartz",
            "title": {
                "fragments": [],
                "text": "What do eyes reveal about the mind?: Algorithmic inference of search targets from fixations"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2537499"
                        ],
                        "name": "Ido Omer",
                        "slug": "Ido-Omer",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Omer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Omer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27379268"
                        ],
                        "name": "M. Werman",
                        "slug": "M.-Werman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Werman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Werman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15322258,
            "fieldsOfStudy": [
                "Art",
                "Computer Science"
            ],
            "id": "6c9f7ddc68ae6d29e6973b2b0b9ca93eef423190",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of deciding whether two pixels in an image have the same real world color is a fundamental problem in computer vision. Many color spaces are used in different applications for discriminating color from intensity to create an informative representation of color. The major drawback of all of these representations is that they assume no color distortion. In practice the colors of real world images are distorted both in the scene itself and in the image capturing process. In this work we introduce color lines, an image specific color representation that is robust to color distortion and provides a compact and useful representation of the colors in a scene."
            },
            "slug": "Color-lines:-image-specific-color-representation-Omer-Werman",
            "title": {
                "fragments": [],
                "text": "Color lines: image specific color representation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work introduces color lines, an image specific color representation that is robust to color distortion and provides a compact and useful representation of the colors in a scene."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2479485"
                        ],
                        "name": "Kostadin Dabov",
                        "slug": "Kostadin-Dabov",
                        "structuredName": {
                            "firstName": "Kostadin",
                            "lastName": "Dabov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kostadin Dabov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49390793"
                        ],
                        "name": "A. Foi",
                        "slug": "A.-Foi",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Foi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Foi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718100"
                        ],
                        "name": "V. Katkovnik",
                        "slug": "V.-Katkovnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Katkovnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Katkovnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683084"
                        ],
                        "name": "K. Egiazarian",
                        "slug": "K.-Egiazarian",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Egiazarian",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Egiazarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8680502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9828f6a4ca3c8e3142108f1ef1b0f376c1150b3",
            "isKey": false,
            "numCitedBy": 400,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an image restoration technique exploiting regularized inversion and the recent block-matching and 3D filtering (BM3D) denoising filter. The BM3D employs a non-local modeling of images by collecting similar image patches in 3D arrays. The so-called collaborative filtering applied on such a 3D array is realized by transformdomain shrinkage. In this work, we propose an extension of the BM3D filter for colored noise, which we use in a two-step deblurring algorithm to improve the regularization after inversion in discrete Fourier domain. The first step of the algorithm is a regularized inversion using BM3D with collaborative hard-thresholding and the seconds step is a regularized Wiener inversion using BM3D with collaborative Wiener filtering. The experimental results show that the proposed technique is competitive with and in most cases outperforms the current best image restoration methods in terms of improvement in signal-to-noise ratio."
            },
            "slug": "Image-restoration-by-sparse-3D-transform-domain-Dabov-Foi",
            "title": {
                "fragments": [],
                "text": "Image restoration by sparse 3D transform-domain collaborative filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An extension of the BM3D filter for colored noise is proposed, which is used in a two-step deblurring algorithm to improve the regularization after inversion in discrete Fourier domain."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66193516"
                        ],
                        "name": "H. Ishikawa",
                        "slug": "H.-Ishikawa",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Ishikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ishikawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2695207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95a85842a726c614206774892e55d1dd1893b6ec",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new technique that can reduce any higher-order Markov random field with binary labels into a first-order one that has the same minima as the original. Moreover, we combine the reduction with the fusion-move and QPBO algorithms to optimize higher-order multi-label problems. While many vision problems today are formulated as energy minimization problems, they have mostly been limited to using first-order energies, which consist of unary and pairwise clique potentials, with a few exceptions that consider triples. This is because of the lack of efficient algorithms to optimize energies with higher-order interactions. Our algorithm challenges this restriction that limits the representational power of the models, so that higher-order energies can be used to capture the rich statistics of natural scenes. To demonstrate the algorithm, we minimize a third-order energy, which allows clique potentials with up to four pixels, in an image restoration problem. The problem uses the fields of experts model, a learned spatial prior of natural images that has been used to test two belief propagation algorithms capable of optimizing higher-order energies. The results show that the algorithm exceeds the BP algorithms in both optimization performance and speed."
            },
            "slug": "Higher-order-clique-reduction-in-binary-graph-cut-Ishikawa",
            "title": {
                "fragments": [],
                "text": "Higher-order clique reduction in binary graph cut"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work introduces a new technique that can reduce any higher-order Markov random field with binary labels into a first-order one that has the same minima as the original, and combines the reduction with the fusion-move and QPBO algorithms to optimize higher- order multi-label problems."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143685864"
                        ],
                        "name": "Yi Yang",
                        "slug": "Yi-Yang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3532973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f04d907ba87261d4e46ed03dd7de0ec7a1a125f",
            "isKey": false,
            "numCitedBy": 710,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for articulated human detection and human pose estimation in static images based on a new representation of deformable part models. Rather than modeling articulation using a family of warped (rotated and foreshortened) templates, we use a mixture of small, nonoriented parts. We describe a general, flexible mixture model that jointly captures spatial relations between part locations and co-occurrence relations between part mixtures, augmenting standard pictorial structure models that encode just spatial relations. Our models have several notable properties: 1) They efficiently model articulation by sharing computation across similar warps, 2) they efficiently model an exponentially large set of global mixtures through composition of local mixtures, and 3) they capture the dependency of global geometry on local appearance (parts look different at different locations). When relations are tree structured, our models can be efficiently optimized with dynamic programming. We learn all parameters, including local appearances, spatial relations, and co-occurrence relations (which encode local rigidity) with a structured SVM solver. Because our model is efficient enough to be used as a detector that searches over scales and image locations, we introduce novel criteria for evaluating pose estimation and human detection, both separately and jointly. We show that currently used evaluation criteria may conflate these two issues. Most previous approaches model limbs with rigid and articulated templates that are trained independently of each other, while we present an extensive diagnostic evaluation that suggests that flexible structure and joint training are crucial for strong performance. We present experimental results on standard benchmarks that suggest our approach is the state-of-the-art system for pose estimation, improving past work on the challenging Parse and Buffy datasets while being orders of magnitude faster."
            },
            "slug": "Articulated-Human-Detection-with-Flexible-Mixtures-Yang-Ramanan",
            "title": {
                "fragments": [],
                "text": "Articulated Human Detection with Flexible Mixtures of Parts"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A general, flexible mixture model that jointly captures spatial relations between part locations and co-occurrence Relations between part mixtures, augmenting standard pictorial structure models that encode just spatial relations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2237194"
                        ],
                        "name": "B. Drost",
                        "slug": "B.-Drost",
                        "structuredName": {
                            "firstName": "Bertram",
                            "lastName": "Drost",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Drost"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983310"
                        ],
                        "name": "M. Ulrich",
                        "slug": "M.-Ulrich",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Ulrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ulrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145587210"
                        ],
                        "name": "Nassir Navab",
                        "slug": "Nassir-Navab",
                        "structuredName": {
                            "firstName": "Nassir",
                            "lastName": "Navab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nassir Navab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46505857"
                        ],
                        "name": "Slobodan Ilic",
                        "slug": "Slobodan-Ilic",
                        "structuredName": {
                            "firstName": "Slobodan",
                            "lastName": "Ilic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slobodan Ilic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12971705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e1f786e79ad9f03436563b4fac53bfde789c697",
            "isKey": false,
            "numCitedBy": 723,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of recognizing free-form 3D objects in point clouds. Compared to traditional approaches based on point descriptors, which depend on local information around points, we propose a novel method that creates a global model description based on oriented point pair features and matches that model locally using a fast voting scheme. The global model description consists of all model point pair features and represents a mapping from the point pair feature space to the model, where similar features on the model are grouped together. Such representation allows using much sparser object and scene point clouds, resulting in very fast performance. Recognition is done locally using an efficient voting scheme on a reduced two-dimensional search space. We demonstrate the efficiency of our approach and show its high recognition performance in the case of noise, clutter and partial occlusions. Compared to state of the art approaches we achieve better recognition rates, and demonstrate that with a slight or even no sacrifice of the recognition performance our method is much faster then the current state of the art approaches."
            },
            "slug": "Model-globally,-match-locally:-Efficient-and-robust-Drost-Ulrich",
            "title": {
                "fragments": [],
                "text": "Model globally, match locally: Efficient and robust 3D object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel method is proposed that creates a global model description based on oriented point pair features and matches that model locally using a fast voting scheme, which allows using much sparser object and scene point clouds, resulting in very fast performance."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7942592"
                        ],
                        "name": "Sunghyun Cho",
                        "slug": "Sunghyun-Cho",
                        "structuredName": {
                            "firstName": "Sunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66593660"
                        ],
                        "name": "Seungyong Lee",
                        "slug": "Seungyong-Lee",
                        "structuredName": {
                            "firstName": "Seungyong",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seungyong Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14206347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0781ce041ee57da83dc400fbabb1209f24e65a8",
            "isKey": false,
            "numCitedBy": 466,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a fast deblurring method that produces a deblurring result from a single image of moderate size in a few seconds. We accelerate both latent image estimation and kernel estimation in an iterative deblurring process by introducing a novel prediction step and working with image derivatives rather than pixel values. In the prediction step, we use simple image processing techniques to predict strong edges from an estimated latent image, which will be solely used for kernel estimation. With this approach, a computationally efficient Gaussian prior becomes sufficient for deconvolution to estimate the latent image, as small deconvolution artifacts can be suppressed in the prediction. For kernel estimation, we formulate the optimization function using image derivatives, and accelerate the numerical process by reducing the number of Fourier transforms needed for a conjugate gradient method. We also show that the formulation results in a smaller condition number of the numerical system than the use of pixel values, which gives faster convergence. Experimental results demonstrate that our method runs an order of magnitude faster than previous work, while the deblurring quality is comparable. GPU implementation facilitates further speed-up, making our method fast enough for practical use."
            },
            "slug": "Fast-motion-deblurring-Cho-Lee",
            "title": {
                "fragments": [],
                "text": "Fast motion deblurring"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A fastdeblurring method that produces a deblurring result from a single image of moderate size in a few seconds by introducing a novel prediction step and working with image derivatives rather than pixel values, which gives faster convergence."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838640"
                        ],
                        "name": "R. Cabral",
                        "slug": "R.-Cabral",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Cabral",
                            "middleNames": [
                                "Silveira"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cabral"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867160"
                        ],
                        "name": "F. D. L. Torre",
                        "slug": "F.-D.-L.-Torre",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Torre",
                            "middleNames": [
                                "De",
                                "la"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. D. L. Torre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848221"
                        ],
                        "name": "J. Costeira",
                        "slug": "J.-Costeira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Costeira",
                            "middleNames": [
                                "Paulo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Costeira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036494"
                        ],
                        "name": "A. Bernardino",
                        "slug": "A.-Bernardino",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Bernardino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bernardino"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 787710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "124ff7a0a83647e15c782199cbb8ecd6989ab90f",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Low rank models have been widely used for the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these low-rank models have alternatively been formulated as convex problems using the nuclear norm regularizer, unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a \"rank continuation'' strategy that outperform state-of-the-art approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data."
            },
            "slug": "Unifying-Nuclear-Norm-and-Bilinear-Factorization-Cabral-Torre",
            "title": {
                "fragments": [],
                "text": "Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A unified approach to bilinear factorization and nuclear norm regularization is proposed, that inherits the benefits of both and proposes a new optimization algorithm and a \"rank continuation'' strategy that outperform state-of-the-art approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443335"
                        ],
                        "name": "M. Unger",
                        "slug": "M.-Unger",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Unger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Unger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2886023"
                        ],
                        "name": "Manuel Werlberger",
                        "slug": "Manuel-Werlberger",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Werlberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manuel Werlberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730097"
                        ],
                        "name": "T. Pock",
                        "slug": "T.-Pock",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Pock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14951819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07fb4ab89d1c1a684853b3cf2258174de5bb78f2",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a unified variational formulation for joint motion estimation and segmentation with explicit occlusion handling. This is done by a multi-label representation of the flow field, where each label corresponds to a parametric representation of the motion. We use a convex formulation of the multi-label Potts model with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our formulation by means of convex constraints. Explicit occlusion handling eliminates errors otherwise created by the regularization. As occlusions can occur only at object boundaries, a large number of objects may be required. By using a fast primal-dual algorithm we are able to handle several hundred motion segments. Results are shown on several classical motion segmentation and optical flow examples."
            },
            "slug": "Joint-motion-estimation-and-segmentation-of-complex-Unger-Werlberger",
            "title": {
                "fragments": [],
                "text": "Joint motion estimation and segmentation of complex scenes with label costs and occlusion modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A unified variational formulation for joint motion estimation and segmentation with explicit occlusion handling using a convex formulation of the multi-label Potts model with label costs and showing that the asymmetric map-uniqueness criterion can be integrated into this formulation by means of convex constraints."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16447573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "isKey": false,
            "numCitedBy": 26038,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
            },
            "slug": "Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "Distributed Representations of Words and Phrases and their Compositionality"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109687910"
                        ],
                        "name": "Xiaoyan Hu",
                        "slug": "Xiaoyan-Hu",
                        "structuredName": {
                            "firstName": "Xiaoyan",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyan Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706145"
                        ],
                        "name": "Philippos Mordohai",
                        "slug": "Philippos-Mordohai",
                        "structuredName": {
                            "firstName": "Philippos",
                            "lastName": "Mordohai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippos Mordohai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5761531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5bd6326f43854c5c6454ddd484a2b9bfa733056",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an extensive evaluation of 17 confidence measures for stereo matching that compares the most widely used measures as well as several novel techniques proposed here. We begin by categorizing these methods according to which aspects of stereo cost estimation they take into account and then assess their strengths and weaknesses. The evaluation is conducted using a winner-take-all framework on binocular and multibaseline datasets with ground truth. It measures the capability of each confidence method to rank depth estimates according to their likelihood for being correct, to detect occluded pixels, and to generate low-error depth maps by selecting among multiple hypotheses for each pixel. Our work was motivated by the observation that such an evaluation is missing from the rapidly maturing stereo literature and that our findings would be helpful to researchers in binocular and multiview stereo."
            },
            "slug": "A-Quantitative-Evaluation-of-Confidence-Measures-Hu-Mordohai",
            "title": {
                "fragments": [],
                "text": "A Quantitative Evaluation of Confidence Measures for Stereo Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "An extensive evaluation of 17 confidence measures for stereo matching that compares the most widely used measures as well as several novel techniques proposed here, and finds that such an evaluation is missing from the rapidly maturing stereo literature."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46867608"
                        ],
                        "name": "Yu Zhang",
                        "slug": "Yu-Zhang",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739816"
                        ],
                        "name": "D. Yeung",
                        "slug": "D.-Yeung",
                        "structuredName": {
                            "firstName": "Dit-Yan",
                            "lastName": "Yeung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Yeung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18237764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c993431e61e524565cd2e86435978e1b47067949",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-task learning is a learning paradigm which seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this paper, we propose a regularization formulation for learning the relationships between tasks in multi-task learning. This formulation can be viewed as a novel generalization of the regularization framework for single-task learning. Besides modeling positive task correlation, our method, called multi-task relationship learning (MTRL), can also describe negative task correlation and identify outlier tasks based on the same underlying principle. Under this regularization framework, the objective function of MTRL is convex. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multi-task learning setting and then generalize it to the asymmetric setting as well. We also study the relationships between MTRL and some existing multi-task learning methods. Experiments conducted on a toy problem as well as several benchmark data sets demonstrate the effectiveness of MTRL."
            },
            "slug": "A-Convex-Formulation-for-Learning-Task-in-Learning-Zhang-Yeung",
            "title": {
                "fragments": [],
                "text": "A Convex Formulation for Learning Task Relationships in Multi-Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a regularization formulation for learning the relationships between tasks in multi-task learning, called MTRL, which can also describe negative task correlation and identify outlier tasks based on the same underlying principle."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2755797"
                        ],
                        "name": "Eunwoo Kim",
                        "slug": "Eunwoo-Kim",
                        "structuredName": {
                            "firstName": "Eunwoo",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eunwoo Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646766"
                        ],
                        "name": "Minsik Lee",
                        "slug": "Minsik-Lee",
                        "structuredName": {
                            "firstName": "Minsik",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minsik Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707151"
                        ],
                        "name": "Chong-Ho Choi",
                        "slug": "Chong-Ho-Choi",
                        "structuredName": {
                            "firstName": "Chong-Ho",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chong-Ho Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160425"
                        ],
                        "name": "Nojun Kwak",
                        "slug": "Nojun-Kwak",
                        "structuredName": {
                            "firstName": "Nojun",
                            "lastName": "Kwak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nojun Kwak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34184385"
                        ],
                        "name": "Songhwai Oh",
                        "slug": "Songhwai-Oh",
                        "structuredName": {
                            "firstName": "Songhwai",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Songhwai Oh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206757091,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8fb28b5b65e11742673d6c56a62c740c6d123961",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Low-rank matrix approximation plays an important role in the area of computer vision and image processing. Most of the conventional low-rank matrix approximation methods are based on the l2-norm (Frobenius norm) with principal component analysis (PCA) being the most popular among them. However, this can give a poor approximation for data contaminated by outliers (including missing data), because the l2-norm exaggerates the negative effect of outliers. Recently, to overcome this problem, various methods based on the l1-norm, such as robust PCA methods, have been proposed for low-rank matrix approximation. Despite the robustness of the methods, they require heavy computational effort and substantial memory for high-dimensional data, which is impractical for realworld problems. In this paper, we propose two efficient low-rank factorization methods based on the l1-norm that find proper projection and coefficient matrices using the alternating rectified gradient method. The proposed methods are applied to a number of low-rank matrix approximation problems to demonstrate their efficiency and robustness. The experimental results show that our proposals are efficient in both execution time and reconstruction performance unlike other state-of-the-art methods."
            },
            "slug": "Efficient-$l_{1}$-Norm-Based-Low-Rank-Matrix-for-Kim-Lee",
            "title": {
                "fragments": [],
                "text": "Efficient  $l_{1}$ -Norm-Based Low-Rank Matrix Approximations for Large-Scale Problems Using Alternating Rectified Gradient Method"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Two efficient low-rank factorization methods based on the l1-norm that find proper projection and coefficient matrices using the alternating rectified gradient method are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks and Learning Systems"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143838040"
                        ],
                        "name": "Julian Quiroga",
                        "slug": "Julian-Quiroga",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Quiroga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Quiroga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909232"
                        ],
                        "name": "Frederic Devernay",
                        "slug": "Frederic-Devernay",
                        "structuredName": {
                            "firstName": "Frederic",
                            "lastName": "Devernay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frederic Devernay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10240196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d4e33eb6e8be3d45e685645d2dee394ae204a5a",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene flow is defined as the motion field in 3D space, and can be computed from a single view when using an RGBD sensor. We propose a new scene flow approach that exploits the local and piecewise rigidity of real world scenes. By modeling the motion as a field of twists, our method encourages piecewise smooth solutions of rigid body motions. We give a general formulation to solve for local and global rigid motions by jointly using intensity and depth data. In order to deal efficiently with a moving camera, we model the motion as a rigid component plus a non-rigid residual and propose an alternating solver. The evaluation demonstrates that the proposed method achieves the best results in the most commonly used scene flow benchmark. Through additional experiments we indicate the general applicability of our approach in a variety of different scenarios."
            },
            "slug": "Dense-Semi-rigid-Scene-Flow-Estimation-from-RGBD-Quiroga-Brox",
            "title": {
                "fragments": [],
                "text": "Dense Semi-rigid Scene Flow Estimation from RGBD Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work proposes a new scene flow approach that exploits the local and piecewise rigidity of real world scenes and gives a general formulation to solve for local and global rigid motions by jointly using intensity and depth data."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259786"
                        ],
                        "name": "Qifa Ke",
                        "slug": "Qifa-Ke",
                        "structuredName": {
                            "firstName": "Qifa",
                            "lastName": "Ke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifa Ke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17144854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "952a6413aeac22d6b1f421e040072a17fcebad23",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Matrix factorization has many applications in computer vision. Singular value decomposition (SVD) is the standard algorithm for factorization. When there are outliers and missing data, which often happen in real measurements, SVD is no longer applicable. For robustness iteratively re-weighted least squares (IRLS) is often used for factorization by assigning a weight to each element in the measurements. Because it uses L/sub 2/ norm, good initialization in IRLS is critical for success, but is nontrivial. In this paper, we formulate matrix factorization as a L/sub 1/ norm minimization problem that is solved efficiently by alternative convex programming. Our formulation 1) is robust without requiring initial weighting, 2) handles missing data straightforwardly, and 3) provides a framework in which constraints and prior knowledge (if available) can be conveniently incorporated. In the experiments we apply our approach to factorization-based structure from motion. It is shown that our approach achieves better results than other approaches (including IRLS) on both synthetic and real data."
            },
            "slug": "Robust-L/sub-1/-norm-factorization-in-the-presence-Ke-Kanade",
            "title": {
                "fragments": [],
                "text": "Robust L/sub 1/ norm factorization in the presence of outliers and missing data by alternative convex programming"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper forms matrix factorization as a L/sub 1/ norm minimization problem that is solved efficiently by alternative convex programming that is robust without requiring initial weighting, handles missing data straightforwardly, and provides a framework in which constraints and prior knowledge can be conveniently incorporated."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066767241"
                        ],
                        "name": "N. Ding",
                        "slug": "N.-Ding",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279670"
                        ],
                        "name": "Andrea Frome",
                        "slug": "Andrea-Frome",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Frome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Frome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056418641"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118204742"
                        ],
                        "name": "Yuan Li",
                        "slug": "Yuan-Li",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2665814"
                        ],
                        "name": "H. Neven",
                        "slug": "H.-Neven",
                        "structuredName": {
                            "firstName": "Hartmut",
                            "lastName": "Neven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Neven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10559817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "924a0f0b1cfcb0d1f8f4ac232e91fe19307861cb",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study how to perform object classification in a principled way that exploits the rich structure of real world labels. We develop a new model that allows encoding of flexible relations between labels. We introduce Hierarchy and Exclusion (HEX) graphs, a new formalism that captures semantic relations between any two labels applied to the same object: mutual exclusion, overlap and subsumption. We then provide rigorous theoretical analysis that illustrates properties of HEX graphs such as consistency, equivalence, and computational implications of the graph structure. Next, we propose a probabilistic classification model based on HEX graphs and show that it enjoys a number of desirable properties. Finally, we evaluate our method using a large-scale benchmark. Empirical results demonstrate that our model can significantly improve object classification by exploiting the label relations."
            },
            "slug": "Large-Scale-Object-Classification-Using-Label-Deng-Ding",
            "title": {
                "fragments": [],
                "text": "Large-Scale Object Classification Using Label Relation Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new model that allows encoding of flexible relations between labels is developed that can significantly improve object classification by exploiting the label relations and a probabilistic classification model based on HEX graphs is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110760400"
                        ],
                        "name": "Ankit Gupta",
                        "slug": "Ankit-Gupta",
                        "structuredName": {
                            "firstName": "Ankit",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ankit Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39678486"
                        ],
                        "name": "Neel Joshi",
                        "slug": "Neel-Joshi",
                        "structuredName": {
                            "firstName": "Neel",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neel Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400248273"
                        ],
                        "name": "Michael F. Cohen",
                        "slug": "Michael-F.-Cohen",
                        "structuredName": {
                            "firstName": "Michael F.",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael F. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800609"
                        ],
                        "name": "B. Curless",
                        "slug": "B.-Curless",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Curless",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Curless"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18417477,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "83f826bd48755e3ea81ad5c921382338f3bb4688",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel single image deblurring method to estimate spatially non-uniform blur that results from camera shake. We use existing spatially invariant deconvolution methods in a local and robust way to compute initial estimates of the latent image. The camera motion is represented as a Motion Density Function (MDF) which records the fraction of time spent in each discretized portion of the space of all possible camera poses. Spatially varying blur kernels are derived directly from the MDF. We show that 6D camera motion is well approximated by 3 degrees of motion (in-plane translation and rotation) and analyze the scope of this approximation. We present results on both synthetic and captured data. Our system out-performs current approaches which make the assumption of spatially invariant blur."
            },
            "slug": "Single-Image-Deblurring-Using-Motion-Density-Gupta-Joshi",
            "title": {
                "fragments": [],
                "text": "Single Image Deblurring Using Motion Density Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A novel single image deblurring method to estimate spatially non-uniform blur that results from camera shake that out-performs current approaches which make the assumption of spatially invariant blur."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103244620"
                        ],
                        "name": "J\u00c3\u00bcrg Portmann",
                        "slug": "J\u00c3\u00bcrg-Portmann",
                        "structuredName": {
                            "firstName": "J\u00c3\u00bcrg",
                            "lastName": "Portmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00c3\u00bcrg Portmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060241"
                        ],
                        "name": "Simon Lynen",
                        "slug": "Simon-Lynen",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lynen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Lynen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1885768"
                        ],
                        "name": "M. Chli",
                        "slug": "M.-Chli",
                        "structuredName": {
                            "firstName": "Margarita",
                            "lastName": "Chli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720483"
                        ],
                        "name": "R. Siegwart",
                        "slug": "R.-Siegwart",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Siegwart",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Siegwart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4472278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "201d116761d9d300193df370107f26d7d475023b",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Detection and tracking of people in visible-light images has been subject to extensive research in the past decades with applications ranging from surveillance to search-and-rescue. Following the growing availability of thermal cameras and the distinctive thermal signature of humans, research effort has been focusing on developing people detection and tracking methodologies applicable to this sensing modality. However, a plethora of challenges arise on the transition from visible-light to thermal images, especially with the recent trend of employing thermal cameras onboard aerial platforms (e.g. in search-and-rescue research) capturing oblique views of the scenery. This paper presents a new, publicly available dataset of annotated thermal image sequences, posing a multitude of challenges for people detection and tracking. Moreover, we propose a new particle filter based framework for tracking people in aerial thermal images. Finally, we evaluate the performance of this pipeline on our dataset, incorporating a selection of relevant, state-of-the-art methods and present a comprehensive discussion of the merits spawning from our study."
            },
            "slug": "People-detection-and-tracking-from-aerial-thermal-Portmann-Lynen",
            "title": {
                "fragments": [],
                "text": "People detection and tracking from aerial thermal views"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new, publicly available dataset of annotated thermal image sequences, posing a multitude of challenges for people detection and tracking is presented and a new particle filter based framework for tracking people in aerial thermal images is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE International Conference on Robotics and Automation (ICRA)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2215623"
                        ],
                        "name": "Oliver Whyte",
                        "slug": "Oliver-Whyte",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Whyte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oliver Whyte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10233982,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "32ad65cfaef5a20377aed938e380a14c582eb5b2",
            "isKey": false,
            "numCitedBy": 629,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Photographs taken in low-light conditions are often blurry as a result of camera shake, i.e. a motion of the camera while its shutter is open. Most existing deblurring methods model the observed blurry image as the convolution of a sharp image with a uniform blur kernel. However, we show that blur from camera shake is in general mostly due to the 3D rotation of the camera, resulting in a blur that can be significantly non-uniform across the image. We propose a new parametrized geometric model of the blurring process in terms of the rotational motion of the camera during exposure. This model is able to capture non-uniform blur in an image due to camera shake using a single global descriptor, and can be substituted into existing deblurring algorithms with only small modifications. To demonstrate its effectiveness, we apply this model to two deblurring problems; first, the case where a single blurry image is available, for which we examine both an approximate marginalization approach and a maximum a posteriori approach, and second, the case where a sharp but noisy image of the scene is available in addition to the blurry image. We show that our approach makes it possible to model and remove a wider class of blurs than previous approaches, including uniform blur as a special case, and demonstrate its effectiveness with experiments on synthetic and real images."
            },
            "slug": "Non-uniform-Deblurring-for-Shaken-Images-Whyte-Sivic",
            "title": {
                "fragments": [],
                "text": "Non-uniform Deblurring for Shaken Images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new parametrized geometric model of the blurring process in terms of the rotational motion of the camera during exposure is proposed, able to capture non-uniform blur in an image due to camera shake using a single global descriptor, and can be substituted into existing deblurring algorithms with only small modifications."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019498"
                        ],
                        "name": "S. Meister",
                        "slug": "S.-Meister",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Meister",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Meister"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703781"
                        ],
                        "name": "B. J\u00e4hne",
                        "slug": "B.-J\u00e4hne",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "J\u00e4hne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. J\u00e4hne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793546"
                        ],
                        "name": "D. Kondermann",
                        "slug": "D.-Kondermann",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kondermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kondermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122506463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "892f0d45074f1d8053ddcd6b8be2f50ebbcf014a",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a high-performance stereo camera system to capture image sequences with high temporal and spatial resolution for the evaluation of various image processing tasks. The system was primarily designed for complex outdoor and traffic scenes that frequently occur in the automotive industry, but is also suited for other applications. For this task the system is equipped with a very accurate inertial measurement unit and global positioning system, which provides exact camera movement and position data. The system is already in active use and has produced several terabytes of challenging image sequences which are partly available for download."
            },
            "slug": "Outdoor-stereo-camera-system-for-the-generation-of-Meister-J\u00e4hne",
            "title": {
                "fragments": [],
                "text": "Outdoor stereo camera system for the generation of real-world benchmark data sets"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A high-performance stereo camera system to capture image sequences with high temporal and spatial resolution for the evaluation of various image processing tasks, primarily designed for complex outdoor and traffic scenes that frequently occur in the automotive industry."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145716041"
                        ],
                        "name": "Han Hu",
                        "slug": "Han-Hu",
                        "structuredName": {
                            "firstName": "Han",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33383055"
                        ],
                        "name": "Zhouchen Lin",
                        "slug": "Zhouchen-Lin",
                        "structuredName": {
                            "firstName": "Zhouchen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhouchen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144698893"
                        ],
                        "name": "Jianjiang Feng",
                        "slug": "Jianjiang-Feng",
                        "structuredName": {
                            "firstName": "Jianjiang",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianjiang Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49640256"
                        ],
                        "name": "Jie Zhou",
                        "slug": "Jie-Zhou",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14807516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b7cb9b97c425b52b2e6f41ba8028836029c4432",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Subspace clustering is a powerful technology for clustering data according to the underlying subspaces. Representation based methods are the most popular subspace clustering approach in recent years. In this paper, we analyze the grouping effect of representation based methods in depth. In particular, we introduce the enforced grouping effect conditions, which greatly facilitate the analysis of grouping effect. We further find that grouping effect is important for subspace clustering, which should be explicitly enforced in the data self-representation model, rather than implicitly implied by the model as in some prior work. Based on our analysis, we propose the SMooth Representation (SMR) model. We also propose a new affinity measure based on the grouping effect, which proves to be much more effective than the commonly used one. As a result, our SMR significantly outperforms the state-of-the-art ones on benchmark datasets."
            },
            "slug": "Smooth-Representation-Clustering-Hu-Lin",
            "title": {
                "fragments": [],
                "text": "Smooth Representation Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is found that grouping effect is important for subspace clustering, which should be explicitly enforced in the data self-representation model, rather than implicitly implied by the model as in some prior work."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144429686"
                        ],
                        "name": "James W. Davis",
                        "slug": "James-W.-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James W. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145696606"
                        ],
                        "name": "Vinay Sharma",
                        "slug": "Vinay-Sharma",
                        "structuredName": {
                            "firstName": "Vinay",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinay Sharma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7356871,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "bfedd15b5c82f97a7cc97df4754d1f255b58a12a",
            "isKey": false,
            "numCitedBy": 323,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Background-subtraction-using-contour-based-fusion-Davis-Sharma",
            "title": {
                "fragments": [],
                "text": "Background-subtraction using contour-based fusion of thermal and visible imagery"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166569"
                        ],
                        "name": "C. Yu",
                        "slug": "C.-Yu",
                        "structuredName": {
                            "firstName": "Chun-Nam",
                            "lastName": "Yu",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10240161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ae20e0bdfddc1888148e0fcde88d937e96318d2",
            "isKey": false,
            "numCitedBy": 714,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. Our proposal covers a large range of application problems, with an optimization problem that can be solved efficiently using Concave-Convex Programming. The generality and performance of the approach is demonstrated through three applications including motiffinding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval."
            },
            "slug": "Learning-structural-SVMs-with-latent-variables-Yu-Joachims",
            "title": {
                "fragments": [],
                "text": "Learning structural SVMs with latent variables"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A large-margin formulation and algorithm for structured output prediction that allows the use of latent variables and the generality and performance of the approach is demonstrated through three applications including motiffinding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690177"
                        ],
                        "name": "M. Ovsjanikov",
                        "slug": "M.-Ovsjanikov",
                        "structuredName": {
                            "firstName": "Maks",
                            "lastName": "Ovsjanikov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ovsjanikov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398681470"
                        ],
                        "name": "M. Ben-Chen",
                        "slug": "M.-Ben-Chen",
                        "structuredName": {
                            "firstName": "Mirela",
                            "lastName": "Ben-Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ben-Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932072"
                        ],
                        "name": "J. Solomon",
                        "slug": "J.-Solomon",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Solomon",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Solomon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983668"
                        ],
                        "name": "Adrian Butscher",
                        "slug": "Adrian-Butscher",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Butscher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian Butscher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744254"
                        ],
                        "name": "L. Guibas",
                        "slug": "L.-Guibas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Guibas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Guibas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207194138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21a39ca716c37ccc133ff96c31cc71565d0c968e",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel representation of maps between pairs of shapes that allows for efficient inference and manipulation. Key to our approach is a generalization of the notion of map that puts in correspondence real-valued functions rather than points on the shapes. By choosing a multi-scale basis for the function space on each shape, such as the eigenfunctions of its Laplace-Beltrami operator, we obtain a representation of a map that is very compact, yet fully suitable for global inference. Perhaps more remarkably, most natural constraints on a map, such as descriptor preservation, landmark correspondences, part preservation and operator commutativity become linear in this formulation. Moreover, the representation naturally supports certain algebraic operations such as map sum, difference and composition, and enables a number of applications, such as function or annotation transfer without establishing point-to-point correspondences. We exploit these properties to devise an efficient shape matching method, at the core of which is a single linear solve. The new method achieves state-of-the-art results on an isometric shape matching benchmark. We also show how this representation can be used to improve the quality of maps produced by existing shape matching methods, and illustrate its usefulness in segmentation transfer and joint analysis of shape collections."
            },
            "slug": "Functional-maps-Ovsjanikov-Ben-Chen",
            "title": {
                "fragments": [],
                "text": "Functional maps"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A novel representation of maps between pairs of shapes that allows for efficient inference and manipulation and supports certain algebraic operations such as map sum, difference and composition, and enables a number of applications, such as function or annotation transfer without establishing point-to-point correspondences."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145914024"
                        ],
                        "name": "R. Lathrop",
                        "slug": "R.-Lathrop",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lathrop",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lathrop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388700951"
                        ],
                        "name": "Tomas Lozano-Perez",
                        "slug": "Tomas-Lozano-Perez",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Lozano-Perez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Lozano-Perez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7398727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c7d38f68fe1150895a186e30b60c02dd89a676a",
            "isKey": false,
            "numCitedBy": 2423,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solving-the-Multiple-Instance-Problem-with-Dietterich-Lathrop",
            "title": {
                "fragments": [],
                "text": "Solving the Multiple Instance Problem with Axis-Parallel Rectangles"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116178059"
                        ],
                        "name": "Feng Xue",
                        "slug": "Feng-Xue",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng Xue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689313"
                        ],
                        "name": "F. Luisier",
                        "slug": "F.-Luisier",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Luisier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Luisier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737799"
                        ],
                        "name": "T. Blu",
                        "slug": "T.-Blu",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Blu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Blu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6646607,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9e50fb4f3bbca0316c13ccfbde759b269047b3b4",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel deconvolution algorithm based on the minimization of a regularized Stein's unbiased risk estimate (SURE), which is a good estimate of the mean squared error. We linearly parametrize the deconvolution process by using multiple Wiener filters as elementary functions, followed by undecimated Haar-wavelet thresholding. Due to the quadratic nature of SURE and the linear parametrization, the deconvolution problem finally boils down to solving a linear system of equations, which is very fast and exact. The linear coefficients, i.e., the solution of the linear system of equations, constitute the best approximation of the optimal processing on the Wiener-Haar-threshold basis that we consider. In addition, the proposed multi-Wiener SURE-LET approach is applicable for both periodic and symmetric boundary conditions, and can thus be used in various practical scenarios. The very competitive (both in computation time and quality) results show that the proposed algorithm, which can be interpreted as a kind of nonlinear Wiener processing, can be used as a basic tool for building more sophisticated deconvolution algorithms."
            },
            "slug": "Multi-Wiener-SURE-LET-Deconvolution-Xue-Luisier",
            "title": {
                "fragments": [],
                "text": "Multi-Wiener SURE-LET Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A novel deconvolution algorithm based on the minimization of a regularized Stein's unbiased risk estimate (SURE), which is a good estimate of the mean squared error, which can be interpreted as a kind of nonlinear Wiener processing on the Wiener-Haar-threshold basis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2496412"
                        ],
                        "name": "Connelly Barnes",
                        "slug": "Connelly-Barnes",
                        "structuredName": {
                            "firstName": "Connelly",
                            "lastName": "Barnes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Connelly Barnes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37737599"
                        ],
                        "name": "A. Finkelstein",
                        "slug": "A.-Finkelstein",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Finkelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976171"
                        ],
                        "name": "Dan B. Goldman",
                        "slug": "Dan-B.-Goldman",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Goldman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan B. Goldman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26169625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10aabf4c13ea57d7106cf809c9edbab63819c277",
            "isKey": false,
            "numCitedBy": 1748,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents interactive image editing tools using a new randomized algorithm for quickly finding approximate nearest-neighbor matches between image patches. Previous research in graphics and vision has leveraged such nearest-neighbor searches to provide a variety of high-level digital image editing tools. However, the cost of computing a field of such matches for an entire image has eluded previous efforts to provide interactive performance. Our algorithm offers substantial performance improvements over the previous state of the art (20-100x), enabling its use in interactive editing tools. The key insights driving the algorithm are that some good patch matches can be found via random sampling, and that natural coherence in the imagery allows us to propagate such matches quickly to surrounding areas. We offer theoretical analysis of the convergence properties of the algorithm, as well as empirical and practical evidence for its high quality and performance. This one simple algorithm forms the basis for a variety of tools -- image retargeting, completion and reshuffling -- that can be used together in the context of a high-level image editing application. Finally, we propose additional intuitive constraints on the synthesis process that offer the user a level of control unavailable in previous methods."
            },
            "slug": "PatchMatch:-a-randomized-correspondence-algorithm-Barnes-Shechtman",
            "title": {
                "fragments": [],
                "text": "PatchMatch: a randomized correspondence algorithm for structural image editing"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents interactive image editing tools using a new randomized algorithm for quickly finding approximate nearest-neighbor matches between image patches, and proposes additional intuitive constraints on the synthesis process that offer the user a level of control unavailable in previous methods."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2009"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38955885"
                        ],
                        "name": "Changchang Wu",
                        "slug": "Changchang-Wu",
                        "structuredName": {
                            "firstName": "Changchang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changchang Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5296119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51b685fe8ce6b917993aecbf9e16cdd07d3b1b7c",
            "isKey": false,
            "numCitedBy": 1114,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The time complexity of incremental structure from motion (SfM) is often known as O(n^4) with respect to the number of cameras. As bundle adjustment (BA) being significantly improved recently by preconditioned conjugate gradient (PCG), it is worth revisiting how fast incremental SfM is. We introduce a novel BA strategy that provides good balance between speed and accuracy. Through algorithm analysis and extensive experiments, we show that incremental SfM requires only O(n) time on many major steps including BA. Our method maintains high accuracy by regularly re-triangulating the feature matches that initially fail to triangulate. We test our algorithm on large photo collections and long video sequences with various settings, and show that our method offers state of the art performance for large-scale reconstructions. The presented algorithm is available as part of VisualSFM at http://homes.cs.washington.edu/~ccwu/vsfm/."
            },
            "slug": "Towards-Linear-Time-Incremental-Structure-from-Wu",
            "title": {
                "fragments": [],
                "text": "Towards Linear-Time Incremental Structure from Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Through algorithm analysis and extensive experiments, it is shown that incremental SfM requires only O(n) time on many major steps including BA, and offers state of the art performance for large-scale reconstructions."
            },
            "venue": {
                "fragments": [],
                "text": "2013 International Conference on 3D Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726249"
                        ],
                        "name": "T. Quack",
                        "slug": "T.-Quack",
                        "structuredName": {
                            "firstName": "Till",
                            "lastName": "Quack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Quack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8922492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01eeff3fb391294cef61d35e75c557575b5b0338",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to automatically find spatial configurations of local features occurring frequently on instances of a given object class, and rarely on the background. The approach is based on computationally efficient data mining techniques and can find frequent configurations among tens of thousands of candidates within seconds. Based on the mined configurations we develop a method to select features which have high probability of lying on previously unseen instances of the object class. The technique is meant as an intermediate processing layer to filter the large amount of clutter features returned by low- level feature extraction, and hence to facilitate the tasks of higher-level processing stages such as object detection."
            },
            "slug": "Efficient-Mining-of-Frequent-and-Distinctive-Quack-Ferrari",
            "title": {
                "fragments": [],
                "text": "Efficient Mining of Frequent and Distinctive Feature Configurations"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A novel approach to automatically find spatial configurations of local features occurring frequently on instances of a given object class, and rarely on the background, to facilitate the tasks of higher-level processing stages such as object detection."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152515382"
                        ],
                        "name": "Thomas Whelan",
                        "slug": "Thomas-Whelan",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Whelan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Whelan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32354334"
                        ],
                        "name": "H. Johannsson",
                        "slug": "H.-Johannsson",
                        "structuredName": {
                            "firstName": "Hordur",
                            "lastName": "Johannsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Johannsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2659339"
                        ],
                        "name": "M. Kaess",
                        "slug": "M.-Kaess",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kaess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kaess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7136913"
                        ],
                        "name": "J. Leonard",
                        "slug": "J.-Leonard",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Leonard",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Leonard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49105537"
                        ],
                        "name": "J. McDonald",
                        "slug": "J.-McDonald",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "McDonald",
                            "middleNames": [
                                "B."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16568724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b07ba7628d47c262fb47eb682d4a2412ebf9feb",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes extensions to the Kintinuous [1] algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused realtime surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features."
            },
            "slug": "Robust-real-time-visual-odometry-for-dense-RGB-D-Whelan-Johannsson",
            "title": {
                "fragments": [],
                "text": "Robust real-time visual odometry for dense RGB-D mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "Extensions to the Kintinuous algorithm for spatially extended KinectFusion are described, incorporating the integration of multiple 6DOF camera odometry estimation methods for robust tracking and a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Robotics and Automation"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144566512"
                        ],
                        "name": "M. Hirsch",
                        "slug": "M.-Hirsch",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hirsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hirsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1814533"
                        ],
                        "name": "Christian J. Schuler",
                        "slug": "Christian-J.-Schuler",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Schuler",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian J. Schuler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734990"
                        ],
                        "name": "S. Harmeling",
                        "slug": "S.-Harmeling",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Harmeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harmeling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3884789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5efb47e755ae6428bc694829b0ed231d3be91e64",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Camera shake leads to non-uniform image blurs. State-of-the-art methods for removing camera shake model the blur as a linear combination of homographically transformed versions of the true image. While this is conceptually interesting, the resulting algorithms are computationally demanding. In this paper we develop a forward model based on the efficient filter flow framework, incorporating the particularities of camera shake, and show how an efficient algorithm for blur removal can be obtained. Comprehensive comparisons on a number of real-world blurry images show that our approach is not only substantially faster, but it also leads to better deblurring results."
            },
            "slug": "Fast-removal-of-non-uniform-camera-shake-Hirsch-Schuler",
            "title": {
                "fragments": [],
                "text": "Fast removal of non-uniform camera shake"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A forward model based on the efficient filter flow framework is developed, incorporating the particularities of camera shake, and it is shown how an efficient algorithm for blur removal can be obtained."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17173992"
                        ],
                        "name": "B. Wrobel",
                        "slug": "B.-Wrobel",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Wrobel",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wrobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44793400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "339093c7ed71919ce59a7e78979a77abd25bad0c",
            "isKey": false,
            "numCitedBy": 16322,
            "numCiting": 222,
            "paperAbstract": {
                "fragments": [],
                "text": "Downloading the book in this website lists can give you more advantages. It will show you the best book collections and completed collections. So many books can be found in this website. So, this is not only this multiple view geometry in computer vision. However, this book is referred to read because it is an inspiring book to give you more chance to get experiences and also thoughts. This is simple, read the soft file of the book and you get it."
            },
            "slug": "Multiple-View-Geometry-in-Computer-Vision-Wrobel",
            "title": {
                "fragments": [],
                "text": "Multiple View Geometry in Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This book is referred to read because it is an inspiring book to give you more chance to get experiences and also thoughts and it will show the best book collections and completed collections."
            },
            "venue": {
                "fragments": [],
                "text": "K\u00fcnstliche Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2430892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3120324069ec20eed853d3f9bbbceb32e4173b93",
            "isKey": false,
            "numCitedBy": 3909,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function's smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an /spl alpha/-/spl beta/-swap: for a pair of labels /spl alpha/,/spl beta/, this move exchanges the labels between an arbitrary set of pixels labeled a and another arbitrary set labeled /spl beta/. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an /spl alpha/-expansion: for a label a, this move assigns an arbitrary set of pixels the label /spl alpha/. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion."
            },
            "slug": "Fast-approximate-energy-minimization-via-graph-cuts-Boykov-Veksler",
            "title": {
                "fragments": [],
                "text": "Fast approximate energy minimization via graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed, and generates a labeling such that there is no expansion move that decreases the energy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14716701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1365bc5de434771f05f186c6be75bb9400b15495",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new model for estimating optical flow based on the motion of planar regions plus local deformations. The approach exploits brightness information to organize and constrain the interpretation of the motion by using segmented regions of piecewise smooth brightness to hypothesize planar regions in the scene. Parametric flow models are estimated in these regions in a two step process which first computes a coarse fit and then estimates the appropriate parametrization of the motion of the region. The initial fit is refined using a generalization of the standard area-based regression approaches. Since the assumption of planarity is likely to be violated, we allow local deformations from the planar assumption in the same spirit as physically-based approaches which model shape using coarse parametric models plus local deformations. This parametric plus deformation model exploits the strong constraints of parametric approaches while retaining the adaptive nature of regularization approaches. Experimental results on a variety of images model produces accurate flow estimates while the incorporation of brightness segmentation boundaries."
            },
            "slug": "Estimating-Optical-Flow-in-Segmented-Images-Using-Black-Jepson",
            "title": {
                "fragments": [],
                "text": "Estimating Optical Flow in Segmented Images Using Variable-Order Parametric Models With Local Deformations"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new model for estimating optical flow based on the motion of planar regions plus local deformations which exploits the strong constraints of parametric approaches while retaining the adaptive nature of regularization approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872189"
                        ],
                        "name": "Martin Byr\u00f6d",
                        "slug": "Martin-Byr\u00f6d",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Byr\u00f6d",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Byr\u00f6d"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144735785"
                        ],
                        "name": "Matthew A. Brown",
                        "slug": "Matthew-A.-Brown",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brown",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew A. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725762"
                        ],
                        "name": "Kalle \u00c5str\u00f6m",
                        "slug": "Kalle-\u00c5str\u00f6m",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "\u00c5str\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kalle \u00c5str\u00f6m"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15026690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1a89dec75199fd9eff5ffc2163d34c84d155d36",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a solution to panoramic image stitching of two images with coinciding optical centers, but unknown focal length and radial distortion. The algorithm operates with a minimal set of corresponding points (three) which means that it is well suited for use in any RANSAC style algorithm for simultaneous estimation of geometry and outlier rejection. Compared to a previous method for this problem, we are able to guarantee that the right solution is found in all cases. The solution is obtained by solving a small system of polynomial equations. The proposed algorithm has been integrated in a complete multi image stitching system and we evaluate its performance on real images with lens distortion. We demonstrate both quantitative and qualitative improvements compared to state of the art methods."
            },
            "slug": "Minimal-Solutions-for-Panoramic-Stitching-with-Byr\u00f6d-Brown",
            "title": {
                "fragments": [],
                "text": "Minimal Solutions for Panoramic Stitching with Radial Distortion"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This paper presents a solution to panoramic image stitching of two images with coinciding optical centers, but unknown focal length and radial distortion, by solving a small system of polynomial equations."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48861660"
                        ],
                        "name": "H. Zou",
                        "slug": "H.-Zou",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Zou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122419596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9f5723859a665c3e54b8d9a1a7eecb8b5084af6",
            "isKey": false,
            "numCitedBy": 13907,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary.\u2002 We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p\u226bn case. An algorithm called LARS\u2010EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso."
            },
            "slug": "Regularization-and-variable-selection-via-the-net-Zou-Hastie",
            "title": {
                "fragments": [],
                "text": "Regularization and variable selection via the elastic net"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is shown that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation, and an algorithm called LARS\u2010EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lamba."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38534744"
                        ],
                        "name": "Ayan Chakrabarti",
                        "slug": "Ayan-Chakrabarti",
                        "structuredName": {
                            "firstName": "Ayan",
                            "lastName": "Chakrabarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ayan Chakrabarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713451"
                        ],
                        "name": "Todd E. Zickler",
                        "slug": "Todd-E.-Zickler",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Zickler",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Todd E. Zickler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5821064,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "d3a41b37b630ab1c25fae96c57d01fcf4b8f8d76",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Blur is caused by a pixel receiving light from multiple scene points, and in many cases, such as object motion, the induced blur varies spatially across the image plane. However, the seemingly straight-forward task of estimating spatially-varying blur from a single image has proved hard to accomplish reliably. This work considers such blur and makes two contributions: a local blur cue that measures the likelihood of a small neighborhood being blurred by a candidate blur kernel; and an algorithm that, given an image, simultaneously selects a motion blur kernel and segments the region that it affects. The methods are shown to perform well on a diversity of images."
            },
            "slug": "Analyzing-spatially-varying-blur-Chakrabarti-Zickler",
            "title": {
                "fragments": [],
                "text": "Analyzing spatially-varying blur"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Two contributions are made: a local blur cue that measures the likelihood of a small neighborhood being blurred by a candidate blur kernel; and an algorithm that, given an image, simultaneously selects a motion blur kernel and segments the region that it affects."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50366818"
                        ],
                        "name": "Richard A. Newcombe",
                        "slug": "Richard-A.-Newcombe",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Newcombe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard A. Newcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79406746"
                        ],
                        "name": "S. Izadi",
                        "slug": "S.-Izadi",
                        "structuredName": {
                            "firstName": "Shahram",
                            "lastName": "Izadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Izadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531379"
                        ],
                        "name": "Otmar Hilliges",
                        "slug": "Otmar-Hilliges",
                        "structuredName": {
                            "firstName": "Otmar",
                            "lastName": "Hilliges",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Otmar Hilliges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032716"
                        ],
                        "name": "D. Molyneaux",
                        "slug": "D.-Molyneaux",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Molyneaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Molyneaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288470"
                        ],
                        "name": "David Kim",
                        "slug": "David-Kim",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052135690"
                        ],
                        "name": "A. Davison",
                        "slug": "A.-Davison",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Davison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Davison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144356949"
                        ],
                        "name": "Steve Hodges",
                        "slug": "Steve-Hodges",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Hodges",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Hodges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11830123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2975deb809f35bd47ac3de34e348675a0cfb3f6",
            "isKey": false,
            "numCitedBy": 3509,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision."
            },
            "slug": "KinectFusion:-Real-time-dense-surface-mapping-and-Newcombe-Izadi",
            "title": {
                "fragments": [],
                "text": "KinectFusion: Real-time dense surface mapping and tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware, which fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real- time."
            },
            "venue": {
                "fragments": [],
                "text": "2011 10th IEEE International Symposium on Mixed and Augmented Reality"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15181392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11bec3f6e2318f334d58bf6d9d80b67ab41355b4",
            "isKey": false,
            "numCitedBy": 677,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A new enhancement of ransac, the locally optimized ransac (lo-ransac), is introduced. It has been observed that, to find an optimal solution (with a given probability), the number of samples drawn in ransac is significantly higher than predicted from the mathematical model. This is due to the incorrect assumption, that a model with parameters computed from an outlier-free sample is consistent with all inliers. The assumption rarely holds in practice. The locally optimized ransac makes no new assumptions about the data, on the contrary \u2013 it makes the above-mentioned assumption valid by applying local optimization to the solution estimated from the random sample."
            },
            "slug": "Locally-Optimized-RANSAC-Chum-Matas",
            "title": {
                "fragments": [],
                "text": "Locally Optimized RANSAC"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The locally optimized ransac makes no new assumptions about the data, on the contrary \u2013 it makes the above-mentioned assumption valid by applying local optimization to the solution estimated from the random sample."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 39450643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1679beddda3a183714d380e944fe6bf586c083cd",
            "isKey": false,
            "numCitedBy": 13766,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such TreeBoost models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed."
            },
            "slug": "Greedy-function-approximation:-A-gradient-boosting-Friedman",
            "title": {
                "fragments": [],
                "text": "Greedy function approximation: A gradient boosting machine."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion, and specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695302"
                        ],
                        "name": "D. Cremers",
                        "slug": "D.-Cremers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cremers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cremers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715959"
                        ],
                        "name": "Stefano Soatto",
                        "slug": "Stefano-Soatto",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Soatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Soatto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17775297,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "5f26571ec4473d8f2efb9deae4aa1ce703be1eba",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel variational approach for segmenting the image plane into a set of regions of parametric motion on the basis of two consecutive frames from an image sequence. Our model is based on a conditional probability for the spatio-temporal image gradient, given a particular velocity model, and on a geometric prior on the estimated motion field favoring motion boundaries of minimal length.Exploiting the Bayesian framework, we derive a cost functional which depends on parametric motion models for each of a set of regions and on the boundary separating these regions. The resulting functional can be interpreted as an extension of the Mumford-Shah functional from intensity segmentation to motion segmentation. In contrast to most alternative approaches, the problems of segmentation and motion estimation are jointly solved by continuous minimization of a single functional. Minimizing this functional with respect to its dynamic variables results in an eigenvalue problem for the motion parameters and in a gradient descent evolution for the motion discontinuity set.We propose two different representations of this motion boundary: an explicit spline-based implementation which can be applied to the motion-based tracking of a single moving object, and an implicit multiphase level set implementation which allows for the segmentation of an arbitrary number of multiply connected moving objects.Numerical results both for simulated ground truth experiments and for real-world sequences demonstrate the capacity of our approach to segment objects based exclusively on their relative motion."
            },
            "slug": "Motion-Competition:-A-Variational-Approach-to-Cremers-Soatto",
            "title": {
                "fragments": [],
                "text": "Motion Competition: A Variational Approach to Piecewise Parametric Motion Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A novel variational approach for segmenting the image plane into a set of regions of parametric motion on the basis of two consecutive frames from an image sequence based on a conditional probability for the spatio-temporal image gradient and a geometric prior on the estimated motion field."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38188040"
                        ],
                        "name": "Dong Xu",
                        "slug": "Dong-Xu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158301"
                        ],
                        "name": "Benyu Zhang",
                        "slug": "Benyu-Zhang",
                        "structuredName": {
                            "firstName": "Benyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benyu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973386"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152290618"
                        ],
                        "name": "Qiang Yang",
                        "slug": "Qiang-Yang",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145676588"
                        ],
                        "name": "Stephen Lin",
                        "slug": "Stephen-Lin",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2426049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69381b5efd97e7c55f51c2730caccab3d632d4d2",
            "isKey": false,
            "numCitedBy": 2193,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions"
            },
            "slug": "Graph-Embedding-and-Extensions:-A-General-Framework-Yan-Xu",
            "title": {
                "fragments": [],
                "text": "Graph Embedding and Extensions: A General Framework for Dimensionality Reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new supervised dimensionality reduction algorithm called marginal Fisher analysis is proposed in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizing the interclass separability."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779676"
                        ],
                        "name": "J\u00fcrgen Sturm",
                        "slug": "J\u00fcrgen-Sturm",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Sturm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00fcrgen Sturm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2337344"
                        ],
                        "name": "Nikolas Engelhard",
                        "slug": "Nikolas-Engelhard",
                        "structuredName": {
                            "firstName": "Nikolas",
                            "lastName": "Engelhard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolas Engelhard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2015943"
                        ],
                        "name": "F. Endres",
                        "slug": "F.-Endres",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Endres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Endres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725973"
                        ],
                        "name": "W. Burgard",
                        "slug": "W.-Burgard",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Burgard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Burgard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695302"
                        ],
                        "name": "D. Cremers",
                        "slug": "D.-Cremers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cremers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cremers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206942855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb4c55740165defacf08329beaae5314d7fbfe6",
            "isKey": false,
            "numCitedBy": 2203,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 \u00d7 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools."
            },
            "slug": "A-benchmark-for-the-evaluation-of-RGB-D-SLAM-Sturm-Engelhard",
            "title": {
                "fragments": [],
                "text": "A benchmark for the evaluation of RGB-D SLAM systems"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system is recorded for the evaluation of RGB-D SLAM systems."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105625747"
                        ],
                        "name": "Kyle Wilson",
                        "slug": "Kyle-Wilson",
                        "structuredName": {
                            "firstName": "Kyle",
                            "lastName": "Wilson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyle Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 653212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b06ce62194a54e59b2fcf44f906b083ebc705618",
            "isKey": false,
            "numCitedBy": 289,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple, effective method for solving structure from motion problems by averaging epipolar geometries. Based on recent successes in solving for global camera rotations using averaging schemes, we focus on the problem of solving for 3D camera translations given a network of noisy pairwise camera translation directions (or 3D point observations). To do this well, we have two main insights. First, we propose a method for removing outliers from problem instances by solving simpler low-dimensional subproblems, which we refer to as 1DSfM problems. Second, we present a simple, principled averaging scheme. We demonstrate this new method in the wild on Internet photo collections."
            },
            "slug": "Robust-Global-Translations-with-1DSfM-Wilson-Snavely",
            "title": {
                "fragments": [],
                "text": "Robust Global Translations with 1DSfM"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a method for removing outliers from problem instances by solving simpler low-dimensional subproblems, which it refers to as 1DSfM problems."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39612999"
                        ],
                        "name": "Srinath Sridhar",
                        "slug": "Srinath-Sridhar",
                        "structuredName": {
                            "firstName": "Srinath",
                            "lastName": "Sridhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srinath Sridhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2663734"
                        ],
                        "name": "Antti Oulasvirta",
                        "slug": "Antti-Oulasvirta",
                        "structuredName": {
                            "firstName": "Antti",
                            "lastName": "Oulasvirta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antti Oulasvirta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680185"
                        ],
                        "name": "C. Theobalt",
                        "slug": "C.-Theobalt",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Theobalt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Theobalt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3058169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6fd336e6f84b13b54968f2cae19a260c91ddded",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Tracking the articulated 3D motion of the hand has important applications, for example, in human-computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multi-view RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-real time performance of 10 fps on a desktop computer."
            },
            "slug": "Interactive-Markerless-Articulated-Hand-Motion-RGB-Sridhar-Oulasvirta",
            "title": {
                "fragments": [],
                "text": "Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization that achieves state-of-the-art accuracy on challenging sequences and near-real time performance on a desktop computer."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294832"
                        ],
                        "name": "Tobias Gurdan",
                        "slug": "Tobias-Gurdan",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Gurdan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tobias Gurdan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 487290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f15aa250d444f4b4a195726e7ad7c709d2c8ebc6",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Pokrass et al. [PBBS12] present a novel sparse modeling approach to non-rigid shape matching using only the ability to detect repeatable regions. They show that such scarce information as two sets of regions in two shapes is sufficient to establish very accurate correspondences between shapes. The paper presents methods from the field of sparse modeling and show how they can be aplied to simultaneously solve for an unknown permutation ordering of the regions on two shapes and for an unknown correspondence in functional representation. It further presents numerical solutions to the resulting optimization problems. The paper also presents results and compare them to state-of-the-art methods. In this seminar paper, we give an overview on their work. Figure 1: In their work on the Sparse Modeling of Intrinsic Correspondences, Pokrass et al. present a novel and first of its kind approach to shape matching. They show how to use tools from the field of sparse modeling to simultaneously search for an approximately diagonal C and permutation \u03a0, bringing a set of regions into correspondence. The latter are given in functional representation by coefficients A and B. These indicator functions can represent any intrinsic property of two near-isometric shapes, e.g. repeatable regions like MSER. Such scarce information is sufficient to achieve high quality matchings with the presented robust permuted sparse coding algorithm, which outperforms state-of-the-art methods."
            },
            "slug": "Sparse-Modeling-of-Intrinsic-Correspondences-Gurdan",
            "title": {
                "fragments": [],
                "text": "Sparse Modeling of Intrinsic Correspondences"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The paper presents methods from the field of sparse modeling and shows how they can be aplied to simultaneously solve for anunknown permutation ordering of the regions on two shapes and for an unknown correspondence in functional representation, and compares them to state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696991"
                        ],
                        "name": "G. Zelinsky",
                        "slug": "G.-Zelinsky",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Zelinsky",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zelinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111015847"
                        ],
                        "name": "Yifan Peng",
                        "slug": "Yifan-Peng",
                        "structuredName": {
                            "firstName": "Yifan",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yifan Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654220"
                        ],
                        "name": "D. Samaras",
                        "slug": "D.-Samaras",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Samaras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Samaras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13977395,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "080bab0f7cb769b05ed5f6e74e5819e578017558",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Is it possible to infer a person's goal by decoding their fixations on objects? Two groups of participants categorically searched for either a teddy bear or butterfly among random category distractors, each rated as high, medium, or low in similarity to the target classes. Target-similar objects were preferentially fixated in both search tasks, demonstrating information about target category in looking behavior. Different participants then viewed the searchers' scanpaths, superimposed over the target-absent displays, and attempted to decode the target category (bear/butterfly). Bear searchers were classified perfectly; butterfly searchers were classified at 77%. Bear and butterfly Support Vector Machine (SVM) classifiers were also used to decode the same preferentially fixated objects and found to yield highly comparable classification rates. We conclude that information about a person's search goal exists in fixation behavior, and that this information can be behaviorally decoded to reveal a search target-essentially reading a person's mind by analyzing their fixations."
            },
            "slug": "Eye-can-read-your-mind:-decoding-gaze-fixations-to-Zelinsky-Peng",
            "title": {
                "fragments": [],
                "text": "Eye can read your mind: decoding gaze fixations to reveal categorical search targets."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is concluded that information about a people's search goal exists in fixation behavior, and that this information can be behaviorally decoded to reveal a search target-essentially reading a person's mind by analyzing their fixations."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38120884"
                        ],
                        "name": "Kamalika Chaudhuri",
                        "slug": "Kamalika-Chaudhuri",
                        "structuredName": {
                            "firstName": "Kamalika",
                            "lastName": "Chaudhuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kamalika Chaudhuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924113"
                        ],
                        "name": "Karen Livescu",
                        "slug": "Karen-Livescu",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Livescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Livescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37564609"
                        ],
                        "name": "Karthik Sridharan",
                        "slug": "Karthik-Sridharan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Sridharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Sridharan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2800834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2ec1f149e494cf589163f6fc46cb6f02e44540f",
            "isKey": false,
            "numCitedBy": 665,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering data in high dimensions is believed to be a hard problem in general. A number of efficient clustering algorithms developed in recent years address this problem by projecting the data into a lower-dimensional subspace, e.g. via Principal Components Analysis (PCA) or random projections, before clustering. Here, we consider constructing such projections using multiple views of the data, via Canonical Correlation Analysis (CCA).\n Under the assumption that the views are un-correlated given the cluster label, we show that the separation conditions required for the algorithm to be successful are significantly weaker than prior results in the literature. We provide results for mixtures of Gaussians and mixtures of log concave distributions. We also provide empirical support from audio-visual speaker clustering (where we desire the clusters to correspond to speaker ID) and from hierarchical Wikipedia document clustering (where one view is the words in the document and the other is the link structure)."
            },
            "slug": "Multi-view-clustering-via-canonical-correlation-Chaudhuri-Kakade",
            "title": {
                "fragments": [],
                "text": "Multi-view clustering via canonical correlation analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Under the assumption that the views are un-correlated given the cluster label, it is shown that the separation conditions required for the algorithm to be successful are significantly weaker than prior results in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679223"
                        ],
                        "name": "S. Seitz",
                        "slug": "S.-Seitz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Seitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13385757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5ebf37ce170f13a905f7feba9fb7096b49fb8b3",
            "isKey": false,
            "numCitedBy": 3203,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites."
            },
            "slug": "Photo-tourism:-exploring-photo-collections-in-3D-Snavely-Seitz",
            "title": {
                "fragments": [],
                "text": "Photo tourism: exploring photo collections in 3D"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work presents a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface that consists of an image-based modeling front end that automatically computes the viewpoint of each photograph and a sparse 3D model of the scene and image to model correspondences."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2006"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40246285"
                        ],
                        "name": "L. Rudin",
                        "slug": "L.-Rudin",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Rudin",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rudin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782265"
                        ],
                        "name": "S. Osher",
                        "slug": "S.-Osher",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Osher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Osher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143749693"
                        ],
                        "name": "E. Fatemi",
                        "slug": "E.-Fatemi",
                        "structuredName": {
                            "firstName": "Emad",
                            "lastName": "Fatemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fatemi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13133466,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "54205667c1f65a320f667d73c354ed8e86f1b9d9",
            "isKey": false,
            "numCitedBy": 13753,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-total-variation-based-noise-removal-Rudin-Osher",
            "title": {
                "fragments": [],
                "text": "Nonlinear total variation based noise removal algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39963722"
                        ],
                        "name": "Raviteja Vemulapalli",
                        "slug": "Raviteja-Vemulapalli",
                        "structuredName": {
                            "firstName": "Raviteja",
                            "lastName": "Vemulapalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raviteja Vemulapalli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998286"
                        ],
                        "name": "F. Arrate",
                        "slug": "F.-Arrate",
                        "structuredName": {
                            "firstName": "Felipe",
                            "lastName": "Arrate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Arrate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1732632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09e1c1a130d98c91a9a5446690e114e2cc14e007",
            "isKey": false,
            "numCitedBy": 1081,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently introduced cost-effective depth sensors coupled with the real-time skeleton estimation algorithm of Shotton et al. [16] have generated a renewed interest in skeleton-based human action recognition. Most of the existing skeleton-based approaches use either the joint locations or the joint angles to represent a human skeleton. In this paper, we propose a new skeletal representation that explicitly models the 3D geometric relationships between various body parts using rotations and translations in 3D space. Since 3D rigid body motions are members of the special Euclidean group SE(3), the proposed skeletal representation lies in the Lie group SE(3)\u00d7.. .\u00d7SE(3), which is a curved manifold. Using the proposed representation, human actions can be modeled as curves in this Lie group. Since classification of curves in this Lie group is not an easy task, we map the action curves from the Lie group to its Lie algebra, which is a vector space. We then perform classification using a combination of dynamic time warping, Fourier temporal pyramid representation and linear SVM. Experimental results on three action datasets show that the proposed representation performs better than many existing skeletal representations. The proposed approach also outperforms various state-of-the-art skeleton-based human action recognition approaches."
            },
            "slug": "Human-Action-Recognition-by-Representing-3D-as-in-a-Vemulapalli-Arrate",
            "title": {
                "fragments": [],
                "text": "Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A new skeletal representation that explicitly models the 3D geometric relationships between various body parts using rotations and translations in 3D space is proposed and outperforms various state-of-the-art skeleton-based human action recognition approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005026919"
                        ],
                        "name": "C. Qian",
                        "slug": "C.-Qian",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112349534"
                        ],
                        "name": "Xiao Sun",
                        "slug": "Xiao-Sun",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732264"
                        ],
                        "name": "Yichen Wei",
                        "slug": "Yichen-Wei",
                        "structuredName": {
                            "firstName": "Yichen",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichen Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9031778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c117be9bc1c62d739705069c2880f010cf73ef0",
            "isKey": false,
            "numCitedBy": 436,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a realtime hand tracking system using a depth sensor. It tracks a fully articulated hand under large viewpoints in realtime (25 FPS on a desktop without using a GPU) and with high accuracy (error below 10 mm). To our knowledge, it is the first system that achieves such robustness, accuracy, and speed simultaneously, as verified on challenging real data. Our system is made of several novel techniques. We model a hand simply using a number of spheres and define a fast cost function. Those are critical for realtime performance. We propose a hybrid method that combines gradient based and stochastic optimization methods to achieve fast convergence and good accuracy. We present new finger detection and hand initialization methods that greatly enhance the robustness of tracking."
            },
            "slug": "Realtime-and-Robust-Hand-Tracking-from-Depth-Qian-Sun",
            "title": {
                "fragments": [],
                "text": "Realtime and Robust Hand Tracking from Depth"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A hybrid method that combines gradient based and stochastic optimization methods to achieve fast convergence and good accuracy is proposed and presented, making it the first system that achieves such robustness, accuracy, and speed simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2783545"
                        ],
                        "name": "Artiom Kovnatsky",
                        "slug": "Artiom-Kovnatsky",
                        "structuredName": {
                            "firstName": "Artiom",
                            "lastName": "Kovnatsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Artiom Kovnatsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732570"
                        ],
                        "name": "M. Bronstein",
                        "slug": "M.-Bronstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bronstein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bronstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49791556"
                        ],
                        "name": "A. Bronstein",
                        "slug": "A.-Bronstein",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Bronstein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bronstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758335"
                        ],
                        "name": "K. Glashoff",
                        "slug": "K.-Glashoff",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Glashoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Glashoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143923265"
                        ],
                        "name": "R. Kimmel",
                        "slug": "R.-Kimmel",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kimmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kimmel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12482548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dd4279e398523aa6c128f976f4748887775c7e1",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of Laplacian eigenbases has been shown to be fruitful in many computer graphics applications. Today, state\u2010of\u2010the\u2010art approaches to shape analysis, synthesis, and correspondence rely on these natural harmonic bases that allow using classical tools from harmonic analysis on manifolds. However, many applications involving multiple shapes are obstacled by the fact that Laplacian eigenbases computed independently on different shapes are often incompatible with each other. In this paper, we propose the construction of common approximate eigenbases for multiple shapes using approximate joint diagonalization algorithms, taking as input a set of corresponding functions (e.g. indicator functions of stable regions) on the two shapes. We illustrate the benefits of the proposed approach on tasks from shape editing, pose transfer, correspondence, and similarity."
            },
            "slug": "Coupled-quasi\u2010harmonic-bases-Kovnatsky-Bronstein",
            "title": {
                "fragments": [],
                "text": "Coupled quasi\u2010harmonic bases"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes the construction of common approximate eigenbases for multiple shapes using approximate joint diagonalization algorithms, taking as input a set of corresponding functions on the two shapes, and illustrates the benefits of the proposed approach on tasks from shape editing, pose transfer, correspondence, and similarity."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Graph. Forum"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3717791"
                        ],
                        "name": "M. P. Kumar",
                        "slug": "M.-P.-Kumar",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Pawan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 524128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8194d30109afd911b5ecffec59b8c281c0b1a81b",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the task of obtaining the maximum a posteriori estimate of discrete pairwise random fields with arbitrary unary potentials and semi-metric pairwise potentials. For this problem, we propose an accurate hierarchical move making strategy where each move is computed efficiently by solving an st-MINCUT problem. Unlike previous move making approaches, e.g. the widely used \u03b1-expansion algorithm, our method obtains the guarantees of the standard linear programming (LP) relaxation for the important special case of metric labeling. Unlike the existing LP relaxation solvers, e.g. interior-point algorithms or tree-reweighted message passing, our method is significantly faster as it uses only the efficient st-MINCUT algorithm in its design. Using both synthetic and real data experiments, we show that our technique outperforms several commonly used algorithms."
            },
            "slug": "MAP-Estimation-of-Semi-Metric-MRFs-via-Hierarchical-Kumar-Koller",
            "title": {
                "fragments": [],
                "text": "MAP Estimation of Semi-Metric MRFs via Hierarchical Graph Cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes an accurate hierarchical move making strategy where each move is computed efficiently by solving an st-MINCUT problem, and obtains the guarantees of the standard linear programming (LP) relaxation for the important special case of metric labeling."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144956994"
                        ],
                        "name": "Andrew Owens",
                        "slug": "Andrew-Owens",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Owens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6779587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bfd614d62e488a0eae0eacb1a3263e1485e336d",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in structure from motion (SfM) has successfully built 3D models from large unstructured collections of images downloaded from the Internet. Most approaches use incremental algorithms that solve progressively larger bundle adjustment problems. These incremental techniques scale poorly as the number of images grows, and can drift or fall into bad local minima. We present an alternative formulation for SfM based on finding a coarse initial solution using a hybrid discrete-continuous optimization, and then improving that solution using bundle adjustment. The initial optimization step uses a discrete Markov random field (MRF) formulation, coupled with a continuous Levenberg-Marquardt refinement. The formulation naturally incorporates various sources of information about both the cameras and the points, including noisy geotags and vanishing point estimates. We test our method on several large-scale photo collections, including one with measured camera positions, and show that it can produce models that are similar to or better than those produced with incremental bundle adjustment, but more robustly and in a fraction of the time."
            },
            "slug": "Discrete-continuous-optimization-for-large-scale-Crandall-Owens",
            "title": {
                "fragments": [],
                "text": "Discrete-continuous optimization for large-scale structure from motion"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents an alternative formulation for SfM based on finding a coarse initial solution using a hybrid discrete-continuous optimization, and then improving that solution using bundle adjustment, and shows that it can produce models that are similar to or better than those produced with incremental bundles adjustment, but more robustly and in a fraction of the time."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3138955"
                        ],
                        "name": "R. Neelamani",
                        "slug": "R.-Neelamani",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Neelamani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neelamani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2942047"
                        ],
                        "name": "Hyeokho Choi",
                        "slug": "Hyeokho-Choi",
                        "structuredName": {
                            "firstName": "Hyeokho",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeokho Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144908066"
                        ],
                        "name": "Richard Baraniuk",
                        "slug": "Richard-Baraniuk",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Baraniuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Baraniuk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 15087667,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "ce3cddcfcf19797b323ba2800b5c1c395b07b6cd",
            "isKey": false,
            "numCitedBy": 473,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an efficient, hybrid Fourier-wavelet regularized deconvolution (ForWaRD) algorithm that performs noise regularization via scalar shrinkage in both the Fourier and wavelet domains. The Fourier shrinkage exploits the Fourier transform's economical representation of the colored noise inherent in deconvolution, whereas the wavelet shrinkage exploits the wavelet domain's economical representation of piecewise smooth signals and images. We derive the optimal balance between the amount of Fourier and wavelet regularization by optimizing an approximate mean-squared error (MSE) metric and find that signals with more economical wavelet representations require less Fourier shrinkage. ForWaRD is applicable to all ill-conditioned deconvolution problems, unlike the purely wavelet-based wavelet-vaguelette deconvolution (WVD); moreover, its estimate features minimal ringing, unlike the purely Fourier-based Wiener deconvolution. Even in problems for which the WVD was designed, we prove that ForWaRD's MSE decays with the optimal WVD rate as the number of samples increases. Further, we demonstrate that over a wide range of practical sample-lengths, ForWaRD improves on WVD's performance."
            },
            "slug": "ForWaRD:-Fourier-wavelet-regularized-deconvolution-Neelamani-Choi",
            "title": {
                "fragments": [],
                "text": "ForWaRD: Fourier-wavelet regularized deconvolution for ill-conditioned systems"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "An efficient, hybrid Fourier-wavelet regularized deconvolution (ForWaRD) algorithm that performs noise regularization via scalar shrinkage in both the Fourier and wavelet domains is proposed and it is found that signals with more economical wavelet representations require less Fourier shrinkage."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": false,
            "numCitedBy": 30942,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116722617"
                        ],
                        "name": "Wei Tang",
                        "slug": "Wei-Tang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11955007"
                        ],
                        "name": "Zhengdong Lu",
                        "slug": "Zhengdong-Lu",
                        "structuredName": {
                            "firstName": "Zhengdong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengdong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783667"
                        ],
                        "name": "I. Dhillon",
                        "slug": "I.-Dhillon",
                        "structuredName": {
                            "firstName": "Inderjit",
                            "lastName": "Dhillon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dhillon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1608993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52c969f7acd3d4ff6257e9551ceb876b8525d41b",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In graph-based learning models, entities are often represented as vertices in an undirected graph with weighted edges describing the relationships between entities. In many real-world applications, however, entities are often associated with relations of different types and/or from different sources, which can be well captured by multiple undirected graphs over the same set of vertices. How to exploit such multiple sources of information to make better inferences on entities remains an interesting open problem. In this paper, we focus on the problem of clustering the vertices based on multiple graphs in both unsupervised and semi-supervised settings. As one of our contributions, we propose Linked Matrix Factorization (LMF) as a novel way of fusing information from multiple graph sources. In LMF, each graph is approximated by matrix factorization with a graph-specific factor and a factor common to all graphs, where the common factor provides features for all vertices. Experiments on SIAM journal data show that (1) we can improve the clustering accuracy through fusing multiple sources of information with several models, and (2) LMF yields superior or competitive results compared to other graph-based clustering methods."
            },
            "slug": "Clustering-with-Multiple-Graphs-Tang-Lu",
            "title": {
                "fragments": [],
                "text": "Clustering with Multiple Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments on SIAM journal data show that LMF can improve the clustering accuracy through fusing multiple sources of information with several models, and LMF yields superior or competitive results compared to other graph-based clustering methods."
            },
            "venue": {
                "fragments": [],
                "text": "2009 Ninth IEEE International Conference on Data Mining"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750012"
                        ],
                        "name": "R. Hartley",
                        "slug": "R.-Hartley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hartley",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hartley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8979544,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e409ee3d6df4f55597d5efe44676cb77f6c599d2",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses the basic role of the trifocal tensor in scene reconstruction from three views. This 3\u00d7 3\u00d7 3 tensor plays a role in the analysis of scenes from three views analogous to the role played by the fundamental matrix in the two-view case. In particular, the trifocal tensor may be computed by a linear algorithm from a set of 13 line correspondences in three views. It is further shown in this paper, that the trifocal tensor is essentially identical to a set of coefficients introduced by Shashua to effect point transfer in the three view case. This observation means that the 13-line algorithm may be extended to allow for the computation of the trifocal tensor given any mixture of sufficiently many line and point correspondences. From the trifocal tensor the camera matrices of the images may be computed, and the scene may be reconstructed. For unrelated uncalibrated cameras, this reconstruction will be unique up to projectivity. Thus, projective reconstruction of a set of lines and points may be carried out linearly from three views."
            },
            "slug": "Lines-and-Points-in-Three-Views-and-the-Trifocal-Hartley",
            "title": {
                "fragments": [],
                "text": "Lines and Points in Three Views and the Trifocal Tensor"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown in this paper, that the trifocal tensor is essentially identical to a set of coefficients introduced by Shashua to effect point transfer in the three view case, which means that the 13-line algorithm may be extended to allow for the computation of the Trifocal Tensor given any mixture of sufficiently many line and point correspondences."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5684350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bdab89bcf6571108454ca6fdb1504791f2905e4",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A problem in uncalibrated stereo reconstruction is that cameras which deviate from the pinhole model have to be pre-calibrated in order to correct for nonlinear lens distortion. If they are not, and point correspondence is attempted using the uncorrected images, the matching constraints provided by the fundamental matrix must be set so loose that point matching is significantly hampered. This paper shows how linear estimation of the fundamental matrix from two-view point correspondences may be augmented to include one term of radial lens distortion. This is achieved by (1) changing from the standard radial-lens model to another which (as we show) has equivalent power, but which takes a simpler form in homogeneous coordinates, and (2) expressing fundamental matrix estimation as a quadratic eigenvalue problem (QEP), for which efficient algorithms are well known. I derive the new estimator, and compare its performance against bundle-adjusted calibration-grid data. The new estimator is fast enough to be included in a RANSAC-based matching loop, and we show cases of matching being rendered possible by its use. I show how the same lens can be calibrated in a natural scene where the lack of straight lines precludes most previous techniques. The modification when the multi-view relation is a planar homography or trifocal tensor is described."
            },
            "slug": "Simultaneous-linear-estimation-of-multiple-view-and-Fitzgibbon",
            "title": {
                "fragments": [],
                "text": "Simultaneous linear estimation of multiple view geometry and lens distortion"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows how linear estimation of the fundamental matrix from two-view point correspondences may be augmented to include one term of radial lens distortion, by expressing fundamental matrix estimation as a quadratic eigenvalue problem (QEP), for which efficient algorithms are well known."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5815325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "755e566d3f10d057bc9e4908e4016ae6f7ca0753",
            "isKey": false,
            "numCitedBy": 771,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the common problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed-form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low-rank representation, and extend the formulation to non-Gaussian noise models such as logistic models. Finally, we apply the methods developed to a collaborative filtering task."
            },
            "slug": "Weighted-Low-Rank-Approximations-Srebro-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Weighted Low-Rank Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work provides a simple and efficient algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed-form solution in general."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3037691"
                        ],
                        "name": "A. Johnson",
                        "slug": "A.-Johnson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Johnson",
                            "middleNames": [
                                "Edie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1377132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df6c0c55864252090b4099237aa821a6c75b52c2",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a 3D shape-based object recognition system for simultaneous recognition of multiple objects in scenes containing clutter and occlusion. Recognition is based on matching surfaces by matching points using the spin image representation. The spin image is a data level shape descriptor that is used to match surfaces represented as surface meshes. We present a compression scheme for spin images that results in efficient multiple object recognition which we verify with results showing the simultaneous recognition of multiple objects from a library of 20 models. Furthermore, we demonstrate the robust performance of recognition in the presence of clutter and occlusion through analysis of recognition trials on 100 scenes."
            },
            "slug": "Using-Spin-Images-for-Efficient-Object-Recognition-Johnson-Hebert",
            "title": {
                "fragments": [],
                "text": "Using Spin Images for Efficient Object Recognition in Cluttered 3D Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A compression scheme for spin images that results in efficient multiple object recognition which is verified with results showing the simultaneous recognition of multiple objects from a library of 20 models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795365"
                        ],
                        "name": "D. Lashkari",
                        "slug": "D.-Lashkari",
                        "structuredName": {
                            "firstName": "Danial",
                            "lastName": "Lashkari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lashkari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729630"
                        ],
                        "name": "P. Golland",
                        "slug": "P.-Golland",
                        "structuredName": {
                            "firstName": "Polina",
                            "lastName": "Golland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Golland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 140296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6e91fc3550d7cd2337f031a571553e25c6a930f",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering is often formulated as the maximum likelihood estimation of a mixture model that explains the data. The EM algorithm widely used to solve the resulting optimization problem is inherently a gradient-descent method and is sensitive to initialization. The resulting solution is a local optimum in the neighborhood of the initial guess. This sensitivity to initialization presents a significant challenge in clustering large data sets into many clusters. In this paper, we present a different approach to approximate mixture fitting for clustering. We introduce an exemplar-based likelihood function that approximates the exact likelihood. This formulation leads to a convex minimization problem and an efficient algorithm with guaranteed convergence to the globally optimal solution. The resulting clustering can be thought of as a probabilistic mapping of the data points to the set of exemplars that minimizes the average distance and the information-theoretic cost of mapping. We present experimental results illustrating the performance of our algorithm and its comparison with the conventional approach to mixture model clustering."
            },
            "slug": "Convex-Clustering-with-Exemplar-Based-Models-Lashkari-Golland",
            "title": {
                "fragments": [],
                "text": "Convex Clustering with Exemplar-Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper introduces an exemplar-based likelihood function that approximates the exact likelihood of a mixture model clustering and presents experimental results illustrating the performance of the algorithm and its comparison with the conventional approach to mixturemodel clustering."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3068758"
                        ],
                        "name": "T. Blumensath",
                        "slug": "T.-Blumensath",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Blumensath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Blumensath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144113976"
                        ],
                        "name": "M. Davies",
                        "slug": "M.-Davies",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Davies",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Davies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13909610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0da75214f4d1f636355bb0212262c5c5e55b746c",
            "isKey": false,
            "numCitedBy": 1099,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse signal expansions represent or approximate a signal using a small number of elements from a large collection of elementary waveforms. Finding the optimal sparse expansion is known to be NP hard in general and non-optimal strategies such as Matching Pursuit, Orthogonal Matching Pursuit, Basis Pursuit and Basis Pursuit De-noising are often called upon. These methods show good performance in practical situations, however, they do not operate on the \u21130 penalised cost functions that are often at the heart of the problem. In this paper we study two iterative algorithms that are minimising the cost functions of interest. Furthermore, each iteration of these strategies has computational complexity similar to a Matching Pursuit iteration, making the methods applicable to many real world problems. However, the optimisation problem is non-convex and the strategies are only guaranteed to find local solutions, so good initialisation becomes paramount. We here study two approaches. The first approach uses the proposed algorithms to refine the solutions found with other methods, replacing the typically used conjugate gradient solver. The second strategy adapts the algorithms and we show on one example that this adaptation can be used to achieve results that lie between those obtained with Matching Pursuit and those found with Orthogonal Matching Pursuit, while retaining the computational complexity of the Matching Pursuit algorithm."
            },
            "slug": "Iterative-Thresholding-for-Sparse-Approximations-Blumensath-Davies",
            "title": {
                "fragments": [],
                "text": "Iterative Thresholding for Sparse Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper studies two iterative algorithms that are minimising the cost functions of interest and adapts the algorithms and shows on one example that this adaptation can be used to achieve results that lie between those obtained with Matching Pursuit and those found with Orthogonal Matching pursuit, while retaining the computational complexity of the Matching pursuit algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805301"
                        ],
                        "name": "Z. Kukelova",
                        "slug": "Z.-Kukelova",
                        "structuredName": {
                            "firstName": "Zuzana",
                            "lastName": "Kukelova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Kukelova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872189"
                        ],
                        "name": "Martin Byr\u00f6d",
                        "slug": "Martin-Byr\u00f6d",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Byr\u00f6d",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Byr\u00f6d"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2659604"
                        ],
                        "name": "Klas Josephson",
                        "slug": "Klas-Josephson",
                        "structuredName": {
                            "firstName": "Klas",
                            "lastName": "Josephson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klas Josephson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758039"
                        ],
                        "name": "T. Pajdla",
                        "slug": "T.-Pajdla",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Pajdla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pajdla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725762"
                        ],
                        "name": "Kalle \u00c5str\u00f6m",
                        "slug": "Kalle-\u00c5str\u00f6m",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "\u00c5str\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kalle \u00c5str\u00f6m"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52372,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e931d257ac4d1088fc0569284526c487095a406",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-and-robust-numerical-solutions-to-minimal-for-Kukelova-Byr\u00f6d",
            "title": {
                "fragments": [],
                "text": "Fast and robust numerical solutions to minimal problems for cameras with radial distortion"
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730844"
                        ],
                        "name": "Atousa Torabi",
                        "slug": "Atousa-Torabi",
                        "structuredName": {
                            "firstName": "Atousa",
                            "lastName": "Torabi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Atousa Torabi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084380984"
                        ],
                        "name": "Guillaume Mass\u00e9",
                        "slug": "Guillaume-Mass\u00e9",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Mass\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Mass\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705256"
                        ],
                        "name": "Guillaume-Alexandre Bilodeau",
                        "slug": "Guillaume-Alexandre-Bilodeau",
                        "structuredName": {
                            "firstName": "Guillaume-Alexandre",
                            "lastName": "Bilodeau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume-Alexandre Bilodeau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16121757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d867d7c8cfefe0f5a297a3c613ae6d79c851f4b9",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-iterative-integrated-framework-for-image-sensor-Torabi-Mass\u00e9",
            "title": {
                "fragments": [],
                "text": "An iterative integrated framework for thermal-visible image registration, sensor fusion, and people tracking for video surveillance applications"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47126776"
                        ],
                        "name": "Ehsan Elhamifar",
                        "slug": "Ehsan-Elhamifar",
                        "structuredName": {
                            "firstName": "Ehsan",
                            "lastName": "Elhamifar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehsan Elhamifar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144020730"
                        ],
                        "name": "R. Vidal",
                        "slug": "R.-Vidal",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Vidal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vidal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10102189,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b19d27e1bda4d039f9eaf2b2eb9d47f75abe9d75",
            "isKey": false,
            "numCitedBy": 1500,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world problems deal with collections of high-dimensional data, such as images, videos, text, and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called sparse subspace clustering, to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm is efficient and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise, sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering."
            },
            "slug": "Sparse-Subspace-Clustering:-Algorithm,-Theory,-and-Elhamifar-Vidal",
            "title": {
                "fragments": [],
                "text": "Sparse Subspace Clustering: Algorithm, Theory, and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes and studies an algorithm, called sparse subspace clustering, to cluster data points that lie in a union of low-dimensional subspaces, and demonstrates the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725054"
                        ],
                        "name": "I. Oikonomidis",
                        "slug": "I.-Oikonomidis",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Oikonomidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Oikonomidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731447"
                        ],
                        "name": "Nikolaos Kyriazis",
                        "slug": "Nikolaos-Kyriazis",
                        "structuredName": {
                            "firstName": "Nikolaos",
                            "lastName": "Kyriazis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolaos Kyriazis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689415"
                        ],
                        "name": "Antonis A. Argyros",
                        "slug": "Antonis-A.-Argyros",
                        "structuredName": {
                            "firstName": "Antonis",
                            "lastName": "Argyros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonis A. Argyros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8677556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfbdb860f10bd0fbd39ddff0999a32ee4f4a684a",
            "isKey": false,
            "numCitedBy": 960,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel solution to the problem of recovering and tracking the 3D position, orientation and full articulation of a human hand from markerless visual observations obtained by a Kinect sensor. We treat this as an optimization problem, seeking for the hand model parameters that minimize the discrepancy between the appearance and 3D structure of hypothesized instances of a hand model and actual hand observations. This optimization problem is effectively solved using a variant of Particle Swarm Optimization (PSO). The proposed method does not require special markers and/or a complex image acquisition setup. Being model based, it provides continuous solutions to the problem of tracking hand articulations. Extensive experiments with a prototype GPU-based implementation of the proposed method demonstrate that accurate and robust 3D tracking of hand articulations can be achieved in near real-time (15Hz)."
            },
            "slug": "Efficient-model-based-3D-tracking-of-hand-using-Oikonomidis-Kyriazis",
            "title": {
                "fragments": [],
                "text": "Efficient model-based 3D tracking of hand articulations using Kinect"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A novel solution to the problem of recovering and tracking the 3D position, orientation and full articulation of a human hand from markerless visual observations obtained by a Kinect sensor is presented."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750012"
                        ],
                        "name": "R. Hartley",
                        "slug": "R.-Hartley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hartley",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hartley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9560939,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "80e4a9ba9d583c291b5db8c9549364757b092828",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper gives a practical rapid algorithm for doing projective reconstruction of a scene consisting of a set of lines seen in three or more images with uncalibrated cameras. The algorithm is evaluated on real and ideal data to determine its performance in the presence of varying degrees of noise. By carefully consideration of sources of error, it is possible to get accurate reconstruction with realistic levels of noise. The algorithm can be applied to images from different cameras or the same camera. For images with the same camera with unknown calibration, it is possible to do a complete Euclidean reconstruction of the image. This extends to the case of uncalibrated cameras previous results on scene reconstruction from lines,.<<ETX>>"
            },
            "slug": "Projective-reconstruction-from-line-correspondences-Hartley",
            "title": {
                "fragments": [],
                "text": "Projective reconstruction from line correspondences"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A practical rapid algorithm for doing projective reconstruction of a scene consisting of a set of lines seen in three or more images with uncalibrated cameras, which can be applied to images from different cameras or the same camera."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211659"
                        ],
                        "name": "Jason D. M. Rennie",
                        "slug": "Jason-D.-M.-Rennie",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Rennie",
                            "middleNames": [
                                "D.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. M. Rennie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5048382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf154c28178370d95510112413dc8cb48120a8",
            "isKey": false,
            "numCitedBy": 1105,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them."
            },
            "slug": "Maximum-Margin-Matrix-Factorization-Srebro-Rennie",
            "title": {
                "fragments": [],
                "text": "Maximum-Margin Matrix Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A novel approach to collaborative prediction is presented, using low-norm instead of low-rank factorizations, inspired by, and has strong connections to, large-margin linear discrimination."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16162039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "isKey": false,
            "numCitedBy": 36470,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described."
            },
            "slug": "Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Regression Shrinkage and Selection via the Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2495855"
                        ],
                        "name": "Fengguang Song",
                        "slug": "Fengguang-Song",
                        "structuredName": {
                            "firstName": "Fengguang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengguang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708869"
                        ],
                        "name": "J. Dongarra",
                        "slug": "J.-Dongarra",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Dongarra",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dongarra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 228
                            }
                        ],
                        "text": "The gap is widened yet further by the use of steadily improving and highly tuned numerical libraries that allow for extremely fast dense matrix multiplication, exploiting the minute details of the underlying CPU or GPU hardware [16, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8741519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74fa03ea483393e7c1b305b023b6161d449ec052",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "While the growing number of cores per chip allows researchers to solve larger scientific and engineering problems, the parallel efficiency of the deployed parallel software starts to decrease. This unscalability problem happens to both vendor-provided and open-source software and wastes CPU cycles and energy. By expecting CPUs with hundreds of cores to be imminent, we have designed a new framework to perform matrix computations for massively many cores. Our performance analysis on manycore systems shows that the unscalability bottleneck is related to Non-Uniform Memory Access (NUMA): memory bus contention and remote memory access latency. To overcome the bottleneck, we have designed NUMA-aware tile algorithms with the help of a dynamic scheduling runtime system to minimize NUMA memory accesses. The main idea is to identify the data that is, either read a number of times or written once by a thread resident on a remote NUMA node, then utilize the runtime system to conduct data caching and movement between different NUMA nodes. Based on the experiments with QR factorizations, we demonstrate that our framework is able to achieve great scalability on a 48-core AMD Opteron system (e.g., parallel efficiency drops only 3% from one core to 48 cores). We also deploy our framework to an extreme-scale shared-memory SGI machine which has 1024 CPU cores and runs a single Linux operating system image. Our framework continues to scale well, and can outperform the vendor-optimized Intel MKL library by up to 750%."
            },
            "slug": "Scaling-up-matrix-computations-on-shared-memory-CPU-Song-Dongarra",
            "title": {
                "fragments": [],
                "text": "Scaling up matrix computations on shared-memory manycore systems with 1000 CPU cores"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A new framework to perform matrix computations for massively many cores with great scalability on a 48-core AMD Opteron system, and can outperform the vendor-optimized Intel MKL library by up to 750%."
            },
            "venue": {
                "fragments": [],
                "text": "ICS '14"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13252401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcee7c85d237b79491a773ef51e746bbbcf48e35",
            "isKey": false,
            "numCitedBy": 13488,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions."
            },
            "slug": "Induction-of-Decision-Trees-Quinlan",
            "title": {
                "fragments": [],
                "text": "Induction of Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail, which is described in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710813"
                        ],
                        "name": "\u00dcmit V. \u00c7ataly\u00fcrek",
                        "slug": "\u00dcmit-V.-\u00c7ataly\u00fcrek",
                        "structuredName": {
                            "firstName": "\u00dcmit",
                            "lastName": "\u00c7ataly\u00fcrek",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00dcmit V. \u00c7ataly\u00fcrek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731638"
                        ],
                        "name": "C. Aykanat",
                        "slug": "C.-Aykanat",
                        "structuredName": {
                            "firstName": "Cevdet",
                            "lastName": "Aykanat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Aykanat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145674689"
                        ],
                        "name": "B. U\u00e7ar",
                        "slug": "B.-U\u00e7ar",
                        "structuredName": {
                            "firstName": "Bora",
                            "lastName": "U\u00e7ar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. U\u00e7ar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12266615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94968ac5c8b2a369aede6f6645c5f76e793aad58",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two-dimensional partitioning of general sparse matrices for parallel sparse matrix-vector multiply operation. We present three hypergraph-partitioning-based methods, each having unique advantages. The first one treats the nonzeros of the matrix individually and hence produces fine-grain partitions. The other two produce coarser partitions, where one of them imposes a limit on the number of messages sent and received by a single processor, and the other trades that limit for a lower communication volume. We also present a thorough experimental evaluation of the proposed two-dimensional partitioning methods together with the hypergraph-based one-dimensional partitioning methods, using an extensive set of public domain matrices. Furthermore, for the users of these partitioning methods, we present a partitioning recipe that chooses one of the partitioning methods according to some matrix characteristics."
            },
            "slug": "On-Two-Dimensional-Sparse-Matrix-Partitioning:-and-\u00c7ataly\u00fcrek-Aykanat",
            "title": {
                "fragments": [],
                "text": "On Two-Dimensional Sparse Matrix Partitioning: Models, Methods, and a Recipe"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work considers two-dimensional partitioning of general sparse matrices for parallel sparse matrix-vector multiply operation and presents three hypergraph-partitioning-based methods, each having unique advantages."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732893"
                        ],
                        "name": "M. Spetsakis",
                        "slug": "M.-Spetsakis",
                        "structuredName": {
                            "firstName": "Minas",
                            "lastName": "Spetsakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Spetsakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697493"
                        ],
                        "name": "Y. Aloimonos",
                        "slug": "Y.-Aloimonos",
                        "structuredName": {
                            "firstName": "Yiannis",
                            "lastName": "Aloimonos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aloimonos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34955132,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "a3b2a1b3a3364c6ff082e3a87e354c79ccd4334e",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A theory is presented for the computation of three-dimensional motion and structure from dynamic imagery, using only line correspondences. The traditional approach of corresponding microfeatures (interesting points-highlights, corners, high-curvature points, etc.) is reviewed and its shortcomings are discussed. Then, a theory is presented that describes a closed form solution to the motion and structure determination problem from line correspondences in three views. The theory is compared with previous ones that are based on nonlinear equations and iterative methods."
            },
            "slug": "Structure-from-motion-using-line-correspondences-Spetsakis-Aloimonos",
            "title": {
                "fragments": [],
                "text": "Structure from motion using line correspondences"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A theory is presented that describes a closed form solution to the motion and structure determination problem from line correspondences in three views, compared with previous ones that are based on nonlinear equations and iterative methods."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814637"
                        ],
                        "name": "Berthold K. P. Horn",
                        "slug": "Berthold-K.-P.-Horn",
                        "structuredName": {
                            "firstName": "Berthold",
                            "lastName": "Horn",
                            "middleNames": [
                                "K.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berthold K. P. Horn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11038004,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2ee07f241c725023494b2a9a61b00a409e5b9708",
            "isKey": false,
            "numCitedBy": 4286,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task . It finds applications i n stereoph and in robotics . I present here a closed-form solution to the least-squares problem for three or more paints . Currently various empirical, graphical, and numerical iterative methods are in use . Derivation of the solution i s simplified by use of unit quaternions to represent rotation . I emphasize a symmetry property that a solution to thi s problem ought to possess . The best translational offset is the difference between the centroid of the coordinates i n one system and the rotated and scaled centroid of the coordinates in the other system . The best scale is equal to th e ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids . These exact results are to be preferred to approximate methods based on measurements of a few selected points . The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue o f a symmetric 4 X 4 matrix . The elements of this matrix are combinations of sums of products of correspondin g coordinates of the points ."
            },
            "slug": "Closed-form-solution-of-absolute-orientation-using-Horn",
            "title": {
                "fragments": [],
                "text": "Closed-form solution of absolute orientation using unit quaternions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A closed-form solution to the least-squares problem for three or more paints is presented, simplified by use of unit quaternions to represent rotation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708497"
                        ],
                        "name": "A. Gretton",
                        "slug": "A.-Gretton",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Gretton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gretton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704422"
                        ],
                        "name": "K. Borgwardt",
                        "slug": "K.-Borgwardt",
                        "structuredName": {
                            "firstName": "Karsten",
                            "lastName": "Borgwardt",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Borgwardt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733256"
                        ],
                        "name": "M. Rasch",
                        "slug": "M.-Rasch",
                        "structuredName": {
                            "firstName": "Malte",
                            "lastName": "Rasch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rasch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1993257,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9bca4d7b932e0854c3325f1578cfd17341dd8ea8",
            "isKey": false,
            "numCitedBy": 1547,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two statistical tests to determine if two samples are from different distributions. Our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel Hilbert space (RKHS). The first test is based on a large deviation bound for the test statistic, while the second is based on the asymptotic distribution of this statistic. The test statistic can be computed in O(m2) time. We apply our approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where our test performs strongly. We also demonstrate excellent performance when comparing distributions over graphs, for which no alternative tests currently exist."
            },
            "slug": "A-Kernel-Method-for-the-Two-Sample-Problem-Gretton-Borgwardt",
            "title": {
                "fragments": [],
                "text": "A Kernel Method for the Two-Sample-Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work proposes two statistical tests to determine if two samples are from different distributions, and applies this approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where the test performs strongly."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145926447"
                        ],
                        "name": "J. Weng",
                        "slug": "J.-Weng",
                        "structuredName": {
                            "firstName": "Juyang",
                            "lastName": "Weng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16276,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "69be4b101fcd312b0b2515a6e7bbfd5af377e342",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This work discusses estimating motion and structure parameters from line correspondences of a rigid scene. The authors present a closed-form solution to motion and structure parameters from line correspondences through three monocular perspective views. The algorithm makes use of redundancy in the data to improve the accuracy of the solutions. The uniqueness of the solution is established, and necessary and sufficient conditions for degenerate spatial line configurations are given. Optimization has been employed to further improve the accuracy of the estimates in the presence of noise. Simulations have shown that the errors of the optimized estimates are close to the theoretical lower error bound. >"
            },
            "slug": "Motion-and-Structure-from-Line-Correspondences;-and-Weng-Huang",
            "title": {
                "fragments": [],
                "text": "Motion and Structure from Line Correspondences; Closed-Form Solution, Uniqueness, and Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The authors present a closed-form solution to motion and structure parameters from line correspondences through three monocular perspective views that makes use of redundancy in the data to improve the accuracy of the solutions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060471826"
                        ],
                        "name": "Manish Mehta",
                        "slug": "Manish-Mehta",
                        "structuredName": {
                            "firstName": "Manish",
                            "lastName": "Mehta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manish Mehta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144947410"
                        ],
                        "name": "R. Agrawal",
                        "slug": "R.-Agrawal",
                        "structuredName": {
                            "firstName": "Rakesh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Agrawal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "435384cc048ce88afa4e9cfb51e38297c4e7a255",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the application of the Minimum Description Length principle for pruning decision trees. We present a new algorithm that intuitively captures the primary goal of reducing the misclassification error. An experimental comparison is presented with three other pruning algorithms. The results show that the MDL pruning algorithm achieves good accuracy, small trees, and fast execution times."
            },
            "slug": "MDL-Based-Decision-Tree-Pruning-Mehta-Rissanen",
            "title": {
                "fragments": [],
                "text": "MDL-Based Decision Tree Pruning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new algorithm is presented that intuitively captures the primary goal of reducing the misclassification error and achieves good accuracy, small trees, and fast execution times in the MDL pruning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 104
                            }
                        ],
                        "text": "The results show that our SST tracker achieves favorable performance than other related sparse trackers [2, 6, 7, 9, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "Although we used CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week, the main limitation being the memory usage."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 106
                            }
                        ],
                        "text": "Recently, sparse representation based generative tracking methods have been developed for object tracking [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 217
                            }
                        ],
                        "text": "Indeed, when knowledge about task relatedness is available, it can be profitably incorporated in multi-task learning approaches for example by designing suitable embedding/coding schemes, kernels or regularizers, see [4, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 165
                            }
                        ],
                        "text": "In addition, our method also preserves the spatial layout structure among the local patches inside each target candidate, which is ignored by the above three models [1, 2, 3, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 35
                            }
                        ],
                        "text": "(a) Global sparse appearance model [3, 4, 6, 7, 9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "The fastest optical flow algorithms run in under a second, but rely on GPUs [7], which are not available on all platforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "(a) Input image [7] (b) Confidence map"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Following [7], we adopt the perspective of reproducing kernel Hilbert spaces for vector-valued functions (RKHSvv) to interpret the multiple tasks f1, \u00b7 \u00b7 \u00b7 , fT that we aim to learn as the components of a vector valued predictor f : X \u2192 RT ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "In this setting we observe that, thanks to a generalization of the Representer theorem to the vector-valued case [7], each task ft can be parametrized as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "Furthermore, we observed a significant improvement for challenging outdoor datasets [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": true,
            "numCitedBy": 25497,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678724"
                        ],
                        "name": "A. Verikas",
                        "slug": "A.-Verikas",
                        "structuredName": {
                            "firstName": "Antanas",
                            "lastName": "Verikas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Verikas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2318816"
                        ],
                        "name": "A. Gelzinis",
                        "slug": "A.-Gelzinis",
                        "structuredName": {
                            "firstName": "Adas",
                            "lastName": "Gelzinis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelzinis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2008202"
                        ],
                        "name": "M. Bacauskiene",
                        "slug": "M.-Bacauskiene",
                        "structuredName": {
                            "firstName": "Marija",
                            "lastName": "Bacauskiene",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bacauskiene"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39661348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e9f1435a51530408db91056a59b7a350aa5ca8b",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mining-data-with-random-forests:-A-survey-and-of-Verikas-Gelzinis",
            "title": {
                "fragments": [],
                "text": "Mining data with random forests: A survey and results of new tests"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145611269"
                        ],
                        "name": "A. Lozano",
                        "slug": "A.-Lozano",
                        "structuredName": {
                            "firstName": "Aur\u00e9lie",
                            "lastName": "Lozano",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lozano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17148395,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ec2e48627c17b002aa3202d2f8ac426faa6f304b",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider multivariate regression problems involving high-dimensional predictor and response spaces. To efficiently address such problems, we propose a variable selection method, Multivariate Group Orthogonal Matching Pursuit, which extends the standard Orthogonal Matching Pursuit technique. This extension accounts for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables, while also taking advantage of the correlation that may exist between the multiple outputs. Within this framework, we then formulate the problem of inferring causal relationships over a collection of high-dimensional time series variables. When applied to time-evolving social media content, our models yield a new family of causality-based influence measures that may be seen as an alternative to the classic PageRank algorithm traditionally applied to hyperlink graphs. Theoretical guarantees, extensive simulations and empirical studies confirm the generality and value of our framework."
            },
            "slug": "Block-Variable-Selection-in-Multivariate-Regression-Lozano-Sindhwani",
            "title": {
                "fragments": [],
                "text": "Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "When applied to time-evolving social media content, these models yield a new family of causality-based influence measures that may be seen as an alternative to the classic PageRank algorithm traditionally applied to hyperlink graphs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2007419"
                        ],
                        "name": "M. Bradley",
                        "slug": "M.-Bradley",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Bradley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bradley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143853826"
                        ],
                        "name": "P. Lang",
                        "slug": "P.-Lang",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17630161,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9576d1573688a05e45ad30e783e7d0c10a10d6d9",
            "isKey": false,
            "numCitedBy": 6890,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Measuring-emotion:-the-Self-Assessment-Manikin-and-Bradley-Lang",
            "title": {
                "fragments": [],
                "text": "Measuring emotion: the Self-Assessment Manikin and the Semantic Differential."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of behavior therapy and experimental psychiatry"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11079833"
                        ],
                        "name": "S. Sutherland",
                        "slug": "S.-Sutherland",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Sutherland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sutherland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35236366,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "ef46c51a9f9db65311accbfa5405b1d0dc280f93",
            "isKey": false,
            "numCitedBy": 444,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Vision and Visual Dysfunction.General editor: John Cronly-Dillon. Macmillan: 1991. 17 volumes. Approximately 5,000 pages. \u00a31,250, $2,295."
            },
            "slug": "Eye,-brain-and-vision-Sutherland",
            "title": {
                "fragments": [],
                "text": "Eye, brain and vision"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703847"
                        ],
                        "name": "Boris Polyak",
                        "slug": "Boris-Polyak",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Polyak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Polyak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754887"
                        ],
                        "name": "A. Juditsky",
                        "slug": "A.-Juditsky",
                        "structuredName": {
                            "firstName": "Anatoli",
                            "lastName": "Juditsky",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Juditsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Polyak averaging [13] was used to create the final model used at inference time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3548228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dc61f37ecc552413606d8c89ffbc46ec98ed887",
            "isKey": false,
            "numCitedBy": 1535,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence."
            },
            "slug": "Acceleration-of-stochastic-approximation-by-Polyak-Juditsky",
            "title": {
                "fragments": [],
                "text": "Acceleration of stochastic approximation by averaging"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Convergence with probability one is proved for a variety of classical optimization and identification problems and it is demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 104
                            }
                        ],
                        "text": "The results show that our SST tracker achieves favorable performance than other related sparse trackers [2, 6, 7, 9, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 106
                            }
                        ],
                        "text": "Recently, sparse representation based generative tracking methods have been developed for object tracking [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 32
                            }
                        ],
                        "text": "choice for image classification [6, 9, 10], while a host of other works have"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "Global methods [6, 9] accumulate votes via a Hough transform and then select the pose with the largest number of votes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 47
                            }
                        ],
                        "text": "Figure 1: Sparse representation based trackers [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 30
                            }
                        ],
                        "text": "CNNs also have a long history [6, 9, 10], and have resurged over the last two years due to good performance on image classification [8], object detection [7], and more recently a wide variety of vision tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 165
                            }
                        ],
                        "text": "In addition, our method also preserves the spatial layout structure among the local patches inside each target candidate, which is ignored by the above three models [1, 2, 3, 5, 6, 7, 9, 10, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 35
                            }
                        ],
                        "text": "(a) Global sparse appearance model [3, 4, 6, 7, 9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] Oliver Woodford, Minh-Tri Pham, Atsuto Maki, Frank Perbet, and Bjorn Stenger."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "67% for the Middlebury dataset [9] in average."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Imagenet large scale visual recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335378"
                        ],
                        "name": "H. Hirschm\u00fcller",
                        "slug": "H.-Hirschm\u00fcller",
                        "structuredName": {
                            "firstName": "Heiko",
                            "lastName": "Hirschm\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hirschm\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18327083,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "04433840795f9add85ac60e98cf4a4b5a14caca1",
            "isKey": false,
            "numCitedBy": 3120,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Semi-Global Matching (SGM) stereo method. It uses a pixelwise, Mutual Information based matching cost for compensating radiometric differences of input images. Pixelwise matching is supported by a smoothness constraint that is usually expressed as a global cost function. SGM performs a fast approximation by pathwise optimizations from all directions. The discussion also addresses occlusion detection, subpixel refinement and multi-baseline matching. Additionally, postprocessing steps for removing outliers, recovering from specific problems of structured environments and the interpolation of gaps are presented. Finally, strategies for processing almost arbitrarily large images and fusion of disparity images using orthographic projection are proposed.A comparison on standard stereo images shows that SGM is among the currently top-ranked algorithms and is best, if subpixel accuracy is considered. The complexity is linear to the number of pixels and disparity range, which results in a runtime of just 1-2s on typical test images. An in depth evaluation of the Mutual Information based matching cost demonstrates a tolerance against a wide range of radiometric transformations. Finally, examples of reconstructions from huge aerial frame and pushbroom images demonstrate that the presented ideas are working well on practical problems."
            },
            "slug": "Stereo-Processing-by-Semiglobal-Matching-and-Mutual-Hirschm\u00fcller",
            "title": {
                "fragments": [],
                "text": "Stereo Processing by Semiglobal Matching and Mutual Information"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper describes the Semi-Global Matching (SGM) stereo method, which uses a pixelwise, Mutual Information based matching cost for compensating radiometric differences of input images and demonstrates a tolerance against a wide range of radiometric transformations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144947410"
                        ],
                        "name": "R. Agrawal",
                        "slug": "R.-Agrawal",
                        "structuredName": {
                            "firstName": "Rakesh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34641476"
                        ],
                        "name": "R. Srikant",
                        "slug": "R.-Srikant",
                        "structuredName": {
                            "firstName": "Ramakrishnan",
                            "lastName": "Srikant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srikant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3131928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5288d14f6a3937df5e10109d4e23d79b7ddf080f",
            "isKey": false,
            "numCitedBy": 10557,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-Algorithms-for-Mining-Association-Rules-in-Agrawal-Srikant",
            "title": {
                "fragments": [],
                "text": "Fast Algorithms for Mining Association Rules in Large Databases"
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725666"
                        ],
                        "name": "L. Tun\u00e7el",
                        "slug": "L.-Tun\u00e7el",
                        "structuredName": {
                            "firstName": "Levent",
                            "lastName": "Tun\u00e7el",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tun\u00e7el"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6103282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7b4c27abb76dff4c017849049541f3fc91e77be",
            "isKey": false,
            "numCitedBy": 1348,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimization-algorithms-on-matrix-manifolds-Tun\u00e7el",
            "title": {
                "fragments": [],
                "text": "Optimization algorithms on matrix manifolds"
            },
            "venue": {
                "fragments": [],
                "text": "Math. Comput."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Howard . Some improvements on deep convolutional neural network based image classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A dataset of 101 human action classes from videos in the wild"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SVMs for multiple-instance learning"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random forests. Machine learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient hierarchical graphbased video segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visualizing and understanding convolutional networks Tom\u00e1s Pajdla, Bernt Schiele, and Tinne Tuytelaars"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision -ECCV 2014 -13th European Conference Proceedings , Part I"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ilya Sutskever, and Ruslan Salakhutdinov . Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207"
            },
            "venue": {
                "fragments": [],
                "text": "Ilya Sutskever, and Ruslan Salakhutdinov . Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Network in network. CoRR, abs/1312"
            },
            "venue": {
                "fragments": [],
                "text": "Network in network. CoRR, abs/1312"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Zeiler and Rob Fergus . Visualizing and understanding convolutional networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Know your meme: We need to go deeper. http://knowyourmeme.com/memes/ we-need-to-go-deeper"
            },
            "venue": {
                "fragments": [],
                "text": "Know your meme: We need to go deeper. http://knowyourmeme.com/memes/ we-need-to-go-deeper"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "//knowyourmeme.com/memes/we-need-to-go-deeper"
            },
            "venue": {
                "fragments": [],
                "text": "//knowyourmeme.com/memes/we-need-to-go-deeper"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eyal Krupka, Andrew Fitzgibbon, and Shahram Izadi. Accurate, robust, and flexible realtime hand tracking"
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "agenet classification with deep convolutional neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised discovery of midlevel discriminative patches"
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Alexander Toshev, and Dragomir Anguelov. Scalable object detection using deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Oded Maron and Tom\u00c3\u0105s Lozano-P\u00c3l'rez. A framework for multipleinstance learning"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1] Know your meme: We need to go deeper"
            },
            "venue": {
                "fragments": [],
                "text": "1] Know your meme: We need to go deeper"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Know your meme: We need to go deeper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blended intrinsic maps. TOG"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "http://knowyourmeme.com/memes/we-need-to-go-deeper"
            },
            "venue": {
                "fragments": [],
                "text": "http://knowyourmeme.com/memes/we-need-to-go-deeper"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 28,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 278,
        "totalPages": 28
    },
    "page_url": "https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327?sort=total-citations"
}