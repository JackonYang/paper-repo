{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1952749"
                        ],
                        "name": "K. Kukich",
                        "slug": "K.-Kukich",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Kukich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kukich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5431215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2044ca37a948fc34ea1f3f87e9090ec8bda4a33",
            "isKey": false,
            "numCitedBy": 1344,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "Research aimed at correcting words in text has focused on three progressively more difficult problems:(1) nonword error detection; (2) isolated-word error correction; and (3) context-dependent work correction. In response to the first problem, efficient pattern-matching and n-gram analysis techniques have been developed for detecting strings that do not appear in a given word list. In response to the second problem, a variety of general and application-specific spelling correction techniques have been developed. Some of them were based on detailed studies of spelling error patterns. In response to the third problem, a few experiments using natural-language-processing tools or statistical-language models have been carried out. This article surveys documented findings on spelling error patterns, provides descriptions of various nonword detection and isolated-word error correction techniques, reviews the state of the art of context-dependent word correction techniques, and discusses research issues related to all three areas of automatic error correction in text."
            },
            "slug": "Techniques-for-automatically-correcting-words-in-Kukich",
            "title": {
                "fragments": [],
                "text": "Techniques for automatically correcting words in text"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Research aimed at correcting words in text has focused on three progressively more difficult problems: nonword error detection; (2) isolated-word error correction; and (3) context-dependent work correction, which surveys documented findings on spelling error patterns."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62706675,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8d59ed7bfde36fd640cec30b2537630f3e30ff7",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new program, CORRECT, which takes words rejected by the Unix\u00ae SPELL program, proposes a list of candidate corrections, and sorts them by probability score. The probability scores are the novel contribution of this work. They are based on a noisy channel model. It is assumed that the typist knows what words he or she wants to type but some noise is added on the way to the keyboard (in the form of typos and spelling errors). Using a classic Bayesian argument of the kind that is popular in recognition applications, especially speech recognition (Jelinek, 1985), one can often recover the intended correction,c, from a typo,t, by finding the correctionc that maximizesPr(c) Pr(t/c). The first factor,Pr(c), is a prior model of word probabilities; the second factor,Pr(t/c), is a model of the noisy channel that accounts for spelling transformations on letter sequences (insertions, deletions, substitutions and reversals). Both sets of probabilities were estimated using data collected from the Associated Press (AP) newswire over 1988 and 1989 as a training set. The AP generates about 1 million words and 500 typos per week.In evaluating the program, we found that human judges were extremely reluctant to cast a vote given only the information available to the program, and that they were much more comfortable when they could see a concordance line or two. The second half of this paper discusses some very simple methods of modeling the context usingn-gram statistics. Althoughn-gram methods are much too simple (compared with much more sophisticated methods used in artificial intelligence and natural language processing), we have found that even these very simple methods illustrate some very interesting estimation problems that will almost certainly come up when we consider more sophisticated models of contexts. The problem is how to estimate the probability of a context that we have not seen. We compare several estimation techniques and find that some are useless. Fortunately, we have found that the Good-Turing method provides an estimate of contextual probabilities that produces a significant improvement in program performance. Context is helpful in this application, but only if it is estimated very carefully.At this point, we have a number of different knowledge sources\u2014the prior, the channel and the context\u2014and there will certainly be more in the future. In general, performance will be improved as more and more knowledge sources are added to the system, as long as each additional knowledge source provides some new (independent) information. As we shall see, it is important to think more carefully about combination rules, especially when there are a large number of different knowledge sources."
            },
            "slug": "Probability-scoring-for-spelling-correction-Church-Gale",
            "title": {
                "fragments": [],
                "text": "Probability scoring for spelling correction"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper describes a new program, CORRECT, which takes words rejected by the Unix\u00ae SPELL program, proposes a list of candidate corrections, and sorts them by probability score, and finds that human judges were extremely reluctant to cast a vote given only the information available to the program."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6922975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cb09327e68400bf05e6f373e046a3a08e82510e",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \"eat a peach\" and \"eat a beach\" is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \"most similar\" words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error."
            },
            "slug": "Similarity-Based-Estimation-of-Word-Cooccurrence-Dagan-Pereira",
            "title": {
                "fragments": [],
                "text": "Similarity-Based Estimation of Word Cooccurrence Probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A probabilistic word association model based on distributional word similarity is described, and it is applied to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650619"
                        ],
                        "name": "Andrew R. Golding",
                        "slug": "Andrew-R.-Golding",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Golding",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew R. Golding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 723379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e334b3fed7561c7799674ba49efd9244a27b8fb5",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of correcting spelling errors that result in valid, though unintended words (such as peace and piece, or quiet and quite) and also the problem of correcting particular word usage errors (such as amount and number, or among and between). Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell. First, we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context. This method uses a small number of parameters compared to previous methods based on word trigrams. However, it is effectively unable to distinguish among words that have the same part of speech. For this case, an alternative feature-based method called Bayes performs better; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints. A hybrid method called Tribayes is then introduced that combines the best of the previous two methods. The improvement in performance of Tribayes over its components is verified experimentally. Tribayes is also compared with the grammar checker in Microsoft Word, and is found to have substantially higher performance."
            },
            "slug": "Combining-Trigram-based-and-Feature-based-Methods-Golding-Schabes",
            "title": {
                "fragments": [],
                "text": "Combining Trigram-based and Feature-based Methods for Context-Sensitive Spelling Correction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A hybrid method called Tribayes is introduced that combines the best of the previous two methods based on word trigrams and is found to have substantially higher performance than the grammar checker in Microsoft Word."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7954978"
                        ],
                        "name": "Lon-Mu Liu",
                        "slug": "Lon-Mu-Liu",
                        "structuredName": {
                            "firstName": "Lon-Mu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lon-Mu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3341148"
                        ],
                        "name": "Y. Babad",
                        "slug": "Y.-Babad",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Babad",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Babad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153199403"
                        ],
                        "name": "Wei Sun",
                        "slug": "Wei-Sun",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32132007"
                        ],
                        "name": "K. Chan",
                        "slug": "K.-Chan",
                        "structuredName": {
                            "firstName": "Ki-Kan",
                            "lastName": "Chan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18198020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "237deadeaaac917eb8048156bb54812ab3d94839",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical Character Recognit ion (OCR) is a convenient and eff ic ient tool for off ice automation and information retrieval, and is becoming more and more important in today's off ice and library environment. Depending on the text to be recognized, and the software and hardware employed, OCR software produces various types of errors in the recognized texts. The error types and their distributions are environment-dependent . In this paper, we first provide a classification for the types of errors that occur. An eff icient approach for the post-processing of recognition errors in OCR text is proposed so that errors can be e f f i ciently detected and corrected with the aid of a computer. The approach also allows for the correction of OCR errors to be partially automated. The major contribution of this approach is the capability of knowledge acquisition for OCR postprocessing which facilitates the eff ic ient correction of OCR errors. Through self-learning, the postprocessor is able to perform better and more accurately as processing proceeds. Experimental results are provided to demonstrate the eff ic iency and the effect iveness of this approach."
            },
            "slug": "Adaptive-post-processing-of-OCR-text-via-knowledge-Liu-Babad",
            "title": {
                "fragments": [],
                "text": "Adaptive post-processing of OCR text via knowledge acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach for the post-processing of recognition errors in OCR text is proposed so that errors can be detected and corrected with the aid of a computer and the correction of OCR errors to be partially automated."
            },
            "venue": {
                "fragments": [],
                "text": "CSC '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650619"
                        ],
                        "name": "Andrew R. Golding",
                        "slug": "Andrew-R.-Golding",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Golding",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew R. Golding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3204825,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19f37330057a76d32f32f92b11f42c53fb6c2a87",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical ``atmosphere'' (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated."
            },
            "slug": "A-Bayesian-Hybrid-Method-for-Context-sensitive-Golding",
            "title": {
                "fragments": [],
                "text": "A Bayesian Hybrid Method for Context-sensitive Spelling Correction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction, and finds that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5371566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b84276fe751ca4f1389549281383b151a746107b",
            "isKey": false,
            "numCitedBy": 1140,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nEugene Charniak breaks new ground in artificial intelligence research by presenting statistical language processing from an artificial intelligence point of view in a text for researchers and scientists with a traditional computer science background. \nNew, exacting empirical methods are needed to break the deadlock in such areas of artificial intelligence as robotics, knowledge representation, machine learning, machine translation, and natural language processing (NLP). It is time, Charniak observes, to switch paradigms. This text introduces statistical language processing techniques -- word tagging, parsing with probabilistic context free grammars, grammar induction, syntactic disambiguation, semantic word classes, word-sense disambiguation -- along with the underlying mathematics and chapter exercises. \nCharniak points out that as a method of attacking NLP problems, the statistical approach has several advantages. It is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning: \"one simply gathers statistics.\" \nLanguage, Speech, and Communication"
            },
            "slug": "Statistical-language-learning-Charniak",
            "title": {
                "fragments": [],
                "text": "Statistical language learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Eugene Charniak points out that as a method of attacking NLP problems, the statistical approach has several advantages and is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145793042"
                        ],
                        "name": "R. Wagner",
                        "slug": "R.-Wagner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wagner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wagner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298802"
                        ],
                        "name": "M. Fischer",
                        "slug": "M.-Fischer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fischer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13381535,
            "fieldsOfStudy": [
                "Mathematics",
                "Education",
                "Physics"
            ],
            "id": "455e1168304e0eb2909093d5ab9b5ec85cda5028",
            "isKey": false,
            "numCitedBy": 3190,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of \u201cedit operations\u201d needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings."
            },
            "slug": "The-String-to-String-Correction-Problem-Wagner-Fischer",
            "title": {
                "fragments": [],
                "text": "The String-to-String Correction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm is presented which solves the string-to-string correction problem in time proportional to the product of the lengths of the two strings."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46191146"
                        ],
                        "name": "C. Chapelle",
                        "slug": "C.-Chapelle",
                        "structuredName": {
                            "firstName": "Carol",
                            "lastName": "Chapelle",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chapelle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 144116247,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "91b52959c7731830f1518ed35e5ab1dabadf79ec",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "specifically on the difficulty of recognizing learning versus language difficulties, that is, how to identify a nonnative-speaking child's need for special education services. They propose a model that administrators can employ that minimizes bias. In a similar vein, the next chapter, by Jeffery Braden and Sandra Fradd, suggests ways that administrators can anticipate difficulties and intervene before such referrals are necessary. William Tikunoff, in the eighth chapter, focuses on instructional leadership. He discusses the characteristics of an effective principal and targets specific areas, such as effective time management. The final chapter, by Beatrice Ward, addresses the greatest resource of any educational institution: the teachers. She describes the clinical approach to teacher development and how it can be implemented. Overall, this book fills a need for basic, factual information about legal requirements, program types, and effective instructional and leadership strategies with respect to the LEP population. Furthermore, it provides guidance on the complex issue of special education for LEP students, particularly the referral process. An additional chapter exploring different models for assessment and program design for these students would have provided depth and balance. Although there is necessarily some overlap between the chapters, it is reinforcing, not repetitive. In addition to being extremely useful to administrators, this book would be of value to school personnel such as psychologists, special education consultants, LEP consultants, instructors-in short, for anyone committed to the design and delivery of effective instructional programs for LEP students."
            },
            "slug": "The-Computational-Analysis-of-English\u2014A-Approach-Chapelle",
            "title": {
                "fragments": [],
                "text": "The Computational Analysis of English\u2014A Corpus\u2010Based Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48892185"
                        ],
                        "name": "J. Teahan",
                        "slug": "J.-Teahan",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Teahan",
                            "middleNames": [
                                "K"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Teahan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6633939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d36910319d11359b995ff5413696aa9e9995e163",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "\\A new data structure for cumulative probability tables\". Soft-\\The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression\"."
            },
            "slug": "\\self-organized-Language-Modeling-for-Speech-In-Teahan-Cleary",
            "title": {
                "fragments": [],
                "text": "\\self-organized Language Modeling for Speech Recognition\". In"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression and a new data structure for cumulative probability tables are studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102429215"
                        ],
                        "name": "Kai-Fu Lee",
                        "slug": "Kai-Fu-Lee",
                        "structuredName": {
                            "firstName": "Kai-Fu",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Fu Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57420724,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0071c960f49d8279e7a5503214a3567cb2237505",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Readings-in-speech-recognition-Waibel-Lee",
            "title": {
                "fragments": [],
                "text": "Readings in speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46734847"
                        ],
                        "name": "P. Kennedy",
                        "slug": "P.-Kennedy",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kennedy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203287107,
            "fieldsOfStudy": [],
            "id": "cf4766d9010c2938f889801c20bbc67e1f02d50c",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Preparing-for-the-21st-Century-Kennedy",
            "title": {
                "fragments": [],
                "text": "Preparing for the 21st Century"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144214753"
                        ],
                        "name": "E. Atwell",
                        "slug": "E.-Atwell",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Atwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Atwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060586058"
                        ],
                        "name": "S. Elliot",
                        "slug": "S.-Elliot",
                        "structuredName": {
                            "firstName": "Sophia",
                            "lastName": "Elliot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Elliot"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60991199,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a50528b42aa93097316532a63d897b1817f9e1bb",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dealing-with-ill-formed-English-text-Atwell-Elliot",
            "title": {
                "fragments": [],
                "text": "Dealing with ill-formed English text"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59710768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6d096d6fa1b39aeeca0a9114b3b3ecdeb960a38",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Self-organizing-language-modeling-for-speech-Jelinek",
            "title": {
                "fragments": [],
                "text": "Self-organizing language modeling for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52029549"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 54152601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f44e0b5372a4fb1d8e1b15ca9e9cab7b4d65dd94",
            "isKey": false,
            "numCitedBy": 1972,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-text-processing-Salton",
            "title": {
                "fragments": [],
                "text": "Automatic text processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153733790"
                        ],
                        "name": "R. Garside",
                        "slug": "R.-Garside",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Garside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Garside"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998726"
                        ],
                        "name": "G. Leech",
                        "slug": "G.-Leech",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Leech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Leech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3215185"
                        ],
                        "name": "G. Sampson",
                        "slug": "G.-Sampson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Sampson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sampson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 146865013,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "49f8abba008bcce99807d9103dc81cff99178575",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "name--see Schiitzenberger (1956). For a summary of the issues, see Ryckman (1986), chap. 5. 2. Carnap and Bar-Hillel (1952), Bar-Hillel (1952). The present book seems in part responsive to this program, having the same title as Bar-Hillel (1964). 3. See papers collected in Hintikka and Suppes (1970). 4. Dretske (1981), Israel and Perry (forthcoming). Peer commentary in Dretske (1983), especially that of Haber, did not accept Dretske's attempted analogies to the metrics of Shannon and Weaver. The notion of \"information pickup\" implies a preestablished harmony of the world and the mind, disregarding the well-known arbitrariness of language. 5. While Fodor (1986) does gives a cogent criticism of attempts to locate information \"in the world\", the alternative \"intentional\" conception that he advocates relies on questionable assumptions of an \"internal code\" wherein such information is \"encoded\". The problem, of course, lies in unpacking this metaphor. Falling into the custom of taking the computational metaphor of mind literally, he resuscitates our old familiar homunculus (in computational disguise as the \"executive\") to provide a way out of the problem of node labels being of higher logical type than the nodes that they label. A simpler resolution follows from Harris's recognition that natural language has no :separate metalanguage. See also Fodor (forthcoming). 6. See especially Harris (1982), and Harris, Gottfried, Ryckman, et al. (in press). 7. This thus cuts deeper than the naive rule-counting metrics for adjudication of grammars advocated not so long ago by generativists (see Ryckman 1986). 8. This work is reported in depth in Harris et al. (in press). These science languages occupy a place between natural language and mathematics, the chief difference from the former being that operator-argument likelihoods are much more strongly defined, amounting in most cases to simple binary selection rather than a graded scale. One of the many interesting aspects of this research is determining empirically the form of argumentation in science. The logical apparatus of deduction and other forms of inference are required only for various uses to which language may be put, rather than being the semantic basis for natural language, as has sometimes been claimed. 9. This is a refinement of the notion of distributional meaning developed in, e.g., Harris (1954). 10. The case of zero likelihood is covered by the word classes of the first constraint. 11. An example is the elision of one of a small set of operators including appear, arrive, show up, which have high likelihood under expect, in I expect John momentarily. The adverb momentarily can only modify the elided to arrive, etc., since neither expect nor John is asserted to be momentary. The infinitive to, the suffix -ly, and the status of momentarily as a modifier are the results of other reductions that are described in detail in Harris (1982). 12. For a computer implementation, see Johnson (1987). I am grateful to Tom Ryckman for helpful comments on an early draft of this review. THE COMPUTATIONAL ANALYSIS OF ENGLISH: A CORPUS-BASED APPROACH"
            },
            "slug": "The-computational-analysis-of-English-:-a-approach-Garside-Leech",
            "title": {
                "fragments": [],
                "text": "The computational analysis of English : a corpus-based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "These science languages occupy a place between natural language and mathematics, the chief difference from the former being that operator-argument likelihoods are much more strongly defined, amounting in most cases to simple binary selection rather than a graded scale."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "tN-wer[Newer] -supp(y[supply] IC conimls[coils] both PWM and power -factor correciiiifl[correction]"
            },
            "venue": {
                "fragments": [],
                "text": "tN-wer[Newer] -supp(y[supply] IC conimls[coils] both PWM and power -factor correciiiifl[correction]"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contextbasedspellingcorrection. Information Processing and Management"
            },
            "venue": {
                "fragments": [],
                "text": "Contextbasedspellingcorrection. Information Processing and Management"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the designer' s job easier by including both power -faction[action], correction and PWM control in one chip. This integrated circuit aids in increasing a supply' s power factor with Thwer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "plotters, pnnters[printers] and other off -line power supplies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "t)esigners[Designers] are focusing more on power -factor correction when creating integrated circuits, due to limited energy supplies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Corrected OCR Text from First-Pass Correction: Note: the correction for a given string is in brackets"
            },
            "venue": {
                "fragments": [],
                "text": "Corrected OCR Text from First-Pass Correction: Note: the correction for a given string is in brackets"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "] and other off -line power supplies. Corrected OCR Text from Feedback Correction: Note: the correction for a given string is in brackets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "t)esigners[Designers] are focusing more on power -factor correction when creating integrated circuits, due to limited energy supplies, new -andards[standards] and the type of office electrical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Example of OCR Correction Original Text Power-supply IC controls both PWM and power-factor correction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 97
                            }
                        ],
                        "text": "Formula 8 may determine that a sequence of four This method is similar to what was described in [Wagner 1974] where the minimum edit distance between two strings was computed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 144
                            }
                        ],
                        "text": "\u2026we can count the cases of substitution, deletion, and insertion using a method similar to computing the minimum edit distance between strings [Wagner 1974] and we can estimate the probabilities using formulas similar to those in [Church & Gale 1991] Obviously, in practice, we typically do\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The stringtostring correction problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OCR Text: tN-wer-supp(y IC conimls both PWM and power-factor correciiiifl"
            },
            "venue": {
                "fragments": [],
                "text": "OCR Text: tN-wer-supp(y IC conimls both PWM and power-factor correciiiifl"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Statistical-Approach-to-Automatic-OCR-Error-in-Tong-Evans/132bb9dd928749af370a87ecb939bbb0b05d7aff?sort=total-citations"
}