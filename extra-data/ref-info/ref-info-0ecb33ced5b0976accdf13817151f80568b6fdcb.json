{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 93
                            }
                        ],
                        "text": "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 405878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844db702be4bc149b06b822b47247e15f5894cc3",
            "isKey": false,
            "numCitedBy": 776,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 F-measure, a 13 relative decrease in F-measure error over the baseline model's score of 88.2. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation."
            },
            "slug": "Discriminative-Reranking-for-Natural-Language-Collins-Koo",
            "title": {
                "fragments": [],
                "text": "Discriminative Reranking for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The boosting approach to ranking problems described in Freund et al. (1998) is applied to parsing the Wall Street Journal treebank, and it is argued that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768480"
                        ],
                        "name": "Liang Huang",
                        "slug": "Liang-Huang",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 104
                            }
                        ],
                        "text": "Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3598758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dfdedcbeb3b67ecaa4d85ca4b9d1aff368b8e2a",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel's implementation of Collins' lexicalized PCFG model, and on Chiang's CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications."
            },
            "slug": "Better-k-best-Parsing-Huang-Chiang",
            "title": {
                "fragments": [],
                "text": "Better k-best Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "It is shown how the improved output of the efficient algorithms for k-best trees in the framework of hypergraph parsing has the potential to improve results from parse reranking systems and other applications."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 99
                            }
                        ],
                        "text": "We\nn-best trees f -score New 0.9102 Collins 0.9037\ntrained then-best parser on sections 2\u201321 of the Penn Treebank, and used section 24 as development data to tune the mixing parameters of the smoothing model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 12
                            }
                        ],
                        "text": "Charniak in (Charniak, 2000) cites an accuracy of 89."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 131
                            }
                        ],
                        "text": "This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 39
                            }
                        ],
                        "text": "897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 538122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76d5e3fa888bee872b7adb7fa810089aa8ab1d58",
            "isKey": false,
            "numCitedBy": 1855,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."
            },
            "slug": "A-Maximum-Entropy-Inspired-Parser-Charniak",
            "title": {
                "fragments": [],
                "text": "A Maximum-Entropy-Inspired Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less and 89.5% when trained and tested on the previously established sections of the Wall Street Journal treebank is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2142738"
                        ],
                        "name": "Tracy Holloway King",
                        "slug": "Tracy-Holloway-King",
                        "structuredName": {
                            "firstName": "Tracy",
                            "lastName": "King",
                            "middleNames": [
                                "Holloway"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tracy Holloway King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803660"
                        ],
                        "name": "R. Kaplan",
                        "slug": "R.-Kaplan",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Kaplan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kaplan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2953252"
                        ],
                        "name": "Dick Crouch",
                        "slug": "Dick-Crouch",
                        "structuredName": {
                            "firstName": "Dick",
                            "lastName": "Crouch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dick Crouch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2265996"
                        ],
                        "name": "John T. Maxwell",
                        "slug": "John-T.-Maxwell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Maxwell",
                            "middleNames": [
                                "T."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John T. Maxwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177145"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 146
                            }
                        ],
                        "text": "Because there may not be a unique best parse for each sentence (i.e.,|Y+(s)| > 1 for some sentences s) we used the variant of MaxEnt described in Riezler et al. (2002) for partially labelled training data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 55
                            }
                        ],
                        "text": "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 78
                            }
                        ],
                        "text": "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6052790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f8f8f33187e20768ae0177780ac5ef78b77feca",
            "isKey": true,
            "numCitedBy": 346,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score."
            },
            "slug": "Parsing-the-Wall-Street-Journal-using-a-Grammar-and-Riezler-King",
            "title": {
                "fragments": [],
                "text": "Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The model combines full and partial parsing techniques to reach full grammar coverage on unseen data, and on a gold standard of manually annotated f-structures for a subset of the WSJ treebank, reaches 79% F-score."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 56
                            }
                        ],
                        "text": "At the end one has a beam-width\u2019s number of best parses (Roark, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 44
                            }
                        ],
                        "text": "A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 34
                            }
                        ],
                        "text": "To put this in perspective, Roark (Roark, 2001) reports oracle results of 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 35
                            }
                        ],
                        "text": "To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "As with Roark\u2019s parser, it is necessary to add a beam-width constraint to make the search tractable."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6237722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e30d29fdf23e14623a2024d4fe0f7f3d5dc889d3",
            "isKey": true,
            "numCitedBy": 368,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model."
            },
            "slug": "Probabilistic-Top-Down-Parsing-and-Language-Roark",
            "title": {
                "fragments": [],
                "text": "Probabilistic Top-Down Parsing and Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A lexicalized probabilistic top-down parser is presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This feature was inspired by a similar feature in Klein and Manning (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11495042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
            },
            "slug": "Accurate-Unlexicalized-Parsing-Klein-Manning",
            "title": {
                "fragments": [],
                "text": "Accurate Unlexicalized Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is demonstrated that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40573589"
                        ],
                        "name": "V\u00edctor M. Jim\u00e9nez",
                        "slug": "V\u00edctor-M.-Jim\u00e9nez",
                        "structuredName": {
                            "firstName": "V\u00edctor",
                            "lastName": "Jim\u00e9nez",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V\u00edctor M. Jim\u00e9nez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064079"
                        ],
                        "name": "A. Marzal",
                        "slug": "A.-Marzal",
                        "structuredName": {
                            "firstName": "Andr\u00e9s",
                            "lastName": "Marzal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Marzal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12457572,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48b915e2d54e93c3a66e9ddc35244ebdef22c101",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Context-Free Grammars are the object of increasing interest in the pattern recognition research community in an attempt to overcome the limited modeling capabilities of the simpler regular grammars, and have application in a variety of fields such as language modeling, speech recognition, optical character recognition, computational biology, etc. \n \nThis paper proposes an efficient algorithm to solve one of the problems associated to the use of weighted and stochastic Context-Free Grammars: the problem of computing the N best parse trees of a given string. After the best parse tree has been computed using the CYK algorithm, a large number of alternative parse trees are obtained, in order by weight (or probability), in a small fraction of the time required by the CYK algorithm to find the best parse tree. This is confirmed by experimental results using grammars from two different domains: a chromosome grammar, and a grammar modeling natural language sentences from the Wall Street Journal corpus."
            },
            "slug": "Computation-of-the-N-Best-Parse-Trees-for-Weighted-Jim\u00e9nez-Marzal",
            "title": {
                "fragments": [],
                "text": "Computation of the N Best Parse Trees for Weighted and Stochastic Context-Free Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An efficient algorithm is proposed to solve one of the problems associated to the use of weighted and stochastic Context-Free Grammars: the problem of computing the N best parse trees of a given string."
            },
            "venue": {
                "fragments": [],
                "text": "SSPR/SPR"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94876749"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2727234"
                        ],
                        "name": "Y. Chow",
                        "slug": "Y.-Chow",
                        "structuredName": {
                            "firstName": "Yen-lu",
                            "lastName": "Chow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 55
                            }
                        ],
                        "text": "The clever way is based upon an algorithm developed by Schwartz and Chow (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The clever way is based upon an algorithm developed by  Schwartz and Chow (1990) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60451180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e4ca575fc5759aff9b6e7450c2f17681c75f8d5",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A search algorithm that provides a simple, clean, and efficient interface between the speech and natural language components of a spoken language system is introduced. The N-best algorithm is a time-synchronous Viterbi-style beam search procedure that is guaranteed to find the N most likely whole sentence alternatives that are within a given beam of the most likely sentence. The computation is linear with the length of the utterance, and faster than linear in N. When used together with a first-order statistical grammar, the correct sentence is usually within the first few sentence choices. The output of the algorithm, which is an ordered set of sentence hypotheses with acoustic and language model scores can easily be processed by natural language knowledge sources without the huge expansion of the search space that would be needed to include all possible knowledge sources in a top-down search. In experiments using a first-order statistical language model, the average rank of the correct answer was 1.8 and was within the first 24 choices 99% of the time.<<ETX>>"
            },
            "slug": "The-N-best-algorithms:-an-efficient-and-exact-for-N-Schwartz-Chow",
            "title": {
                "fragments": [],
                "text": "The N-best algorithms: an efficient and exact procedure for finding the N most likely sentence hypotheses"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The N-best algorithm is a time-synchronous Viterbi-style beam search procedure that is guaranteed to find the N most likely whole sentence alternatives that are within a given beam of the most likely sentence."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2023469"
                        ],
                        "name": "D. Bikel",
                        "slug": "D.-Bikel",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bikel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bikel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 862713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0606291dae96446e812ea8f09d9fbdc6acc3ec37",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins' (1999) thesis, this article contains all information necessary to duplicate Collins' benchmark results. Indeed, these as-yet-unpublished details account for an 11 relative increase in error from an implementation including all details to a clean-room implementation of Collins' model. We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins' parser. We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech."
            },
            "slug": "Intricacies-of-Collins'-Parsing-Model-Bikel",
            "title": {
                "fragments": [],
                "text": "Intricacies of Collins' Parsing Model"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A large set of heretofore unpublished details Collins used in his parser are documents, such that, along with Collins' (1999) thesis, this article contains all information necessary to duplicate Collins' benchmark results."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 19
                            }
                        ],
                        "text": "The Collins parser (Collins, 1997) does use dynamic programming in its search."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ffa423a5283396c88ff3d4033d541796bd039cc",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96)."
            },
            "slug": "Three-Generative,-Lexicalised-Models-for-Parsing-Collins",
            "title": {
                "fragments": [],
                "text": "Three Generative, Lexicalised Models for Statistical Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new statistical parsing model is proposed, which is a generative model of lexicalised context-free grammar and extended to include a probabilistic treatment of both subcategorisation and wh-movement."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764192"
                        ],
                        "name": "R. Bod",
                        "slug": "R.-Bod",
                        "structuredName": {
                            "firstName": "Rens",
                            "lastName": "Bod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bod"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 101
                            }
                        ],
                        "text": "This result is only slightly higher than the highest reported result for this test-set, Bod\u2019s (.907) (Bod, 2003)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 16072122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9d59174cc50b119ee4be19b3e65177431e37003",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFG-reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank. Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence."
            },
            "slug": "An-efficient-implementation-of-a-new-DOP-model-Bod",
            "title": {
                "fragments": [],
                "text": "An efficient implementation of a new DOP model"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper proposes an integration of the two apparently opposing DOP models which outperforms each of them separately, and together with a PCFG-reduction of DOP gets improved accuracy and efficiency on the Wall Street Journal treebank."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152299194"
                        ],
                        "name": "G. Satta",
                        "slug": "G.-Satta",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Satta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Satta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs require O(m3) states, so a n-best parser would require O(nm3) states."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 20
                            }
                        ],
                        "text": "As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs requireO(m3) states, so an-best parser would requireO(nm3) states."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 333410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96395cd7494bcfa43cd62c14b4cf428bddbd3063",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n5). For a common special case that was known to allow O(n3) parsing (Eisner, 1997), we present an O(n3) algorithm with an improved grammar constant."
            },
            "slug": "Efficient-Parsing-for-Bilexical-Context-Free-and-Eisner-Satta",
            "title": {
                "fragments": [],
                "text": "Efficient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents O(n4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O( n5) by one step and an improved grammar constant by another."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17446277"
                        ],
                        "name": "J. Goodman",
                        "slug": "J.-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5848469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65dc3dc3a4c78f5a5875da24b152ab6a2b2da5e5",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level. We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement. We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms."
            },
            "slug": "Global-Thresholding-and-Multiple-Pass-Parsing-Goodman",
            "title": {
                "fragments": [],
                "text": "Global Thresholding and Multiple-Pass Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level, and uses a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152465203"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47428006"
                        ],
                        "name": "S. Canon",
                        "slug": "S.-Canon",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Canon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Canon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140471"
                        ],
                        "name": "Zhiyi Chi",
                        "slug": "Zhiyi-Chi",
                        "structuredName": {
                            "firstName": "Zhiyi",
                            "lastName": "Chi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiyi Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 56
                            }
                        ],
                        "text": "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17435621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "463dbd690d912b23d29b7581fb6b253b36f50394",
            "isKey": true,
            "numCitedBy": 233,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Log-linear models provide a statistically sound framework for Stochastic \"Unification-Based\" Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar."
            },
            "slug": "Estimators-for-Stochastic-\"Unification-Based\"-Johnson-Geman",
            "title": {
                "fragments": [],
                "text": "Estimators for Stochastic \"Unification-Based\" Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Two computationally-tractable ways of estimating the parameters of Stochastic \"Unification-Based\" Grammars from a training corpus of syntactic analyses are described and applied to estimate a stochastic version of Lexical-Functional Grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145804005"
                        ],
                        "name": "Robert Malouf",
                        "slug": "Robert-Malouf",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Malouf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Malouf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 116
                            }
                        ],
                        "text": ", 2004) to find the optimal feature weight\u015d\u03b8 because this method seems substantially faster than comparable methods (Malouf, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6249194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "878783964ab23c97052ea82685368099d85c500d",
            "isKey": false,
            "numCitedBy": 741,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices."
            },
            "slug": "A-Comparison-of-Algorithms-for-Maximum-Entropy-Malouf",
            "title": {
                "fragments": [],
                "text": "A Comparison of Algorithms for Maximum Entropy Parameter Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A number of algorithms for estimating the parameters of ME models are considered, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24275527"
                        ],
                        "name": "W. Bauer",
                        "slug": "W.-Bauer",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Bauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69505222"
                        ],
                        "name": "J. Emonds",
                        "slug": "J.-Emonds",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Emonds",
                            "middleNames": [
                                "Embley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Emonds"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 168
                            }
                        ],
                        "text": "An additional parameter controls whether the feature is specialized to embedded or non-embedded clauses, which roughly corresponds to Emonds\u2019 \u201cnonroot\u201d and \u201croot\u201d contexts (Emonds, 1976)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123465810,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5989de8370d5de663ef0c3c6cb30cc7dcccb6bb6",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Imagine that you get such certain awesome experience and knowledge by only reading a book. How can? It seems to be greater when a book can be the best thing to discover. Books now will appear in printed and soft file collection. One of them is this book a transformational approach to english syntax root structure preserving and local transformations. It is so usual with the printed books. However, many people sometimes have no space to bring the book for them; this is why they can't read the book wherever they want."
            },
            "slug": "A-Transformational-Approach-to-English-Syntax:-and-Bauer-Emonds",
            "title": {
                "fragments": [],
                "text": "A Transformational Approach to English Syntax: Root, Structure-Preserving, and Local Transformations"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This book a transformational approach to english syntax root structure preserving and local transformations is presented, which will appear in printed and soft file collection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72498580"
                        ],
                        "name": "J. Grimshaw",
                        "slug": "J.-Grimshaw",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Grimshaw",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Grimshaw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 15
                            }
                        ],
                        "text": "Recall the standard MaxEnt conditional probability model for a parsey \u2208 Y:\n(2002) is just the negative log conditional likelihood outputY(s):\nLD(\u03b8) = \u2212 n\u2032\u2211\ni=1\nlog P\u03b8(Y+(si)|Y(si)), where\nP\u03b8(Y+|Y) = \u2211\ny\u2208Y+\nP\u03b8(y|Y)\nThe partial derivatives of this loss function, which are required by the numerical\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2582613,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c8e6e777e8dd4642b9a588f5b927f95a40c03277",
            "isKey": false,
            "numCitedBy": 651,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "This article argues that inversion of the subject and auxiliary in English matrix questions and elsewhere is the effect of a violable optimality-theoretic constraint that requires head positions to be filled. When no other auxiliary is available, do-support occurs to satisfy this constraint, resulting in the presence of an expletive verb. When a higher-ranked constraint prohibits inversion, no inversion or do-support is found. The argument is then extended to cases where the complementizer that is obligatory, which are shown to offer best satisfaction of the proposed set of violable constraints"
            },
            "slug": "Projection,-heads,-and-optimality-Grimshaw",
            "title": {
                "fragments": [],
                "text": "Projection, heads, and optimality"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145033312"
                        ],
                        "name": "S. Benson",
                        "slug": "S.-Benson",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Benson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Benson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763017"
                        ],
                        "name": "L. McInnes",
                        "slug": "L.-McInnes",
                        "structuredName": {
                            "firstName": "Lois",
                            "lastName": "McInnes",
                            "middleNames": [
                                "Curfman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. McInnes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3018158"
                        ],
                        "name": "J. Sarich",
                        "slug": "J.-Sarich",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Sarich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sarich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "We used the Limited Memory Variable Metric optimization algorithm from thePETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights\u0302\u03b8 because this method seems substantially faster than comparable methods (Malouf, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 105
                            }
                        ],
                        "text": "We used the Limited Memory Variable Metric optimization algorithm from thePETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weight\u015d\u03b8 because this method seems substantially faster than comparable methods (Malouf, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59817029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce5c17a5884497840a2bbf0529ae43a6c8c94a5e",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The Toolkit for Advanced Optimization (TAO) focuses on the design and implementation of component-based optimization software for the solution of large-scale optimization applications on high-performance architectures. Their approach is motivated by the scattered support for parallel computations and lack of reuse of linear algebra software in currently available optimization software. The TAO design allows the reuse of toolkits that provide lower-level support (parallel sparse matrix data structures, preconditioners, solvers), and thus they are able to build on top of these toolkits instead of having to redevelop code. The advantages in terms of efficiency and development time are significant. The TAO design philosophy uses object-oriented techniques of data and state encapsulation, abstract classes, and limited inheritance to create a flexible optimization toolkit. This chapter provides a short introduction to the design philosophy by describing the objectives in TAO and the importance of this design. Since a major concern in the TAO project is the performance and scalability of optimization algorithms on large problems, they also present some performance results."
            },
            "slug": "TAO-users-manual.-Benson-McInnes",
            "title": {
                "fragments": [],
                "text": "TAO users manual."
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The Toolkit for Advanced Optimization design philosophy uses object-oriented techniques of data and state encapsulation, abstract classes, and limited inheritance to create a flexible optimization toolkit that allows the reuse of toolkits that provide lower-level support."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "We used the Limited Memory Variable Metric optimization algorithm from thePETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights\u0302\u03b8 because this method seems substantially faster than comparable methods (Malouf, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tao users manual. Technical Report ANL/MCS-TM-242-Revision 1.6, Argonne National Laboratory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 7
                            }
                        ],
                        "text": "parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 144
                            }
                        ],
                        "text": "The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins\nparser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intricacies of collins parsing model.Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler"
            },
            "venue": {
                "fragments": [],
                "text": "1999. Estimators for stochas-"
            },
            "year": 1876
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 20
                            }
                        ],
                        "text": "As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs requireO(m3) states, so an-best parser would requireO(nm3) states."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Annual 2 This probably underestimates the actual improvement. There are no currently accepted figures for inter-annotater agreement on Penn WSJ, but it is no doubt well short of 100%"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 37th"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 14,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Coarse-to-Fine-n-Best-Parsing-and-MaxEnt-Reranking-Charniak-Johnson/0ecb33ced5b0976accdf13817151f80568b6fdcb?sort=total-citations"
}