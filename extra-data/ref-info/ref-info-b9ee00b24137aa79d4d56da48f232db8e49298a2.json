{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074746847"
                        ],
                        "name": "P. Buehler",
                        "slug": "P.-Buehler",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Buehler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Buehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 226
                            }
                        ],
                        "text": "[3] and Cooper and Bowden [7] were able to automatically extract sign-video pairs from TV broadcasts; these automatically extracted sign-video pairs can then be used as supervisory material to train a sign language classifier [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17953939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a4828b273979b4b03dde6b0a23d18c08c80b9a7",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present several contributions towards automatic recognition of BSL signs from continuous signing video sequences: (i) automatic detection and tracking of the hands using a generative model of the image; (ii) automatic learning of signs from TV broadcasts of single signers, using only the supervisory information available from subtitles; (iii) discriminative signer-independent sign recognition using automatically extracted training data from a single signer. Our source material consists of many hours of video with continuous signing and aligned subtitles recorded from BBC digital television. This is very challenging material visually in detecting and tracking the signer for a number of reasons, including self-occlusions, self-shadowing, motion blur, and in particular the changing background; it is also a challenging learning situation since the supervision provided by the subtitles is both weak and noisy."
            },
            "slug": "Employing-signed-TV-broadcasts-for-automated-of-Buehler-Everingham",
            "title": {
                "fragments": [],
                "text": "Employing signed TV broadcasts for automated learning of British Sign Language"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Several contributions towards automatic recognition of BSL signs from continuous signing video sequences are presented, including automatic detection and tracking of the hands using a generative model of the image and discriminative signer-independent sign recognition using automatically extracted training data from a single signer."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074746847"
                        ],
                        "name": "P. Buehler",
                        "slug": "P.-Buehler",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Buehler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Buehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] and Cooper and Bowden [7] were able to automatically extract sign-video pairs from TV broadcasts; these automatically extracted sign-video pairs can then be used as supervisory material to train a sign language classifier [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15326394,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6b47a0ee5149be524aa585aef6ca35e6a903fe6",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to automatically learn a large number of British sign language (BSL) signs from TV broadcasts. We achieve this by using the supervisory information available from subtitles broadcast simultaneously with the signing. This supervision is both weak and noisy: it is weak due to the correspondence problem since temporal distance between sign and subtitle is unknown and signing does not follow the text order; it is noisy because subtitles can be signed in different ways, and because the occurrence of a subtitle word does not imply the presence of the corresponding sign. The contributions are: (i) we propose a distance function to match signing sequences which includes the trajectory of both hands, the hand shape and orientation, and properly models the case of hands touching; (ii) we show that by optimizing a scoring function based on multiple instance learning, we are able to extract the sign of interest from hours of signing footage, despite the very weak and noisy supervision. The method is automatic given the English target word of the sign to be learnt. Results are presented for 210 words including nouns, verbs and adjectives."
            },
            "slug": "Learning-sign-language-by-watching-TV-(using-weakly-Buehler-Zisserman",
            "title": {
                "fragments": [],
                "text": "Learning sign language by watching TV (using weakly aligned subtitles)"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a distance function to match signing sequences which includes the trajectory of both hands, the hand shape and orientation, and properly models the case of hands touching and shows that by optimizing a scoring function based on multiple instance learning, it is able to extract the sign of interest from hours of signing footage, despite the very weak and noisy supervision."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056025532"
                        ],
                        "name": "H. Cooper",
                        "slug": "H.-Cooper",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Cooper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145398628"
                        ],
                        "name": "R. Bowden",
                        "slug": "R.-Bowden",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Bowden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bowden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 88
                            }
                        ],
                        "text": "Previous approaches have concentrated on obtaining the hands by using their skin colour [1, 3, 18] or by detectors based on AdaBoost hand classifiers [8, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14220924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "654de15066ef74c7843459ae679c96912849f7d9",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to large lexicon sign recognition that does not require tracking. This overcomes the issues of how to accurately track the hands through self occlusion in unconstrained video, instead opting to take a detection strategy, where patterns of motion are identified. It is demonstrated that detection can be achieved with only minor loss of accuracy compared to a perfectly tracked sequence using coloured gloves. The approach uses two levels of classification. In the first, a set of viseme classifiers detects the presence of sub-Sign units of activity. The second level then assembles visemes into word level Sign using Markov chains. The system is able to cope with a large lexicon and is more expandable than traditional word level approaches. Using as few as 5 training examples the proposed system has classification rates as high as 74.3% on a randomly selected 164 sign vocabulary performing at a comparable level to other tracking based systems."
            },
            "slug": "Large-Lexicon-Detection-of-Sign-Language-Cooper-Bowden",
            "title": {
                "fragments": [],
                "text": "Large Lexicon Detection of Sign Language"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An approach to large lexicon sign recognition that does not require tracking overcomes the issues of how to accurately track the hands through self occlusion in unconstrained video, instead opting to take a detection strategy, where patterns of motion are identified."
            },
            "venue": {
                "fragments": [],
                "text": "ICCV-HCI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "Their great advantage is the low complexity of inference [4] and hence they have been used in numerous applications [14, 15, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Our method owes some inspiration to [15], which detects a distinctive lateral walking pose to initialise a person-specific colour model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5574410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14eacd0e48a160bfc935cd4d419772f0110b1a0f",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an algorithm for finding and kinematically tracking multiple people in long sequences. Our basic assumption is that people tend to take on certain canonical poses, even when performing unusual activities like throwing a baseball or figure skating. We build a person detector that quite accurately detects and localizes limbs of people in lateral walking poses. We use the estimated limbs from a detection to build a discriminative appearance model; we assume the features that discriminate a figure in one frame will discriminate the figure in other frames. We then use the models as limb detectors in a pictorial structure framework, detecting figures in unrestricted poses in both previous and successive frames. We have run our tracker on hundreds of thousands of frames, and present and apply a methodology for evaluating tracking on such a large scale. We test our tracker on real sequences including a feature-length film, an hour of footage from a public park, and various sports sequences. We find that we can quite accurately automatically find and track multiple people interacting with each other while performing fast and unusual motions."
            },
            "slug": "Strike-a-pose:-tracking-people-by-finding-stylized-Ramanan-Forsyth",
            "title": {
                "fragments": [],
                "text": "Strike a pose: tracking people by finding stylized poses"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A person detector that quite accurately detects and localizes limbs of people in lateral walking poses is built, and an algorithm for finding and kinematically tracking multiple people in long sequences is developed."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074819081"
                        ],
                        "name": "A. Fossati",
                        "slug": "A.-Fossati",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Fossati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fossati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070926374"
                        ],
                        "name": "M. Dimitrijevic",
                        "slug": "M.-Dimitrijevic",
                        "structuredName": {
                            "firstName": "Miodrag",
                            "lastName": "Dimitrijevic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dimitrijevic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689738"
                        ],
                        "name": "V. Lepetit",
                        "slug": "V.-Lepetit",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Lepetit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lepetit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "It is similar in form to that of [7, 9, 10, 11, 16] and described in detail in Section 2."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8587499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4454103bdbd4b9c8a5ded6c32a8eac3d3bf486f3",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We combine detection and tracking techniques to achieve robust 3-D motion recovery of people seen from arbitrary viewpoints by a single and potentially moving camera. We rely on detecting key postures, which can be done reliably, using a motion model to infer 3-D poses between consecutive detections, and finally refining them over the whole sequence using a generative model. We demonstrate our approach in the case of people walking against cluttered backgrounds and filmed using a moving camera, which precludes the use of simple background subtraction techniques. In this case, the easy-to-detect posture is the one that occurs at the end of each step when people have their legs furthest apart."
            },
            "slug": "Bridging-the-Gap-between-Detection-and-Tracking-for-Fossati-Dimitrijevic",
            "title": {
                "fragments": [],
                "text": "Bridging the Gap between Detection and Tracking for 3D Monocular Video-Based Motion Capture"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work combines detection and tracking techniques to achieve robust 3-D motion recovery of people seen from arbitrary viewpoints by a single and potentially moving camera, and relies on detecting key postures, which can be done reliably, using a motion model to infer3-D poses between consecutive detections, and refining them over the whole sequence using a generative model."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784015"
                        ],
                        "name": "R. Navaratnam",
                        "slug": "R.-Navaratnam",
                        "structuredName": {
                            "firstName": "Ramanan",
                            "lastName": "Navaratnam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Navaratnam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707867"
                        ],
                        "name": "A. Thayananthan",
                        "slug": "A.-Thayananthan",
                        "structuredName": {
                            "firstName": "Arasanathan",
                            "lastName": "Thayananthan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Thayananthan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "Head and torso segmentation: In a similar manner to [12], the outline of the head and the torso are detected first before identifying the arm configuration."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7990570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9cb8b846987a3ff1814d0addaeb708b41b2769b",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of automatic detection and recovery of three-dimensional human body pose from monocular video sequences for HCI applications. We propose a new hierarchical part-based pose estimation method for the upper-body that efficiently searches the high dimensional articulation space. The body is treated as a collection of parts linked in a kinematic structure. Search for configurations of this collection is commenced from the most reliably detectable part. The rest of the parts are searched based on the detected locations of this anchor as they all are kinematically linked. Each part is represented by a set of 2D templates created from a 3D model, hence inherently encoding the 3D joint angles. The tree data structure is exploited to efficiently search through these templates. Multiple hypotheses are computed for each frame. By modelling these with a HMM, temporal coherence of body motion is exploited to find a smooth trajectory of articulation between frames using a modified Viterbi algorithm. Experimental results show that the proposed technique produces good estimates of the human 3D pose on a range of test videos in a cluttered environment."
            },
            "slug": "Hierarchical-Part-Based-Human-Body-Pose-Estimation-Navaratnam-Thayananthan",
            "title": {
                "fragments": [],
                "text": "Hierarchical Part-Based Human Body Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new hierarchical part-based pose estimation method for the upper-body that efficiently searches the high dimensional articulation space using a modified Viterbi algorithm for smooth trajectory of articulation between frames."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398347979"
                        ],
                        "name": "Manuel J. Mar\u00edn-Jim\u00e9nez",
                        "slug": "Manuel-J.-Mar\u00edn-Jim\u00e9nez",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Mar\u00edn-Jim\u00e9nez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manuel J. Mar\u00edn-Jim\u00e9nez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10], with the FG clamped in areas based on the face location (Figure 3c)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 184
                            }
                        ],
                        "text": "Detection of both frontal and profile view faces is done by choosing between the OpenCV face detector (high recall for frontal faces) and a face detector based on upper body detection [10] (lower recall but detects profile views) according to their confidence values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2845360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7b6bd15f32ec49906e3500cac1abd7ed6a7c01a",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this paper is to estimate 2D human pose as a spatial configuration of body parts in TV and movie video shots. Such video material is uncontrolled and extremely challenging. We propose an approach that progressively reduces the search space for body parts, to greatly improve the chances that pose estimation will succeed. This involves two contributions: (i) a generic detector using a weak model of pose to substantially reduce the full pose search space; and (ii) employing 'grabcut' initialized on detected regions proposed by the weak model, to further prune the search space. Moreover, we also propose (Hi) an integrated spatio- temporal model covering multiple frames to refine pose estimates from individual frames, with inference using belief propagation. The method is fully automatic and self-initializing, and explains the spatio-temporal volume covered by a person moving in a shot, by soft-labeling every pixel as belonging to a particular body part or to the background. We demonstrate upper-body pose estimation by an extensive evaluation over 70000 frames from four episodes of the TV series Buffy the vampire slayer, and present an application to full- body action recognition on the Weizmann dataset."
            },
            "slug": "Progressive-search-space-reduction-for-human-pose-Ferrari-Mar\u00edn-Jim\u00e9nez",
            "title": {
                "fragments": [],
                "text": "Progressive search space reduction for human pose estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An approach that progressively reduces the search space for body parts, to greatly improve the chances that pose estimation will succeed, and an integrated spatio- temporal model covering multiple frames to refine pose estimates from individual frames, with inference using belief propagation."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "Their great advantage is the low complexity of inference [4] and hence they have been used in numerous applications [14, 15, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6049714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c404f6577297bb1f90a702dc13951c35f958a37",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to find all occurrences of a particular person in a sequence of photographs taken over a short period of time. For identification, we assume each individual\u2019s hair and clothing stays the same throughout the sequence. Even with these assumptions, the task remains challenging as people can move around, change their pose and scale, and partially occlude each other. We propose a two stage method. First, individuals are identified by clustering frontal face detections using color clothing information. Second, a color based pictorial structure model is used to find occurrences of each person in images where their frontal face detection was missed. Two extensions improving the pictorial structure detections are also described. In the first extension, we obtain a better clothing segmentation to improve the accuracy of the clothing color model. In the second extension, we simultaneously consider multiple detection hypotheses of all people potentially present in the shot. Our results show that people can be re-detected in images where they do not face the camera. Results are presented on several sequences from a personal photo collection."
            },
            "slug": "Finding-People-in-Repeated-Shots-of-the-Same-Scene-Sivic-Zitnick",
            "title": {
                "fragments": [],
                "text": "Finding People in Repeated Shots of the Same Scene"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results show that people can be re-detected in images where they do not face the camera, and two extensions improving the pictorial structure detections are described."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689738"
                        ],
                        "name": "V. Lepetit",
                        "slug": "V.-Lepetit",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Lepetit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lepetit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 232
                            }
                        ],
                        "text": "In recent years there has been increasing interest in random forest/fern-based methods for tasks such as image classification [2, 19], object detection [8, 11], segmentation [12, 25], head pose estimation [1] and feature extraction [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12980089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa6557c658aed10ff303aa90fe8d0952332a43c0",
            "isKey": false,
            "numCitedBy": 783,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In many 3D object-detection and pose-estimation problems, runtime performance is of critical importance. However, there usually is time to train the system, which we would show to be very useful. Assuming that several registered images of the target object are available, we developed a keypoint-based approach that is effective in this context by formulating wide-baseline matching of keypoints extracted from the input images to those found in the model images as a classification problem. This shifts much of the computational burden to a training phase, without sacrificing recognition performance. As a result, the resulting algorithm is robust, accurate, and fast-enough for frame-rate performance. This reduction in runtime computational complexity is our first contribution. Our second contribution is to show that, in this context, a simple and fast keypoint detector suffices to support detection and tracking even under large perspective and scale variations. While earlier methods require a detector that can be expected to produce very repeatable results, in general, which usually is very time-consuming, we simply find the most repeatable object keypoints for the specific target object during the training phase. We have incorporated these ideas into a real-time system that detects planar, nonplanar, and deformable objects. It then estimates the pose of the rigid ones and the deformations of the others"
            },
            "slug": "Keypoint-recognition-using-randomized-trees-Lepetit-Fua",
            "title": {
                "fragments": [],
                "text": "Keypoint recognition using randomized trees"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A keypoint-based approach is developed that is effective in this context by formulating wide-baseline matching of keypoints extracted from the input images to those found in the model images as a classification problem, which shifts much of the computational burden to a training phase, without sacrificing recognition performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3155984"
                        ],
                        "name": "Ben Benfold",
                        "slug": "Ben-Benfold",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Benfold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Benfold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 205
                            }
                        ],
                        "text": "In recent years there has been increasing interest in random forest/fern-based methods for tasks such as image classification [2, 19], object detection [8, 11], segmentation [12, 25], head pose estimation [1] and feature extraction [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "A pixel q is represented by xq = (x1 q,x(2) q,x(3) q) where x1 q, x2 q, x3 q are the skin, torso and background colour posterior values at pixel q respectively [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 317550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3250371b30ae553f35e55852407fe362c9be4abd",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an algorithm for the classification of head pose in low resolution video. Invariance to skin, hair and background colours is achieved by classifying using an ensemble of randomised ferns which have been trained on labelled images. The ferns are used to simultaneously classify the head pose and to identify the most likely hypothesis for the mapping between colours and labels. Results from video sequences demonstrate that an improved posterior estimation using learnt colour distributions reduces classification error and provides accurate pose information in images where the head occupies as little as 10 pixels square. In systems which automatically monitor surveillance video, knowledge of head pose provides an important cue for higher level behavioural analysis. The focus of an individual\u2019s attention often indicates their desired destination whereas mutual attention between people indicates familiarity, and any single object or person receiving attention from a large number of people is likely to be worthy of further investigation. In systems controlling dynamic cameras, a pose estimation from a low resolution head image can be used to determine whether or not a close-up from a dynamic camera would provide a face image that is suitable for identification. Surveillance cameras tend to have a fairly wide field of view, making the region of the video that is occupied by a person\u2019s head fairly small. The low resolution of the head image prevents the application of techniques which require detail such as those which track feature points or detect facial features [6, 4]. The majority of research into head pose measurement in low resolution video involves the use of labelled training examples which are used to train various types of classifiers such as neural networks [11, 2, 13], support vector machines [14] or nearest neighbour and tree based classifiers [10, 7, 1]. Other approaches model the head as an ellipsoid and either learn a texture from training data [15] or fit a reprojected head image to find a relative rotation [9]. For a head pose classifier to be effective in real-world situations it must be able to cope with different skin and hair colours as well as wide variations in lighting direction, intensity and colour. Most existing classifiers are susceptible to these variations and require examples with different combinations of lighting conditions and skin/hair colour variations in order to make an accurate classification."
            },
            "slug": "Colour-Invariant-Head-Pose-Classification-in-Low-Benfold-Reid",
            "title": {
                "fragments": [],
                "text": "Colour Invariant Head Pose Classification in Low Resolution Video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results from video sequences demonstrate that an improved posterior estimation using learnt colour distributions reduces classification error and provides accurate pose information in images where the head occupies as little as 10 pixels square."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649483"
                        ],
                        "name": "M. Lee",
                        "slug": "M.-Lee",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144422657"
                        ],
                        "name": "I. Cohen",
                        "slug": "I.-Cohen",
                        "structuredName": {
                            "firstName": "Isaac",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "It is similar in form to that of [7, 9, 10, 11, 16] and described in detail in Section 2."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 11936520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd56c0a9be12c7501de6cdb3ce04780e51d7e654",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimating human body poses in static images is important for many image understanding applications including semantic content extraction and image database query and retrieval. This problem is challenging due to the presence of clutter in the image, ambiguities in image observation, unknown human image boundary, and high-dimensional state space due to the complex articulated structure of the human body. Human pose estimation can be made more robust by integrating the detection of body components such as face and limbs, with the highly constrained structure of the articulated body. In this paper, a data-driven approach based on Markov chain Monte Carlo (DD-MCMC) is used, where component detection results generate state proposals for 3D pose estimation. To translate these observations into pose hypotheses, we introduce the use of \"proposal maps,\" an efficient way of consolidating the evidence and generating 3D pose candidates during the MCMC search. Experimental results on a set of test images show that the method is able to estimate the human pose in static images of real scenes."
            },
            "slug": "A-model-based-approach-for-estimating-human-3D-in-Lee-Cohen",
            "title": {
                "fragments": [],
                "text": "A model-based approach for estimating human 3D poses in static images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A data-driven approach based on Markov chain Monte Carlo (DD-MCMC) is used, where component detection results generate state proposals for 3D pose estimation, and experimental results show that the method is able to estimate the human pose in static images of real scenes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056025532"
                        ],
                        "name": "H. Cooper",
                        "slug": "H.-Cooper",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Cooper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145398628"
                        ],
                        "name": "R. Bowden",
                        "slug": "R.-Bowden",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Bowden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bowden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "[3] and Cooper and Bowden [7] were able to automatically extract sign-video pairs from TV broadcasts; these automatically extracted sign-video pairs can then be used as supervisory material to train a sign language classifier [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2741978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46b658f9e291b1ff1684aa9f778c31ed1c92a23a",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a fully automated, unsupervised method to recognise sign from subtitles. It does this by using data mining to align correspondences in sections of videos. Based on head and hand tracking, a novel temporally constrained adaptation of a priori mining is used to extract similar regions of video, with the aid of a proposed contextual negative selection method. These regions are refined in the temporal domain to isolate the occurrences of similar signs in each example. The system is shown to automatically identify and segment signs from standard news broadcasts containing a variety of topics."
            },
            "slug": "Learning-signs-from-subtitles:-A-weakly-supervised-Cooper-Bowden",
            "title": {
                "fragments": [],
                "text": "Learning signs from subtitles: A weakly supervised approach to sign language recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A fully automated, unsupervised method to recognise sign from subtitles is introduced by using data mining to align correspondences in sections of videos using head and hand tracking and a proposed contextual negative selection method."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143894891"
                        ],
                        "name": "Eng-Jon Ong",
                        "slug": "Eng-Jon-Ong",
                        "structuredName": {
                            "firstName": "Eng-Jon",
                            "lastName": "Ong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eng-Jon Ong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145398628"
                        ],
                        "name": "R. Bowden",
                        "slug": "R.-Bowden",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Bowden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bowden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 150
                            }
                        ],
                        "text": "Previous approaches have concentrated on obtaining the hands by using their skin colour [1, 3, 18] or by detectors based on AdaBoost hand classifiers [8, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6324953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4625c6c0d45fdc19e21ceb43f12e2f7d3278f498",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to detect a persons unconstrained hand in a natural video sequence has applications in sign language, gesture recognition and HCl. This paper presents a novel, unsupervised approach to training an efficient and robust detector which is capable of not only detecting the presence of human hands within an image but classifying the hand shape. A database of images is first clustered using a k-method clustering algorithm with a distance metric based upon shape context. From this, a tree structure of boosted cascades is constructed. The head of the tree provides a general hand detector while the individual branches of the tree classify a valid shape as belong to one of the predetermined clusters exemplified by an indicative hand shape. Preliminary experiments carried out showed that the approach boasts a promising 99.8% success rate on hand detection and 97.4% success at classification. Although we demonstrate the approach within the domain of hand shape it is equally applicable to other problems where both detection and classification are required for objects that display high variability in appearance."
            },
            "slug": "A-boosted-classifier-tree-for-hand-shape-detection-Ong-Bowden",
            "title": {
                "fragments": [],
                "text": "A boosted classifier tree for hand shape detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel, unsupervised approach to training an efficient and robust detector which is capable of not only detecting the presence of human hands within an image but classifying the hand shape."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3717791"
                        ],
                        "name": "M. P. Kumar",
                        "slug": "M.-P.-Kumar",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Pawan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "It is similar in form to that of [7, 9, 10, 11, 16] and described in detail in Section 2."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 5546735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3fd2ce852195082537248767912983b7ae36bcf",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe present an unsupervised approach for learning a layered representation of a scene from a video for motion segmentation. Our method is applicable to any video containing piecewise parametric motion. The learnt model is a composition of layers, which consist of one or more segments. The shape of each segment is represented using a binary matte and its appearance is given by the rgb value for each point belonging to the matte. Included in the model are the effects of image projection, lighting, and motion blur. Furthermore, spatial continuity is explicitly modeled resulting in contiguous segments. Unlike previous approaches, our method does not use reference frame(s) for initialization. The two main contributions of our method are: (i)\u00a0A novel algorithm for obtaining the initial estimate of the model by dividing the scene into rigidly moving components using efficient loopy belief propagation; and (ii)\u00a0Refining the initial estimate using \u03b1\u03b2-swap and \u03b1-expansion algorithms, which guarantee a strong local minima. Results are presented on several classes of objects with different types of camera motion, e.g. videos of a human walking shot with static or translating cameras. We compare our method with the state of the art and demonstrate significant improvements.\n"
            },
            "slug": "Learning-Layered-Motion-Segmentations-of-Video-Kumar-Torr",
            "title": {
                "fragments": [],
                "text": "Learning Layered Motion Segmentations of Video"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An unsupervised approach for learning a layered representation of a scene from a video for motion segmentation applicable to any video containing piecewise parametric motion using \u03b1\u03b2-swap and \u03b1-expansion algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "Their great advantage is the low complexity of inference [4] and hence they have been used in numerous applications [14, 15, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8170470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd0597f8513dc100cd0bc1b493768cde45098a9",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. This problem is hard because of the large number of degrees of freedom to be estimated. Following a established line of research, pose estimation is framed as inference in a probabilistic model. In our experience however, the success of many approaches often lie in the power of the features. Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database."
            },
            "slug": "Learning-to-parse-images-of-articulated-bodies-Ramanan",
            "title": {
                "fragments": [],
                "text": "Learning to parse images of articulated bodies"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work considers the machine vision task of pose estimation from static images, specifically for the case of articulated objects, and casts visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145002914"
                        ],
                        "name": "Joshua Weaver",
                        "slug": "Joshua-Weaver",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Weaver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua Weaver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 88
                            }
                        ],
                        "text": "Previous approaches have concentrated on obtaining the hands by using their skin colour [1, 3, 18] or by detectors based on AdaBoost hand classifiers [8, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13308074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccd238120e1eb620f4bcf235dc549619560f2857",
            "isKey": false,
            "numCitedBy": 1357,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon."
            },
            "slug": "Real-Time-American-Sign-Language-Recognition-Using-Starner-Weaver",
            "title": {
                "fragments": [],
                "text": "Real-Time American Sign Language Recognition Using Desk and Wearable Computer Based Video"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264779"
                        ],
                        "name": "T. Kadir",
                        "slug": "T.-Kadir",
                        "structuredName": {
                            "firstName": "Timor",
                            "lastName": "Kadir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kadir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145398628"
                        ],
                        "name": "R. Bowden",
                        "slug": "R.-Bowden",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Bowden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bowden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143894891"
                        ],
                        "name": "Eng-Jon Ong",
                        "slug": "Eng-Jon-Ong",
                        "structuredName": {
                            "firstName": "Eng-Jon",
                            "lastName": "Ong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eng-Jon Ong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 150
                            }
                        ],
                        "text": "Previous approaches have concentrated on obtaining the hands by using their skin colour [1, 3, 18] or by detectors based on AdaBoost hand classifiers [8, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9330070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a835d1f4f8fc3f1338bd1e408b0bba651c33636",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a flexible monocular system capable of recognising sign lexicons far greater in number than previous approaches. The power of the system is due to four key elements: (i) Head and hand detection based upon boosting which removes the need for temperamental colour segmentation; (ii) A body centred description of activity which overcomes issues with camera placement, calibration and user; (iii) A two stage classification in which stage I generates a high level linguistic description of activity which naturally generalises and hence reduces training; (iv) A stage II classifier bank which does not require HMMs, further reducing training requirements. The outcome of which is a system capable of running in real-time, and generating extremely high recognition rates for large lexicons with as little as a single training instance per sign. We demonstrate classification rates as high as 92% for a lexicon of 164 words with extremely low training requirements outperforming previous approaches where thousands of training examples are required."
            },
            "slug": "Minimal-Training,-Large-Lexicon,-Unconstrained-Sign-Kadir-Bowden",
            "title": {
                "fragments": [],
                "text": "Minimal Training, Large Lexicon, Unconstrained Sign Language Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A flexible monocular system capable of recognising sign lexicons far greater in number than previous approaches and generating extremely high recognition rates for large lexicons with as little as a single training instance per sign is presented."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "It is similar in form to that of [7, 9, 10, 11, 16] and described in detail in Section 2."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1570800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46626dce354feb5e21fde1095cd436e2a7d0c03a",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Part-based tree-structured models have been widely used for 2D articulated human pose-estimation. These approaches admit efficient inference algorithms while capturing the important kinematic constraints of the human body as a graphical model. These methods often fail however when multiple body parts fit the same image region resulting in global pose estimates that poorly explain the overall image evidence. Attempts to solve this problem have focused on the use of strong prior models that are limited to learned activities such as walking. We argue that the problem actually lies with the image observations and not with the prior. In particular, image evidence for each body part is estimated independently of other parts without regard to self-occlusion. To address this we introduce occlusion-sensitive local likelihoods that approximate the global image likelihood using per-pixel hidden binary variables that encode the occlusion relationships between parts. This occlusion reasoning introduces interactions between non-adjacent body parts creating loops in the underlying graphical model. We deal with this using an extension of an approximate belief propagation algorithm (PAMPAS). The algorithm recovers the real-valued 2D pose of the body in the presence of occlusions, does not require strong priors over body pose and does a quantitatively better job of explaining image evidence than previous methods."
            },
            "slug": "Measure-Locally,-Reason-Globally:-Articulated-Pose-Sigal-Black",
            "title": {
                "fragments": [],
                "text": "Measure Locally, Reason Globally: Occlusion-sensitive Articulated Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An extension of an approximate belief propagation algorithm (PAMPAS) that recovers the real-valued 2D pose of the body in the presence of occlusions, does not require strong priors over body pose and does a quantitatively better job of explaining image evidence than previous methods."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 24
                            }
                        ],
                        "text": "Co-segmentation methods [6, 14, 16, 22] consider sets of images where the appearance of foreground and/or background share some similarities, and exploit these similarities to obtain accurate foreground-background segmentations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1041733,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fbfd6a9daa73756196ec663d65019a7a9b58600",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the term cosegmentation which denotes the task of segmenting simultaneously the common parts of an image pair. A generative model for cosegmentation is presented. Inference in the model leads to minimizing an energy with an MRF term encoding spatial coherency and a global constraint which attempts to match the appearance histograms of the common parts. This energy has not been proposed previously and its optimization is challenging and NP-hard. For this problem a novel optimization scheme which we call trust region graph cuts is presented. We demonstrate that this framework has the potential to improve a wide range of research: Object driven image retrieval, video tracking and segmentation, and interactive image editing. The power of the framework lies in its generality, the common part can be a rigid/non-rigid object (or scene), observed from different viewpoints or even similar objects of the same class."
            },
            "slug": "Cosegmentation-of-Image-Pairs-by-Histogram-Matching-Rother-Minka",
            "title": {
                "fragments": [],
                "text": "Cosegmentation of Image Pairs by Histogram Matching - Incorporating a Global Constraint into MRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is demonstrated that this generative model for cosegmentation has the potential to improve a wide range of research: Object driven image retrieval, video tracking and segmentation, and interactive image editing."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183557"
                        ],
                        "name": "Pei Yin",
                        "slug": "Pei-Yin",
                        "structuredName": {
                            "firstName": "Pei",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pei Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 174
                            }
                        ],
                        "text": "In recent years there has been increasing interest in random forest/fern-based methods for tasks such as image classification [2, 19], object detection [8, 11], segmentation [12, 25], head pose estimation [1] and feature extraction [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9442002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0fc9eb62a153d0059da1cf846f0a1968465148e1",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an algorithm for the automatic segmentation of monocular videos into foreground and background layers. Correct segmentations are produced even in the presence of large background motion with nearly stationary foreground. There are three key contributions. The first is the introduction of a novel motion representation, \"motons\", inspired by research in object recognition. Second, we propose learning the segmentation likelihood from the spatial context of motion. The learning is efficiently performed by Random Forests. The third contribution is a general taxonomy of tree-based classifiers, which facilitates theoretical and experimental comparisons of several known classification algorithms, as well as spawning new ones. Diverse visual cues such as motion, motion context, colour, contrast and spatial priors are fused together by means of a conditional random field (CRF) model. Segmentation is then achieved by binary min-cut. Our algorithm requires no initialization. Experiments on many video-chat type sequences demonstrate the effectiveness of our algorithm in a variety of scenes. The segmentation results are comparable to those obtained by stereo systems."
            },
            "slug": "Tree-based-Classifiers-for-Bilayer-Video-Yin-Criminisi",
            "title": {
                "fragments": [],
                "text": "Tree-based Classifiers for Bilayer Video Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel motion representation, \"motons\", inspired by research in object recognition is introduced, and the segmentation likelihood from the spatial context of motion is proposed, which is efficiently performed by Random Forests."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698689"
                        ],
                        "name": "N. Jojic",
                        "slug": "N.-Jojic",
                        "structuredName": {
                            "firstName": "Nebojsa",
                            "lastName": "Jojic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jojic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[15, 17] since part of the background in the video is always moving so we have a dynamic rather than fixed layer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11402717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c0b9240a66110b125bc94e7f6f0c062dd38647c",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a technique for automatically learning layers of \"flexible sprites\" (probabilistic 2-dimensional appearance maps and masks of moving, occluding objects). The model explains each input image as a layered composition of flexible sprites. A variational expectation maximization algorithm is used to learn a mixture of sprites from a video sequence. For each input image, probabilistic inference is used to infer the sprite class, translation, mask values and pixel intensities (including obstructed pixels) in each layer. Exact inference is intractable, but we show how a variational inference technique can be used to process 320/spl times/240 images at 1 frame/second. The only inputs to the learning algorithm are the video sequence, the number of layers and the number of flexible sprites. We give results on several tasks, including summarizing a video sequence with sprites, point-and-click video stabilization, and point-and-click object removal."
            },
            "slug": "Learning-flexible-sprites-in-video-layers-Jojic-Frey",
            "title": {
                "fragments": [],
                "text": "Learning flexible sprites in video layers"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A technique for automatically learning layers of \"flexible sprites\" (probabilistic 2-dimensional appearance maps and masks of moving, occluding objects) is proposed and it is shown how a variational inference technique can be used to process 320/spl times/240 images at 1 frame/second."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6714943"
                        ],
                        "name": "Ryan White",
                        "slug": "Ryan-White",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "White",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 88
                            }
                        ],
                        "text": "Previous approaches have concentrated on obtaining the hands by using their skin colour [1, 3, 18] or by detectors based on AdaBoost hand classifiers [8, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8874010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd357c773c5531b35d9050fcb066fdd35211611f",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We build word models for American Sign Language (ASL) that transfer between different signers and different aspects. This is advantageous because one could use large amounts of labelled avatar data in combination with a smaller amount of labelled human data to spot a large number of words in human data. Transfer learning is possible because we represent blocks of video with novel intermediate discriminative features based on splits of the data. By constructing the same splits in avatar and human data and clustering appropriately, our features are both discriminative and semantically similar: across signers similar features imply similar words. We demonstrate transfer learning in two scenarios: from avatar to a frontally viewed human signer and from an avatar to human signer in a 3/4 view."
            },
            "slug": "Transfer-Learning-in-Sign-language-Farhadi-Forsyth",
            "title": {
                "fragments": [],
                "text": "Transfer Learning in Sign language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work builds word models for ASL that transfer between different signers and different aspects and demonstrates transfer learning in two scenarios: from avatar to a frontally viewed human signer and from an avatar tohuman signer in a 3/4 view."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "(iv) Sharpening instead of smoothing the probability distribution: In [4] the authors recommend that samples be drawn from a smoothed probability distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "The posterior distribution from which samples are drawn is given [4] as"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "Their great advantage is the low complexity of inference [4] and hence they have been used in numerous applications [14, 15, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [4], the colour likelihood of a part is given as a function of the foreground and the surrounding background."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "We therefore adopt a stochastic search for each arm, using an efficient sampling method [4] to propose likely candidate configurations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "2) using the marginal distributions (see [4])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2277383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd9ab441df8b24f473a3635370c69620b00c1e60",
            "isKey": false,
            "numCitedBy": 2423,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images."
            },
            "slug": "Pictorial-Structures-for-Object-Recognition-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Pictorial Structures for Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A computationally efficient framework for part-based modeling and recognition of objects, motivated by the pictorial structure models introduced by Fischler and Elschlager, that allows for qualitative descriptions of visual appearance and is suitable for generic recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717379"
                        ],
                        "name": "D. Hochbaum",
                        "slug": "D.-Hochbaum",
                        "structuredName": {
                            "firstName": "Dorit",
                            "lastName": "Hochbaum",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hochbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711711"
                        ],
                        "name": "Vikas Singh",
                        "slug": "Vikas-Singh",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vikas Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 24
                            }
                        ],
                        "text": "Co-segmentation methods [6, 14, 16, 22] consider sets of images where the appearance of foreground and/or background share some similarities, and exploit these similarities to obtain accurate foreground-background segmentations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9715883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a6e95ffeb450a869bc758dd720bc79e3e30b02f",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is focused on the Co-segmentation problem [1] - where the objective is to segment a similar object from a pair of images. The background in the two images may be arbitrary; therefore, simultaneous segmentation of both images must be performed with a requirement that the appearance of the two sets of foreground pixels in the respective images are consistent. Existing approaches [1, 2] cast this problem as a Markov Random Field (MRF) based segmentation of the image pair with a regularized difference of the two histograms - assuming a Gaussian prior on the foreground appearance [1] or by calculating the sum of squared differences [2]. Both are interesting formulations but lead to difficult optimization problems, due to the presence of the second (histogram difference) term. The model proposed here bypasses measurement of the histogram differences in a direct fashion; we show that this enables obtaining efficient solutions to the underlying optimization model. Our new algorithm is similar to the existing methods in spirit, but differs substantially in that it can be solved to optimality in polynomial time using a maximum flow procedure on an appropriately constructed graph. We discuss our ideas and present promising experimental results."
            },
            "slug": "An-efficient-algorithm-for-Co-segmentation-Hochbaum-Singh",
            "title": {
                "fragments": [],
                "text": "An efficient algorithm for Co-segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The model proposed here bypasses measurement of the histogram differences in a direct fashion and enables obtaining efficient solutions to the underlying optimization model, and can be solved to optimality in polynomial time using a maximum flow procedure on an appropriately constructed graph."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740145"
                        ],
                        "name": "V. Lempitsky",
                        "slug": "V.-Lempitsky",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lempitsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lempitsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 152
                            }
                        ],
                        "text": "In recent years there has been increasing interest in random forest/fern-based methods for tasks such as image classification [2, 19], object detection [8, 11], segmentation [12, 25], head pose estimation [1] and feature extraction [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1532594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f281b87ef58c0dd244ec4743fb1f899b4948308",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for the detection of instances of an object class, such as cars or pedestrians, in natural images. Similarly to some previous works, this is accomplished via generalized Hough transform, where the detections of individual object parts cast probabilistic votes for possible locations of the centroid of the whole object; the detection hypotheses then correspond to the maxima of the Hough image that accumulates the votes from all parts. However, whereas the previous methods detect object parts using generative codebooks of part appearances, we take a more discriminative approach to object part detection. Towards this end, we train a class-specific Hough forest, which is a random forest that directly maps the image patch appearance to the probabilistic vote about the possible location of the object centroid. We demonstrate that Hough forests improve the results of the Hough-transform object detection significantly and achieve state-of-the-art performance for several classes and datasets."
            },
            "slug": "Class-specific-Hough-forests-for-object-detection-Gall-Lempitsky",
            "title": {
                "fragments": [],
                "text": "Class-specific Hough forests for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that Hough forests improve the results of the Hough-transform object detection significantly and achieve state-of-the-art performance for several classes and datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145527707"
                        ],
                        "name": "Zhe L. Lin",
                        "slug": "Zhe-L.-Lin",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Lin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe L. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31423777"
                        ],
                        "name": "D. DeMenthon",
                        "slug": "D.-DeMenthon",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "DeMenthon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeMenthon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "It is similar in form to that of [7, 9, 10, 11, 16] and described in detail in Section 2."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 9521687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f66589967af31dc1e8d597e4b3928fa0bac5a992",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "An interactive human segmentation approach is described. Given regions of interest provided by users, the approach iteratively estimates segmentation via a generalized EM algorithm. Specifically, it encodes both spatial and color information in a nonparametric kernel density estimator, and incorporates local MRF constraints and global pose inferences to propagate beliefs over image space iteratively to determine a coherent segmentation. This ensures the segmented humans resemble the shapes of human poses. Additionally, a layered occlusion model and a probabilistic occlusion reasoning method are proposed to handle segmentation of multiple humans in occlusion. The approach is tested on a wide variety of images containing single or multiple occluded humans, and the segmentation performance is evaluated quantitatively."
            },
            "slug": "An-Interactive-Approach-to-Pose-Assisted-and-of-Lin-Davis",
            "title": {
                "fragments": [],
                "text": "An Interactive Approach to Pose-Assisted and Appearance-based Segmentation of Humans"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "An interactive human segmentation approach that encodes both spatial and color information in a nonparametric kernel density estimator, and incorporates local MRF constraints and global pose inferences to propagate beliefs over image space iteratively to determine a coherent segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144423856"
                        ],
                        "name": "Anna Bosch",
                        "slug": "Anna-Bosch",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Bosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Bosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062941326"
                        ],
                        "name": "X. Mu\u00f1oz",
                        "slug": "X.-Mu\u00f1oz",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Mu\u00f1oz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Mu\u00f1oz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 126
                            }
                        ],
                        "text": "In recent years there has been increasing interest in random forest/fern-based methods for tasks such as image classification [2, 19], object detection [8, 11], segmentation [12, 25], head pose estimation [1] and feature extraction [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17584818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d175a196816e44c08928ad05e30fd774468d69aa",
            "isKey": false,
            "numCitedBy": 1366,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the problem of classifying images by the object categories they contain in the case of a large number of object categories. To this end we combine three ingredients: (i) shape and appearance representations that support spatial pyramid matching over a region of interest. This generalizes the representation of Lazebnik et al., (2006) from an image to a region of interest (ROI), and from appearance (visual words) alone to appearance and local shape (edge distributions); (ii) automatic selection of the regions of interest in training. This provides a method of inhibiting background clutter and adding invariance to the object instance 's position; and (iii) the use of random forests (and random ferns) as a multi-way classifier. The advantage of such classifiers (over multi-way SVM for example) is the ease of training and testing. Results are reported for classification of the Caltech-101 and Caltech-256 data sets. We compare the performance of the random forest/ferns classifier with a benchmark multi-way SVM classifier. It is shown that selecting the ROI adds about 5% to the performance and, together with the other improvements, the result is about a 10% improvement over the state of the art for Caltech-256."
            },
            "slug": "Image-Classification-using-Random-Forests-and-Ferns-Bosch-Zisserman",
            "title": {
                "fragments": [],
                "text": "Image Classification using Random Forests and Ferns"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that selecting the ROI adds about 5% to the performance and, together with the other improvements, the result is about a 10% improvement over the state of the art for Caltech-256."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828154"
                        ],
                        "name": "Duan Tran",
                        "slug": "Duan-Tran",
                        "structuredName": {
                            "firstName": "Duan",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duan Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Adding HOG features also improved the proportion of high overlaps significantly."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "In our experiments we only calculate the HOG appearance term for the left and right forearm since these are often not occluded, and provide the strongest constraint on the wrist position."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "We achieved the best results using a HOG descriptor with cell size of 8\u00d78 pixels, block size of 2\u00d72 cells, and 6 orientation bins."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 232
                            }
                        ],
                        "text": "We define the probability of a given configuration L conditioned on the image I to be\nP(L|I) \u221d P(L) N\n\u220f i=1 p(xi|\u03bbi) \u220f j\u2208{LF,RF} p(y j|l j) (1)\nwhere N is the number of pixels in the input image, xi is the colour of pixel i, and y j is the HOG descriptor computed for limb j (see Section 2.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Adding HOG appearance features to the complete\ncost function did improve the number of highly accurate left and right arm positions, but at the expense of not finding the left arm in some frames."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 186
                            }
                        ],
                        "text": "The appearance term, p(zi|li), is composed of the product of pixel likelihoods using colour distributions modelled by mixtures of Gaussians, and edge and illumination cues added through HOG descriptors (see Section 2.1 for details)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 68
                            }
                        ],
                        "text": "configuration of a part using Histogram of Oriented Gradients (HOG) [2, 19] templates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "The agreement between the template of a given part with configuration li, and the HOG descriptor of the image is evaluated using a distance function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 149
                            }
                        ],
                        "text": "Learning the model: Manually labelled data is required to learn part-specific colour distributions, to build the head and torso model, and to create HOG templates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "HOG templates were learned from 39 images where the true arm configuration was manually specified."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "The image area covered by a template could be used to scale the importance of the HOG cue, although we found that this degraded the results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "The distance function for the HOG appearance term, p(y j|l j), is computed by evaluating a truncated L2 norm and normalising it to be in the range [0,1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "We exploit both boundary and internal features to determine the position and\nconfiguration of a part using Histogram of Oriented Gradients (HOG) [2, 19] templates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "The second appearance term, p(y j|l j), models the likelihood of HOG descriptors extracted for the left and right forearms {LF,RF}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": "A HOG template is learned from manually labelled data for each part, scale and orientation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13902758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35570297681daa3973498eabead361d0be961672",
            "isKey": true,
            "numCitedBy": 63,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Fair discriminative pedestrian finders are now available. In fact, these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data, for example, mounting a bicycle. This is undesirable. However, the human configuration can itself be estimated discriminatively using structure learning. We demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset. We then present features (local histogram of oriented gradient and local PCA of gradient) based on that configuration to an SVM classifier. We show, using the INRIA Person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder."
            },
            "slug": "Configuration-Estimates-Improve-Pedestrian-Finding-Tran-Forsyth",
            "title": {
                "fragments": [],
                "text": "Configuration Estimates Improve Pedestrian Finding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown, using the INRIA Person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319608"
                        ],
                        "name": "Armand Joulin",
                        "slug": "Armand-Joulin",
                        "structuredName": {
                            "firstName": "Armand",
                            "lastName": "Joulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Armand Joulin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 24
                            }
                        ],
                        "text": "Co-segmentation methods [6, 14, 16, 22] consider sets of images where the appearance of foreground and/or background share some similarities, and exploit these similarities to obtain accurate foreground-background segmentations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16611295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d385429e7d8e0552824366d1f70d13781ef789f",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Purely bottom-up, unsupervised segmentation of a single image into foreground and background regions remains a challenging task for computer vision. Co-segmentation is the problem of simultaneously dividing multiple images into regions (segments) corresponding to different object classes. In this paper, we combine existing tools for bottom-up image segmentation such as normalized cuts, with kernel methods commonly used in object recognition. These two sets of techniques are used within a discriminative clustering framework: the goal is to assign foreground/background labels jointly to all images, so that a supervised classifier trained with these labels leads to maximal separation of the two classes. In practice, we obtain a combinatorial optimization problem which is relaxed to a continuous convex optimization problem, that can itself be solved efficiently for up to dozens of images. We illustrate the proposed method on images with very similar foreground objects, as well as on more challenging problems with objects with higher intra-class variations."
            },
            "slug": "Discriminative-clustering-for-image-co-segmentation-Joulin-Bach",
            "title": {
                "fragments": [],
                "text": "Discriminative clustering for image co-segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper combines existing tools for bottom-up image segmentation such as normalized cuts, with kernel methods commonly used in object recognition, used within a discriminative clustering framework to obtain a combinatorial optimization problem which is relaxed to a continuous convex optimization problem that can be solved efficiently for up to dozens of images."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34574494"
                        ],
                        "name": "D. Robertson",
                        "slug": "D.-Robertson",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Robertson",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Robertson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796918"
                        ],
                        "name": "E. Konukoglu",
                        "slug": "E.-Konukoglu",
                        "structuredName": {
                            "firstName": "Ender",
                            "lastName": "Konukoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Konukoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 152
                            }
                        ],
                        "text": "In recent years there has been increasing interest in random forest/fern-based methods for tasks such as image classification [2, 19], object detection [8, 11], segmentation [12, 25], head pose estimation [1] and feature extraction [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18830026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84fe699bea32c11560c8c2d8cf89cb614659a41f",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes multi-class random regression forests as an algorithm for the efficient, automatic detection and localization of anatomical structures within three-dimensional CT scans. \n \nRegression forests are similar to the more popular classification forests, but trained to predict continuous outputs. We introduce a new, continuous parametrization of the anatomy localization task which is effectively addressed by regression forests. This is shown to be a more natural approach than classification. \n \nA single pass of our probabilistic algorithm enables the direct mapping from voxels to organ location and size; with training focusing on maximizing the confidence of output predictions. As a by-product, our method produces salient anatomical landmarks; i.e. automatically selected \"anchor\" regions which help localize organs of interest with high confidence. Quantitative validation is performed on a database of 100 highly variable CT scans. Localization errors are shown to be lower (and more stable) than those from global affine registration approaches. The regressor's parallelism and the simplicity of its context-rich visual features yield typical runtimes of only 1s. Applications include semantic visual navigation, image tagging for retrieval, and initializing organ-specific processing."
            },
            "slug": "Regression-Forests-for-Efficient-Anatomy-Detection-Criminisi-Shotton",
            "title": {
                "fragments": [],
                "text": "Regression Forests for Efficient Anatomy Detection and Localization in CT Studies"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper introduces a new, continuous parametrization of the anatomy localization task which is effectively addressed by regression forests, and shows to be a more natural approach than classification."
            },
            "venue": {
                "fragments": [],
                "text": "MCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 97
                            }
                        ],
                        "text": "There has been much previous work on 2D human pose estimation, mainly using pictorial structures [5, 6] based on tree structured graphical models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9018871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6deeed19c56ec6e737a909de9ee172b93f0d0a89",
            "isKey": false,
            "numCitedBy": 449,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A pictorial structure is a collection of parts arranged in a deformable configuration. Each part is represented using a simple appearance model and the deformable configuration is represented by spring-like connections between pairs of parts. While pictorial structures were introduced a number of years ago, they have not been broadly applied to matching and recognition problems. This has been due in part to the computational difficulty of matching pictorial structures to images. In this paper we present an efficient algorithm for finding the best global match of a pictorial stucture to an image. With this improved algorithm, pictorial structures provide a practical and powerful framework for quantitative descriptions of objects and scenes, and are suitable for many generic image recognition problems. We illustrate the approach using simple models of a person and a car."
            },
            "slug": "Efficient-matching-of-pictorial-structures-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Efficient matching of pictorial structures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An efficient algorithm for finding the best global match of a pictorial stucture to an image is presented and it is shown that this approach is suitable for many generic image recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Adding HOG features also improved the proportion of high overlaps significantly."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "In our experiments we only calculate the HOG appearance term for the left and right forearm since these are often not occluded, and provide the strongest constraint on the wrist position."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "We achieved the best results using a HOG descriptor with cell size of 8\u00d78 pixels, block size of 2\u00d72 cells, and 6 orientation bins."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 232
                            }
                        ],
                        "text": "We define the probability of a given configuration L conditioned on the image I to be\nP(L|I) \u221d P(L) N\n\u220f i=1 p(xi|\u03bbi) \u220f j\u2208{LF,RF} p(y j|l j) (1)\nwhere N is the number of pixels in the input image, xi is the colour of pixel i, and y j is the HOG descriptor computed for limb j (see Section 2.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Adding HOG appearance features to the complete\ncost function did improve the number of highly accurate left and right arm positions, but at the expense of not finding the left arm in some frames."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 186
                            }
                        ],
                        "text": "The appearance term, p(zi|li), is composed of the product of pixel likelihoods using colour distributions modelled by mixtures of Gaussians, and edge and illumination cues added through HOG descriptors (see Section 2.1 for details)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 68
                            }
                        ],
                        "text": "configuration of a part using Histogram of Oriented Gradients (HOG) [2, 19] templates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "The agreement between the template of a given part with configuration li, and the HOG descriptor of the image is evaluated using a distance function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 149
                            }
                        ],
                        "text": "Learning the model: Manually labelled data is required to learn part-specific colour distributions, to build the head and torso model, and to create HOG templates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "HOG templates were learned from 39 images where the true arm configuration was manually specified."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "The image area covered by a template could be used to scale the importance of the HOG cue, although we found that this degraded the results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "The distance function for the HOG appearance term, p(y j|l j), is computed by evaluating a truncated L2 norm and normalising it to be in the range [0,1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "We exploit both boundary and internal features to determine the position and\nconfiguration of a part using Histogram of Oriented Gradients (HOG) [2, 19] templates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "The second appearance term, p(y j|l j), models the likelihood of HOG descriptors extracted for the left and right forearms {LF,RF}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": "A HOG template is learned from manually labelled data for each part, scale and orientation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": true,
            "numCitedBy": 29264,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143945334"
                        ],
                        "name": "Matthew Johnson",
                        "slug": "Matthew-Johnson",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": ") at the nodes of the trees which only compare pairs of pixel values [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 232
                            }
                        ],
                        "text": "In recent years there has been increasing interest in random forest/fern-based methods for tasks such as image classification [2, 19], object detection [8, 11], segmentation [12, 25], head pose estimation [1] and feature extraction [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9952478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d136d77dcdfb34381d8f581f3866d10293a519fd",
            "isKey": false,
            "numCitedBy": 1004,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose semantic texton forests, efficient and powerful new low-level features. These are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of filter-bank responses or local descriptors. They are extremely fast to both train and test, especially compared with k-means clustering and nearest-neighbor assignment of feature descriptors. The nodes in the trees provide (i) an implicit hierarchical clustering into semantic textons, and (ii) an explicit local classification estimate. Our second contribution, the bag of semantic textons, combines a histogram of semantic textons over an image region with a region prior category distribution. The bag of semantic textons is computed over the whole image for categorization, and over local rectangular regions for segmentation. Including both histogram and region prior allows our segmentation algorithm to exploit both textural and semantic context. Our third contribution is an image-level prior for segmentation that emphasizes those categories that the automatic categorization believes to be present. We evaluate on two datasets including the very challenging VOC 2007 segmentation dataset. Our results significantly advance the state-of-the-art in segmentation accuracy, and furthermore, our use of efficient decision forests gives at least a five-fold increase in execution speed."
            },
            "slug": "Semantic-texton-forests-for-image-categorization-Shotton-Johnson",
            "title": {
                "fragments": [],
                "text": "Semantic texton forests for image categorization and segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The proposed semantic texton forests are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of filter-bank responses or local descriptors, and give at least a five-fold increase in execution speed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "The segmentation uses GrabCut [21], with the FG colour model provided by the initialisation set and, as in Ferrari et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6202829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f26d35d2e32934150cd27b030d4d769942126184",
            "isKey": false,
            "numCitedBy": 5202,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for \"border matting\" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools."
            },
            "slug": "\"GrabCut\":-interactive-foreground-extraction-using-Rother-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "\"GrabCut\": interactive foreground extraction using iterated graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A more powerful, iterative version of the optimisation of the graph-cut approach is developed and the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2571487"
                        ],
                        "name": "C. Yanover",
                        "slug": "C.-Yanover",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Yanover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yanover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144538257"
                        ],
                        "name": "Y. Weiss",
                        "slug": "Y.-Weiss",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Our reasoning is similar to that in [20], where the max-marginal is applied to problems where there are multiple possible valid solutions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1571829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d20c904347191acdb8d32b0ddd758c8a14e238be",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Loopy belief propagation (BP) has been successfully used in a number of difficult graphical models to find the most probable configuration of the hidden variables. In applications ranging from protein folding to image analysis one would like to find not just the best configuration but rather the top M. While this problem has been solved using the junction tree formalism, in many real world problems the clique size in the junction tree is prohibitively large. In this work we address the problem of finding the M best configurations when exact inference is impossible. \n \nWe start by developing a new exact inference algorithm for calculating the best configurations that uses only max-marginals. For approximate inference, we replace the max-marginals with the beliefs calculated using max-product BP and generalized BP. We show empirically that the algorithm can accurately and rapidly approximate the M best configurations in graphs with hundreds of variables."
            },
            "slug": "Finding-the-M-Most-Probable-Configurations-using-Yanover-Weiss",
            "title": {
                "fragments": [],
                "text": "Finding the M Most Probable Configurations using Loopy Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work develops a new exact inference algorithm for calculating the best configurations that uses only max-marginals, and shows empirically that the algorithm can accurately and rapidly approximate the M best configurations in graphs with hundreds of variables."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465976"
                        ],
                        "name": "M. Fischler",
                        "slug": "M.-Fischler",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Fischler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394928"
                        ],
                        "name": "R. Elschlager",
                        "slug": "R.-Elschlager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Elschlager",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elschlager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 97
                            }
                        ],
                        "text": "There has been much previous work on 2D human pose estimation, mainly using pictorial structures [5, 6] based on tree structured graphical models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14554383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "719da2a0ddd38e78151e1cb2db31703ea8b2e490",
            "isKey": false,
            "numCitedBy": 1527,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection."
            },
            "slug": "The-Representation-and-Matching-of-Pictorial-Fischler-Elschlager",
            "title": {
                "fragments": [],
                "text": "The Representation and Matching of Pictorial Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The primary problem dealt with in this paper is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689882"
                        ],
                        "name": "R. Mar\u00e9e",
                        "slug": "R.-Mar\u00e9e",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "Mar\u00e9e",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mar\u00e9e"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50206577"
                        ],
                        "name": "P. Geurts",
                        "slug": "P.-Geurts",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Geurts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Geurts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772389"
                        ],
                        "name": "J. Piater",
                        "slug": "J.-Piater",
                        "structuredName": {
                            "firstName": "Justus",
                            "lastName": "Piater",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Piater"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695713"
                        ],
                        "name": "L. Wehenkel",
                        "slug": "L.-Wehenkel",
                        "structuredName": {
                            "firstName": "Louis",
                            "lastName": "Wehenkel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wehenkel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 126
                            }
                        ],
                        "text": "In recent years there has been increasing interest in random forest/fern-based methods for tasks such as image classification [2, 19], object detection [8, 11], segmentation [12, 25], head pose estimation [1] and feature extraction [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 877899,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c6530c66ca72fe0d9a2ec732664b2e63e9a59e5",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel, generic image classification method based on a recent machine learning algorithm (ensembles of extremely randomized decision trees). Images are classified using randomly extracted subwindows that are suitably normalized to yield robustness to certain image transformations. Our method is evaluated on four very different, publicly available datasets (COIL-100, ZuBuD, ETH-80, WANG). Our results show that our automatic approach is generic and robust to illumination, scale, and viewpoint changes. An extension of the method is proposed to improve its robustness with respect to rotation changes."
            },
            "slug": "Random-subwindows-for-robust-image-classification-Mar\u00e9e-Geurts",
            "title": {
                "fragments": [],
                "text": "Random subwindows for robust image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This work presents a novel, generic image classification method based on a recent machine learning algorithm (ensembles of extremely randomized decision trees) that is generic and robust to illumination, scale, and viewpoint changes."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pictorial structures for object recognition. IJCV"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 15,
            "result": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 38,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Long-Term-Arm-and-Hand-Tracking-for-Continuous-Sign-Buehler-Everingham/b9ee00b24137aa79d4d56da48f232db8e49298a2?sort=total-citations"
}