{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2302447"
                        ],
                        "name": "N. Kambhatla",
                        "slug": "N.-Kambhatla",
                        "structuredName": {
                            "firstName": "Nanda",
                            "lastName": "Kambhatla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kambhatla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3222903"
                        ],
                        "name": "T. Leen",
                        "slug": "T.-Leen",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Leen",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 147780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb4b0ff68d58d8ef8c1c2ee2b8bdc0a60cbeb4b4",
            "isKey": false,
            "numCitedBy": 664,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Reducing or eliminating statistical redundancy between the components of high-dimensional vector data enables a lower-dimensional representation without significant loss of information. Recognizing the limitations of principal component analysis (PCA), researchers in the statistics and neural network communities have developed nonlinear extensions of PCA. This article develops a local linear approach to dimension reduction that provides accurate representations and is fast to compute. We exercise the algorithms on speech and image data, and compare performance with PCA and with neural network implementations of nonlinear PCA. We find that both nonlinear techniques can provide more accurate representations than PCA and show that the local linear techniques outperform neural network implementations."
            },
            "slug": "Dimension-Reduction-by-Local-Principal-Component-Kambhatla-Leen",
            "title": {
                "fragments": [],
                "text": "Dimension Reduction by Local Principal Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A local linear approach to dimension reduction that provides accurate representations and is fast to compute is developed and it is shown that the local linear techniques outperform neural network implementations."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": false,
            "numCitedBy": 13976,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13407,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094778056"
                        ],
                        "name": "V. De Silva",
                        "slug": "V.-De-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "De Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. De Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221338160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "isKey": false,
            "numCitedBy": 12180,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
            },
            "slug": "A-global-geometric-framework-for-nonlinear-Tenenbaum-Silva",
            "title": {
                "fragments": [],
                "text": "A global geometric framework for nonlinear dimensionality reduction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set and efficiently computes a globally optimal solution, and is guaranteed to converge asymptotically to the true structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398229863"
                        ],
                        "name": "R. Hecht-Nielsen",
                        "slug": "R.-Hecht-Nielsen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hecht-Nielsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hecht-Nielsen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33988163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c42ee8d7811dec703207001b81177edc51620a86",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Replicator neural networks self-organize by using their inputs as desired outputs; they internally form a compressed representation for the input data. A theorem shows that a class of replicator networks can, through the minimization of mean squared reconstruction error (for instance, by training on raw data examples), carry out optimal data compression for arbitrary data vector sources. Data manifolds, a new general model of data sources, are then introduced and a second theorem shows that, in a practically important limiting case, optimal-compression replicator networks operate by creating an essentially unique natural coordinate system for the manifold."
            },
            "slug": "Replicator-neural-networks-for-universal-optimal-Hecht-Nielsen",
            "title": {
                "fragments": [],
                "text": "Replicator neural networks for universal optimal source coding."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A theorem shows that a class of replicator networks can, through the minimization of mean squared reconstruction error, carry out optimal data compression for arbitrary data vector sources."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546518"
                        ],
                        "name": "D. Plaut",
                        "slug": "D.-Plaut",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Plaut",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plaut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14299616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b59ebc8abee160f1cb34a992a6b07d1c9f7bafb",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-sets-of-filters-using-back-propagation-Plaut-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning sets of filters using back-propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60565534,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "69d7086300e7f5322c06f2f242a565b3a182efb5",
            "isKey": false,
            "numCitedBy": 4647,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Bill Baird { Publications References 1] B. Baird. Bifurcation analysis of oscillating neural network model of pattern recognition in the rabbit olfactory bulb. In D. 3] B. Baird. Bifurcation analysis of a network model of the rabbit olfactory bulb with periodic attractors stored by a sequence learning algorithm. 5] B. Baird. Bifurcation theory methods for programming static or periodic attractors and their bifurcations in dynamic neural networks."
            },
            "slug": "In-Advances-in-Neural-Information-Processing-Hanson",
            "title": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 784288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98b4d4e24aab57ab4e1124ff8106909050645cfa",
            "isKey": false,
            "numCitedBy": 14027,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."
            },
            "slug": "Neural-networks-and-physical-systems-with-emergent-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neural networks and physical systems with emergent collective computational abilities."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A model of a system having a large number of simple equivalent components, based on aspects of neurobiology but readily adapted to integrated circuits, produces a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2204972"
                        ],
                        "name": "M. V. Rossum",
                        "slug": "M.-V.-Rossum",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Rossum",
                            "middleNames": [
                                "C.",
                                "W.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. V. Rossum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2281536,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Lecture Notes for the MSc/DTC module. The brain is a complex computing machine which has evolved to give the ttest output to a given input. Neural computation has as goal to describe the function of the nervous system in mathematical and computational terms. By analysing or simulating the resulting equations, one can better understand its function, research how changes in parameters would eect the function, and try to mimic the nervous system in hardware or software implementations. Neural Computation is a bit like physics, that has been successful in describing numerous physical phenomena. However, approaches developed in those elds not always work for neural computation, because: 1. Physical systems are best studied in reduced, simplied circumstances, but the nervous system is hard to study in isolation. Neurons require a narrow range of operating conditions (temperature, oxygen, presence of other neurons, ion concentrations, ...) under which they work as they should. These conditions are hard to reproduce outside the body. Secondly, the neurons form a highly interconnected network. The function of the nervous systems depends on this connectivity and interaction, by trying to isolate the components, you are likely to alter the function. 2. It is not clear how much detail one needs to describe the computations in the brain. In these lectures we shall see various description levels. 3. Neural signals and neural connectivity are hard to measure, especially, if disturbance and damage to the nervous system is to be kept minimal. Perhaps Neural Computation has more in common with trying to gure out how a complicated machine, such as a computer or car works. Knowledge of the basic physics helps, but is not sucient. Luckily there are factors which perhaps make understanding the brain easier than understanding an arbitrary complicated machine: 1. There is a high degree of conservation across species. This means that animal studies can be used to gain information about the human brain. Furthermore, study of, say, the visual system might help to understand the auditory system. 2. The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives. Therefore it might be possible to nd the organising principles and develop a brain from there. This would be easier than guring out the complete 'wiring diagram'. 3. The nervous system is exible and robust, neurons die everyday. This stands \u2026"
            },
            "slug": "Neural-Computation-Rossum",
            "title": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives, and it might be possible to develop a brain from there."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15726,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195881635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8bd8c8bc854adbfca92b82ce4e76aafa4779fcf3",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following topics are dealt with: document analysis and recognition; multiple classifiers; feature analysis; document understanding; hidden Markov models; text segmentation; character recognition; graphics recognition; non-Latin alphabets - Korean/Hangul; Web documents; word recognition; image processing; Kanji alphabets; Chinese alphabets evaluation and datasets; feature selection; postal automation and cheque processing; image quality and style; classifiers/algorithms; Arabic alphabets; Indian alphabets; forms and tables; writer identification; graphical on-line recognition; online handwriting recognition; handwriting features; and handwriting/document models/algorithms."
            },
            "slug": "Proceedings-Seventh-International-Conference-on-and",
            "title": {
                "fragments": [],
                "text": "Proceedings Seventh International Conference on Document Analysis and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The following topics are dealt with: document analysis and recognition; multiple classifiers; feature analysis; document understanding; hidden Markov models; text segmentation; character recognition; graphics recognition; non-Latin alphabets - Korean/Hangul; Web documents."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 97
                            }
                        ],
                        "text": ", images) can be modeled using a two-layer network called a Brestricted Boltzmann machine[ (RBM) (5, 6) in which stochastic, binary pixels"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "While training higher level RBMs, the visible units were set to the activation probabilities of the hidden units in the previous RBM, but the hidden units of every RBM except the top one had stochastic binary values."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The first layer of feature detectors then become the visible units for learning the next RBM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The hidden units of the top RBM had stochastic real-valued states drawn from a unit variance Gaussian whose mean was determined by the input from that RBM_s logistic visible units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "The pixels correspond to Bvisible[ units of the RBM because their states are observed; the feature detectors correspond to Bhidden[ units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "For continuous data, the hidden units of the first-level RBM remain binary, but the visible units are replaced by linear units with Gaussian noise (10)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 127
                            }
                        ],
                        "text": "An ensemble of binary vectors (e.g., images) can be modeled using a two-layer network called a Brestricted Boltzmann machine[ (RBM) (5, 6) in which stochastic, binary pixels are connected to stochastic, binary feature detectors using symmetrically weighted connections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "In all our experiments, the visible units of every RBM had real-valued activities, which were in the range E0, 1^ for logistic units."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59877636,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "4ca847c502df52aa59980b3564c4b72d7f293a74",
            "isKey": true,
            "numCitedBy": 202,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1,227,914. A die for laminated films. NORTHERN PETROCHEMICAL CO. 25 April, 1969 [26 April, 1968], No. 21298/69. Heading B5B. [Also in Division F2] A die for the extrusion of laminated plastic tubing is shown in Fig. 1. Material for the inner ply is supplied through tube 18, the conduit 19 the axial tube 20 and the four radial tubes 21 to the annular chamber 22 and material for the outer ply is supplied through the tube 28 and the feed hole 31 to the chamber 30. From chambers 22 and 30 the inner and outer ply materials are delivered to the annular conical chamber 43, in a molten state and flow under conditions of laminar flow to the die mouth 44. A molten laminate is produced in chamber 43 by virtue of the ring 42. Air from the supply 35 is delivered to the interior of the tube so formed via the hydraulic coupling 33, the tube 32, the hydraulic coupling 34 and the outlet 36 in order to inflate and hence support the extruded tubing. A mandrel 14, the rear end of which threadably engages a nut 15 and the forward end of which threadably engages mandrel head 25, fits slidably in the cylindrical die casing 10. The chambers 22, 30, 43 and the ring 42 are formed by shaping of the mandrel, the die casing and the mandrel head. As can be seen from Fig. 1 the thickness of the film, and the relative thicknesses of the plies can be varied by relative axial movements of the mandrel 14, mandrel head 25 and the die casing 10. Other features of the die include a nut 15 for adjusting the mandrel position, holes 26 in the mandrel head to engage a spanner to facilitate the adjustment of the mandrel head and an adapter head 17. A modification of the above die is shown in Fig. 2 (not shown) having the following features: a sleeve (54) which separates the mandrel from the die casing; the use of 0-rings for producing seals and an adjustment which allows the eccentricity of the die head to be varied."
            },
            "slug": "Parallel-Distributed-Processing-Volume-1:-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing Volume 1: Foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060364618"
                        ],
                        "name": "J. Ashby",
                        "slug": "J.-Ashby",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Ashby",
                            "middleNames": [
                                "Gary"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ashby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116066461,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ebef523c81a685e695202c99c731e9ee47c6a341",
            "isKey": false,
            "numCitedBy": 3656,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "our experimentation could eventually be used to discredit our findings, should they happen not to agree with the original observations. It seems important that all experiments in the rapidly expanding area of endocrine disruption toxicology should be carefully designed and fully reported. The use of concurrent positive and negative control groups also seems to be prudent. These needs are independent of who conducts or sponsors studies. Good science is good science. Finally, it should be noted that the only formal retraction of endocrine disruption data currently encountered derived from an academic laboratory (15), a salutary counterbalance to the assertions that stimulated this letter (1-3)."
            },
            "slug": "References-and-Notes-Ashby",
            "title": {
                "fragments": [],
                "text": "References and Notes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "135606783"
                        ],
                        "name": "\u59da\u5229\u6653",
                        "slug": "\u59da\u5229\u6653",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\u59da\u5229\u6653",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u59da\u5229\u6653"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87278592"
                        ],
                        "name": "\u8521\u5e7c\u6c11",
                        "slug": "\u8521\u5e7c\u6c11",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\u8521\u5e7c\u6c11",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u8521\u5e7c\u6c11"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87218869"
                        ],
                        "name": "\u6797\u77eb\u77eb",
                        "slug": "\u6797\u77eb\u77eb",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\u6797\u77eb\u77eb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u6797\u77eb\u77eb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71778270"
                        ],
                        "name": "\u5218\u91d1\u660e",
                        "slug": "\u5218\u91d1\u660e",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\u5218\u91d1\u660e",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5218\u91d1\u660e"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 179733831,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "35d2b9da90cde935dcf1ace9b4e12b33ce889279",
            "isKey": false,
            "numCitedBy": 1859,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\u5165\u4fb5\u75c5\u539f\u4f53\u4e0e\u5bbf\u4e3b\u4e4b\u95f4\u5448\u52a8\u6001\u5e73\u8861\uff0c\u4ee5\u7ef4\u6301\u75c5\u539f\u4f53\u6210\u529f\u5730\u5bc4\u751f\u5728\u5bbf\u4e3b\u4f53\u5185\u800c\u4e0d\u81f4\u5bbf\u4e3b\u6b7b\u4ea1\uff0c\u8fd9\u662f\u8bb8\u591a\u5bc4\u751f\u866b\u611f\u67d3\u7684\u4e00\u4e2a\u91cd\u8981\u7279\u5f81\u3002\u5305\u62ec\u66fc\u6c0f\u8840\u5438\u866b\u5728\u5185\u7684\u8bb8\u591a\u8815\u866b\u611f\u67d3\u4e2d\uff0c\u6301\u7eed\u7684\u708e\u75c7\u53cd\u5e94\u6bd4\u75c5\u539f\u4f53\u672c\u8eab\u5bf9\u5bbf\u4e3b\u7684\u5371\u5bb3\u66f4\u5927\uff0c\u964d\u4f4e\u5bbf\u4e3b\u7684\u514d\u75ab\u53cd\u5e94\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u66fc\u6c0f\u8840\u5438\u866b\u611f\u67d3\u540e\uff0c\u5bbf\u4e3b\u6d3b\u5316CD4^\uff0bTh2\u7ec6\u80de\uff0c\u5206\u6cccIL-4\u3001IL-5\u548cIL-13\u3002\u6700\u8fd1\u7814\u7a76\u8868\u660eIL-13\u662f\u809d\u7ec4\u7ec7\u7ea4\u7ef4\u5316\u7684\u91cd\u8981\u8c03\u8282\u56e0\u5b50\u3002"
            },
            "slug": "IL-13\u53d7\u4f53\u03b12\u964d\u4f4e\u8840\u5438\u866b\u75c5\u8089\u82bd\u80bf\u7684\u708e\u75c7\u53cd\u5e94\u5e76\u5ef6\u957f\u5bbf\u4e3b\u5b58\u6d3b\u65f6\u95f4[\u82f1]/Mentink-Kane-U-\u59da\u5229\u6653-\u8521\u5e7c\u6c11",
            "title": {
                "fragments": [],
                "text": "IL-13\u53d7\u4f53\u03b12\u964d\u4f4e\u8840\u5438\u866b\u75c5\u8089\u82bd\u80bf\u7684\u708e\u75c7\u53cd\u5e94\u5e76\u5ef6\u957f\u5bbf\u4e3b\u5b58\u6d3b\u65f6\u95f4[\u82f1]/Mentink-Kane MM,Cheever AW\uff0cThompson RW\uff0cet al//Proc Natl Acad Sci U S A"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143673845"
                        ],
                        "name": "P. Slusallek",
                        "slug": "P.-Slusallek",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Slusallek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Slusallek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144033462"
                        ],
                        "name": "P. Shirley",
                        "slug": "P.-Shirley",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Shirley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shirley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913999"
                        ],
                        "name": "W. Mark",
                        "slug": "W.-Mark",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Mark",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Mark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34574034"
                        ],
                        "name": "G. Stoll",
                        "slug": "G.-Stoll",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Stoll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Stoll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145357585"
                        ],
                        "name": "I. Wald",
                        "slug": "I.-Wald",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Wald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Wald"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36180426,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cf7d7684600d3ebe916ca093eda123a9dad41459",
            "isKey": false,
            "numCitedBy": 3110,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Parallel-&-distributed-processing-Slusallek-Shirley",
            "title": {
                "fragments": [],
                "text": "Parallel & distributed processing"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH Courses"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For the conjugate gradient fine-tuning, we used Carl Rasmussen's \" minimize \" code available at http"
            },
            "venue": {
                "fragments": [],
                "text": "For the conjugate gradient fine-tuning, we used Carl Rasmussen's \" minimize \" code available at http"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Olivetti face data set is available at www"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The MNIST data set is available at http"
            },
            "venue": {
                "fragments": [],
                "text": "The MNIST data set is available at http"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Comput"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Comput"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Microw. Theory Tech"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Microw. Theory Tech"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Roweis for helpful discussions, and the Natural Sciences and Engineering Research Council of Canada for funding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The 20 newsgroups dataset (called 20news-bydate.tar.gz) is available at http://people"
            },
            "venue": {
                "fragments": [],
                "text": "The 20 newsgroups dataset (called 20news-bydate.tar.gz) is available at http://people"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Roweis for helpful discussions, and the Natural Sciences and Engineering Research Council of Canada for funding. G.E.H. is a fellow of the Canadian Institute for Advanced Research"
            },
            "venue": {
                "fragments": [],
                "text": "Roweis for helpful discussions, and the Natural Sciences and Engineering Research Council of Canada for funding. G.E.H. is a fellow of the Canadian Institute for Advanced Research"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phys. Rev. B"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. B"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phys. Rev. B"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. B"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phys. Rev. B"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. B"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matlab code for generating the images of curves is available at http"
            },
            "venue": {
                "fragments": [],
                "text": "Matlab code for generating the images of curves is available at http"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Supporting Online Material www.sciencemag.org/cgi/content/full/313/5786/504/DC1 Materials and Methods Figs. S1 to S5 Matlab Code 20"
            },
            "venue": {
                "fragments": [],
                "text": "Supporting Online Material www.sciencemag.org/cgi/content/full/313/5786/504/DC1 Materials and Methods Figs. S1 to S5 Matlab Code 20"
            },
            "year": 1126
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soukoulis for discussions. The research of M"
            },
            "venue": {
                "fragments": [],
                "text": "W. is supported by the Leibniz award 2000 of the Deutsche Forschungsgemeinschaft (DFG), that of S.L. through a Helmholtz-Hochschul-Nachwuchsgruppe (VH-NG-232). References and Notes"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci"
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "See supporting material on Science Online"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Opt. Express"
            },
            "venue": {
                "fragments": [],
                "text": "Opt. Express"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matlab code for LLE is available at http"
            },
            "venue": {
                "fragments": [],
                "text": "Matlab code for LLE is available at http"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extreme Nonlinear Optics"
            },
            "venue": {
                "fragments": [],
                "text": "Extreme Nonlinear Optics"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Opt. Lett"
            },
            "venue": {
                "fragments": [],
                "text": "Opt. Lett"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Journal of Machine Learning Research"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Machine Learning Research"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Materials and Methods are available as supporting material on Science Online"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Reuter Corpus Volume 2 is available at http"
            },
            "venue": {
                "fragments": [],
                "text": "The Reuter Corpus Volume 2 is available at http"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e?sort=total-citations"
}