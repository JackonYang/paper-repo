{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 51
                            }
                        ],
                        "text": "This is an extended version of an earlier article (Amari, 1996), including new results."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 139
                            }
                        ],
                        "text": "A preliminary investigation into the performance of the natural gradient learning algorithm has been undertaken by Douglas, Chichocki, and Amari (1996) and Amari et al. (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "(See Murata, Mu\u0308ller, Ziehe, and Amari (1996) for a more general case.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7589577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa0c75a9b5f39d166dd875005580687716a236bb",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The parameter space of neural networks has a Riemannian metric structure. The natural Riemannian gradient should be used instead of the conventional gradient, since the former denotes the true steepest descent direction of a loss function in the Riemannian space. The behavior of the stochastic gradient learning algorithm is much more effective if the natural gradient is used. The present paper studies the information-geometrical structure of perceptrons and other networks, and prove that the on-line learning method based on the natural gradient is asymptotically as efficient as the optimal batch algorithm. Adaptive modification of the learning constant is proposed and analyzed in terms of the Riemannian measure and is shown to be efficient. The natural gradient is finally applied to blind separation of mixtured independent signal sources."
            },
            "slug": "Neural-Learning-in-Structured-Parameter-Spaces-Amari",
            "title": {
                "fragments": [],
                "text": "Neural Learning in Structured Parameter Spaces - Natural Riemannian Gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The present paper studies the information-geometrical structure of perceptrons and other networks, and proves that the on-line learning method based on the natural gradient is asymptotically as efficient as the optimal batch algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051036"
                        ],
                        "name": "Hyeyoung Park",
                        "slug": "Hyeyoung-Park",
                        "structuredName": {
                            "firstName": "Hyeyoung",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeyoung Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693668"
                        ],
                        "name": "K. Fukumizu",
                        "slug": "K.-Fukumizu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Fukumizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukumizu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6471036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55a5dbc05fe8e362e0050b36dbbc4886011c4a32",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-natural-gradient-learning-algorithms-for-Park-Amari",
            "title": {
                "fragments": [],
                "text": "Adaptive natural gradient learning algorithms for various stochastic models"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051036"
                        ],
                        "name": "Hyeyoung Park",
                        "slug": "Hyeyoung-Park",
                        "structuredName": {
                            "firstName": "Hyeyoung",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeyoung Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693668"
                        ],
                        "name": "K. Fukumizu",
                        "slug": "K.-Fukumizu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Fukumizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukumizu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6468689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c20d6c5bd45b7f8d484bf24b01f43d3f7d46d57",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The natural gradient learning method is known to have ideal performances for on-line training of multilayer perceptrons. It avoids plateaus, which give rise to slow convergence of the backpropagation method. It is Fisher efficient, whereas the conventional method is not. However, for implementing the method, it is necessary to calculate the Fisher information matrix and its inverse, which is practically very difficult. This article proposes an adaptive method of directly obtaining the inverse of the Fisher information matrix. It generalizes the adaptive Gauss-Newton algorithms and provides a solid theoretical justification of them. Simulations show that the proposed adaptive method works very well for realizing natural gradient learning."
            },
            "slug": "Adaptive-Method-of-Realizing-Natural-Gradient-for-Amari-Park",
            "title": {
                "fragments": [],
                "text": "Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An adaptive method of directly obtaining the inverse of the Fisher information matrix is proposed and it generalizes the adaptive Gauss-Newton algorithms and provides a solid theoretical justification of them."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8035080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "650319158e20a678bb6bc9352270c1f41a108479",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Several studies have shown that natural gradient descent for on-line learning is much more efficient than standard gradient descent. In this article, we derive natural gradients in a slightly different manner and discuss implications for batch-mode learning and pruning, linking them to existing algorithms such as Levenberg-Marquardt optimization and optimal brain surgeon. The Fisher matrix plays an important role in all these algorithms. The second half of the article discusses a layered approximation of the Fisher matrix specific to multilayered perceptrons. Using this approximation rather than the exact Fisher matrix, we arrive at much faster natural learning algorithms and more robust pruning procedures."
            },
            "slug": "On-Natural-Learning-and-Pruning-in-Multilayered-Heskes",
            "title": {
                "fragments": [],
                "text": "On Natural Learning and Pruning in Multilayered Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article derives natural gradients in a slightly different manner and discusses implications for batch-mode learning and pruning, linking them to existing algorithms such as Levenberg-Marquardt optimization and optimal brain surgeon."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22304550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e88407f3a5591ca5c46c4c26751bdeba1e42a41",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The natural gradient descent method is applied to train an n-m-1 multilayer perceptron. Based on an efficient scheme to represent the Fisher information matrix for an n-m-1 stochastic multilayer perceptron, a new algorithm is proposed to calculate the natural gradient without inverting the Fisher information matrix explicitly. When the input dimension n is much larger than the number of hidden neurons m, the time complexity of computing the natural gradient is O(n)."
            },
            "slug": "Complexity-Issues-in-Natural-Gradient-Descent-for-Yang-Amari",
            "title": {
                "fragments": [],
                "text": "Complexity Issues in Natural Gradient Descent Method for Training Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new algorithm is proposed to calculate the natural gradient without inverting the Fisher information matrix explicitly, when the input dimension n is much larger than the number of hidden neurons m."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391278"
                        ],
                        "name": "Richard Hans Robert Hahnloser",
                        "slug": "Richard-Hans-Robert-Hahnloser",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hahnloser",
                            "middleNames": [
                                "Hans",
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Hans Robert Hahnloser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15923382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "683406c6b438496664e69827325092283eddc0d7",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this article is to investigate a mechanical description of learning. A framework for local and simple learning algorithms based on interpreting a neural network as a set of configuration constraints is proposed. For any architectural design and learning task, unsupervised and supervised algorithms can be derived, optionally using unconstrained and hidden neurons. Unlike algorithms based on the gradient in weight space, the proposed tangential correlation (TC) algorithms move along the gradient in state space. This results in optimal scaling properties and simple expressions for the weight updates. The number of synapses is much larger than the number of neurons. A constraint for neural states does not impose a unique constraint for synaptic weights. Which weights to assign credit to can be selected from a parametrization of all weight changes equivalently satisfying the state constraints. At the heart of the parametrization are minimal weight changes. Two supervised algorithms (differing by their parametrizations) operating on a three-layer perceptron are compared with standard backpropagation. The successful training of fixed points of recurrent networks is demonstrated. The unsupervised learning of oscillations with variable frequencies is performed on standard and more sophisticated recurrent networks. The results presented here can be useful both for the analysis and for the synthesis of learning algorithms."
            },
            "slug": "Learning-algorithms-based-on-linearization.-Hahnloser",
            "title": {
                "fragments": [],
                "text": "Learning algorithms based on linearization."
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A framework for local and simple learning algorithms based on interpreting a neural network as a set of configuration constraints is proposed and can be useful both for the analysis and for the synthesis of learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30067553"
                        ],
                        "name": "Barkai",
                        "slug": "Barkai",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Barkai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barkai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076815081"
                        ],
                        "name": "Seung",
                        "slug": "Seung",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Seung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30053069"
                        ],
                        "name": "Sompolinsky",
                        "slug": "Sompolinsky",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sompolinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sompolinsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sompolinsky, Barkai, and Seung (1995), and  Barkai, Seung, and Sompolinsky (1995)  proposed an adaptive method of adjusting the learning rate (see also Amari, 1967)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Sompolinsky et al. (1995) (see also  Barkai et al., 1995 ) proposed a rule of adaptive change of \u00b7t, which is applicable to the pattern classification problem where the expected loss L.w/ is not differentiable atw\u2044."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 43
                            }
                        ],
                        "text": "Sompolinsky, Barkai, and Seung (1995), and Barkai, Seung, and Sompolinsky (1995) proposed an adaptive method of adjusting the learning rate (see also Amari, 1967)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 36
                            }
                        ],
                        "text": "Sompolinsky et al. (1995) (see also Barkai et al., 1995) proposed a rule of adaptive change of \u03b7t, which is applicable to the pattern classification problem where the expected loss L(w) is not differentiable atw\u2217."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4306442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e41aa33aeb98fbeb3c055d6688366e7f247c5b1",
            "isKey": true,
            "numCitedBy": 15,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the performance of an on-line algorithm for learning di-chotomies, with a dynamical error-dependent learning rate. The asymp-totic scaling form of the solution to the associated Markov equations is derived, assuming certain smoothness conditions. We show that the system converges to the optimal solution and the generalization error vanishes inversely with the number of examples. The system is capable of escaping from local minima, and adapts rapidly to shifts in the target function. The general theory is illustrated for the perceptron and committee machine. Much of learning theory has analyzed the paradigm of batch learning, in which the learner has free access to a xed set of examples stored in memory. This paradigm leads naturally to an equilibrium statistical mechanical approach based on an energy function that is the learner's error on the training set. The theoretical advantage of this equilibrium formulation is that the learner's performance as a function of the number of examples can be studied without addressing the complex dynamic aspects of learning. The disadvantage is that it gives no information about the learner's performance as a function of training time. Many learning phenomena, especially biological ones, t more naturally in an on-line learning paradigm, rather than a batch one. At each time step, the learner receives a single new example drawn at random, and is unable to store previous examples in memory (see e.g. 1]{{4]). In the simplest cases, the change in the learner depends only on the new example and its current state. Hence training time is proportional to the number of examples. Interesting theoretical results have been derived for on-line learning of functions that are smooth functions of the system's parameters, using stochastic approximation theoryy5] and statistical mechanical methodss1]{{4]. In this Letter , we focus on learning of dichotomies, to which most of this existing theory 1"
            },
            "slug": "Local-and-global-convergence-of-on-line-learning.-Barkai-Seung",
            "title": {
                "fragments": [],
                "text": "Local and global convergence of on-line learning."
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This Letter studies the performance of an on-line algorithm for learning di-chotomies, with a dynamical error-dependent learning rate, and shows that the system converges to the optimal solution and the generalization error vanishes inversely with the number of examples."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693668"
                        ],
                        "name": "K. Fukumizu",
                        "slug": "K.-Fukumizu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Fukumizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukumizu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11198551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27b89885140da973ee04469c032ebeb02f921053",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Local-minima-and-plateaus-in-hierarchical-of-Fukumizu-Amari",
            "title": {
                "fragments": [],
                "text": "Local minima and plateaus in hierarchical structures of multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The independent component analysis or the mutual information criterion also gives a similar loss function (Comon, 1994;  Amari et al., 1996;  see also Oja & Karhunen, 1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "gradient in  Amari, Cichocki, and Yang (1996) ; see also Yang and Amari (1997) for detail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399027888"
                        ],
                        "name": "\u00c1. Navia-V\u00e1zquez",
                        "slug": "\u00c1.-Navia-V\u00e1zquez",
                        "structuredName": {
                            "firstName": "\u00c1ngel",
                            "lastName": "Navia-V\u00e1zquez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c1. Navia-V\u00e1zquez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398493114"
                        ],
                        "name": "A. Figueiras-Vidal",
                        "slug": "A.-Figueiras-Vidal",
                        "structuredName": {
                            "firstName": "An\u00edbal",
                            "lastName": "Figueiras-Vidal",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Figueiras-Vidal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36196421,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19683a8e7584e4553e219867dde6d3065c693bbd",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The attractive possibility of applying layerwise block training algorithms to multilayer perceptrons MLP, which offers initial advantages in computational effort, is refined in this article by means of introducing a sensitivity correction factor in the formulation. This results in a clear performance advantage, which we verify in several applications. The reasons for this advantage are discussed and related to implicit relations with second-order techniques, natural gradient formulations through Fisher's information matrix, and sample selection. Extensions to recurrent networks and other research lines are suggested at the close of the article."
            },
            "slug": "Efficient-Block-Training-of-Multilayer-Perceptrons-Navia-V\u00e1zquez-Figueiras-Vidal",
            "title": {
                "fragments": [],
                "text": "Efficient Block Training of Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The attractive possibility of applying layerwise block training algorithms to multilayer perceptrons MLP, which offers initial advantages in computational effort, is refined in this article by means of introducing a sensitivity correction factor in the formulation."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2685825"
                        ],
                        "name": "K. Kurata",
                        "slug": "K.-Kurata",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Kurata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kurata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145930529"
                        ],
                        "name": "H. Nagaoka",
                        "slug": "H.-Nagaoka",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Nagaoka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nagaoka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It was shown in  Amari et al. (1992)  that the dynamic behavior of natural gradient in the Boltzmann machine is excellent."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Riemannian metric structures are introduced by means of information geometry (Amari, 1985; Murray and Rice, 1993; Amari, 1997a;  Amari, Kurata, & Nagoska, 1992 )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 16
                            }
                        ],
                        "text": "It was shown in Amari et al. (1992) that the dynamic behavior of natural gradient in the Boltzmann machine is excellent."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10534535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e13bcc9abc8520e29db1a2064213f297078b370",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule."
            },
            "slug": "Information-geometry-of-Boltzmann-machines-Amari-Kurata",
            "title": {
                "fragments": [],
                "text": "Information geometry of Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established and the meaning of geometrical structures is elucidated from the stochastic and the statistical point of view."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145749654"
                        ],
                        "name": "M. Rattray",
                        "slug": "M.-Rattray",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Rattray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rattray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2506116"
                        ],
                        "name": "D. Saad",
                        "slug": "D.-Saad",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Saad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Saad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121666312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e6adf581dd8dfdf259a7e25c0c7158d50a4293",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural gradient descent is an on-line variable-metric optimization algorithm which utilizes an underlying Riemannian parameter space. We analyze the dynamics of natural gradient descent beyond the asymptotic regime by employing an exact statistical mechanics description of learning in two-layer feed-forward neural networks. For a realizable learning scenario we find significant improvements over standard gradient descent for both the transient and asymptotic stages of learning, with a slower power law increase in learning time as task complexity grows."
            },
            "slug": "Natural-gradient-descent-for-on-line-learning-Rattray-Saad",
            "title": {
                "fragments": [],
                "text": "Natural gradient descent for on-line learning"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work analyzes the dynamics of natural gradient descent beyond the asymptotic regime by employing an exact statistical mechanics description of learning in two-layer feed-forward neural networks and finds significant improvements over standard gradient descent for both the transient and asymPTotic stages of learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29964577"
                        ],
                        "name": "Heskes",
                        "slug": "Heskes",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Heskes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heskes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120185866"
                        ],
                        "name": "Kappen",
                        "slug": "Kappen",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Kappen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kappen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Heskes and Kappen (1991) obtained similar results, which ignited research into online learning."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 17571646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58df58ec0ebe600f0e15d8595785415134ca3c79",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the learning dynamics of neural networks from a general point of view. The environment from which the network learns is defined as a set of input stimuli. At discrete points in time, one of these stimuli is presented and an incremental learning step takes place. If the time between learning steps is drawn from a Poisson distribution, the dynamics of an ensemble of learning processes is described by a continuous-time master equation. A learning algorithm that enables a neural network to adapt to a changing environment must have a nonzero learning parameter. This constant adaptability, however, goes at cost of fluctuations in the plasticities, such as synapses and thresholds. The ensemble description allows us to study the asymptotic behavior of the plasticities for a large class of neural networks. For small learning parameters, we derive an expression for the size of the fluctuations in an unchanging environment. In a changing environment, there is a trade-off between adaptability and accuracy (i.e., size of the fluctuations). We use the networks of Grossberg [J. Stat. Phys. 48, 105 (1969)] and Oja [J. Math. Biol. 15, 267 (1982)] as simple examples to analyze and simulate the performance of neural networks in a changing environment. In some cases an optimal learning parameter can be calculated."
            },
            "slug": "Learning-processes-in-neural-networks.-Heskes-Kappen",
            "title": {
                "fragments": [],
                "text": "Learning processes in neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The ensemble description allows us to study the asymptotic behavior of the plasticities for a large class of neural networks and derives an expression for the size of the fluctuations in an unchanging environment for small learning parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. A, Atomic, molecular, and optical physics"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 101
                            }
                        ],
                        "text": "This principle was used to derive the natural gradient in Amari, Cichocki, and Yang (1996); see also Yang and Amari (1997) for detail."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 104
                            }
                        ],
                        "text": "However, this is much better than the direct inversion of the original (n+ 1)m-dimensional matrix of G. Yang and Amari (1997) performed a preliminary study on the performance of the natural gradient learning algorithm for a simple multilayer perceptron."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "However, a preliminary analysis (Yang & Amari, 1997), by using a simple model, shows that the performance of natural gradient learning is remarkably good, and it is sometimes free from being trapped in plateaus, which give rise to slow convergence of the backpropagation learning method (Saad &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1188335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8880484e4be05f8eabf408e7a9267e8214e411fe",
            "isKey": false,
            "numCitedBy": 368,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "There are two major approaches for blind separation: maximum entropy (ME) and minimum mutual information (MMI). Both can be implemented by the stochastic gradient descent method for obtaining the demixing matrix. The MI is the contrast function for blind separation; the entropy is not. To justify the ME, the relation between ME and MMI is first elucidated by calculating the first derivative of the entropy and proving that the mean subtraction is necessary in applying the ME and at the solution points determined by the MI, the ME will not update the demixing matrix in the directions of increasing the cross-talking. Second, the natural gradient instead of the ordinary gradient is introduced to obtain efficient algorithms, because the parameter space is a Riemannian space consisting of matrices. The mutual information is calculated by applying the Gram-Charlier expansion to approximate probability density functions of the outputs. Finally, we propose an efficient learning algorithm that incorporates with an adaptive method of estimating the unknown cumulants. It is shown by computer simulation that the convergence of the stochastic descent algorithms is improved by using the natural gradient and the adaptively estimated cumulants."
            },
            "slug": "Adaptive-Online-Learning-Algorithms-for-Blind-and-Yang-Amari",
            "title": {
                "fragments": [],
                "text": "Adaptive Online Learning Algorithms for Blind Separation: Maximum Entropy and Minimum Mutual Information"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown by computer simulation that the convergence of the stochastic descent algorithms is improved by using the natural gradient and the adaptively estimated cumulants."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2184725"
                        ],
                        "name": "Tianping Chen",
                        "slug": "Tianping-Chen",
                        "structuredName": {
                            "firstName": "Tianping",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianping Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33315484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7a64bc8947a1889e8f827917e3bca5babc4369f",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stability-Analysis-of-Learning-Algorithms-for-Blind-Amari-Chen",
            "title": {
                "fragments": [],
                "text": "Stability Analysis of Learning Algorithms for Blind Source Separation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9700808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d9673dd6af71d6a3c19e72e1815b3990bc60743",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning algorithms and underlying basic mathematical ideas are presented for the problem of adaptive blind signal processing, especially instantaneous blind separation and multichannel blind deconvolution/equalization of independent source signals. We discuss developments of adaptive learning algorithms based on the natural gradient approach and their properties concerning convergence, stability, and efficiency. Several promising schemas are proposed and reviewed in the paper. Emphasis is given to neural networks or adaptive filtering models and associated online adaptive nonlinear learning algorithms. Computer simulations illustrate the performances of the developed algorithms. Some results presented in this paper are new and are being published for the first time."
            },
            "slug": "Adaptive-blind-signal-processing-neural-network-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "Adaptive blind signal processing-neural network approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Learning algorithms and underlying basic mathematical ideas are presented for the problem of adaptive blind signal processing, especially instantaneous blind separation and multichannel blind deconvolution/equalization of independent source signals."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 64
                            }
                        ],
                        "text": "Such a loss function is obtained by the maximum entropy method (Bell & Sejnowski, 1995), independent component analysis (Comon, 1994), or the statistical likelihood method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Such a loss function is obtained by the maximum entropy method ( Bell & Sejnowski, 1995 ), independent component analysis (Comon, 1994), or the statistical likelihood method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 269
                            }
                        ],
                        "text": "\u2026loss is\nL(W) = E[l(x,W)],\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf by guest on 18 Septem ber 2021\nwhich represents the entropy of the output y after a componentwise nonlinear transformation (Nadal & Parga, 1994; Bell & Sejnowski, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "which represents the entropy of the outputy after a componentwise nonlinear transformation (Nadal & Parga, 1994;  Bell & Sejnowski, 1995 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": true,
            "numCitedBy": 8756,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653061"
                        ],
                        "name": "N. Murata",
                        "slug": "N.-Murata",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Murata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Murata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 166
                            }
                        ],
                        "text": "The results can be stated in terms of the generalization error instead of the covariance of the estimator, and we can obtain more universal results (see Amari, 1993; Amari & Murata, 1993)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The results can be stated in terms of the generalization error instead of the covariance of the estimator, and we can obtain more universal results (see Amari, 1993;  Amari & Murata, 1993 )."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 11148511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "882b6899d07ce3498a04195885150ef87c02d1c3",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m/(2t), while the training error converges as H0 - m/(2t), where t is the number of examples and m shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples."
            },
            "slug": "Statistical-Theory-of-Learning-Curves-under-Loss-Amari-Murata",
            "title": {
                "fragments": [],
                "text": "Statistical Theory of Learning Curves under Entropic Loss Criterion"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A universal property of learning curves is elucidated, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastics machine is improved as the number of training examples increases."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48988892"
                        ],
                        "name": "S. Douglas",
                        "slug": "S.-Douglas",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Douglas",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Douglas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109418851"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "H.-H.",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "A preliminary investigation into the performance of the natural gradient learning algorithm has been undertaken by Douglas, Chichocki, and Amari (1996) and Amari et al. (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10177216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "692d4c7bfc4da8592e95edc6c3a31edbf0cb4976",
            "isKey": false,
            "numCitedBy": 370,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Multichannel deconvolution and equalization is an important task for numerous applications in communications, signal processing, and control. We extend the efficient natural gradient search method of Amari, Cichocki and Yang (see Advances in Neural Information Processing Systems, p.752-63, 1995) to derive a set of on-line algorithms for combined multichannel blind source separation and time-domain deconvolution/equalization of additive, convolved signal mixtures. We prove that the doubly-infinite multichannel equalizer based on the maximum entropy cost function with natural gradient possesses the so-called \"equivariance property\" such that its asymptotic performance depends on the normalized stochastic distribution of the source signals and not on the characteristics of the unknown channel. Simulations indicate the ability of the algorithm to perform efficient simultaneous multichannel signal deconvolution and source separation."
            },
            "slug": "Multichannel-blind-deconvolution-and-equalization-Amari-Douglas",
            "title": {
                "fragments": [],
                "text": "Multichannel blind deconvolution and equalization using the natural gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that the doubly-infinite multichannel equalizer based on the maximum entropy cost function with natural gradient possesses the so-called \"equivariance property\" such that its asymptotic performance depends on the normalized stochastic distribution of the source signals and not on the characteristics of the unknown channel."
            },
            "venue": {
                "fragments": [],
                "text": "First IEEE Signal Processing Workshop on Signal Processing Advances in Wireless Communications"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The results can be stated in terms of the generalization error instead of the covariance of the estimator, and we can obtain more universal results (see  Amari, 1993;  Amari & Murata, 1993)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 153
                            }
                        ],
                        "text": "The results can be stated in terms of the generalization error instead of the covariance of the estimator, and we can obtain more universal results (see Amari, 1993; Amari & Murata, 1993)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6195781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa34ab3c4532b22af861ba55db3dafdaf1b6f616",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-universal-theorem-on-learning-curves-Amari",
            "title": {
                "fragments": [],
                "text": "A universal theorem on learning curves"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235517"
                        ],
                        "name": "J. Basak",
                        "slug": "J.-Basak",
                        "structuredName": {
                            "firstName": "Jayanta",
                            "lastName": "Basak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Basak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1203934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc32c1d0a0be56a38bce830373233b5879cd7459",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A new, efficient algorithm for blind separation of uniformly distributed sources is proposed. The mixing matrix is assumed to be orthogonal by prewhitening the observed signals. The learning rule adaptively estimates the mixing matrix by conceptually rotating a unit hypercube so that all output signal components are contained within or on the hypercube. Under some ideal constraints, it has been theoretically shown that the algorithm is very similar to an ideal convergent algorithm, which is much faster than the existing convergent algorithms. The algorithm has been generalized to take care of the noisy signals by adaptively dilating the hypercube in conjunction with its rotation."
            },
            "slug": "Blind-Separation-of-a-Mixture-of-Uniformly-Source-A-Basak-Amari",
            "title": {
                "fragments": [],
                "text": "Blind Separation of a Mixture of Uniformly Distributed Source Signals: A Novel Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A new, efficient algorithm for blind separation of uniformly distributed sources is proposed that is very similar to an ideal convergent algorithm, which is much faster than the existing convergent algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "The superefficiency of some algorithms has been also proved in Amari (1997b) under certain conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 79
                            }
                        ],
                        "text": "This was given in Amari (1987) from the point of view of information geometry (Amari, 1985, 1997a; Murray & Rice, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 40
                            }
                        ],
                        "text": "However, a preliminary analysis (Yang & Amari, 1997), by using a simple model, shows that the performance of natural gradient learning is remarkably good, and it is sometimes free from being trapped in plateaus, which give rise to slow convergence of the backpropagation learning method (Saad &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 113
                            }
                        ],
                        "text": "However, this is much better than the direct inversion of the original (n+ 1)m-dimensional matrix of G. Yang and Amari (1997) performed a preliminary study on the performance of the natural gradient learning algorithm for a simple multilayer perceptron."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The superefficiency of some algorithms has been also proved in  Amari (1997b)  under certain conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "The Riemannian metric structures are introduced by means of information geometry (Amari, 1985; Murray and Rice, 1993; Amari, 1997a; Amari, Kurata, & Nagoska, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "This principle was used to derive the natural gradient in Amari, Cichocki, and Yang (1996); see also Yang and Amari (1997) for detail."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12691606,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0998320b465fad2e3260bb164ee9147302b9e21",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Blind source separation is the problem of extracting independent signals from their mixtures without knowing the mixing coefficients nor the probability distributions of source signals and may be applied to EEG and MEG imaging of the brain. It is already known that certain algorithms work well for the extraction of independent components. The present paper is concerned with superefficiency of these based on the statistical and dynamical analysis. In a statistical estimation using t examples, the covariance of any two extracted independent signals converges to 0 of the order of 1/t. On-line dynamics shows that the covariance is of the order of /spl eta/ when the learning rate /spl eta/ is fixed to a small constant. In contrast with the above general properties, a surprising superefficiency holds in blind source separation under certain conditions where superefficiency implies that covariance decreases in the order of 1/t/sup 2/ or of /spl eta//sup 2/. The paper uses the natural gradient learning algorithm and method of estimating functions to obtain superefficient procedures for both batch estimation and on-line learning. A standardized estimating function is introduced to this end. Superefficiency does not imply that the error variances of the extracted signals decrease in the order of 1/t/sup 2/ or /spl eta//sup 2/ but implies that their covariances (and independencies) do."
            },
            "slug": "Superefficiency-in-blind-source-separation-Amari",
            "title": {
                "fragments": [],
                "text": "Superefficiency in blind source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The paper uses the natural gradient learning algorithm and method of estimating functions to obtain superefficient procedures for both batch estimation and on-line learning and introduces a standardized estimating function."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "where \u00b7t is a learning rate that may depend on t and C.w/ is a suitably chosen positive definite matrix (see  Amari, 1967 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 280
                            }
                        ],
                        "text": "\u2026general as\nwt+1 = wt \u2212 \u03b7tC(wt)\u2207l(zt,wt), (3.2)\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf by guest on 18 Septem ber 2021\nwhere \u03b7t is a learning rate that may depend on t and C(w) is a suitably chosen positive definite matrix (see Amari, 1967)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 46
                            }
                        ],
                        "text": "The stochastic gradient method (Widrow, 1963; Amari, 1967; Tsypkin, 1973; Rumelhart, Hinton, & Williams, 1986) is a popular learning method in the general nonlinear optimization framework."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The dynamical behavior of the learning rule (see equation 3.2) was studied in  Amari (1967)  when\u00b7t is a small constant\u00b7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "Sompolinsky, Barkai, and Seung (1995), and Barkai, Seung, and Sompolinsky (1995) proposed an adaptive method of adjusting the learning rate (see also Amari, 1967)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 32
                            }
                        ],
                        "text": "The averaged learning equation (Amari, 1967, 1977) is written as\nd dt wt = \u2212\u03b7tG\u22121(wt)\n\u2329 \u2202\n\u2202w l(x,y;wt)\n\u232a , (5.5)\nd dt \u03b7t = \u03b1\u03b7t{\u03b2\u3008l(x,y;wt)\u3009 \u2212 \u03b7t}, (5.6)\nwhere \u3008 \u3009 denotes the average over the current (x,y)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 44
                            }
                        ],
                        "text": "Such online dynamics were first analyzed in Amari (1967) and then by many researchers recently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 50
                            }
                        ],
                        "text": "An idea of an adaptive change of \u03b7 was discussed in Amari (1967) and was called \u201clearning of learning rules.\u201d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Sompolinsky, Barkai, and Seung (1995), and Barkai, Seung, and Sompolinsky (1995) proposed an adaptive method of adjusting the learning rate (see also  Amari, 1967 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 157
                            }
                        ],
                        "text": "The gradient online learning rule\nw\u0303t+1 = w\u0303t \u2212 \u03b7tC\u2202l(xt,yt; w\u0303t) \u2202w ,\nwas proposed where C is a positive-definite matrix, and its dynamical behavior was studied by Amari (1967) when the learning constant \u03b7t = \u03b7 is fixed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The averaged learning equation ( Amari, 1967, 1977 ) is written as"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 78
                            }
                        ],
                        "text": "The dynamical behavior of the learning rule (see equation 3.2) was studied in Amari (1967) when \u03b7t is a small constant \u03b7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The stochastic gradient method (Widrow, 1963;  Amari, 1967;  Tsypkin, 1973; Rumelhart, Hinton, & Williams, 1986) is a popular learning method in the general nonlinear optimization framework."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Such online dynamics were first analyzed in  Amari (1967)  and then by many researchers recently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "was proposed where C is a positive-definite matrix, and its dynamical behavior was studied by  Amari (1967)  when the learning constant \u00b7tD \u00b7 is fixed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An idea of an adaptive change of \u00b7 was discussed in  Amari (1967)  and was called \u201clearning of learning rules.\u201d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31220579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1339348aeef592802288d9d929a085cb3ae61c4b",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions."
            },
            "slug": "A-Theory-of-Adaptive-Pattern-Classifiers-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Adaptive Pattern Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions, and there is an important tradeoff between speed and accuracy of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12429323,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0dcd931d10aafdd34f54d8ce9ed0fc38496894fc",
            "isKey": false,
            "numCitedBy": 284,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The semiparametric statistical model is used to formulate the problem of blind source separation. The method of estimating functions is applied to this problem. It is shown that an estimator of the mixing matrix or its learning version can be described in terms of an estimating function. The statistical efficiencies of these algorithms are studied. The main results are as follows. (1) The space consisting of all the estimating functions is derived. (2) The space is decomposed into the orthogonal sum of the admissible part and a redundant ancillary part. For any estimating function, one can find a better or equally good estimator in the admissible part. (3) The Fisher efficient (that is, asymptotically best) estimating functions are derived. (4) The stability of learning algorithms is studied."
            },
            "slug": "Blind-source-separation-semiparametric-statistical-Amari-Cardoso",
            "title": {
                "fragments": [],
                "text": "Blind source separation-semiparametric statistical approach"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that an estimator of the mixing matrix or its learning version can be described in terms of an estimating function, which means that one can find a better or equally good estimator in the admissible part."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 298602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a651655d617b7ee2e0bff14abf30c8c889cf3450",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Many recent analysis-by-synthesis density estimation models of cortical learning and processing have made the crucial simplifying assumption that units within a single layer are mutually independent given the states of units in the layer below or the layer above. In this article, we suggest using either a Markov random field or an alternative stochastic sampling architecture to capture explicitly particular forms of dependence within each layer. We develop the architectures in the context of real and binary Helmholtz machines. Recurrent sampling can be used to capture correlations within layers in the generative or the recognition models, and we also show how these can be combined."
            },
            "slug": "Recurrent-Sampling-Models-for-the-Helmholtz-Machine-Dayan",
            "title": {
                "fragments": [],
                "text": "Recurrent Sampling Models for the Helmholtz Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article suggests using either a Markov random field or an alternative stochastic sampling architecture to capture explicitly particular forms of dependence within each layer to capture correlations within layers in the generative or the recognition models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10122319,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "65d921a416b708e2d140e56457fc4ebcbc1c4940",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "This article studies a general theory of estimating functions of independent component analysis when the independent source signals are temporarily correlated. Estimating functions are used for deriving both batch and on-line learning algorithms, and they are applicable to blind cases where spatial and temporal probability structures of the sources are unknown. Most algorithms proposed so far can be analyzed in the framework of estimating functions. An admissible class of estimating functions is derived, and related efficient on-line learning algorithms are introduced. We analyze dynamical stability and statistical efficiency of these algorithms. Different from the independently and identically distributed case, the algorithms work even when only the second-order moments are used. The method of simultaneous diagonalization of cross-covariance matrices is also studied from the point of view of estimating functions."
            },
            "slug": "Estimating-Functions-of-Independent-Component-for-Amari",
            "title": {
                "fragments": [],
                "text": "Estimating Functions of Independent Component Analysis for Temporally Correlated Signals"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This article studies a general theory of estimating functions of independent component analysis when the independent source signals are temporarily correlated to analyze dynamical stability and statistical efficiency of these algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 249258,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "f55156fedf0bb586273f835c8987546b6bd65249",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis or blind source separation is a new technique of extracting independent signals from mixtures. It is applicable even when the number of independent sources is unknown and is larger or smaller than the number of observed mixture signals. This article extends the natural gradient learning algorithm to be applicable to these overcomplete and undercomplete cases. Here, the observed signals are assumed to be whitened by preprocessing, so that we use the natural Riemannian gradient in Stiefel manifolds."
            },
            "slug": "Natural-Gradient-Learning-for-Over-and-Bases-in-ICA-Amari",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Learning for Over- and Under-Complete Bases in ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The natural gradient learning algorithm is extended to be applicable to these overcomplete and undercomplete cases, where the observed signals are assumed to be whitened by preprocessing, so that the natural Riemannian gradient in Stiefel manifolds is used."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 110
                            }
                        ],
                        "text": "This has recently been solved by using the Riemannian structure (Amari, Chen, & Chichocki, in press; see also Cardoso & Laheld, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Hessian is decomposed into diagonal elements and two-by-two diagonal blocks (see also  Cardoso & Laheld, 1996 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 90
                            }
                        ],
                        "text": "The Hessian is decomposed into diagonal elements and two-by-two diagonal blocks (see also Cardoso & Laheld, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This has recently been solved by using the Riemannian structure (Amari, Chen, & Chichocki, in press; see also  Cardoso & Laheld, 1996 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17839672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8637f042e3d2a2d45de41566b4203646987a8424",
            "isKey": true,
            "numCitedBy": 1501,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Source separation consists of recovering a set of independent signals when only mixtures with unknown coefficients are observed. This paper introduces a class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called equivariant adaptive separation via independence (EASI). The EASI algorithms are based on the idea of serial updating. This specific form of matrix updates systematically yields algorithms with a simple structure for both real and complex mixtures. Most importantly, the performance of an EASI algorithm does not depend on the mixing matrix. In particular, convergence rates, stability conditions, and interference rejection levels depend only on the (normalized) distributions of the source signals. Closed-form expressions of these quantities are given via an asymptotic performance analysis. The theme of equivariance is stressed throughout the paper. The source separation problem has an underlying multiplicative structure. The parameter space forms a (matrix) multiplicative group. We explore the (favorable) consequences of this fact on implementation, performance, and optimization of EASI algorithms."
            },
            "slug": "Equivariant-adaptive-source-separation-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called EASI, which yields algorithms with a simple structure for both real and complex mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207739442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642f41bc18b36dead2b85e45a93bcfb8379224a2",
            "isKey": false,
            "numCitedBy": 1710,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions. This was achieved by using a simple type of learning rule first derived by Girolami (1997) by choosing negentropy as a projection pursuit index. Parameterized probability distributions that have sub- and supergaussian regimes were used to derive a general learning rule that preserves the simple architecture proposed by Bell and Sejnowski (1995), is optimized using the natural gradient by Amari (1998), and uses the stability analysis of Cardoso and Laheld (1996) to switch between sub- and supergaussian regimes. We demonstrate that the extended infomax algorithm is able to separate 20 sources with a variety of source distributions easily. Applied to high-dimensional data from electroencephalographic recordings, it is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain."
            },
            "slug": "Independent-Component-Analysis-Using-an-Extended-Lee-Girolami",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis Using an Extended Infomax Algorithm for Mixed Subgaussian and Supergaussian Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions and is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 248
                            }
                        ],
                        "text": "\u2026loss is\nL(W) = E[l(x,W)],\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf by guest on 18 Septem ber 2021\nwhich represents the entropy of the output y after a componentwise nonlinear transformation (Nadal & Parga, 1994; Bell & Sejnowski, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115302789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "698aedd44c51da829228e2c7d243960345efeb94",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focusing on the case of nonlinear transfer functions. We assume that both receptive fields (synaptic efficacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow. We also show that this result is valid for linear and, more generally, unbounded, transfer functions, provided optimization is performed under an additive constraint, i.e. which can be written as a sum of terms, each one being specific to one output neuron. Finally, we study the effect of a non-zero input noise. We find that, to first order in the input noise, assumed to be small in comparison with th..."
            },
            "slug": "Nonlinear-neurons-in-the-low-noise-limit:-a-code-5-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer Network 5"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998402"
                        ],
                        "name": "Liqing Zhang",
                        "slug": "Liqing-Zhang",
                        "structuredName": {
                            "firstName": "Liqing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liqing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16746638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bf8fbbb76a47316a2c60724f071d8ab77ea34b7",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the natural gradient approach to blind separation of overdetermined mixtures. First we introduce a Lie group on the manifold of overdetermined mixtures, and endow a Riemannian metric on the manifold based on the property of the Lie group. Then we derive the natural gradient on the manifold using the isometry of the Riemannian metric. Using the natural gradient, we present a new learning algorithm based on the minimization of mutual information."
            },
            "slug": "Natural-gradient-algorithm-for-blind-separation-of-Zhang-Cichocki",
            "title": {
                "fragments": [],
                "text": "Natural gradient algorithm for blind separation of overdetermined mixture with additive noise"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Using the natural gradient, this work presents a new learning algorithm based on the minimization of mutual information that derives a natural gradient on the manifold using the isometry of the Riemannian metric."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653061"
                        ],
                        "name": "N. Murata",
                        "slug": "N.-Murata",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Murata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Murata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522238"
                        ],
                        "name": "A. Ziehe",
                        "slug": "A.-Ziehe",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ziehe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ziehe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "under certain conditions (see  Kushner & Clark, 1978 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6696976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cca990eeca167893d063d20000c8de47e0c93dcc",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive on-line algorithm extending the learning of learning idea is proposed and theoretically motivated. Relying only on gradient flow information it can be applied to learning continuous functions or distributions, even when no explicit loss function is given and the Hessian is not available. Its efficiency is demonstrated for a non-stationary blind separation task of acoustic signals."
            },
            "slug": "Adaptive-On-line-Learning-in-Changing-Environments-Murata-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Adaptive On-line Learning in Changing Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An adaptive on-line algorithm extending the learning of learning idea can be applied to learning continuous functions or distributions, even when no explicit loss function is given and the Hessian is not available."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2184725"
                        ],
                        "name": "Tianping Chen",
                        "slug": "Tianping-Chen",
                        "structuredName": {
                            "firstName": "Tianping",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianping Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 25496839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "467feeed433afcf194fd693964a2fcdbe225ef62",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis or blind source separation extracts independent signals from their linear mixtures without assuming prior knowledge of their mixing coefficients. It is known that the independent signals in the observed mixtures can be successfully extracted except for their order and scales. In order to resolve the indeterminacy of scales, most learning algorithms impose some constraints on the magnitudes of the recovered signals. However, when the source signals are nonstationary and their average magnitudes change rapidly, the constraints force a rapid change in the magnitude of the separating matrix. This is the case with most applications (e.g., speech sounds, electroencephalogram signals). It is known that this causes numerical instability in some cases. In order to resolve this difficulty, this article introduces new nonholonomic constraints in the learning algorithm. This is motivated by the geometrical consideration that the directions of change in the separating matrix should be orthogonal to the equivalence class of separating matrices due to the scaling indeterminacy. These constraints are proved to be nonholonomic, so that the proposed algorithm is able to adapt to rapid or intermittent changes in the magnitudes of the source signals. The proposed algorithm works well even when the number of the sources is overestimated, whereas the existent algorithms do not (assuming the sensor noise is negligibly small), because they amplify the null components not included in the sources. Computer simulations confirm this desirable property."
            },
            "slug": "Nonholonomic-Orthogonal-Learning-Algorithms-for-Amari-Chen",
            "title": {
                "fragments": [],
                "text": "Nonholonomic Orthogonal Learning Algorithms for Blind Source Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "New nonholonomic constraints in the learning algorithm are introduced, motivated by the geometrical consideration that the directions of change in the separating matrix should be orthogonal to the equivalence class of separating matrices due to the scaling indeterminacy."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235517"
                        ],
                        "name": "J. Basak",
                        "slug": "J.-Basak",
                        "structuredName": {
                            "firstName": "Jayanta",
                            "lastName": "Basak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Basak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23320000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "230e2579a2c8f059868015cbc11f56fa15c63e42",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A general algorithm for blind separation of uniformly distributed signals is presented. First maximum likelihood equations are obtained for dealing with this task. It is difficult to obtain a closed form maximum likelihood solution for arbitrary mixing matrix. The learning rules are obtained based on the geometric interpretation of the maximum likelihood estimator. The algorithm, under special constraint of orthogonal mixing matrix, is the same as the O(1/T2) convergent algorithm. Special noise correction mechanisms are incorporated in the algorithm, and it has been found that the algorithm exhibits stable performance even in the presence of large amount of noise."
            },
            "slug": "Blind-separation-of-uniformly-distributed-signals:-Basak-Amari",
            "title": {
                "fragments": [],
                "text": "Blind separation of uniformly distributed signals: a general approach"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A general algorithm for blind separation of uniformly distributed signals is presented, and it has been found that the algorithm exhibits stable performance even in the presence of large amount of noise."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145749654"
                        ],
                        "name": "M. Rattray",
                        "slug": "M.-Rattray",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Rattray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rattray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2506116"
                        ],
                        "name": "D. Saad",
                        "slug": "D.-Saad",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Saad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Saad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119340710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c28639a46dec461c05522edb66767bd4492246f4",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural gradient descent (NGD) is an on-line algorithm for redefining the steepest descent direction. An analysis of NGD for training a multilayer neural network is presented using statistical mechanics. The performance can be significantly improved using NGD algorithm and can be used for both the transient and asymptotic stages of learning."
            },
            "slug": "Analysis-of-natural-gradient-descent-for-multilayer-Rattray-Saad",
            "title": {
                "fragments": [],
                "text": "Analysis of natural gradient descent for multilayer neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An analysis of NGD for training a multilayer neural network is presented using statistical mechanics and the performance can be significantly improved using NGD algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608039"
                        ],
                        "name": "P. V. D. Laar",
                        "slug": "P.-V.-D.-Laar",
                        "structuredName": {
                            "firstName": "Pi\u00ebrre",
                            "lastName": "Laar",
                            "middleNames": [
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. V. D. Laar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 307376,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e40d7395a15902bce675e16f7f56c7c7a4d1780a",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we introduce a measure of optimality for architecture selection algorithms for neural networks: the distance from the original network to the new network in a metric defined by the probability distributions of all possible networks. We derive two pruning algorithms, one based on a metric in parameter space and the other based on a metric in neuron space, which are closely related to well-known architecture selection algorithms, such as GOBS. Our framework extends the theoretically range of validity of GOBS and therefore can explain results observed in previous experiments. In addition, we give some computational improvements for these algorithms."
            },
            "slug": "Pruning-Using-Parameter-and-Neuronal-Metrics-Laar-Heskes",
            "title": {
                "fragments": [],
                "text": "Pruning Using Parameter and Neuronal Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A measure of optimality for architecture selection algorithms for neural networks: the distance from the original network to the new network in a metric defined by the probability distributions of all possible networks is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 121
                            }
                        ],
                        "text": "Such a loss function is obtained by the maximum entropy method (Bell & Sejnowski, 1995), independent component analysis (Comon, 1994), or the statistical likelihood method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "The independent component analysis or the mutual information criterion also gives a similar loss function (Comon, 1994; Amari et al., 1996; see also Oja & Karhunen, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143858836"
                        ],
                        "name": "A. Taleb",
                        "slug": "A.-Taleb",
                        "structuredName": {
                            "firstName": "Anisse",
                            "lastName": "Taleb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Taleb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17278094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c357a5e544e6c2f306c300b4095484c07f854ba3",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of separation of mutually independent sources in nonlinear mixtures. First, we propose theoretical results and prove that in the general case, it is not possible to separate the sources without nonlinear distortion. Therefore, we focus our work on specific nonlinear mixtures known as post-nonlinear mixtures. These mixtures constituted by a linear instantaneous mixture (linear memoryless channel) followed by an unknown and invertible memoryless nonlinear distortion, are realistic models in many situations and emphasize interesting properties i.e., in such nonlinear mixtures, sources can be estimated with the same indeterminacies as in instantaneous linear mixtures. The separation structure of nonlinear mixtures is a two-stage system, namely, a nonlinear stage followed by a linear stage, the parameters of which are updated to minimize an output independence criterion expressed as a mutual information criterion. The minimization of this criterion requires knowledge or estimation of source densities or of their log-derivatives. A first algorithm based on a Gram-Charlier expansion of densities is proposed. Unfortunately, it fails for hard nonlinear mixtures. A second algorithm based on an adaptive estimation of the log-derivative of densities leads to very good performance, even with hard nonlinearities. Experiments are proposed to illustrate these results."
            },
            "slug": "Source-separation-in-post-nonlinear-mixtures-Taleb-Jutten",
            "title": {
                "fragments": [],
                "text": "Source separation in post-nonlinear mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is proved that in the general case, it is not possible to separate the sources without nonlinear distortion, and this work focuses on specific nonlinear mixtures known as post-nonlinear mixture, which are realistic models in many situations and emphasize interesting properties i.e., in such nonlinearmixtures, sources can be estimated with the same indeterminacies as in instantaneous linear mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 169671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ec3a3b0475b26c2d35d5ce644eea13440fed410",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised classification algorithm is derived by modeling observed data as a mixture of several mutually exclusive classes that are each described by linear combinations of independent, non-Gaussian densities. The algorithm estimates the density of each class and is able to model class distributions with non-Gaussian structure. The new algorithm can improve classification accuracy compared with standard Gaussian mixture models. When applied to blind source separation in nonstationary environments, the method can switch automatically between classes, which correspond to contexts with different mixing properties. The algorithm can learn efficient codes for images containing both natural scenes and text. This method shows promise for modeling non-Gaussian structure in high-dimensional data and has many potential applications."
            },
            "slug": "ICA-Mixture-Models-for-Unsupervised-Classification-Lee-Lewicki",
            "title": {
                "fragments": [],
                "text": "ICA Mixture Models for Unsupervised Classification of Non-Gaussian Classes and Automatic Context Switching in Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The algorithm estimates the density of each class and is able to model class distributions with non-Gaussian structure and can improve classification accuracy compared with standard Gaussian mixture models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781965"
                        ],
                        "name": "S. Cruces",
                        "slug": "S.-Cruces",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Cruces",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cruces"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406719094"
                        ],
                        "name": "L. Castedo",
                        "slug": "L.-Castedo",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Castedo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Castedo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7349843,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "817c3deb518179bccd942c0dfa97f84c1be5e69e",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an iterative inversion (II) approach to blind source separation (BSS). It consists of a quasi-Newton method for the resolution of an estimating equation obtained from the implicit inversion of a robust estimate of the mixing system. The resulting learning rule includes several existing algorithms for BSS as particular cases giving them a novel and unified interpretation.It also provides a justification of the Cardoso and Laheld step size normalization. The II method is first presented for instantaneous mixtures and then extended to the problem of blind separation of convolutive mixtures. Finally, we derive the necessary and sufficient asymptotic stability conditions for both the instantaneous and convolutive methods to converge."
            },
            "slug": "An-iterative-inversion-approach-to-blind-source-Cruces-Cichocki",
            "title": {
                "fragments": [],
                "text": "An iterative inversion approach to blind source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "An iterative inversion approach to blind source separation (BSS) consisting of a quasi-Newton method for the resolution of an estimating equation obtained from the implicit inversion of a robust estimate of the mixing system is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716788"
                        ],
                        "name": "M. Kawanabe",
                        "slug": "M.-Kawanabe",
                        "structuredName": {
                            "firstName": "Motoaki",
                            "lastName": "Kawanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawanabe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, we can apply the information-geometrical theory of estimating functions ( Amari & Kawanabe, 1997 ) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The efficiency of this equation is studied from the statistical and information geometrical point of view ( Amari & Kawanabe, 1997;  Amari & Cardoso, in press)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 82
                            }
                        ],
                        "text": "However, we can apply the information-geometrical theory of estimating functions (Amari & Kawanabe, 1997) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 204
                            }
                        ],
                        "text": "(7.10)\nThis gives \u2202L/\u2202X, and the natural gradient learning equation is\ndW dt = \u03b7t(I \u2212 \u03d5(y)Ty)W. (7.11)\nThe efficiency of this equation is studied from the statistical and information geometrical point of view (Amari & Kawanabe, 1997; Amari & Cardoso, in press)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120424907,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd3136d05928f7636e197382205b0f84f85eada4",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "For semi-parametric statistical estimation, when an estimating function exists, it often provides an e\u0081cient or a good consistent estimator of the parameter of interest against nuisance parameters of in\u00aenite dimensions. The present paper elucidates the structure of estimating functions, based on the dual di\u0080erential geometry of statistical inference and its extension to \u00aebre bundles. The paper studies the following problems. First, when does an estimating function exist and what is the set of all the estimating functions? Second, how are the asymptotic variances of the estimators derived from estimating functions and when are the estimators e\u0081cient? Third, how do we adaptively choose a practically good (quasi-)estimating function from the observed data? The concept of m-curvature freeness plays a fundamental role in solving the above problems."
            },
            "slug": "Information-geometry-of-estimating-functions-in-Amari-Kawanabe",
            "title": {
                "fragments": [],
                "text": "Information geometry of estimating functions in semi-parametric statistical models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Heskes and Kappen (1991)  obtained similar results, which ignited research into online learning."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 145
                            }
                        ],
                        "text": "Blind source separation is the problem of recovering the original signals s(t), t = 1, 2, . . . from the observed signals x(t), t = 1, 2, . . . (Jutten & He\u0301rault, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": false,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48988892"
                        ],
                        "name": "S. Douglas",
                        "slug": "S.-Douglas",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Douglas",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Douglas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10095341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd305b04c4bc558bd79676df15a6f1e6292d6ebc",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the problem of selective transmission-the dual of the blind source separation task-in which a set of independent source signals are adaptively premixed prior to a nondispersive physical mixing process so that each source can be independently monitored in the far field. Following similar procedures for information-theoretic blind source separation, we derive a stochastic gradient algorithm for iteratively estimating the premixing matrix in the selective transmission problem, and through a simple modification, we obtain a second algorithm whose performance is equivariant with respect to the channel's mixing characteristics. The local stability conditions for the algorithms about any selective transmission solution are shown to be the same as those for similar source separation algorithms. Practical implementation issues are discussed, including the estimation of the combined system matrix and the reordering and scaling of the received signals within the algorithm. Mean square error-based selective transmission algorithms are also derived for performance comparison purposes. Simulations indicate the useful behavior of the premixing algorithms for selective transmission."
            },
            "slug": "Equivariant-adaptive-selective-transmission-Douglas",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive selective transmission"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Following similar procedures for information-theoretic blind source separation, a stochastic gradient algorithm is derived for iteratively estimating the premixing matrix in the selective transmission problem and a second algorithm is obtained whose performance is equivariant with respect to the channel's mixing characteristics."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144806321"
                        ],
                        "name": "L. Campbell",
                        "slug": "L.-Campbell",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Campbell",
                            "middleNames": [
                                "Lorne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Campbell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is the only invariant metric to be given to the statistical model (Chentsov, 1972;  Campbell, 1985;  Amari, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 88
                            }
                        ],
                        "text": "This is the only invariant metric to be given to the statistical model (Chentsov, 1972; Campbell, 1985; Amari, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21748757,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4068d449d2f5e1e912f0a0252300f4101d3ac355",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-relation-between-information-theory-and-the-to-Campbell",
            "title": {
                "fragments": [],
                "text": "The relation between information theory and the differential geometry approach to statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792428"
                        ],
                        "name": "H. H. Yang",
                        "slug": "H.-H.-Yang",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Yang",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Yang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206467692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7720404bc0a750b260f78e3f896f8fb2f071cc8c",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In the context of blind source separation, the method of scoring based on the inverse of the Fisher information matrix (FIM) becomes the serial updating learning rule with an equivariant property. This learning rule can be simplified to a low-complexity algorithm by using the asymptotic form of the FTM around the equilibrium. The simplified learning rule is still general enough to include some existing equivariant blind separation algorithms as its special cases."
            },
            "slug": "Serial-updating-rule-for-blind-separation-derived-Yang",
            "title": {
                "fragments": [],
                "text": "Serial updating rule for blind separation derived from the method of scoring"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "In the context of blind source separation, the method of scoring based on the inverse of the Fisher information matrix (FIM) becomes the serial updating learning rule with an equivariant property that can be simplified to a low-complexity algorithm by using the asymptotic form of the FTM around the equilibrium."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2184725"
                        ],
                        "name": "Tianping Chen",
                        "slug": "Tianping-Chen",
                        "structuredName": {
                            "firstName": "Tianping",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianping Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114097399"
                        ],
                        "name": "Q. Lin",
                        "slug": "Q.-Lin",
                        "structuredName": {
                            "firstName": "Qin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10015437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "539bfa9e723e1e772d6e1ee1604123fc4b482795",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-unified-algorithm-for-principal-and-minor-Chen-Amari",
            "title": {
                "fragments": [],
                "text": "A unified algorithm for principal and minor components extraction"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48988892"
                        ],
                        "name": "S. Douglas",
                        "slug": "S.-Douglas",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Douglas",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Douglas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A preliminary investigation into the performance of the natural gradient learning algorithm has been undertaken by  Douglas, Chichocki, and Amari (1996)  and Amari et al. (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 29
                            }
                        ],
                        "text": "We show here only ideas (see Douglas et al., 1996; Amari, Douglas, Cichocki, & Yang, 1997, for preliminary studies)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We show here only ideas (see  Douglas et al., 1996;  Amari, Douglas, Cichocki, & Yang, 1997, for preliminary studies)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123312174,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8144e93fd974d82a428d14ac92d84b5213a29bbe",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a simple extension of the standard Bussgang blind equalisation algorithms that significantly improves their convergence properties. The technique uses the inverse channel estimate to filter the regressor signal. The modified algorithms provide quasi-Newton convergence in the vicinity of a local minimum of the chosen cost function with only a modest increase in the overall computational complexity of the system. An example of the technique as applied to the constant-modulus algorithm indicates its superior convergence behaviour."
            },
            "slug": "Fast-convergence-filtered-regressor-algorithms-for-Douglas-Cichocki",
            "title": {
                "fragments": [],
                "text": "Fast-convergence filtered regressor algorithms for blind equalisation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144532256"
                        ],
                        "name": "C. R. Rao",
                        "slug": "C.-R.-Rao",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Rao",
                            "middleNames": [
                                "Radhakrishna"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Riemannian structure of the parameter space of a statistical model is defined by the Fisher information (Rao, 1945; Amari, 1985)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117034671,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c4f675cdb7d03aa1458091e4b1cf04e492ffc9f5",
            "isKey": false,
            "numCitedBy": 1658,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The earliest method of estimation of statistical parameters is the method of least squares due to Mark off. A set of observations whose expectations are linear functions of a number of unknown parameters being given, the problem which Markoff posed for solution is to find out a linear function of observations whose expectation is an assigned linear function of the unknown parameters and whose variance is a minimum. There is no assumption about the distribution of the observations except that each has a finite variance."
            },
            "slug": "Information-and-the-Accuracy-Attainable-in-the-of-Rao",
            "title": {
                "fragments": [],
                "text": "Information and the Accuracy Attainable in the Estimation of Statistical Parameters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387480163"
                        ],
                        "name": "J. J. Murillo-Fuentes",
                        "slug": "J.-J.-Murillo-Fuentes",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Murillo-Fuentes",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Murillo-Fuentes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398532110"
                        ],
                        "name": "F. Gonz\u00e1lez-Serrano",
                        "slug": "F.-Gonz\u00e1lez-Serrano",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Gonz\u00e1lez-Serrano",
                            "middleNames": [
                                "Javier"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gonz\u00e1lez-Serrano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122351311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95d569a7736d78d2f5c3e310cc7605acda2d4aa7",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A new online learning algorithm is derived for blind separation of mixed signals with symmetric distributions. The stability of the method is analysed and compared with the natural gradient method. It is proved that the set of stability conditions obtained is less stringent. Some experiments are included."
            },
            "slug": "Improving-stability-in-blind-source-separation-with-Murillo-Fuentes-Gonz\u00e1lez-Serrano",
            "title": {
                "fragments": [],
                "text": "Improving stability in blind source separation with stochastic median gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new online learning algorithm is derived for blind separation of mixed signals with symmetric distributions and it is proved that the set of stability conditions obtained is less stringent than the natural gradient method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29996782"
                        ],
                        "name": "Opper",
                        "slug": "Opper",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Opper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 283
                            }
                        ],
                        "text": "\u2026nloaded from http://direct.m it.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf by guest on 18 Septem ber 2021\nof asymptotic statistics when the loss function is differentiable, so that it is asymptotically equivalent to the optimal batch procedure (see also Amari, 1995; Opper, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This article answers the question affirmatively, by giving an efficient online learning rule (see Amari, 1995; see also  Opper, 1996 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "of asymptotic statistics when the loss function is differentiable, so that it is asymptotically equivalent to the optimal batch procedure (see also Amari, 1995;  Opper, 1996 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 120
                            }
                        ],
                        "text": "This article answers the question affirmatively, by giving an efficient online learning rule (see Amari, 1995; see also Opper, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31220453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "291fd6323d4369529cc9994ce3d504fc3a3f0d8a",
            "isKey": true,
            "numCitedBy": 55,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose a general model of on-line learning from random examples which, when applied to a smooth realizable stochastic rule, yields the same asymptotic generalization error rate as optimal batch algorithms. The approach is based on an iterative Gaussian approximation to the posterior Gibbs distribution of rule parameters."
            },
            "slug": "On-line-versus-Off-line-Learning-from-Random-Opper",
            "title": {
                "fragments": [],
                "text": "On-line versus Off-line Learning from Random Examples: General Results."
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A general model of on-line learning from random examples which, when applied to a smooth realizable stochastic rule, yields the same asymptotic generalization error rate as optimal batch algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6298687,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "b8da1aec8d5542abe815f58caaa99febef525aa7",
            "isKey": false,
            "numCitedBy": 470,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Empirical results were obtained for the blind source separation of more sources than mixtures using a previously proposed framework for learning overcomplete representations. This technique assumes a linear mixing model with additive noise and involves two steps: (1) learning an overcomplete representation for the observed data and (2) inferring sources given a sparse prior on the coefficients. We demonstrate that three speech signals can be separated with good fidelity given only two mixtures of the three signals. Similar results were obtained with mixtures of two speech signals and one music signal."
            },
            "slug": "Blind-source-separation-of-more-sources-than-using-Lee-Lewicki",
            "title": {
                "fragments": [],
                "text": "Blind source separation of more sources than mixtures using overcomplete representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that three speech signals can be separated with good fidelity given only two mixtures of the three signals."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102673564"
                        ],
                        "name": "T. Ens",
                        "slug": "T.-Ens",
                        "structuredName": {
                            "firstName": "Toulouse",
                            "lastName": "Ens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10175271,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "61bb90c1769d2076e9a5538bd3e4bffcb3f6a1b7",
            "isKey": false,
            "numCitedBy": 804,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Blind signal separation (BSS) and independent component analysis (ICA) are emerging techniques of array processing and data analysis, aiming at recovering unobserved signals or \u2018sources\u2019 from observed mixtures (typically, the output of an array of sensors), exploiting only the assumption of mutual independence between the signals. The weakness of the assumptions makes it a powerful approach but requires to venture beyond familiar second order statistics. The objective of this paper is to review some of the approaches that have been recently developed to address this exciting problem, to show how they stem from basic principles and how they relate to each other. Keywords\u2014 Signal separation, blind source separation, independent component analysis."
            },
            "slug": "Blind-signal-separation-:-statistical-principles-Ens",
            "title": {
                "fragments": [],
                "text": "Blind signal separation : statistical principles"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23897777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f57f89e93e5244c1337395a2fb3f503484625fe6",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This article develops an extended independent component analysis algorithm for mixtures of arbitrary subgaussian and supergaussian sources. The gaussian mixture model of Pearson is employed in deriving a closed-form generic score function for strictly subgaussian sources. This is combined with the score function for a unimodal supergaussian density to provide a computationally simple yet powerful algorithm for performing independent component analysis on arbitrary mixtures of nongaussian sources."
            },
            "slug": "An-Alternative-Perspective-on-Adaptive-Independent-Girolami",
            "title": {
                "fragments": [],
                "text": "An Alternative Perspective on Adaptive Independent Component Analysis Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The gaussian mixture model of Pearson is employed in deriving a closed-form generic score function for strictly subgaussian sources to provide a computationally simple yet powerful algorithm for performing independent component analysis on arbitrary mixtures of nongaussian sources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083360580"
                        ],
                        "name": "Filipe Aires",
                        "slug": "Filipe-Aires",
                        "structuredName": {
                            "firstName": "Filipe",
                            "lastName": "Aires",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filipe Aires"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2396737"
                        ],
                        "name": "A. Ch\u00e9din",
                        "slug": "A.-Ch\u00e9din",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Ch\u00e9din",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ch\u00e9din"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152240749"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 129306085,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "8a61b9df59eb69e08b306f98530b2bc9d5bbe8d5",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analyse-de-s\u00e9ries-temporelles-g\u00e9ophysiques-et-de-en-Aires-Ch\u00e9din",
            "title": {
                "fragments": [],
                "text": "Analyse de s\u00e9ries temporelles g\u00e9ophysiques et th\u00e9orie de l'information: L'analyse en composantes ind\u00e9pendantes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16019590"
                        ],
                        "name": "T. Nakada",
                        "slug": "T.-Nakada",
                        "structuredName": {
                            "firstName": "Tsutomu",
                            "lastName": "Nakada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nakada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50355248"
                        ],
                        "name": "Kiyotaka Suzuki",
                        "slug": "Kiyotaka-Suzuki",
                        "structuredName": {
                            "firstName": "Kiyotaka",
                            "lastName": "Suzuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kiyotaka Suzuki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713355"
                        ],
                        "name": "Y. Fujii",
                        "slug": "Y.-Fujii",
                        "structuredName": {
                            "firstName": "Yukihiko",
                            "lastName": "Fujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Fujii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7006279"
                        ],
                        "name": "H. Matsuzawa",
                        "slug": "H.-Matsuzawa",
                        "structuredName": {
                            "firstName": "Hitoshi",
                            "lastName": "Matsuzawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Matsuzawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4633092"
                        ],
                        "name": "I. Kwee",
                        "slug": "I.-Kwee",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Kwee",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kwee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11567901,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "cc830991396f66e7f330c8af69f2edcfabe3b476",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-cross-correlation-sequential-Nakada-Suzuki",
            "title": {
                "fragments": [],
                "text": "Independent component-cross correlation-sequential epoch (ICS) analysis of high field fMRI time series: direct visualization of dual representation of the primary motor cortex in human"
            },
            "venue": {
                "fragments": [],
                "text": "Neuroscience Research"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229585"
                        ],
                        "name": "D. Clark",
                        "slug": "D.-Clark",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Clark",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Clark"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 243
                            }
                        ],
                        "text": "\u2026it.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf by guest on 18 Septem ber 2021\nE [ \u22022l(xt,yt;w\u2217)\n\u2202w\u2202w\n] = G(w\u2217), (4.8)\nG(w\u0303t) = G(w\u2217)+O (\n1 t\n) ,\nbecause w\u0303t converges to w\u2217 as guaranteed by stochastic approximation under certain conditions (see Kushner & Clark, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117328031,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0009c5a2b4b07751a99bcf407d95e911a3064d0f",
            "isKey": true,
            "numCitedBy": 948,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I. Introduction.- 1.1. General Remarks.- 1.2. The Robbins-Monro Process.- 1.3. A \"Continuous\" Process Version of Section 2.- 1.4. Regulation of a Dynamical System a simple example.- 1.5. Function Minimization: The Kiefer-Wolfowitz Procedure.- 1.6. Constrained Problems.- 1.7. An Economics Example.- II. Convergence w.p.1 for Unconstrained Systems.- 2.1. Preliminaries and Motivation.- 2.2. The Robbins-Monro and Kiefer-Wolfowitz Algorithms: Conditions and Discussion.- 2.3. Convergence Proofs for RM and KW-like Procedures.- 2.3.1. A Basic RM-like Procedure.- 2.3.2. One Dimensional RM and Accelerated RM Procedures.- 2.3.3. A Continuous Parameter RM Procedure.- 2.3.4. The Basic Kiefer-Wolfowitz Procedure.- 2.3.5. Random Directions KW Methods.- 2.4. A General Robbins-Monro Process: \"Exogenous Noise\".- 2.4.1. The Case of Bounded h(*,*).- 2.4.2. Unbounded h(*,*): Exogenous Noise.- 2.5. A General RM Process State Dependent Noise.- 2.5.1. Extensions and Localizations of Theorem 2.5.2.- 2.6. Some Applications.- 2.7. Mensov-Rademacher Estimates.- III. Weak Convergence of Probability Measures.- IV. Weak Convergence for Unconstrained Systems.- 4.1. Conditions and General Discussion.- 4.2. The Robbins-Monro and Kiefer-Wolfowitz Procedures.- 4.2.1. The Basic Robbins-Monro Procedure.- 4.2.2. The One-Dimensional Robbins-Monro Procedure.- 4.2.3. The Kiefer-Wolfowitz Procedure.- 4.2.4. A Case Where the Limit Satisfies a Generalized ODE.- 4.2.5. A Continuous Parameter KW Procedure.- 4.3. A General Robbins-Monro Process: Exogenous Noise.- 4.4. A General RM Process: State Dependent Noise.- 4.5. The Identification Problem.- 4.6. A Counter-Example to Tightness.- 4.7. Boundedness of {Xn} and Tightness of {Xn(*)}.- V. Convergence w.p.1 For Constrained Systems.- 5.1. A Penalty-Multiplier Algorithm for Equality Constraints.- 5.1.1. A Basic RM-like Algorithm, Conditions and Discussion.- 5.1.2. The Noise Condition, Discussion and Generalization.- 5.1.3. Boundedness of {Xn}.- 5.1.4. Proof of the Main Theorem.- 5.1.5. Constrained Function Minimization and Other Extensions.- 5.2. A Lagrangian Method for Inequality Constraints.- 5.2.1. The Algorithm and Conditions.- 5.2.2. The Convergence Theorem 18.- 5.2.3. A Non-Convergent but Useful Algorithm.- 5.2.4. An Application to the Identification Problem.- 5.3. A Projection Algorithm.- 5.4. A Penalty-Multiplier Method for Inequality Constraints.- VI. Weak Convergence: Constrained Systems.- 6.1. A Multiplier Type Algorithm for Equality Constraints.- 6.1.1. Boundedness of {Xn}.- 6.1.2. The Noise Condition, Discussion.- 6.1.3. The Convergence Theorem.- 6.2. The Lagrangian Method.- 6.3. A Projection Algorithm.- 6.4. A Penalty-Multiplier Algorithm for Inequality Constraints.- VII. Rates of Convergence.- 7.1. The Problem Formulation.- 7.2. Conditions and Discussions.- 7.3. Rates of Convergence for Case 1, the KW Algorithm.- 7.4. Discussion of Rates of Convergence for Two KW Algorithms."
            },
            "slug": "wchastic.-approximation-methods-for-constrained-and-Kushner-Clark",
            "title": {
                "fragments": [],
                "text": "wchastic. approximation methods for constrained and unconstrained systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Robbins-Monro and Kiefer-Wolfowitz Algorithm for Inequality Constraints and the Weak Convergence of Probability Measures, a simple example, and the Convergence Theorem, a proof of the Main Theorem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678771"
                        ],
                        "name": "P. Bickel",
                        "slug": "P.-Bickel",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bickel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bickel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119754792,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2ea931fb55701a5f5b82aa491a64c0e431fb4b38",
            "isKey": false,
            "numCitedBy": 1391,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction.- Asymptotic Inference for (Finite-Dimensional) Parametric Models.- Information Bounds for Euclidean Parameters in Infinite-Dimensional Models.- Euclidean Parameters: Further Examples.- Information Bounds for Infinite-Dimensional Parameters.- Infinite-Dimensional Parameters: Further Examples: Construction of Examples."
            },
            "slug": "Efficient-and-Adaptive-Estimation-for-Models-Bickel",
            "title": {
                "fragments": [],
                "text": "Efficient and Adaptive Estimation for Semiparametric Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The stochastic gradient method (Widrow, 1963; Amari, 1967; Tsypkin, 1973;  Rumelhart, Hinton, & Williams, 1986 ) is a popular learning method in the general nonlinear optimization framework."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69425235"
                        ],
                        "name": "I\ufe20a\ufe21. Z. T\ufe20S\ufe21ypkin",
                        "slug": "I\ufe20a\ufe21.-Z.-T\ufe20S\ufe21ypkin",
                        "structuredName": {
                            "firstName": "I\ufe20a\ufe21.",
                            "lastName": "T\ufe20S\ufe21ypkin",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I\ufe20a\ufe21. Z. T\ufe20S\ufe21ypkin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 59
                            }
                        ],
                        "text": "The stochastic gradient method (Widrow, 1963; Amari, 1967; Tsypkin, 1973; Rumelhart, Hinton, & Williams, 1986) is a popular learning method in the general nonlinear optimization framework."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60871581,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "ded14685a23df94d93e8662578d4132c9f4aa1c7",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Foundations-of-the-theory-of-learning-systems-T\ufe20S\ufe21ypkin",
            "title": {
                "fragments": [],
                "text": "Foundations of the theory of learning systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102711000"
                        ],
                        "name": "N. \u010cencov",
                        "slug": "N.-\u010cencov",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "\u010cencov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. \u010cencov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 72
                            }
                        ],
                        "text": "This is the only invariant metric to be given to the statistical model (Chentsov, 1972; Campbell, 1985; Amari, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117133642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1d92fc8a8d8736a17a790d832d71fec48625250",
            "isKey": false,
            "numCitedBy": 651,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-Decision-Rules-and-Optimal-Inference-\u010cencov",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Rules and Optimal Inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24914074"
                        ],
                        "name": "M. Murray",
                        "slug": "M.-Murray",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Murray",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96297020"
                        ],
                        "name": "J. Rice",
                        "slug": "J.-Rice",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Rice",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rice"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "The Riemannian metric structures are introduced by means of information geometry (Amari, 1985; Murray and Rice, 1993; Amari, 1997a; Amari, Kurata, & Nagoska, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 99
                            }
                        ],
                        "text": "This was given in Amari (1987) from the point of view of information geometry (Amari, 1985, 1997a; Murray & Rice, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121757409,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6c1656e91d7a0e64c523ab481e024aa851cc4ff9",
            "isKey": false,
            "numCitedBy": 289,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The Geometry of Exponential Families. Calculus on Manifolds. Statistical Manifolds. Connections. Curvature. Information Metrices and Statistical Divergences. Asymptotics. Bundles and Tensors. Higher Order Geometry. References. Index."
            },
            "slug": "Differential-Geometry-and-Statistics-Murray-Rice",
            "title": {
                "fragments": [],
                "text": "Differential Geometry and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 104
                            }
                        ],
                        "text": "This is the only invariant metric to be given to the statistical model (Chentsov, 1972; Campbell, 1985; Amari, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 120
                            }
                        ],
                        "text": "The Riemannian structure of the parameter space of a statistical model is defined by the Fisher information (Rao, 1945; Amari, 1985)\ngij(w) = E [ \u2202 log p(x,w)\n\u2202wi\n\u2202 log p(x,w) \u2202wj\n] (3.5)\nin the component form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 79
                            }
                        ],
                        "text": "This was given in Amari (1987) from the point of view of information geometry (Amari, 1985, 1997a; Murray & Rice, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 21
                            }
                        ],
                        "text": "Information geometry (Amari, 1985) shows that the Riemannian structure is given to the parameter space of multilayer networks by the Fisher information matrix,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 108
                            }
                        ],
                        "text": "The Riemannian structure of the parameter space of a statistical model is defined by the Fisher information (Rao, 1945; Amari, 1985)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 29
                            }
                        ],
                        "text": "(3.10)\nInformation geometry (Amari, 1985) shows that the Riemannian structure is given to the parameter space of multilayer networks by the Fisher information matrix,\ngij(w) = E [ \u2202 log p(x,y;w)\n\u2202wi\n\u2202p(x,y;w) \u2202wj\n] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 82
                            }
                        ],
                        "text": "The Riemannian metric structures are introduced by means of information geometry (Amari, 1985; Murray and Rice, 1993; Amari, 1997a; Amari, Kurata, & Nagoska, 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 116956004,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "420322994c59e9081786b46b31e2c82a9753e23a",
            "isKey": false,
            "numCitedBy": 1490,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Differential-geometrical-methods-in-statistics-Amari",
            "title": {
                "fragments": [],
                "text": "Differential-geometrical methods in statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720547"
                        ],
                        "name": "H. Sompolinsky",
                        "slug": "H.-Sompolinsky",
                        "structuredName": {
                            "firstName": "Haim",
                            "lastName": "Sompolinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sompolinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144074514"
                        ],
                        "name": "N. Barkai",
                        "slug": "N.-Barkai",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Barkai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Barkai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69020362"
                        ],
                        "name": "S. SeungH",
                        "slug": "S.-SeungH",
                        "structuredName": {
                            "firstName": "S",
                            "lastName": "SeungH",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. SeungH"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 0
                            }
                        ],
                        "text": "Sompolinsky, Barkai, and Seung (1995), and Barkai, Seung, and Sompolinsky (1995) proposed an adaptive method of adjusting the learning rate (see also Amari, 1967)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Sompolinsky et al. (1995)  (see also Barkai et al., 1995) proposed a rule of adaptive change of \u00b7t, which is applicable to the pattern classification problem where the expected loss L.w/ is not differentiable atw\u2044."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Sompolinsky et al. (1995) (see also Barkai et al., 1995) proposed a rule of adaptive change of \u03b7t, which is applicable to the pattern classification problem where the expected loss L(w) is not differentiable atw\u2217."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Sompolinsky, Barkai, and Seung (1995) , and Barkai, Seung, and Sompolinsky (1995) proposed an adaptive method of adjusting the learning rate (see also Amari, 1967)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59736932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7e98de0af00c1752ae418406fd1593b6121b2c4",
            "isKey": true,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-line-Learning-of-Dichotomies:-Algorithms-and-Sompolinsky-Barkai",
            "title": {
                "fragments": [],
                "text": "On-line Learning of Dichotomies: Algorithms and Learning Curves."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1995"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 82
                            }
                        ],
                        "text": "However, we can apply the information-geometrical theory of estimating functions (Amari & Kawanabe, 1997) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 204
                            }
                        ],
                        "text": "(7.10)\nThis gives \u2202L/\u2202X, and the natural gradient learning equation is\ndW dt = \u03b7t(I \u2212 \u03d5(y)Ty)W. (7.11)\nThe efficiency of this equation is studied from the statistical and information geometrical point of view (Amari & Kawanabe, 1997; Amari & Cardoso, in press)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry of estimating functions in semiparametric statistical models, Bernoulli"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716788"
                        ],
                        "name": "M. Kawanabe",
                        "slug": "M.-Kawanabe",
                        "structuredName": {
                            "firstName": "Motoaki",
                            "lastName": "Kawanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawanabe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 82
                            }
                        ],
                        "text": "However, we can apply the information-geometrical theory of estimating functions (Amari & Kawanabe, 1997) to this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 204
                            }
                        ],
                        "text": "(7.10)\nThis gives \u2202L/\u2202X, and the natural gradient learning equation is\ndW dt = \u03b7t(I \u2212 \u03d5(y)Ty)W. (7.11)\nThe efficiency of this equation is studied from the statistical and information geometrical point of view (Amari & Kawanabe, 1997; Amari & Cardoso, in press)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117292930,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "406c862847b286a65ee0dd05cf92572ae19203d5",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-Functions-in-Semiparametric-Statistical-Amari-Kawanabe",
            "title": {
                "fragments": [],
                "text": "Estimating Functions in Semiparametric Statistical Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 270
                            }
                        ],
                        "text": "\u2026nloaded from http://direct.m it.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf by guest on 18 Septem ber 2021\nof asymptotic statistics when the loss function is differentiable, so that it is asymptotically equivalent to the optimal batch procedure (see also Amari, 1995; Opper, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "of asymptotic statistics when the loss function is differentiable, so that it is asymptotically equivalent to the optimal batch procedure (see also  Amari, 1995;  Opper, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This article answers the question affirmatively, by giving an efficient online learning rule (see  Amari, 1995;  see also Opper, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 98
                            }
                        ],
                        "text": "This article answers the question affirmatively, by giving an efficient online learning rule (see Amari, 1995; see also Opper, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56859321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a1f29c682961b00137eb787295366349f21ea47",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-and-statistical-inference-Amari",
            "title": {
                "fragments": [],
                "text": "Learning and statistical inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153129968"
                        ],
                        "name": "K. Do",
                        "slug": "K.-Do",
                        "structuredName": {
                            "firstName": "Kim-Anh",
                            "lastName": "Do",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Do"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678771"
                        ],
                        "name": "P. Bickel",
                        "slug": "P.-Bickel",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bickel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2857518"
                        ],
                        "name": "C. Klaassen",
                        "slug": "C.-Klaassen",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Klaassen",
                            "middleNames": [
                                "A.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Klaassen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100532061"
                        ],
                        "name": "Y. Ritov",
                        "slug": "Y.-Ritov",
                        "structuredName": {
                            "firstName": "Yaacov",
                            "lastName": "Ritov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ritov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144952368"
                        ],
                        "name": "J. Wellner",
                        "slug": "J.-Wellner",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Wellner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wellner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125283279,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b8160ac3f34352b4f1fa99cb49bd72acef31240e",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-and-Adaptive-Estimation-for-Models.-Do-Bickel",
            "title": {
                "fragments": [],
                "text": "Efficient and Adaptive Estimation for Semiparametric Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145749654"
                        ],
                        "name": "M. Rattray",
                        "slug": "M.-Rattray",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Rattray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rattray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2506116"
                        ],
                        "name": "D. Saad",
                        "slug": "D.-Saad",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Saad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Saad"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120826888,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "984235b33013c0ec86a055008cb10842be385fa2",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ANALYSIS-OF-ON-LINE-TRAINING-WITH-OPTIMAL-LEARNING-Rattray-Saad",
            "title": {
                "fragments": [],
                "text": "ANALYSIS OF ON-LINE TRAINING WITH OPTIMAL LEARNING RATES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145389998"
                        ],
                        "name": "M. Palaniswami",
                        "slug": "M.-Palaniswami",
                        "structuredName": {
                            "firstName": "Marimuthu",
                            "lastName": "Palaniswami",
                            "middleNames": [
                                "Swami"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Palaniswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719111"
                        ],
                        "name": "Y. Attikiouzel",
                        "slug": "Y.-Attikiouzel",
                        "structuredName": {
                            "firstName": "Yianni",
                            "lastName": "Attikiouzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Attikiouzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144883753"
                        ],
                        "name": "T. Fukuda",
                        "slug": "T.-Fukuda",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Fukuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Fukuda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "The independent component analysis or the mutual information criterion also gives a similar loss function (Comon, 1994; Amari et al., 1996; see also Oja & Karhunen, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 56507380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6196e9de5a41a342ff7b989fb9370548bb6cae7a",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computational-Intelligence:-A-Dynamic-System-Palaniswami-Attikiouzel",
            "title": {
                "fragments": [],
                "text": "Computational Intelligence: A Dynamic System Perspective"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97541485"
                        ],
                        "name": "Saad",
                        "slug": "Saad",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Saad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30006126"
                        ],
                        "name": "Solla",
                        "slug": "Solla",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Solla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Solla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 280
                            }
                        ],
                        "text": "\u2026a preliminary analysis (Yang & Amari, 1997), by using a simple model, shows that the performance of natural gradient learning is remarkably good, and it is sometimes free from being trapped in plateaus, which give rise to slow convergence of the backpropagation learning method (Saad & Solla, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31719180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74665b24e316cc379f3179e09443f4d23e07a6bc",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-line-learning-in-soft-committee-machines.-Saad-Solla",
            "title": {
                "fragments": [],
                "text": "On-line learning in soft committee machines."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30060814"
                        ],
                        "name": "Van den Broeck C",
                        "slug": "Van-den-Broeck-C",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Van den Broeck C",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Van den Broeck C"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88157200"
                        ],
                        "name": "Reimann",
                        "slug": "Reimann",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Reimann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reimann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "When the loss function is nondifferentiable, the accuracy of asymptotic online learning is worse than batch learning by a factor of 2 (see, for example,  Van den Broeck & Reimann, 1996 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 161
                            }
                        ],
                        "text": "When the loss function is nondifferentiable, the accuracy of asymptotic online learning is worse than batch learning by a factor of 2 (see, for example, Van den Broeck & Reimann, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26628823,
            "fieldsOfStudy": [
                "Physics",
                "Medicine"
            ],
            "id": "715f80f0b2cebeb5bc3d17622cd332acbe3a08ac",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unsupervised-learning-by-examples:-On-line-versus-VandenBroeck-Reimann",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning by examples: On-line versus off-line."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The independent component analysis or the mutual information criterion also gives a similar loss function (Comon, 1994; Amari et al., 1996; see also  Oja & Karhunen, 1995 )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "The independent component analysis or the mutual information criterion also gives a similar loss function (Comon, 1994; Amari et al., 1996; see also Oja & Karhunen, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13894957,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "b305880be83958da87a269c123d943f7063c9ed5",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Signal-Separation-by-Nonlinear-Hebbian-Learning-Oja-Karhunen",
            "title": {
                "fragments": [],
                "text": "Signal Separation by Nonlinear Hebbian Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive on-line learning algorithms for blind separation-Maximum entropy and minimal mutual information"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "The stochastic gradient method (Widrow, 1963; Amari, 1967; Tsypkin, 1973; Rumelhart, Hinton, & Williams, 1986) is a popular learning method in the general nonlinear optimization framework."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A statistical theory of adaptation"
            },
            "venue": {
                "fragments": [],
                "text": "Oxford: Pergamon Press."
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 101
                            }
                        ],
                        "text": "This principle was used to derive the natural gradient in Amari, Cichocki, and Yang (1996); see also Yang and Amari (1997) for detail."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 104
                            }
                        ],
                        "text": "However, this is much better than the direct inversion of the original (n+ 1)m-dimensional matrix of G. Yang and Amari (1997) performed a preliminary study on the performance of the natural gradient learning algorithm for a simple multilayer perceptron."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "However, a preliminary analysis (Yang & Amari, 1997), by using a simple model, shows that the performance of natural gradient learning is remarkably good, and it is sometimes free from being trapped in plateaus, which give rise to slow convergence of the backpropagation learning method (Saad &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Application of natural gradient in training multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind source separation \u2014 Semi - parametric statistical approach"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . on Signal Processing"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 121
                            }
                        ],
                        "text": "Such a loss function is obtained by the maximum entropy method (Bell & Sejnowski, 1995), independent component analysis (Comon, 1994), or the statistical likelihood method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "The independent component analysis or the mutual information criterion also gives a similar loss function (Comon, 1994; Amari et al., 1996; see also Oja & Karhunen, 1995)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Heskes and Kappen (1991) obtained similar results, which ignited research into online learning."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning process in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Physical Review,"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 20,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 78,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Natural-Gradient-Works-Efficiently-in-Learning-Amari/5a767a341364de1f75bea85e0b12ba7d3586a461?sort=total-citations"
}