{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763321"
                        ],
                        "name": "E. Saund",
                        "slug": "E.-Saund",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Saund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 119
                            }
                        ],
                        "text": "An interesting problem relevant to vision is that of extracting independent horizontal and vertical bars from an image [Foldiak 1990; Saund 1995; Zemel 1993; Dayan and Zemel 1995; Hinton et al. 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 133
                            }
                        ],
                        "text": "An interesting problem relevant to vision is that of extracting independent horizontal andvertical bars from an image [Foldiak 1990; Saund 1995; Zemel 1993; Dayan and Zemel 1995;Hinton et al. 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18231498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd7d7767129ed180db39d38be28c1ae389481d2f",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data. Unlike the hard k-means clustering algorithm and the soft mixture model, each of which assumes that a single hidden event generates each data point, a multiple cause model accounts for observed data by combining assertions from many hidden causes, each of which can pertain to varying degree to any subset of the observable dimensions. We employ an objective function and iterative gradient descent learning algorithm resembling the conventional mixture model. A crucial issue is the mixing function for combining beliefs from different cluster centers in order to generate data predictions whose errors are minimized both during recognition and learning. The mixing function constitutes a prior assumption about underlying structural regularities of the data domain; we demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing, and offer alternative forms of the nonlinearity for two types of data domain. Results are presented demonstrating the algorithm's ability successfully to discover coherent multiple causal representations in several experimental data sets."
            },
            "slug": "A-Multiple-Cause-Mixture-Model-for-Unsupervised-Saund",
            "title": {
                "fragments": [],
                "text": "A Multiple Cause Mixture Model for Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data, which employs an objective function and iterative gradient descent learning algorithm resembling the conventional mixture model and demonstrates its ability to discover coherent multiple causal representations in several experimental data sets."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1889982"
                        ],
                        "name": "F. Kschischang",
                        "slug": "F.-Kschischang",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Kschischang",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kschischang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 222
                            }
                        ],
                        "text": "However, it turns out thatthe turbo-code can be concisely described as a multiply-connected Bayesian network, andthat the turbo-decoding algorithm is just probability propagation in this network [Freyand Kschischang 1996; Kschischang and Frey 1997; MacKay, McEliece and Cheng 1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 240
                            }
                        ],
                        "text": "\u2026that the turbo-decoding algorithm for these turbo-codes [Berrouand Glavieux 1996] is just the probability propagation algorithm discussed in Section 2.1applied to a code network like the one shown in Figure 1.5b [Frey and Kschischang 1996;Kschischang and Frey 1997; MacKay, McEliece and Cheng 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 197
                            }
                        ],
                        "text": "However, it turns out that the turbo-code can be concisely described as a multiply-connected Bayesian network, and that the turbo-decoding algorithm is just probability propagation in this network [Frey and Kschischang 1996; Kschischang and Frey 1997; MacKay, McEliece and Cheng 1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6522238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbd45449e1cdadbf1f0c06a9510b5ac247cb70b9",
            "isKey": false,
            "numCitedBy": 425,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unified graphical model framework for describing compound codes and deriving iterative decoding algorithms. After reviewing a variety of graphical models (Markov random fields, Tanner graphs, and Bayesian networks), we derive a general distributed marginalization algorithm for functions described by factor graphs. From this general algorithm, Pearl's (1986) belief propagation algorithm is easily derived as a special case. We point out that iterative decoding algorithms for various codes, including \"turbo decoding\" of parallel-concatenated convolutional codes, may be viewed as probability propagation in a graphical model of the code. We focus on Bayesian network descriptions of codes, which give a natural input/state/output/channel description of a code and channel, and we indicate how iterative decoders can be developed for parallel-and serially concatenated coding systems, product codes, and low-density parity-check codes."
            },
            "slug": "Iterative-Decoding-of-Compound-Codes-by-Probability-Kschischang-Frey",
            "title": {
                "fragments": [],
                "text": "Iterative Decoding of Compound Codes by Probability Propagation in Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is pointed out that iterative decoding algorithms for various codes, including \"turbo decoding\" of parallel-concatenated convolutional codes, may be viewed as probability propagation in a graphical model of the code."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Sel. Areas Commun."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031175"
                        ],
                        "name": "G. Ungerboeck",
                        "slug": "G.-Ungerboeck",
                        "structuredName": {
                            "firstName": "Gottfried",
                            "lastName": "Ungerboeck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ungerboeck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 13
                            }
                        ],
                        "text": "coset codes) [Ungerboeck 1982; Calderbank and Sloane 1987; Forney 1988], which your telephone modem probably uses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 62
                            }
                        ],
                        "text": "Ways ofdoing this include trellis codes (a.k.a. coset codes) [Ungerboeck 1982; Calderbank andSloane 1987; Forney 1988], which your telephone modem probably uses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20715426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "171f365e7156b71805031f7636fc78cff2fc5480",
            "isKey": false,
            "numCitedBy": 3810,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A coding technique is described which improves error performance of synchronous data links without sacrificing data rate or requiring more bandwidth. This is achieved by channel coding with expanded sets of multilevel/phase signals in a manner which increases free Euclidean distance. Soft maximum--likelihood (ML) decoding using the Viterbi algorithm is assumed. Following a discussion of channel capacity, simple hand-designed trellis codes are presented for 8 phase-shift keying (PSK) and 16 quadrature amplitude-shift keying (QASK) modulation. These simple codes achieve coding gains in the order of 3-4 dB. It is then shown that the codes can be interpreted as binary convolutional codes with a mapping of coded bits into channel signals, which we call \"mapping by set partitioning.\" Based on a new distance measure between binary code sequences which efficiently lower-bounds the Euclidean distance between the corresponding channel signal sequences, a search procedure for more powerful codes is developed. Codes with coding gains up to 6 dB are obtained for a variety of multilevel/phase modulation schemes. Simulation results are presented and an example of carrier-phase tracking is discussed."
            },
            "slug": "Channel-coding-with-multilevel/phase-signals-Ungerboeck",
            "title": {
                "fragments": [],
                "text": "Channel coding with multilevel/phase signals"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A coding technique is described which improves error performance of synchronous data links without sacrificing data rate or requiring more bandwidth by channel coding with expanded sets of multilevel/phase signals in a manner which increases free Euclidean distance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 220
                            }
                        ],
                        "text": "Although this nonsystematic ITC code does not perform as well as a turbo-codewith the same K and N , it does perform signi cantly better than the best rate 1/2 low-density parity-check code published to date [MacKay and Neal 1996] with K = 32; 671 andN = 65; 389."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 281
                            }
                        ],
                        "text": "\u2026corresponding to a low-density parity-check code consists of a large number of simpleparity-check trellises, where each constraint ensures that one nonsystematic branch variablefrom each of a very small number of trellises are equal (two or three trellises are used in[MacKay and Neal 1996])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 108
                            }
                        ],
                        "text": "Interestingly, the standard iterative decoders for low-densityparity-check codes [Gallager 1963; MacKay and Neal 1996] process the soft decisions foreach parity-check equation by applying the forward-backward algorithm to a parity-checktrellis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 115
                            }
                        ],
                        "text": "Figure 5.10b shows the Bayesian network for a low-density parity-check code [Gallager1963; Tanner 1981; MacKay and Neal 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "However, it appears they do not comeas close to Shannon's limit as do turbo-codes for rates of 1/3 and 1/2 [MacKay and Neal1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 235
                            }
                        ],
                        "text": "\u2026not depend on ) to obtain a probabilityP (vjD) = Z P (vj )P ( jD)d : (3.51)For example, this integral can be approximated using Laplace's approximation [Spiegelhal-ter and Lauritzen 1990], Markov chain Monte Carlo methods [Neal 1993; Neal 1996] orvariational techniques [Jaakkola and Jordan 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 150
                            }
                        ],
                        "text": "51) For example, this integral can be approximated using Laplace's approximation [Spiegelhalter and Lauritzen 1990], Markov chain Monte Carlo methods [Neal 1993; Neal 1996] or variational techniques [Jaakkola and Jordan 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": false,
            "numCitedBy": 3642,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 71
                            }
                        ],
                        "text": "A common parametric Bayesian network is the sigmoidal Bayesian network [Neal 1992; Jordan 1995; Saul, Jaakkola and Jordan 1996], whose random variables are all binary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 132
                            }
                        ],
                        "text": "Binary Bayesian networks which use logistic regression for the conditional distributions are often called binary sigmoidal networks [Neal 1992] and are sometimes called stochastic multi-layer perceptrons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 59
                            }
                        ],
                        "text": "These include Markov chain Monte Carlo methods[Pearl 1987; Neal 1992], \\Helmholtz machines\" [Hinton et al. 1995; Dayan et al. 1995], andvariational techniques [Saul, Jaakkola and Jordan 1996; Jaakkola, Saul and Jordan 1996;Frey 1997b].1.1 A probabilistic perspectiveO hand, it is not obvious that\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 172
                            }
                        ],
                        "text": "59) Networks of these variables can represent a broad range of structures, including deterministic multilayer perceptrons [Bishop 1995], binary sigmoidal Bayesian networks [Neal 1992] (see Section 3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 141
                            }
                        ],
                        "text": "The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods, and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992] as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 200
                            }
                        ],
                        "text": "\u2026of the variables of interest in h.The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods,and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992]as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 132
                            }
                        ],
                        "text": "Binary Bayesian networks which use logistic regression for the conditionaldistributions are often called binary sigmoidal networks [Neal 1992] and are sometimescalled stochastic multi-layer perceptrons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 143
                            }
                        ],
                        "text": "\u2026can represent a broad range of structures, including deter-ministic multilayer perceptrons [Bishop 1995], binary sigmoidal Bayesian networks [Neal1992] (see Section 3.4), mixture models, mixture of expert models [Jacobs et al. 1991],hierarchical mixture of expert models [Jordan and Jacobs\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 72
                            }
                        ],
                        "text": "A common parametric Bayesian network is the sigmoidal Bayesian network [Neal 1992;Jordan 1995; Saul, Jaakkola and Jordan 1996], whose random variables are all binary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 47
                            }
                        ],
                        "text": "These include Markov chain Monte Carlo methods [Pearl 1987; Neal 1992], \\Helmholtz machines\" [Hinton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14290328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5",
            "isKey": true,
            "numCitedBy": 601,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-of-Belief-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning of Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3637824"
                        ],
                        "name": "R. M. Tanner",
                        "slug": "R.-M.-Tanner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tanner",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. M. Tanner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 67
                            }
                        ],
                        "text": "10b shows the Bayesian network for a low-density parity-check code [Gallager 1963; Tanner 1981; MacKay and Neal 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 91
                            }
                        ],
                        "text": "Figure 5.10b shows the Bayesian network for a low-density parity-check code [Gallager1963; Tanner 1981; MacKay and Neal 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 754232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "157218bae792b6ef550dfd0f73e688d83d98b3d7",
            "isKey": false,
            "numCitedBy": 2971,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is described for constructing long error-correcting codes from one or more shorter error-correcting codes, referred to as subcodes, and a bipartite graph. A graph is shown which specifies carefully chosen subsets of the digits of the new codes that must be codewords in one of the shorter subcodes. Lower bounds to the rate and the minimum distance of the new code are derived in terms of the parameters of the graph and the subeodes. Both the encoders and decoders proposed are shown to take advantage of the code's explicit decomposition into subcodes to decompose and simplify the associated computational processes. Bounds on the performance of two specific decoding algorithms are established, and the asymptotic growth of the complexity of decoding for two types of codes and decoders is analyzed. The proposed decoders are able to make effective use of probabilistic information supplied by the channel receiver, e.g., reliability information, without greatly increasing the number of computations required. It is shown that choosing a transmission order for the digits that is appropriate for the graph and the subcodes can give the code excellent burst-error correction abilities. The construction principles"
            },
            "slug": "A-recursive-approach-to-low-complexity-codes-Tanner",
            "title": {
                "fragments": [],
                "text": "A recursive approach to low complexity codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that choosing a transmission order for the digits that is appropriate for the graph and the subcodes can give the code excellent burst-error correction abilities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1489457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03a60c3bca776ecd84c580f86766dec52ef8d0e3",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Sigmoid type belief networks, a class of probabilistic neural networks, provide a natural framework for compactly representing probabilistic information in a variety of unsupervised and supervised learning problems. Often the parameters used in these networks need to be learned from examples. Unfortunately, estimating the parameters via exact probabilistic calculations (i.e, the EM-algorithm) is intractable even for networks with fairly small numbers of hidden units. We propose to avoid the infeasibility of the E step by bounding likelihoods instead of computing them exactly. We introduce extended and complementary representations for these networks and show that the estimation of the network parameters can be made fast (reduced to quadratic optimization) by performing the estimation in either of the alternative domains. The complementary networks can be used for continuous density estimation as well."
            },
            "slug": "Fast-Learning-by-Bounding-Likelihoods-in-Sigmoid-Jaakkola-Saul",
            "title": {
                "fragments": [],
                "text": "Fast Learning by Bounding Likelihoods in Sigmoid Type Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes to avoid the infeasibility of the E step by bounding likelihoods instead of computing them exactly, and shows that the estimation of the network parameters can be made fast by performing the estimation in either of the alternative domains."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 82
                            }
                        ],
                        "text": "A common parametric Bayesian network is the sigmoidal Bayesian network [Neal 1992;Jordan 1995; Saul, Jaakkola and Jordan 1996], whose random variables are all binary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 71
                            }
                        ],
                        "text": "A common parametric Bayesian network is the sigmoidal Bayesian network [Neal 1992; Jordan 1995; Saul, Jaakkola and Jordan 1996], whose random variables are all binary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14540707,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bed107422083bcf98a7bc509929ce5b71e6e5023",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a tutorial introduction to the logistic function as a statistical object. Beyond the discussion of the whys and wherefores of the logistic function, I also hope to illuminate the general distinction between the \\generative/causal/class-conditional\" and the \\discriminative/diagnostic/ predictive\" directions for the modeling of data. Crudely put, the belief network community has tended to focus on the former while the neural network community has tended to focus on the latter (although there are numerous papers in both communities going against their respective grains). It is the author's view that these two directions are two sides of the same coin, a corollary of which is that the two network-based communities are in closer contact than one might otherwise think. To illustrate some of the issues involved, I discuss the simplest nonlinear neural network|a logistic function of a linear combination of the input variables (also known in statistics as a logistic regression). The logistic function has had a lengthy history in classical statistics and in neural networks. In statistics it plays a leading role in the methodology of logistic regression, where it makes an important contribution to the literature on classi cation. The logistic function has also appeared in many guises in neural network research. In early work, in which continuous time formalisms tended to dominate, it was justi ed via its being the solution to a particular di erential equation. In later work, with the emphasis on discrete time, it was generally used more heuristically as one of the many possible smooth, monotonic \\squashing\" functions that map real values into a bounded interval. More recently, however, with the increasing focus on learning, the probabilistic properties of the logistic function have begun to"
            },
            "slug": "Why-the-logistic-function-A-tutorial-discussion-on-Jordan",
            "title": {
                "fragments": [],
                "text": "Why the logistic function? A tutorial discussion on probabilities and neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is the author's view that these two directions are two sides of the same coin, a corollary of which is that the two network-based communities are in closer contact than one might otherwise think."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741200"
                        ],
                        "name": "J. Movellan",
                        "slug": "J.-Movellan",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Movellan",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movellan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 16635309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe74763afa5741c00263ca479068337b647252d4",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article we present symmetric diffusion networks, a family of networks that instantiate the principles of continuous, stochastic, adaptive and interactive propagation of information. Using methods of Markovion diffusion theory, we formalize the activation dynamics of these networks and then show that they can be trained to reproduce entire multivariate probability distributions on their outputs using the contrastive Hebbion learning rule (CHL). We show that CHL performs gradient descent on an error function that captures differences between desired and obtained continuous multivariate probability distributions. This allows the learning algorithm to go beyond expected values of output units and to approximate complete probability distributions on continuous multivariate activation spaces. We argue that learning continuous distributions is an important task underlying a variety of real-life situations that were beyond the scope of previous connectionist networks. Deterministic networks, like back propagation, cannot learn this task because they are limited to learning average values of independent output units. Previous stochastic connectionist networks could learn probability distributions but they were limited to discrete variables. Simulations show that symmetric diffusion networks can be trained with the CHL rule to approximate discrete and continuous probability distributions of various types."
            },
            "slug": "Learning-Continuous-Probability-Distributions-with-Movellan-McClelland",
            "title": {
                "fragments": [],
                "text": "Learning Continuous Probability Distributions with Symmetric Diffusion Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Simulations show that symmetric diffusion networks can be trained with the CHL rule to approximate discrete and continuous probability distributions of various types."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 93
                            }
                        ],
                        "text": "The forward-backward algorithmcan be viewed simply as a combination of probabilistic \\ ows\" [McEliece 1996] computedin the forward direction and in the backward direction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 93
                            }
                        ],
                        "text": "The forward-backward algorithm can be viewed simply as a combination of probabilistic \\ ows\" [McEliece 1996] computed in the forward direction and in the backward direction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9207542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f6a15214a73abf7a05c604ecd08847614bf8a03",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In this semi-tutorial paper, we will investigate the computational complexity of an abstract version of the Viterbi algorithm on a trellis, and show that if the trellis has e edges, the complexity of the Viterbi algorithm is /spl Theta/(e). This result suggests that the \"best\" trellis representation for a given linear block code is the one with the fewest edges. We will then show that, among all trellises that represent a given code, the original trellis introduced by Bahl, Cocke, Jelinek, and Raviv in 1974, and later rediscovered by Wolf (1978), Massey (1978), and Forney (1988), uniquely minimizes the edge count, as well as several other figures of merit. Following Forney and Kschischang and Sorokine (1995), we will also discuss \"trellis-oriented\" or \"minimal-span\" generator matrices, which facilitate the calculation of the size of the BCJR trellis, as well as the actual construction of it."
            },
            "slug": "On-the-BCJR-trellis-for-linear-block-codes-McEliece",
            "title": {
                "fragments": [],
                "text": "On the BCJR trellis for linear block codes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that, among all trellises that represent a given code, the original trellis introduced by Bahl, Cocke, Jelinek, and Raviv in 1974, and later rediscovered by Wolf, Massey, and Forney, uniquely minimizes the edge count."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13723620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb0419bccc2244ed33c9c42341f342511262daa3",
            "isKey": false,
            "numCitedBy": 2148,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called \"hidden causes.\" It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves."
            },
            "slug": "Fusion,-Propagation,-and-Structuring-in-Belief-Pearl",
            "title": {
                "fragments": [],
                "text": "Fusion, Propagation, and Structuring in Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network."
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 119
                            }
                        ],
                        "text": "An interesting problem relevant to vision is that of extracting independent horizontal and vertical bars from an image [Foldiak 1990; Saund 1995; Zemel 1993; Dayan and Zemel 1995; Hinton et al. 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 145
                            }
                        ],
                        "text": "An interesting problem relevant to vision is that of extracting independent horizontal andvertical bars from an image [Foldiak 1990; Saund 1995; Zemel 1993; Dayan and Zemel 1995;Hinton et al. 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117036852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b2bffdf5b62305bec4c0f1ea7e3c1ba66fccb5",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental problem in learning and reasoning about a set of information is finding the right representation. The primary goal of an unsupervised learning procedure is to optimize the quality of a system's internal representation. In this thesis, we present a general framework for describing unsupervised learning procedures based on the Minimum Description Length (MDL) principle. The MDL principle states that the best model is one that minimizes the summed description length of the model and the data with respect to the model. Applying this approach to the unsupervised learning problem makes explicit a key trade off between the accuracy of a representation (i.e., how concise a description of the input may be generated from it) and its succinctness (i.e., how compactly the representation itself can be described). \nViewing existing unsupervised learning procedures in terms of the framework exposes their implicit assumptions about the type of structure assumed to underlie the data. While these existing algorithms typically minimize the data description using a fixed length representation, we use the framework to derive a class of objective functions for training self-supervised neural networks, where the goal is to minimize the description length of the representation simultaneously with that of the data. Formulating a description of the representation forces assumptions about the structure of the data to be made explicit, which in turn leads to a particular network configuration as well as an objective function that can be used to optimize the network parameters. We describe three new learning algorithms derived in this manner from the MDL framework. Each algorithm embodies a different scheme for describing the internal representation, and is therefore suited to a range of datasets based on the structure underlying the data. Simulations demonstrate the applicability of these algorithms on some simple computational vision tasks."
            },
            "slug": "A-minimum-description-length-framework-for-learning-Zemel",
            "title": {
                "fragments": [],
                "text": "A minimum description length framework for unsupervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis presents a general framework for describing unsupervised learning procedures based on the Minimum Description Length (MDL) principle, and describes three new learning algorithms derived in this manner from the MDL framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690846"
                        ],
                        "name": "S. Wicker",
                        "slug": "S.-Wicker",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wicker",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wicker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 49
                            }
                        ],
                        "text": "There are many techniques for algebraic decoding [Lin and Costello 1983; Blahut 1990; Wicker 1995] and algebraic decoders usually take advantage of special structure that is built into the code to make decoding easier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 5
                            }
                        ],
                        "text": "See [Wicker 1995] for a textbook treatment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 85
                            }
                        ],
                        "text": "There are many techniques for algebraicdecoding [Lin and Costello 1983; Blahut 1990; Wicker 1995] and algebraic decoders usuallytake advantage of special structure that is built into the code to make decoding easier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122288638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69373b93ec632055582d8666619b4b6e3f49dd32",
            "isKey": false,
            "numCitedBy": 1885,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Error Control Coding for Digital Communication Systems. 2. Galois Fields. 3. Polynomials over Galois Fields. 4. Linear Block Codes. 5. Cyclic Codes. 6. Hadamard, Quadratic Residue, and Golay Codes. 7. Reed-Muller Codes 8. BCH and Reed-Solomon Codes. 9. Decoding BCH and Reed-Solomon Codes. 10. The Analysis of the Performance of Block Codes. 11. Convolutional Codes. 12. The Viterbi Decoding Algorithm. 13. The Sequential Decoding Algorithms. 14. Trellis Coded Modulation. 15. Error Control for Channels with Feedback. 16. Applications. Appendices: A. Binary Primitive Polynomials. B. Add-on Tables and Vector Space Representations for GF(8) Through GF(1024). C. Cyclotronic Cosets Modulo 2m-1. D. Minimal Polynomials for Elements in GF (2m). E. Generator Polynomials of Binary BCH Codes of Lengths Through 511. Bibliography."
            },
            "slug": "Error-Control-Systems-for-Digital-Communication-and-Wicker",
            "title": {
                "fragments": [],
                "text": "Error Control Systems for Digital Communication and Storage"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work has shown that polynomials over Galois Fields, particularly the Hadamard, Quadratic Residue, and Golay Codes, are good candidates for Error Control Coding for Digital Communication Systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2445072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f",
            "isKey": false,
            "numCitedBy": 958,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes."
            },
            "slug": "Autoencoders,-Minimum-Description-Length-and-Free-Hinton-Zemel",
            "title": {
                "fragments": [],
                "text": "Autoencoders, Minimum Description Length and Helmholtz Free Energy"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7835,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2718299"
                        ],
                        "name": "N. Wiberg",
                        "slug": "N.-Wiberg",
                        "structuredName": {
                            "firstName": "Niclas",
                            "lastName": "Wiberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 115168171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb44d50bce92b4ce2c0ea53bd8ede95f628ee3cb",
            "isKey": false,
            "numCitedBy": 1007,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Iterative decoding techniques have become a viable alternative for constructing high performance coding systems. In particular, the recent success of turbo codes indicates that performance close to the Shannon limit may be achieved. In this thesis, it is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding. The min-sum and sum-product algorithms are developed and presented as generalized trellis algorithms, where the time axis of the trellis is replaced by an arbitrary graph, the \u201cTanner graph\u201d. With cycle-free Tanner graphs, the resulting decoding algorithms (e.g., Viterbi decoding) are maximum-likelihood but suffer from an exponentially increasing complexity. Iterative decoding occurs when the Tanner graph has cycles (e.g., turbo codes); the resulting algorithms are in general suboptimal, but significant complexity reductions are possible compared to the cycle-free case. Several performance estimates for iterative decoding are developed, including a generalization of the union bound used with Viterbi decoding and a characterization of errors that are uncorrectable after infinitely many decoding iterations."
            },
            "slug": "Codes-and-Decoding-on-General-Graphs-Wiberg",
            "title": {
                "fragments": [],
                "text": "Codes and Decoding on General Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122200499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2861a5e59de4d61bcd8a9ae4785978ac11fc9c1",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-neural-networks-and-density-networks-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian neural networks and density networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 81
                            }
                        ],
                        "text": "51) For example, this integral can be approximated using Laplace's approximation [Spiegelhalter and Lauritzen 1990], Markov chain Monte Carlo methods [Neal 1993; Neal 1996] or variational techniques [Jaakkola and Jordan 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10739577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcce2a3564685657c23d1afa00155c03560e76ac",
            "isKey": false,
            "numCitedBy": 615,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A directed acyclic graph or influence diagram is frequently used as a representation for qualitative knowledge in some domains in which expert system techniques have been applied, and conditional probability tables on appropriate sets of variables form the quantitative part of the accumulated experience. It is shown how one can introduce imprecision into such probabilities as a data base of cases accumulates. By exploiting the graphical structure, the updating can be performed locally, either approximately or exactly, and the setup makes it possible to take advantage of a range of well-established statistical techniques. As examples we discuss discrete models, models based on Dirichlet distributions and models of the logistic regression type."
            },
            "slug": "Sequential-updating-of-conditional-probabilities-on-Spiegelhalter-Lauritzen",
            "title": {
                "fragments": [],
                "text": "Sequential updating of conditional probabilities on directed graphical structures"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is shown how one can introduce imprecision into such probabilities as a data base of cases accumulates and how to take advantage of a range of well-established statistical techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2343452"
                        ],
                        "name": "Shu Lin",
                        "slug": "Shu-Lin",
                        "structuredName": {
                            "firstName": "Shu",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690091"
                        ],
                        "name": "D. Costello",
                        "slug": "D.-Costello",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Costello",
                            "middleNames": [
                                "J."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Costello"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28213032,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "003643595acb19117ddff6c8ed854ddd30c1aff5",
            "isKey": false,
            "numCitedBy": 4443,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Coding for Reliable Digital Transmission and Storage. 2. Introduction to Algebra. 3. Linear Block Codes. 4. Important Linear Block Codes. 5. Cyclic Codes. 6. Binary BCH Codes. 7. Nonbinary BCH Codes, Reed-Solomon Codes, and Decoding Algorithms. 8. Majority-Logic Decodable Codes. 9. Trellises for Linear Block Codes. 10. Reliability-Based Soft-Decision Decoding Algorithms for Linear Block Codes. 11. Convolutional Codes. 12. Trellis-Based Decoding Algorithms for Convolutional Codes. 13. Sequential and Threshold Decoding of Convolutional Codes. 14. Trellis-Based Soft-Decision Algorithms for Linear Block Codes. 15. Concatenated Coding, Code Decomposition ad Multistage Decoding. 16. Turbo Coding. 17. Low Density Parity Check Codes. 18. Trellis Coded Modulation. 19. Block Coded Modulation. 20. Burst-Error-Correcting Codes. 21. Automatic-Repeat-Request Strategies."
            },
            "slug": "Error-control-coding-fundamentals-and-applications-Lin-Costello",
            "title": {
                "fragments": [],
                "text": "Error control coding - fundamentals and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This book explains coding for Reliable Digital Transmission and Storage using Trellis-Based Soft-Decision Decoding Algorithms for Linear Block Codes and Convolutional Codes, and some of the techniques used in this work."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall computer applications in electrical engineering series"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51006626"
                        ],
                        "name": "H. Imai",
                        "slug": "H.-Imai",
                        "structuredName": {
                            "firstName": "Hideki",
                            "lastName": "Imai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Imai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761929"
                        ],
                        "name": "S. Hirakawa",
                        "slug": "S.-Hirakawa",
                        "structuredName": {
                            "firstName": "Shuji",
                            "lastName": "Hirakawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hirakawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 121
                            }
                        ],
                        "text": "Alternatively,Wachsmann and Huber [1995] and Forney [1997] have shown that by using a techniquecalled multilevel coding [Imai and Hirakawa 1977], we can achieve the capacity in (5.2) by\n5.1 Simplifying the playing eld 121combining several relatively simple linear binary codes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 122
                            }
                        ],
                        "text": "Alternatively, Wachsmann and Huber [1995] and Forney [1997] have shown that by using a technique called multilevel coding [Imai and Hirakawa 1977], we can achieve the capacity in (5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 111
                            }
                        ],
                        "text": "It turns out that the solution to this problem has far-reaching consequences in multi-level (nonbinary) coding [Imai and Hirakawa 1977], mainly due to recent proofs by Wachsmann and Huber [1995] and Forney [1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29554334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40f8d4f755b6495fde051f26adb4aca72dd51c7b",
            "isKey": true,
            "numCitedBy": 918,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A new multilevel coding method that uses several error-correcting codes is proposed. The transmission symbols are constructed by combining symbols of codewords of these codes. Usually, these codes are binary error-correcting codes and have different error-correcting capabilities. For various channels, efficient systems can be obtained by choosing these codes appropriately. Encoding and decoding procedures for this method are relatively simple compared with those of other multilevel coding methods. In addition, this method makes effective use of soft-decisions to improve the performance of decoding. The decoding error probability is analyzed for multiphase modulation, and numerical comparisons to other multilevel coding systems are made. When equally complex systems are compared, the new system is superior to other multilevel coding systems."
            },
            "slug": "A-new-multilevel-coding-method-using-codes-Imai-Hirakawa",
            "title": {
                "fragments": [],
                "text": "A new multilevel coding method using error-correcting codes"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new multilevel coding method that uses several error-correcting codes that makes effective use of soft-decisions to improve the performance of decoding and is superior to other multileVEL coding systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 60
                            }
                        ],
                        "text": "In this section, I review a technique called slice sampling [Neal 1997; Frey 1997a], that can be used for drawing a value z from a univariate probability density p(z) | in the context of inference, p(z) is the conditional distribution p(zkjfzj = z( 1) j gNj=1;j 6=k)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 70
                            }
                        ],
                        "text": "A new value is obtained for each hidden variable using slice sampling [Neal 1997] (see Section 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 60
                            }
                        ],
                        "text": "In this section, I review a technique called slicesampling [Neal 1997; Frey 1997a], that can be used for drawing a value z from a univariateprobability density p(z) | in the context of inference, p(z) is the conditional distributionp(zkjfzj = z( 1)j gNj=1;j 6=k)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 70
                            }
                        ],
                        "text": "A new value is obtained for each hidden variable using slice sampling[Neal 1997] (see Section 2.2.4), based on the distribution for the variable conditioned on all\n92 Pattern Classi cationother variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15736158,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5f22bf9d37c49a7bbbca45d3a331b56a4a4143c5",
            "isKey": true,
            "numCitedBy": 100,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "One way to sample from a distribution is to sample uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizonta\u00ecslice' deened by the current vertical position. Variations on such`slice sampling' methods can easily be implemented for univariate distributions , and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling, and may be more eecient than easily-constructed versions of the Metropolis algorithm. Slice sampling is therefore attractive in routine Markov chain Monte Carlo applications, and for use by software that automatically generates a Markov chain sampler from a model specii-cation. One can also easily devise overrelaxed versions of slice sampling, which sometimes greatly improve sampling eeciency by suppressing random walk behaviour. Random walks can also be avoided in some slice sampling schemes that simultaneously update all variables."
            },
            "slug": "Markov-Chain-Monte-Carlo-Methods-Based-on-`Slicing'-Neal",
            "title": {
                "fragments": [],
                "text": "Markov Chain Monte Carlo Methods Based on `Slicing' the Density Function"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Slice sampling is attractive in routine Markov chain Monte Carlo applications, and for use by software that automatically generates aMarkov chain sampler from a model specii-cation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3203547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b59f6b9461aaa1abad3bc1d1d44c7de32c589e21",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a mean eld theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean eld theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition|the classiication of handwritten digits."
            },
            "slug": "Mean-Field-Theory-for-Sigmoid-Belief-NetworksMean-Saul-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Mean Field Theory for Sigmoid Belief NetworksMean Field Theory for Sigmoid Belief"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The mean eld theory for sigmoid belief networks provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2718299"
                        ],
                        "name": "N. Wiberg",
                        "slug": "N.-Wiberg",
                        "structuredName": {
                            "firstName": "Niclas",
                            "lastName": "Wiberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143681410"
                        ],
                        "name": "H. Loeliger",
                        "slug": "H.-Loeliger",
                        "structuredName": {
                            "firstName": "Hans-Andrea",
                            "lastName": "Loeliger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Loeliger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7715701"
                        ],
                        "name": "R. Koetter",
                        "slug": "R.-Koetter",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Koetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koetter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 263
                            }
                        ],
                        "text": "\u2026by hispeers, only to be rediscovered nearly 35 years later independently by at least three researchgroups, and to be shown to yield unprecedented performance in error-correcting codingapplications [Berrou, Glavieux and Thitimajshima 1993; Wiberg, Loeliger and K otter 1995;MacKay and Neal 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36630145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "848822f6c3446842730587cb4373a53f69e38720",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Until recently, most known decoding procedures for error-correcting codes were based either on algebraically calculating the error pattern or on some sort of tree or trellis search. With the advent of turbo coding, a third decoding principle has finally had its breakthrough: iterative decoding. With respect to Viterbi decoding, a code is most naturally described by means of a trellis diagram. The main thesis of the present paper is that, with respect to iterative decoding, the natural way of describing a code is by means of a Tanner graph, which may be viewed as a generalized trellis. More precisely, it is the \"time axis\" of a trellis that is generalized to a Tanner graph."
            },
            "slug": "Codes-and-iterative-decoding-on-general-graphs-Wiberg-Loeliger",
            "title": {
                "fragments": [],
                "text": "Codes and iterative decoding on general graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The main thesis of the present paper is that, with respect to iterative decoding, the natural way of describing a code is by means of a Tanner graph, which may be viewed as a generalized trellis."
            },
            "venue": {
                "fragments": [],
                "text": "Eur. Trans. Telecommun."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Spiegelhalter, D. J. (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 131
                            }
                        ],
                        "text": "Although there are procedures for attempting to convert an original networkto one that is appropriate for probability propagation [Spiegelhalter 1986; Lauritzen andSpiegelhalter 1988], these procedures are not practically fruitful when the number of multiplepaths is large."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 202
                            }
                        ],
                        "text": "Probabilisticstructure has been most extensively developed in the arti cial intelligence literature, withapplications ranging from taxonomic hierarchies [Woods 1975; Schubert 1976] to medicaldiagnosis [Spiegelhalter 1990]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 131
                            }
                        ],
                        "text": "Although there are procedures for attempting to convert an original network to one that is appropriate for probability propagation [Spiegelhalter 1986; Lauritzen and Spiegelhalter 1988], these procedures are not practically fruitful when the number of multiple paths is large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 98
                            }
                        ],
                        "text": "1 Exact inference in singly-connected Bayesian networksIn the late 1980's, Pearl [1986; 1988] and Lauritzen and Spiegelhalter [Spiegelhalter 1986;Lauritzen and Spiegelhalter 1988] independently published the exact probability propaga-tion algorithm for inferring the distributions over individual variables in singly-connectedBayesian networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Spiegelhalter, D. J. and Lauritzen, S. L. (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 21
                            }
                        ],
                        "text": "Lauritzen, S. L. and Spiegelhalter, D. J. (1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Spiegelhalter, D. J. (1986)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 127
                            }
                        ],
                        "text": "1 Exact inference in singly-connected Bayesian networks In the late 1980's, Pearl [1986; 1988] and Lauritzen and Spiegelhalter [Spiegelhalter 1986; Lauritzen and Spiegelhalter 1988] independently published the exact probability propagation algorithm for inferring the distributions over individual variables in singly-connected Bayesian networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 146
                            }
                        ],
                        "text": "1 Exact inference in singly-connected Bayesian networksIn the late 1980's, Pearl [1986; 1988] and Lauritzen and Spiegelhalter [Spiegelhalter 1986;Lauritzen and Spiegelhalter 1988] independently published the exact probability propaga-tion algorithm for inferring the distributions over individual\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58792451,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0a3767909649cf31d32e087693d93171af28ebe0",
            "isKey": true,
            "numCitedBy": 4303,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Local-computations-with-probabilities-on-graphical-Lauritzen-Spiegelhalter",
            "title": {
                "fragments": [],
                "text": "Local computations with probabilities on graphical structures and their application to expert systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703841"
                        ],
                        "name": "Lin-nan Lee",
                        "slug": "Lin-nan-Lee",
                        "structuredName": {
                            "firstName": "Lin-nan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin-nan Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 75
                            }
                        ],
                        "text": "Byusing this approach, substantial coding gains have been reported by Lee [Lee 1977], Collins[Collins 1993], and Hagenauer et al. [Hagenauer, O er and Papke 1993]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "See Lee and Messerschmitt [1994] for moredetails.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Lee, E. A. and Messerschmitt, D. G. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Kluwer AcademicPublishers, Norwell MA.Lee, L. (1977)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 75
                            }
                        ],
                        "text": "By using this approach, substantial coding gains have been reported by Lee [Lee 1977], Collins [Collins 1993], and Hagenauer et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62190598,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "79b2dedd52d12b3455de6213d5fff8c37491fc55",
            "isKey": true,
            "numCitedBy": 44,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Concatenated coding systems utilizing a convolutional code as the inner code and a Reed-Solomon code as the outer code are considered. In order to obtain very reliable communications over a very noisy channel with relatively modest coding complexity, it is proposed to concatenate a byte-oriented unit-memory convolutional code with an RS outer code whose symbol size is one byte. It is further proposed to utilize a real-time minimal-byte-error probability decoding algorithm, together with feedback from the outer decoder, in the decoder for the inner convolutional code. The performance of the proposed concatenated coding system is studied, and the improvement over conventional concatenated systems due to each additional feature is isolated."
            },
            "slug": "Concatenated-Coding-Systems-Employing-a-Unit-Memory-Lee",
            "title": {
                "fragments": [],
                "text": "Concatenated Coding Systems Employing a Unit-Memory Convolutional Code and a Byte-Oriented Decoding Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "In order to obtain very reliable communications over a very noisy channel with relatively modest coding complexity, it is proposed to concatenate a byte-oriented unit-memory convolutional code with an RS outer code whose symbol size is one byte."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 47
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower thana certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 110
                            }
                        ],
                        "text": "There are several proofs that each EM iteration is guaranteed to increase the likelihood of the training data [Baum and Petrie 1966; Dempster, Laird and Rubin 1977; Meng and Rubin 1992; Neal and Hinton 1993]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 184
                            }
                        ],
                        "text": "There are several proofs that each EM iteration is guaranteed to increasethe likelihood of the training data [Baum and Petrie 1966; Dempster, Laird and Rubin 1977;Meng and Rubin 1992; Neal and Hinton 1993]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 18
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton 1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan 1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower than a certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17947141,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9f87a11a523e4680e61966e36ea2eac516096f23",
            "isKey": false,
            "numCitedBy": 2597,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."
            },
            "slug": "A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton",
            "title": {
                "fragments": [],
                "text": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step is shown empirically to give faster convergence in a mixture estimation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 71
                            }
                        ],
                        "text": "However, it turns out that they have excellent theoretical performance [MacKay 1997] and that the iterative decoder proposed"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 71
                            }
                        ],
                        "text": "However, it turns out that they haveexcellent theoretical performance [MacKay 1997] and that the iterative decoder proposed\n5.3 Interleaved trellis-constraint codes (ITC codes) 139by Gallager is in fact equivalent to probability propagation in the network shown above."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17285553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9ae39a71308a0bfe12fd5c1ba13165547be3cbd",
            "isKey": true,
            "numCitedBy": 494,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of error-correcting codes for the binary symmetric channel. These codes are designed to encode a sparse source, and are defined in terms of very sparse invertible matrices, in such a way that the decoder can treat the signal and the noise symmetrically. The decoding problem involves only very sparse matrices and sparse vectors, and so is a promising candidate for practical decoding."
            },
            "slug": "Good-Codes-Based-on-Very-Sparse-Matrices-Mackay",
            "title": {
                "fragments": [],
                "text": "Good Codes Based on Very Sparse Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new family of error-correcting codes for the binary symmetric channel is presented, designed to encode a sparse source, and are defined in terms of very sparse invertible matrices, in such a way that the decoder can treat the signal and the noise symmetrically."
            },
            "venue": {
                "fragments": [],
                "text": "IMACC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31579914"
                        ],
                        "name": "R. Hofmann",
                        "slug": "R.-Hofmann",
                        "structuredName": {
                            "firstName": "Reimar",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6576069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa002ad55209cfbdb62fc88682ddfbac1b5d9d53",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We study Bayesian networks for continuous variables using nonlinear conditional density estimators. We demonstrate that useful structures can be extracted from a data set in a self-organized way and we present sampling techniques for belief update based on Markov blanket conditional density models."
            },
            "slug": "Discovering-Structure-in-Continuous-Variables-Using-Hofmann-Tresp",
            "title": {
                "fragments": [],
                "text": "Discovering Structure in Continuous Variables Using Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is demonstrated that useful structures can be extracted from a data set in a self-organized way and sampling techniques for belief update based on Markov blanket conditional density models are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 230
                            }
                        ],
                        "text": "\u2026including deter-ministic multilayer perceptrons [Bishop 1995], binary sigmoidal Bayesian networks [Neal1992] (see Section 3.4), mixture models, mixture of expert models [Jacobs et al. 1991],hierarchical mixture of expert models [Jordan and Jacobs 1994], and factor analysis models[Everitt 1984]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 45
                            }
                        ],
                        "text": "1991], hierarchical mixture of expert models [Jordan and Jacobs 1994], and factor analysis models [Everitt 1984]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": true,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 199
                            }
                        ],
                        "text": "51) For example, this integral can be approximated using Laplace's approximation [Spiegelhalter and Lauritzen 1990], Markov chain Monte Carlo methods [Neal 1993; Neal 1996] or variational techniques [Jaakkola and Jordan 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 272
                            }
                        ],
                        "text": "\u2026not depend on ) to obtain a probabilityP (vjD) = Z P (vj )P ( jD)d : (3.51)For example, this integral can be approximated using Laplace's approximation [Spiegelhal-ter and Lauritzen 1990], Markov chain Monte Carlo methods [Neal 1993; Neal 1996] orvariational techniques [Jaakkola and Jordan 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16037413,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "e407ea7fda6d152d2186f4b5e27aa04ec2d32dcd",
            "isKey": true,
            "numCitedBy": 210,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a logistic regression model with a Gaussian prior distribution over the parameters. We show that accurate variational techniques can be used to obtain a closed form posterior distribution over the parameters given the data thereby yielding a posterior predictive model. The results are readily extended to (binary) belief networks. For belief networks we also derive closed form posteriors in the presence of missing values. Finally, we show that the dual of the regression problem gives a latent variable density model, the variational formulation of which leads to exactly solvable EM updates."
            },
            "slug": "A-variational-approach-to-Bayesian-logistic-and-Jaakkola-Jordan",
            "title": {
                "fragments": [],
                "text": "A variational approach to Bayesian logistic regression problems and their extensions"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is shown that accurate variational techniques can be used to obtain a closed form posterior distribution over the parameters given the data thereby yielding a posterior predictive model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688852"
                        ],
                        "name": "G. Potamianos",
                        "slug": "G.-Potamianos",
                        "structuredName": {
                            "firstName": "Gerasimos",
                            "lastName": "Potamianos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Potamianos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2599527"
                        ],
                        "name": "J. Goutsias",
                        "slug": "J.-Goutsias",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Goutsias",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goutsias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 124
                            }
                        ],
                        "text": "There are a variety of practical algorithms for obtaining such an approximation, including Markov chain Monte Carlo methods [Geman and Geman 1984; Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld methods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan 1996], and inverse model methods [Hinton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 142
                            }
                        ],
                        "text": "\u2026for obtaining such anapproximation, including Markov chain Monte Carlo methods [Geman and Geman 1984;Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld meth-ods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan1996], and inverse\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34832523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e81f9bb1ea659c41ca3b63d6cc9a2e06c98d98ec",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A Monte Carlo simulation technique for estimating the partition function of a general Gibbs random field image is proposed. By expressing the partition function as an expectation, an importance sampling approach for estimating it using Monte Carlo simulations is developed. As expected, the resulting estimators are unbiased and consistent. Computations can be performed iteratively by using simple Monte Carlo algorithms with remarkable success, as demonstrated by simulations. The work concentrates on binary, second-order Gibbs random fields defined on a rectangular lattice. However, the proposed methods can be easily extended to more general Gibbs random fields. Their potential contribution to optimal parameter estimation and hypothesis testing problems for general Gibbs random field images using a likelihood approach is anticipated. >"
            },
            "slug": "Partition-function-estimation-of-Gibbs-random-field-Potamianos-Goutsias",
            "title": {
                "fragments": [],
                "text": "Partition function estimation of Gibbs random field images using Monte Carlo simulations"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "By expressing the partition function of a general Gibbs random field image as an expectation, an importance sampling approach for estimating it using Monte Carlo simulations is developed and the resulting estimators are unbiased and consistent."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 18
                            }
                        ],
                        "text": "Arithmetic coding [Rissanen and Langdon 1976; Witten, Neal and Cleary 1987] is a practical algorithm for producing near-optimal codewords when the source distribution Pr(v) is known."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 73
                            }
                        ],
                        "text": "To address this problem, consider the operation of an arithmetic decoder [Rissanen and Langdon 1976; Witten, Neal and Cleary 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 143
                            }
                        ],
                        "text": "\u2026`(v) is the length of the codeword for v in bits, and H is the entropy of the sourcein bits: H = Xv Pr(v) log2 Pr(v): (1.5)Arithmetic coding [Rissanen and Langdon 1976; Witten, Neal and Cleary 1987] is apractical algorithm for producing near-optimal codewords when the source\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 19
                            }
                        ],
                        "text": "Arithmetic coding [Rissanen and Langdon 1976; Witten, Neal and Cleary 1987] is apractical algorithm for producing near-optimal codewords when the source distributionPr(v) is known."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 73
                            }
                        ],
                        "text": "To address thisproblem, consider the operation of an arithmetic decoder [Rissanen and Langdon 1976;Witten, Neal and Cleary 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 21
                            }
                        ],
                        "text": "5) Arithmetic coding [Rissanen and Langdon 1976; Witten, Neal and Cleary 1987] is a practical algorithm for producing near-optimal codewords when the source distribution Pr(v) is known."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39909636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20d673dc3200ac1742ee0827535a291eb6e051f8",
            "isKey": true,
            "numCitedBy": 759,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases. An outstanding feature of this technique is that alphabet extensions are not required. A complete decodability analysis is given. The relationship of arithmetic coding to other known nonblock codes is illuminated."
            },
            "slug": "Arithmetic-Coding-Rissanen-Langdon",
            "title": {
                "fragments": [],
                "text": "Arithmetic Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3343393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd23c9168418324e81881365f297fb6a1caa3a07",
            "isKey": false,
            "numCitedBy": 3142,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method. Arithmetic coding gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding."
            },
            "slug": "Arithmetic-coding-for-data-compression-Witten-Neal",
            "title": {
                "fragments": [],
                "text": "Arithmetic coding for data compression"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method, which gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 102
                            }
                        ],
                        "text": "2 Monte Carlo inferenceThe Monte Carlo method [Hammersley and Handscomb 1964; Kalos and Whitlock 1986;Ripley 1987] makes use of pseudo-random numbers in order to perform computations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 124
                            }
                        ],
                        "text": "There are a variety of practical algorithms for obtaining such an approximation, including Markov chain Monte Carlo methods [Geman and Geman 1984; Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld methods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan 1996], and inverse model methods [Hinton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026practical algorithms for obtaining such anapproximation, including Markov chain Monte Carlo methods [Geman and Geman 1984;Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld meth-ods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan1996],\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 120
                            }
                        ],
                        "text": "E cient sampling methods forseveral special types of continuous parametric distribution are given in [Devroye 1986] and[Ripley 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 47
                            }
                        ],
                        "text": "2 Monte Carlo inference The Monte Carlo method [Hammersley and Handscomb 1964; Kalos and Whitlock 1986; Ripley 1987] makes use of pseudo-random numbers in order to perform computations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 121
                            }
                        ],
                        "text": "E cient sampling methods for several special types of continuous parametric distribution are given in [Devroye 1986] and [Ripley 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6151205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4180abb06c3e6931474c4a3e0ff079d2d5a0382",
            "isKey": true,
            "numCitedBy": 2209,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "One fifth (4 of 20) of the research articles published in the Journal of Educational Statistics in 1988 include simulation studies that justify or illustrate the authors' conclusions. A similar fraction (6 of 33) of the articles in the 1988 volume of Psychometrika include simulations; comparable proportions could be expected in other journals at the boundary of theoretical statistics and social/psychological applications. Due in part to the complexity of the problems tackled today and in part to the availability of cheap, powerful computing\u2014by no means independent influences\u2014simulation and Monte Carlo methods have become both necessary and practical tools for statisticians and applied workers in quantitative areas of education and psychology. Simulation has become popular\u2014not only in the quantitative social sciences, but in all of the mathematical sciences from physics to operations research to number theory\u2014because it is almost always easy to do. This ease of use makes the simulation experimenter vulnerable to two common pitfalls. Selection of the basic source of \"random numbers\" is often passive: Whatever is available in the computer's standard subroutine library is used. However, the fact that a pseudo-random number generator appears in a popular software package or operating system is hardly reason to trust it, as is shown by the infamous RANDU generator, once popular on IBM mainframes and PDP mini-computers, and by the generators burned into RAM on today's PCs. Simulation design and reporting also deserve special care. Some attempt must be made to assess the accuracy of the simulation estimates: One should accurately estimate and report SE (6) as well as 6. In addition, enough detail should be reported that the interested reader can replicate the study and check the results, just as with other experiments. Yet these considerations are also easy to overlook. Brian D. Ripley's Stochastic Simulation is a short, yet ambitious, survey of modern simulation techniques. Three themes run throughout the book. First, one shoud not take basic simulation subroutines for granted, especially on minior microcomputers where they tend to be poor implementations, implementations of poor algorithms, or both. Second, design of experiments, or variance reduction as it is known in this field, deserves greater consideration. Third, modern methods make it possible to simulate and analyze processes that are dependent over time, and using such processes opens the door to new simulation techniques, such as simulated annealing in optimization. Ripley intends this book to be a \"comprehensive guide,\" and it is indeed most accurately described as a researcher's handbook with examples and"
            },
            "slug": "Stochastic-simulation-Ripley",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Brian D. Ripley's Stochastic Simulation is a short, yet ambitious, survey of modern simulation techniques, and three themes run throughout the book."
            },
            "venue": {
                "fragments": [],
                "text": "Wiley series in probability and mathematical statistics : applied probability and statistics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32536207"
                        ],
                        "name": "D. Camp",
                        "slug": "D.-Camp",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Camp",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Camp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713302"
                        ],
                        "name": "M. Revow",
                        "slug": "M.-Revow",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Revow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Revow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2601226"
                        ],
                        "name": "R. Kustra",
                        "slug": "R.-Kustra",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "Kustra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kustra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123772066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c678012d28757921853ef98dbbbe36a1a804e69",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This manual describes the preliminary release of the DELVE environment. Some features described here have not yet implemented, as noted. Support for regression tasks is presently somewhat more developed than that for classiication tasks. We recommend that you exercise caution when using this version of DELVE for real work, as it is possible that bugs remain in the software. We hope that you will send us reports of any problems you encounter, as well as any other comments you may have on the software or manual, at the e-mail address below. Please mention the version number of the manual and/or the software with any comments you send. All Rights Reserved Permission to use, copy, modify, and distribute this software and its documentation for non-commercial purposes only is hereby granted without fee, provided that the above copyright notice appears in all copies and that both the copyright notice and this permission notice appear in supporting documentation, and that the name of The University of Toronto not be used in advertising or publicity pertaining to distribution of the software without speciic, written prior permission. The University of Toronto makes no representations about the suitability of this software for any purpose. It is provided \\as is\" without express or implied warranty. The University of Toronto disclaims all warranties with regard to this software, including all implied warranties of merchantability and tness. In no event shall the University of Toronto be liable for any special, indirect or consequential damages or any damages whatsoever resulting from loss of use, data or proots, whether in an action of contract, negligence or other tortious action, arising out of or in connection with the use or performance of this software. If you publish results obtained using DELVE, please cite this manual, and mention the version number of the software that you used."
            },
            "slug": "The-delve-manual-Rasmussen-Neal",
            "title": {
                "fragments": [],
                "text": "The delve manual"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This manual describes the preliminary release of the DELVE environment, and recommends that you exercise caution when using this version of DELVE for real work, as it is possible that bugs remain in the software."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38718801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b36ed2f1e0e99120726298e80b93cd0319d87284",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Although classifiation is perhaps the oldest practical application of MML inference, the early algorithm was subject to weakly inconsistent estimation. The same problem is inherent in any MML inference which infers many discrete \"nuisance\" parameters. A solution has been found using a novel coding trick, which could be useful in many inductive inferences."
            },
            "slug": "Classification-by-Minimum-Message-Length-Inference-Wallace",
            "title": {
                "fragments": [],
                "text": "Classification by Minimum-Message-Length Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A solution has been found using a novel coding trick, which could be useful in many inductive inferences in any MML inference which infers many discrete \"nuisance\" parameters."
            },
            "venue": {
                "fragments": [],
                "text": "ICCI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 47
                            }
                        ],
                        "text": "These include Markov chain Monte Carlo methods [Pearl 1987; Neal 1992], \\Helmholtz machines\" [Hinton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 47
                            }
                        ],
                        "text": "These include Markov chain Monte Carlo methods[Pearl 1987; Neal 1992], \\Helmholtz machines\" [Hinton et al. 1995; Dayan et al. 1995], andvariational techniques [Saul, Jaakkola and Jordan 1996; Jaakkola, Saul and Jordan 1996;Frey 1997b].1.1 A probabilistic perspectiveO hand, it is not obvious that\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 141
                            }
                        ],
                        "text": "The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods, and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992] as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 176
                            }
                        ],
                        "text": "\u2026of the variables of interest in h.The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods,and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992]as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12334765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5669148e6026516720940558086def2ab92b04e2",
            "isKey": true,
            "numCitedBy": 394,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evidential-Reasoning-Using-Stochastic-Simulation-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Evidential Reasoning Using Stochastic Simulation of Causal Models"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2404386"
                        ],
                        "name": "Lenhart K. Schubert",
                        "slug": "Lenhart-K.-Schubert",
                        "structuredName": {
                            "firstName": "Lenhart",
                            "lastName": "Schubert",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lenhart K. Schubert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "Probabilisticstructure has been most extensively developed in the arti cial intelligence literature, withapplications ranging from taxonomic hierarchies [Woods 1975; Schubert 1976] to medicaldiagnosis [Spiegelhalter 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 155
                            }
                        ],
                        "text": "Probabilistic structure has been most extensively developed in the arti cial intelligence literature, with applications ranging from taxonomic hierarchies [Woods 1975; Schubert 1976] to medical diagnosis [Spiegelhalter 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6508429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a51229e8f8310efd66e6a479172917d49e34a0a1",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extending-The-Expressive-Power-Of-Semantic-Networks-Schubert",
            "title": {
                "fragments": [],
                "text": "Extending The Expressive Power Of Semantic Networks"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121653378,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "07ce26d312a343d81331e34be7d8c73df4ef838e",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A principal curve (Hastie and Stuetzle, 1989) is a smooth curve passing through the \u2018middle\u2019 of a distribution or data cloud, and is a generalization of linear principal components. We give an alternative definition of a principal curve, based on a mixture model. Estimation is carried out through an EM algorithm. Some comparisons are made to the Hastie-Stuetzle definition."
            },
            "slug": "Principal-curves-revisited-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Principal curves revisited"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158255"
                        ],
                        "name": "U. Wachsmann",
                        "slug": "U.-Wachsmann",
                        "structuredName": {
                            "firstName": "Udo",
                            "lastName": "Wachsmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Wachsmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373576"
                        ],
                        "name": "J. Huber",
                        "slug": "J.-Huber",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Huber",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Huber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12608318,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "470b7c92607969c9364c915c995138d144f73178",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently proposed Turbo codes are applied to bandwidth efficient modulation schemes via multilevel coding. For this purpose, Turbo codes are extended for a wide range of fine tunable rates by puncturing. A straightforward derivation of iterative Turbo decoding and the concept of extrinsic information is presented. New design rules for multilevel codes with arbitrary component codes and codeword lengths are derived from information theory. Simulation results show that application of Turbo codes to properly designed multilevel coding schemes leads to digital transmission schemes with high power and bandwidth efficiency."
            },
            "slug": "Power-and-bandwidth-efficient-digital-communication-Wachsmann-Huber",
            "title": {
                "fragments": [],
                "text": "Power and bandwidth efficient digital communication using turbo codes in multilevel codes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A straightforward derivation of iterative Turbo decoding and the concept of extrinsic information is presented and simulation results show that application of Turbo codes to properly designed multilevel coding schemes leads to digital transmission schemes with high power and bandwidth efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "Eur. Trans. Telecommun."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2690359"
                        ],
                        "name": "D. Huffman",
                        "slug": "D.-Huffman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Huffman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huffman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "If the codeword selection distribution is dyadic2, Hu man decoding [Hu -man 1952] can be used to pick codewords."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10606404,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "a068df4c5f829bd0b85bf72fd919006c563c6345",
            "isKey": false,
            "numCitedBy": 4112,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryAn optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized."
            },
            "slug": "A-method-for-the-construction-of-minimum-redundancy-Huffman",
            "title": {
                "fragments": [],
                "text": "A method for the construction of minimum-redundancy codes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IRE"
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9079926"
                        ],
                        "name": "W. Woods",
                        "slug": "W.-Woods",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Woods",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Woods"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 154
                            }
                        ],
                        "text": "Probabilisticstructure has been most extensively developed in the arti cial intelligence literature, withapplications ranging from taxonomic hierarchies [Woods 1975; Schubert 1976] to medicaldiagnosis [Spiegelhalter 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 155
                            }
                        ],
                        "text": "Probabilistic structure has been most extensively developed in the arti cial intelligence literature, with applications ranging from taxonomic hierarchies [Woods 1975; Schubert 1976] to medical diagnosis [Spiegelhalter 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15796055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "534177394b6b3ed4b0a8ba42c0e63d66f43029d9",
            "isKey": false,
            "numCitedBy": 967,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What's-in-a-Link:-Foundations-for-Semantic-Networks-Woods",
            "title": {
                "fragments": [],
                "text": "What's in a Link: Foundations for Semantic Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 208
                            }
                        ],
                        "text": "Although this nonsystematic ITC code does not perform as well as a turbo-code with the same K and N , it does perform signi cantly better than the best rate 1/2 lowdensity parity-check code published to date [MacKay and Neal 1996] with K = 32; 671 and N = 65; 389."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "Although this nonsystematic ITC code does not perform as well as a turbo-codewith the same K and N , it does perform signi cantly better than the best rate 1/2 low-density parity-check code published to date [MacKay and Neal 1996] with K = 32; 671 andN = 65; 389."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 270
                            }
                        ],
                        "text": "\u2026corresponding to a low-density parity-check code consists of a large number of simpleparity-check trellises, where each constraint ensures that one nonsystematic branch variablefrom each of a very small number of trellises are equal (two or three trellises are used in[MacKay and Neal 1996])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 97
                            }
                        ],
                        "text": "Interestingly, the standard iterative decoders for low-densityparity-check codes [Gallager 1963; MacKay and Neal 1996] process the soft decisions foreach parity-check equation by applying the forward-backward algorithm to a parity-checktrellis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 284
                            }
                        ],
                        "text": "The ITC code corresponding to a low-density parity-check code consists of a large number of simple parity-check trellises, where each constraint ensures that one nonsystematic branch variable from each of a very small number of trellises are equal (two or three trellises are used in [MacKay and Neal 1996])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 82
                            }
                        ],
                        "text": "Interestingly, the standard iterative decoders for low-density parity-check codes [Gallager 1963; MacKay and Neal 1996] process the soft decisions for each parity-check equation by applying the forward-backward algorithm to a parity-check trellis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 104
                            }
                        ],
                        "text": "Figure 5.10b shows the Bayesian network for a low-density parity-check code [Gallager1963; Tanner 1981; MacKay and Neal 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 108
                            }
                        ],
                        "text": "However, it appears they do not comeas close to Shannon's limit as do turbo-codes for rates of 1/3 and 1/2 [MacKay and Neal1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 67
                            }
                        ],
                        "text": "10b shows the Bayesian network for a low-density parity-check code [Gallager 1963; Tanner 1981; MacKay and Neal 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "However, it appears they do not come as close to Shannon's limit as do turbo-codes for rates of 1/3 and 1/2 [MacKay and Neal 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122801915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33f275df4188cf8d51f3a85bd95ed2afa64196e4",
            "isKey": true,
            "numCitedBy": 2785,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels. They show that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of turbo codes."
            },
            "slug": "Near-Shannon-limit-performance-of-low-density-check-Mackay-Neal",
            "title": {
                "fragments": [],
                "text": "Near Shannon limit performance of low density parity check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels, showing that performance substantially better than that of standard convolutional and concatenated codes can be achieved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410117753"
                        ],
                        "name": "N. N. Vorob\u2019ev",
                        "slug": "N.-N.-Vorob\u2019ev",
                        "structuredName": {
                            "firstName": "Nikolay",
                            "lastName": "Vorob\u2019ev",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. N. Vorob\u2019ev"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 160
                            }
                        ],
                        "text": "Few new developments were made until the 1960's when statisticians began using graphs to describe restrictions in statistical models called \\log-linear models\" [Vorobev 1962; Goodman 1970]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 159
                            }
                        ],
                        "text": "Few new developments were made until the 1960's when statisticiansbegan using graphs to describe restrictions in statistical models called \\log-linear models\"[Vorobev 1962; Goodman 1970]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118915216,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e2d367cbb64f5d52caf884b056b3b962d17819b7",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Let $\\Sigma $ be a family of Borel fields of subsets of a set S and $\\mu_\\mathfrak{S} $ probabilistic measures on measurable spaces $\\langle {\\mathfrak{S},S} \\rangle $, where $\\mathfrak{S} \\in \\Sigma $. The family of measures $\\mu_\\mathfrak{S} $, $\\mathfrak{S} \\in \\Sigma $ is denoted by $\\mu_\\Sigma $.The measures $\\mu_{\\mathfrak{S}_1 } $ and $\\mu_{\\mathfrak{S}_2 } $ are said to be consistent if $\\mu_{\\mathfrak{S}_1 } (A) = \\mu_{\\mathfrak{S}_2 } (A)$ for any $A \\in \\mathfrak{S}_1 \\cap \\mathfrak{S}_2 $. If any pair of measures of the family $\\mu_\\Sigma $ is consistent, the family itself is referred to as consistent.The consistent family $\\mu_\\Sigma $ is said to be extendable if there is a measure $\\mu_{[\\Sigma ]} $ on the measurable space $\\langle {[\\Sigma ],S} \\rangle $ consistent with each measure of $\\mu_\\Sigma $ ($[\\Sigma ]$ is the smallest Borel field containing all $\\mathfrak{S} \\in \\Sigma $).For the purposes of the theory of games the following special case of extendability is important. Let ${\\bf \\m..."
            },
            "slug": "Consistent-Families-of-Measures-and-Their-Vorob\u2019ev",
            "title": {
                "fragments": [],
                "text": "Consistent Families of Measures and Their Extensions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 59
                            }
                        ],
                        "text": "For example, the latent variables in a hidden Markov model [Rabiner 1989] with a xed state space size can be integrated out in a way so that the encoding complexity is linear in the number of latent variables (number of time steps)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "For example, the latent variables in a hidden Markov model [Rabiner1989] with a xed state space size can be integrated out in a way so that the encodingcomplexity is linear in the number of latent variables (number of time steps).4.2 Communicating extra information through the choice ofcodewordIn\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": true,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Spiegelhalter, D. J. (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 131
                            }
                        ],
                        "text": "Although there are procedures for attempting to convert an original networkto one that is appropriate for probability propagation [Spiegelhalter 1986; Lauritzen andSpiegelhalter 1988], these procedures are not practically fruitful when the number of multiplepaths is large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 202
                            }
                        ],
                        "text": "Probabilisticstructure has been most extensively developed in the arti cial intelligence literature, withapplications ranging from taxonomic hierarchies [Woods 1975; Schubert 1976] to medicaldiagnosis [Spiegelhalter 1990]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 131
                            }
                        ],
                        "text": "Although there are procedures for attempting to convert an original network to one that is appropriate for probability propagation [Spiegelhalter 1986; Lauritzen and Spiegelhalter 1988], these procedures are not practically fruitful when the number of multiple paths is large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 98
                            }
                        ],
                        "text": "1 Exact inference in singly-connected Bayesian networksIn the late 1980's, Pearl [1986; 1988] and Lauritzen and Spiegelhalter [Spiegelhalter 1986;Lauritzen and Spiegelhalter 1988] independently published the exact probability propaga-tion algorithm for inferring the distributions over individual variables in singly-connectedBayesian networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Spiegelhalter, D. J. and Lauritzen, S. L. (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 21
                            }
                        ],
                        "text": "Lauritzen, S. L. and Spiegelhalter, D. J. (1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Spiegelhalter, D. J. (1986)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 127
                            }
                        ],
                        "text": "1 Exact inference in singly-connected Bayesian networks In the late 1980's, Pearl [1986; 1988] and Lauritzen and Spiegelhalter [Spiegelhalter 1986; Lauritzen and Spiegelhalter 1988] independently published the exact probability propagation algorithm for inferring the distributions over individual variables in singly-connected Bayesian networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 127
                            }
                        ],
                        "text": "1 Exact inference in singly-connected Bayesian networksIn the late 1980's, Pearl [1986; 1988] and Lauritzen and Spiegelhalter [Spiegelhalter 1986;Lauritzen and Spiegelhalter 1988] independently published the exact probability propaga-tion algorithm for inferring the distributions over individual\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41744030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e9c168f6d744174efad3764e03522fe55be5ada",
            "isKey": true,
            "numCitedBy": 149,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Reasoning-in-Predictive-Expert-Spiegelhalter",
            "title": {
                "fragments": [],
                "text": "Probabilistic Reasoning in Predictive Expert Systems"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 124
                            }
                        ],
                        "text": "There are a variety of practical algorithms for obtaining such an approximation, including Markov chain Monte Carlo methods [Geman and Geman 1984; Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld methods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan 1996], and inverse model methods [Hinton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 35
                            }
                        ],
                        "text": "For example, the Boltzmannmachine [Hinton and Sejnowski 1986] (a Markov random eld that learns) is poorly suitedto data compression, because it does not decompose P (v) in a way that is suitable fore cient piece-wise compression."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 35
                            }
                        ],
                        "text": "For example, the Boltzmann machine [Hinton and Sejnowski 1986] (a Markov random eld that learns) is poorly suited to data compression, because it does not decompose P (v) in a way that is suitable for e cient piece-wise compression."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 211
                            }
                        ],
                        "text": "The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods, and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992] as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 267
                            }
                        ],
                        "text": "\u2026of the variables of interest in h.The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods,and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992]as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 145
                            }
                        ],
                        "text": "There are a variety of practical algorithms for obtaining such anapproximation, including Markov chain Monte Carlo methods [Geman and Geman 1984;Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld meth-ods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49715681"
                        ],
                        "name": "P. Games",
                        "slug": "P.-Games",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Games",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Games"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 145546766,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "6fbe9b95e35bed4294b8089056c3c2548c454745",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractPage and various collaborators (Keith, Page, & Robertson, 1984; Page, 1981; Page & Keith, 1981, 1982) have defended the use of data from investigations (correlational studies) to make causative conclusions. These methods are not defensible logically or statistically. They can only suggest hypotheses that then should be tested by proper experiments. At worst, as in the Coleman studies, they have been used to make social policy based on unjustified conclusions. The logical flaw of the methods of path analysis or structural equation modeling is shown. A proper evaluation of the role of investigations versus experiments is cited in the work of Cochran (as described by Rubin [1983])."
            },
            "slug": "Correlation-and-Causation:-A-Logical-Snafu-Games",
            "title": {
                "fragments": [],
                "text": "Correlation and Causation: A Logical Snafu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32922277"
                        ],
                        "name": "N. Metropolis",
                        "slug": "N.-Metropolis",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Metropolis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Metropolis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91743329"
                        ],
                        "name": "A. W. Rosenbluth",
                        "slug": "A.-W.-Rosenbluth",
                        "structuredName": {
                            "firstName": "Arianna",
                            "lastName": "Rosenbluth",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. W. Rosenbluth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991661"
                        ],
                        "name": "M. Rosenbluth",
                        "slug": "M.-Rosenbluth",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Rosenbluth",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rosenbluth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46516796"
                        ],
                        "name": "A. H. Teller",
                        "slug": "A.-H.-Teller",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Teller",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. H. Teller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3840350"
                        ],
                        "name": "E. Teller",
                        "slug": "E.-Teller",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Teller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Teller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1046577,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f6a13f116e270dde9d67848495f801cdb8efa25d",
            "isKey": false,
            "numCitedBy": 32417,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two\u2010dimensional rigid\u2010sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four\u2010term virial coefficient expansion."
            },
            "slug": "Equation-of-state-calculations-by-fast-computing-Metropolis-Rosenbluth",
            "title": {
                "fragments": [],
                "text": "Equation of state calculations by fast computing machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "Itturns out that an AWGN channel simply adds independent Gaussian noise to each input\n118 Channel codingvalue ai, where the variance of the noise is related to N0 by 2 = N0=2:p(yja) = N 1Yi=0 p(yija) = N 1Yi=0 p(yijai)p(yijai) = 1p2 2e (yi ai)2=2 2 ; i = 0; 1; : : : ; N 1: (5.3)If the decoder applies a low-pass lter with a higher bandwidth, then frequency componentsof the AWGN that are aboveW Hz will increase the e ective noise on the sequence fyigN 1i=0 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 80
                            }
                        ],
                        "text": "In these cases the source model maps each symbol to two codewords| one for each Gaussian | producing a multi-valued source code."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 160
                            }
                        ],
                        "text": "We rst constructa table that maps each information vector u to a real value cu in a way so that a uniformdistribution over information vectors induces a nearly Gaussian distribution over cu."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 184
                            }
                        ],
                        "text": "A simple example will help illustrate this procedure.4.2.1 Example: A simple mixture modelConsider a source that outputs real numbers that are distributed according to a mixtureof two Gaussians."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 175
                            }
                        ],
                        "text": "Stochastic linear mode: For a given mean, if the squashing function is approximatelylinear over the span of the added noise, the postsigmoid distribution will be approximatelyGaussian with the mean and standard deviation linearly transformed (see Figure 3.15c)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 185
                            }
                        ],
                        "text": "A discrete set of values (called a constellation) is then judiciously chosenwithin each n-dimensional subspace in a way that leads to marginal signalling distributionsthat are close to Gaussian."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 32
                            }
                        ],
                        "text": "Since p(yi) is a mixture of two Gaussians, the second term isZyihe (yi 1)2=2 22p2 2 + e (yi+1)2=2 22p2 2 i log2he (yi 1)2=2 22p2 2 + e (yi+1)2=2 22p2 2 idyi; (5.14)which can be approximated quite well using a Monte Carlo method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 115
                            }
                        ],
                        "text": "Since the all-zero# codeword is always sent, a decoder network is built, plus a noise# vector network (independent Gaussian units).# Set up the constantsset LOGFILE berrou0.6.logset SNR 0.6set K 65536set NBLOCKS 530set NITERS 18set NUM {1 0 0 0 1}set DEN {1 1 1 1 1}set K2 [expr 2*$K]; set Km1 [expr $K-1]set RATE [expr 1.0*$K/$K2]; set VAR [expr pow(10.0,-($SNR/10.0))/2.0/$RATE]# Build the recursive convolutional encoder link.source lfsr.tclset S [buildLFSR $NUM $DEN u->s us->s s->x]if { [expr $S == -1] } { puts \"Error: Could not build encoder link.\""
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 87
                            }
                        ],
                        "text": "For example, try mapping 1bit of information to a variable whose distribution close to Gaussian!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 224
                            }
                        ],
                        "text": "In the above example, we decide that a sinusoidal manifoldis a reasonable compromise between our prior expectations regarding continuity and theobserved data within class B.Simple parametric models, such as multidimensional Gaussian density functions, canbe used to obtain some degree of generalization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 82
                            }
                        ],
                        "text": "(a)shows a source with a single binary hidden variable which identi es from which Gaussian, G1 or G2,the symbol value v is sampled."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 14
                            }
                        ],
                        "text": "Evaluation of Gaussian Processes and OtherMethods for Non-Linear Regression."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "(See [Pearl 1988] for an extensive discussion of dependency-separation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 227
                            }
                        ],
                        "text": "Stochastic nonlinear mode: If the variance of a variable in the stochastic linear modeis increased so that the squashing function is used in its nonlinear region, a variety ofdistributions are producible that range from skewed Gaussian to uniform to bimodal (seeFigure 3.15d)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 148
                            }
                        ],
                        "text": "In the prototypicalproblem, the transmitter sends a discrete-time binary sequence of +1's and 1's, and eachof these values is corrupted by additive Gaussian noise."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "The value of zi(its postsigmoid activity) is obtained by passing the presigmoid activity through a sigmoidalcumulative Gaussian squashing function:zi (xi) Z x 1 1p2 e 2=2d : (3.59)Networks of these variables can represent a broad range of structures, including deter-ministic multilayer perceptrons [Bishop 1995], binary sigmoidal Bayesian networks [Neal1992] (see Section 3.4), mixture models, mixture of expert models [Jacobs et al. 1991],hierarchical mixture of expert models [Jordan and Jacobs 1994], and factor analysis models[Everitt 1984]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 114
                            }
                        ],
                        "text": "Even if a good code is used with these signalling points, the marginal signallingdistributions are quite far from Gaussian and so the rate will be below capacity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 6
                            }
                        ],
                        "text": "(See [Pearl 1988] for an extensive discussion of dependency-separation.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 103
                            }
                        ],
                        "text": "Examples of graphi-cal models include Markov random elds [Kinderman and Snell 1980], Bayesian networks[Pearl 1988], and chain graphs [Lauritzen and Wermuth 1989]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "This prior distribution has two sources of variability: aGaussian prior over the parameters i, and a uniform distribution over the inputs fvkgi 1k=1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 89
                            }
                        ],
                        "text": "A signalling technique has good shape if the marginal signalling distributionsare nearly Gaussian."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 24
                            }
                        ],
                        "text": "1175.1.1 Additive white Gaussian noise (AWGN) . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 191
                            }
                        ],
                        "text": "Work done on continuous-valued Bayesian networkshas focussed mainly on Gaussian random variables that are linked linearly such that thejoint distribution over all variables is also Gaussian [Pearl 1988; Heckerman and Geiger1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 192
                            }
                        ],
                        "text": "Work done on continuous-valued Bayesian networks has focussed mainly on Gaussian random variables that are linked linearly such that the joint distribution over all variables is also Gaussian [Pearl 1988; Heckerman and Geiger 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 81
                            }
                        ],
                        "text": "Using this method, each marginal distribution p(ai) can be made to be asclose to Gaussian as desired, by increasing N and re ning the map from u to cu."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 35
                            }
                        ],
                        "text": "Since the receiver also knowswhich Gaussian was selected, the receiver can recover the queued auxiliary data bit that wasused to make the choice."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 20
                            }
                        ],
                        "text": "The optimality of a Gaussian signalling distribution leads to a new type of coding conceptcalled shaping."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 56
                            }
                        ],
                        "text": "Values of v near v0 are likely to have come from either Gaussian."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 29
                            }
                        ],
                        "text": "However, the identity of the Gaussian that produceda given symbol is often ambiguous."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 64
                            }
                        ],
                        "text": "Over- tting was prevented by using MAP estimation with a scaled Gaussian parameter prior."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 102
                            }
                        ],
                        "text": "Examples of graphical models include Markov random elds [Kinderman and Snell 1980], Bayesian networks [Pearl 1988], and chain graphs [Lauritzen and Wermuth 1989]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 71
                            }
                        ],
                        "text": "It may be a good idea to let the biases in the network have a separate Gaussian prior,although I have not yet explored this possibility experimentally.3.2.4 Ensembles of autoregressive networksAn autoregressive network is speci ed by chosing an order for the variables v1; : : : ; vN ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 99
                            }
                        ],
                        "text": "The most natural source code to use in this case is one that requires one bit to specifyfrom which Gaussian a given symbol was produced plus however many bits are needed tocode the symbol using that Gaussian."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 260
                            }
                        ],
                        "text": "Suppose the\n2.3 Variational inference 41(a) q(zj ) p(z) z (b) q(zj ) p(z) zFigure 2.7: The e ect of using (a) Dqkp versus (b) Dpkq when tting a variational distributionq(zj ) that is unimodel to a bimodal distribution p(z).variational distribution q(zj ) is a Gaussian with consisting of a mean and a variance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 81
                            }
                        ],
                        "text": "Lauritzen et al. [1990] have included discrete random variables within the linearGaussian framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 503,
                                "start": 495
                            }
                        ],
                        "text": "(z 6= z0jy)]L\u0302i( ) approximation to L( ) produced at iteration i of iterative decoding`( ) `(v) is the length of the source codeword for v`( ; ) `(v;h) is the length of the hth codeword for v, in a multi-valued source codelog( ) natural logarithmlogx( ) logarithm to the base xN number of variables in a network; or number of visible variables; or number ofcodeword variablesNt number of constituent trellises in an interleaved trellis-constraint codeN0 single-sided spectral density for a white Gaussian processnk set of variables containing the nondescendents of zk (excluding zk)P transmitter power for channel codingP set of all distributions that can be represented by a Bayesian networkP ( ) probability mass function for a Bayesian network; or the probabilityfunction for a mixed (discrete and continuous) set of variablesP\u0302 ( ) estimate of P ( )PAiZi conditional probability matrix for variable zi, PAiZiaizi = P (zijai)\nvip( ) probability density function for a Bayesian networkQ( ) general variational distribution; or general recognition network distributionq vector of parity-check variablesq( ) variational probability density; or recognition network probability densityR rate of a binary channel code in information bits / codeword bitsS( ; ) state transition function for a LFSR: sk = S(sk 1; uk)s set of discrete LFSR state variables, where si is the state at time it training case index, 1 t Tu vector of binary information variablesu\u0302 an estimate of the true information vector uv vector of visible (observed) variablesv(t) vector of visible (observed) variables for training case tx vector of binary codeword variables; or dummy vector variablexi;j the jth branch variable that participates in constraint i of an interleavedtrellis-constraint codey vector of real channel output variables; or dummy vector variabley( ) channel output waveform for channel codingyo observed value of a variable y 2 v in a Bayesian networkz set of variables used to discuss properties of Bayesian networks in general normalization constant ( ; ) delta function: (x; y) = 1 if x = y and 0 otherwise learning rate for steepest descent parameter estimation set of all parameters for a parameterized Bayesian network i set of parameters associated with variable i ij parameter associated with the connection from variable j to variable i i0 constant (bias) parameter associated with variable i H set of parameters associated with the set of hidden variables h V set of parameters associated with the set of visible variables v Z1Z2 child-parent message sent from z1 to z2 | has jz2j elements variational parameters ( ) permutation function that maps integers in [1; N ] to [1; N ], for some integer N Z1Z2 parent-child message sent from z1 to z2 | has jz1j elements time in a Markov chain Monte Carlo simulation ( ) cummulative distribution for a standard normal p.d.f. recognition network parameters (see i and ij for re ned details)\nviij j jzj = number of variables in z; or jzkj = number of values zk can take onPx summation over all possible con gurations of xPx0 :x0i=xi summation over the con gurations of x0 for which element x0i = xif: : :g set or vector of variables, fzkg means the set containing zk alone addition modulo 2\nviii AcknowledgementsDuring my Ph.D. program here at the University of Toronto, I have been fortunateto bene t from interactions with several excellent researchers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "This density wastes coding space because it is wrongly shaped and has an area signi cantly less thanunity.from either Gaussian."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 323,
                                "start": 315
                            }
                        ],
                        "text": "For an AWGN channel with +1= 1 signalling, the mutual information as a function of thenoise variance isM( 2) = Xai2f 1;+1gZyi p(ai; yi) log2 p(ai; yi)p(ai)p(yi)dyi= Xai2f 1;+1gZyi p(ai; yi) log2 p(yijai)daidyi Zyi p(yi) log2 p(yi)dyi (5.13)The rst term is the entropy of yi given ai, which is just the entropy of a Gaussian distri-bution, 0:5 log2(2 2e)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 243
                            }
                        ],
                        "text": "The package is tailored to coding applications, but can be usedto propagate probabilities in any Bayesian network where the real-valued variables areobserved and where the conditional probabilities for the real-valued variables are mixturesof Gaussians."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 14
                            }
                        ],
                        "text": "Additivewhite Gaussian noise with single-sided spectral density N0 is added to the signalling wave-form to obtain the channel output waveform."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 15
                            }
                        ],
                        "text": "In contrast, a Gaussian mixture model would require many morecomponents for the second task as compared to the rst."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 441,
                                "start": 433
                            }
                        ],
                        "text": "By substituting (4.9) into (4.8), we nd that theoptimal bits-back coding rate isF = Xv Pr(v) log2hXh 2 `(v;h)i: (4.10)This rate is the same as the rate for a single-valued source code that has codeword lengthswhich properly re ect the total codeword space associated with each symbol in the multi-valued source code.1The codewords may have fractional lengths produced, say, by arithmetic coding\n102 Data CompressionIn the mixture of Gaussians example, where for symbol v0 we had `1 = 3 bits and `2 = 4bits, Q (G1jv0) = 2 3=(2 3 + 2 4) = 2=3"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 53
                            }
                        ],
                        "text": "90 Pattern Classi cation(a) Variableziai ixiZero-meanGaussiannoise withvariance 2izi (b) 4 x 4 (x) p(x) 0 z 1p(z) (c) 4 x 4 (x) p(x) 0 z 1p(z)(d) 4 x 4 (x)p(x) 0 z 1p(z) (e) 400 x 400p(x) (x) 0 z 1p(z)Figure 3.15: (a) schematically shows the dependence of the proposed variable on its parents."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 51
                            }
                        ],
                        "text": "Including the single bit required to specify which Gaussian is being used, anoptimal source code (where the Gaussian identity is explicit) will thus have codewords withlengths `1 = 3 bits and `2 = 4 bits."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 141
                            }
                        ],
                        "text": "The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods, and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992] as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 188
                            }
                        ],
                        "text": "\u2026of the variables of interest in h.The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods,and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992]as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 124
                            }
                        ],
                        "text": "For reasonable power levels, it isnot possible to deterministically map C bits of information to a value ai that will have aGaussian distribution (or one that is even close to Gaussian)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 47
                            }
                        ],
                        "text": "I used binarysignalling over an additive white Gaussian noise (AWGN) channel with Eb=N0 = 0:2 dB."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 239
                            }
                        ],
                        "text": "In this section, I simplify the codingproblem in the ways described above, while attempting to argue that if done properly, thesimpli cation will lead to a communication rate that is practically very close to capacity.5.1.1 Additive white Gaussian noise (AWGN)A channel model that is simple and works well in practice is the AWGN channel."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 132
                            }
                        ],
                        "text": "That is, we assign a new codeword toeach symbol based on its total probability mass, obtained by summing the contributionsfrom each Gaussian."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 142
                            }
                        ],
                        "text": "One way to pose\n124 Channel coding(a) -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\n1/32 1/16 1/8 1/51/4 1/3 1/2 1\n+1/-1 signalling Gaussian signalling Rate R (bits/channel usage)Minimum E b=N 0(dB) (b) 1e-5 1e-4 1e-3 1e-2 1e-1 -2 -1.5 -1 -0.5 0 0.5 1Eb=N0 (dB)Minim umBER R = 1R = 1=2R = 1=3R = 1=4R = 1=5R = 1=8Figure 5.2: (a) The minimumEb=N0 needed for error-free communication with a rate R code, overan AWGN channel using +1= 1 signalling and optimal (Gaussian) signalling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 208
                            }
                        ],
                        "text": "Afterrearrangement, we have Eb=N0 > 12RM 1(R) : (5.15)This bound (based on an interpolated inverse of a Monte Carlo estimate ofM( 2)) is shownin Figure 5.2a, along with the minimum Eb=N0 required by optimal (Gaussian) signalling(see Section 5.1.2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 60
                            }
                        ],
                        "text": "When decoding, the receiver reads o the bit that says which Gaussian was used andthen determines the rounded value v0 from the codeword."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 580,
                                "start": 572
                            }
                        ],
                        "text": "; exit }# Create information bit variables, state variables, codeword bit variables,# and received signal variables for constituent codes 1 and 2.crVars d du 2 $K; crVars d ds1 $S $K; crVars d ds2 $S $K; crVars d dx 2 $K2crVars r dy $K2# Create the noise vector variables.crVars r ns $K2# Create a 50/50 prior link for the info bitscrLink u d 2; addtoLink u 0.5 0; addtoLink u 0.5 1\nB.3 Scripts used to decode a turbo-code 183# Create a link used for the systematic codeword bits.crLink u->x d 2 2; addtoLink u->x 1.0 0 0; addtoLink u->x 1.0 1 1# Create the channel link (Gaussian distribution).crLink x->y r 2addtoLink x->y 1.0 -1.0 $VAR 0; addtoLink x->y 1.0 1.0 $VAR 1# Create the noise vector link (Gaussian distribution).crLink ns r; addtoLink ns 1.0 -1.0 $VAR# Build the interleaver.set P [permute $K]# Connect up the noise links to the noise variables.loop i 0 $K2 { linkVars ns ns-$i }# Connect up the variables for the decoder network."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 56
                            }
                        ],
                        "text": "It turns out that the maximum in (5.6) is obtained by a Gaussian signalling distribution\n5.1 Simplifying the playing eld 119with variance P (see [Cover and Thomas 1991]), and the capacity isC = 12 log2 1 + P 2 : (5.7)For example, if P = 3 2, then C = 1 bit/channel usage."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 49
                            }
                        ],
                        "text": "Assuming that the channel simply addsindependent Gaussian noise with variance 2 to the +1/-1 signals described above, theconditional distributions for the received channel output signals arep(ykjxk) = 8 : 1p2 2 e (yk 1:0)2=2 2 if xk = 11p2 2 e (yk+1:0)2=2 2 if xk = 0: (1.22)Given an information vector u, encoding and channel transmission can be simulated by onesweep of ancestral simulation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 107
                            }
                        ],
                        "text": "3.7 Simultaneous extraction of continuous and categorical structure 91This mode is useful for representing Gaussian noise e ects such as those found in mixturemodels, the outputs of mixture of expert models, and factor analysis models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 57437891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bf6f01402e1648b7d1e6c9200ede6cb1af30123",
            "isKey": true,
            "numCitedBy": 4579,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30752333"
                        ],
                        "name": "J. Omura",
                        "slug": "J.-Omura",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Omura",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Omura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 129
                            }
                        ],
                        "text": "Thisrecursive systematic convolutional code was designed to maximize the minimum Hammingdistance between all pairs of codewords [Viterbi and Omura 1979; Lin and Costello 1983](dmin = 10)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 130
                            }
                        ],
                        "text": "This recursive systematic convolutional code was designed to maximize the minimum Hamming distance between all pairs of codewords [Viterbi and Omura 1979; Lin and Costello 1983] (dmin = 10)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195895025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "893caf4196d0fce0c287e7c8099beda28abf0ada",
            "isKey": false,
            "numCitedBy": 943,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Principles of digital communication and coding , Principles of digital communication and coding , \u0645\u0631\u06a9\u0632 \u0641\u0646\u0627\u0648\u0631\u06cc \u0627\u0637\u0644\u0627\u0639\u0627\u062a \u0648 \u0627\u0637\u0644\u0627\u0639 \u0631\u0633\u0627\u0646\u06cc \u06a9\u0634\u0627\u0648\u0631\u0632\u06cc"
            },
            "slug": "Principles-of-Digital-Communication-and-Coding-Viterbi-Omura",
            "title": {
                "fragments": [],
                "text": "Principles of Digital Communication and Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The principles of digital communication and coding are presented and a practical application of these principles, called \"Principles of Digital Communication and coding\", are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 113
                            }
                        ],
                        "text": "Relative to Rissanen's work, bits-back coding provides a tractable way to approximate the stochastic complexity [Rissanen1989] and furthermore communicate at this rate.4.3 Relationship to maximum likelihood estimation and itsapproximationsThe whole idea of a multi-valued source code may seem absurd."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 111
                            }
                        ],
                        "text": "Relative to Rissanen's work, bitsback coding provides a tractable way to approximate the stochastic complexity [Rissanen 1989] and furthermore communicate at this rate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 93
                            }
                        ],
                        "text": "In fact, the optimal bits-back coding rate is equivalent to Rissanen's stochastic complexity [Rissanen 1989] if we interpret the choice of codeword as a model parameter."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 93
                            }
                        ],
                        "text": "In fact,the optimal bits-back coding rate is equivalent to Rissanen's stochastic complexity [Rissanen1989] if we interpret the choice of codeword as a model parameter."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9365056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72247e4e34de0dd2d0428522ded24b49fb1632be",
            "isKey": true,
            "numCitedBy": 1772,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Complexity-in-Statistical-Inquiry-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity in Statistical Inquiry"
            },
            "venue": {
                "fragments": [],
                "text": "World Scientific Series in Computer Science"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Cooper [1990] has shown that probabilistic inference in Bayesian networks is in general NP-hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 822,
                                "start": 22
                            }
                        ],
                        "text": "gorithms presented in Chapter 2 to perform decoding. For this reason. I begin this chapter by distilling out the essence of the channel coding problem and presenting a simple prototypical problem that will be t he focus for the remainder of the chapber. In the prototypical problem. the transmitter sends a discrete-time b i n q sequence of -i 1's and - 1's. and each of these values is corrupted by additive Gaussian noise. So, the encoder maps each information sequence to a binary signailing sequence, and given a received noisy sequence. the decoder rnakes a mess at the binary information sequence. It turns out that the solution to t his pro blem has far-reaching consequences in mu1 ti-level (nonbinary ) coding [Imai and Hirakawa 19771. rnainly due to recent proofs by Wachsmann and Huber [1995] and Forney [1997]. In Section 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Berrou and Glaview [1996] have argued that a nice compromise between these codes is a systematic recursive convolutional code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 804,
                                "start": 22
                            }
                        ],
                        "text": "gorithms presented in Chapter 2 to perform decoding. For this reason. I begin this chapter by distilling out the essence of the channel coding problem and presenting a simple prototypical problem that will be t he focus for the remainder of the chapber. In the prototypical problem. the transmitter sends a discrete-time b i n q sequence of -i 1's and - 1's. and each of these values is corrupted by additive Gaussian noise. So, the encoder maps each information sequence to a binary signailing sequence, and given a received noisy sequence. the decoder rnakes a mess at the binary information sequence. It turns out that the solution to t his pro blem has far-reaching consequences in mu1 ti-level (nonbinary ) coding [Imai and Hirakawa 19771. rnainly due to recent proofs by Wachsmann and Huber [1995] and Forney [1997]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 586,
                                "start": 0
                            }
                        ],
                        "text": "Cooper [1990] has shown that probabilistic inference in Bayesian networks is in general NP-hard. Summations relevant to inference. such as the ones in (1.3). (1.7). and (1.9). contain an exponential number of terms and it appears that in generd these siimmations cannot be simplified. Researchers have thus focused on developing exact inference algorithms for restricted classes of networks (e -g . . probability propagation for singly-comected networks) , and on developing approximate inference algorit hms for networks t hat are intractable (assuming P # NP) . In fact. Dagum [1993] (see &O [Dagurn and Chavez 19931) has shown that for general Bayesian networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mat hematical t heory of communication"
            },
            "venue": {
                "fragments": [],
                "text": "Bell S p t e m Technlcal Journal"
            },
            "year": 1948
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 121
                            }
                        ],
                        "text": "The variational bound was increased at each generalized E-step using the following itera-tive method [Saul, Jaakkola and Jordan 1996]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "\u2026dx = Z i 1 exp[ x2=2 2i ]p2 2i dx = i i : (3.60)This sort of stochastic activation is found in binary sigmoidal belief networks [Jaakkola,Saul and Jordan 1996] and in the decision-making components of mixture of expert modelsand hierarchical mixture of expert models.3.7.2 Inference using slice\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 138
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower thana certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "\u2026Carlo methods[Pearl 1987; Neal 1992], \\Helmholtz machines\" [Hinton et al. 1995; Dayan et al. 1995], andvariational techniques [Saul, Jaakkola and Jordan 1996; Jaakkola, Saul and Jordan 1996;Frey 1997b].1.1 A probabilistic perspectiveO hand, it is not obvious that sophisticated probability\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 164
                            }
                        ],
                        "text": "Alternatively, we may express each conditional distribution P (zk jak) in terms of condi-tional distributions over an extended set of variables [Jaakkola, Saul and Jordan 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 85
                            }
                        ],
                        "text": "In the Bayesian network literature, variational inference methods [Saul,Jaakkola and Jordan 1996; Ghahramani and Jordan 1996; Jaakkola, Saul and Jordan 1996]were introduced as an alternative variation on the central theme of Helmholtz machines[Hinton et al. 1995; Dayan et al. 1995], which are\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "\u20261992], \\Helmholtz machines\" [Hinton et al. 1995; Dayan et al. 1995], andvariational techniques [Saul, Jaakkola and Jordan 1996; Jaakkola, Saul and Jordan 1996;Frey 1997b].1.1 A probabilistic perspectiveO hand, it is not obvious that sophisticated probability models are needed to solve problemsin\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 201
                            }
                        ],
                        "text": "\u2026Carlo methods [Geman and Geman 1984;Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld meth-ods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan1996], and inverse model methods [Hinton et al. 1995; Dayan et al. 1995] (see Section 4.5)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 187
                            }
                        ],
                        "text": "In these cases, it may be possible to derive an upper boundon the distance that does not depend on h, and then try to minimize the bound instead of thedistance itself [Saul, Jaakkola and Jordan 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "A common parametric Bayesian network is the sigmoidal Bayesian network [Neal 1992;Jordan 1995; Saul, Jaakkola and Jordan 1996], whose random variables are all binary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 25
                            }
                        ],
                        "text": "(See [Saul, Jaakkola and Jordan 1996] fordetails.)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mean field theory for sigrnoid belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Artificial Intelligence Research"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 93
                            }
                        ],
                        "text": "A standard statistical method for predicting a binary-valued variable is logistic regression [McCullagh and Nelder 1983], in which the conditional probability for zk given ak is P (zkjak ; k) = 8<:1=(1 + exp[ k0 P8j:zj2ak kjzj ]) if zk = 1; 1 1=(1 + exp[ k0 P8j:zj2ak kjzj ]) if zk = 0; (1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 116
                            }
                        ],
                        "text": "1 The logistic autoregressive network If the pattern consists of binary variables (vi 2 f0; 1g) logistic regression [McCullagh and Nelder 1983] (see Section 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 93
                            }
                        ],
                        "text": "A standard statistical method for predictinga binary-valued variable is logistic regression [McCullagh and Nelder 1983], in which theconditional probability for zk given ak isP (zkjak ; k) = 8<:1=(1 + exp[ k0 P8j:zj2ak kjzj ]) if zk = 1;1 1=(1 + exp[ k0 P8j:zj2ak kjzj ]) if zk = 0; (1.23)where the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117796698,
            "fieldsOfStudy": [],
            "id": "ea54f0c0568cab38619ee51e6ac021e76d6a6559",
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MacKay, D. J. C., McEliece, R. J., and Cheng, J. F. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "Bibliography 193MacKay, D. J. C. and Neal, R. M. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 63
                            }
                        ],
                        "text": "I greatly appreciaterecent energetic collaborations with David MacKay, who I nd is an engaging collaborator."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 279
                            }
                        ],
                        "text": "It turns out that the turbo-decoding algorithm for these turbo-codes [Berrouand Glavieux 1996] is just the probability propagation algorithm discussed in Section 2.1applied to a code network like the one shown in Figure 1.5b [Frey and Kschischang 1996;Kschischang and Frey 1997; MacKay, McEliece and Cheng 1997]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MacKay, D. J. C. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MacKay, D. J. C. (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 257
                            }
                        ],
                        "text": "However, it turns out thatthe turbo-code can be concisely described as a multiply-connected Bayesian network, andthat the turbo-decoding algorithm is just probability propagation in this network [Freyand Kschischang 1996; Kschischang and Frey 1997; MacKay, McEliece and Cheng 1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MacKay, D. J. C. and Neal, R. M. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 275
                            }
                        ],
                        "text": "\u2026that the turbo-decoding algorithm for these turbo-codes [Berrouand Glavieux 1996] is just the probability propagation algorithm discussed in Section 2.1applied to a code network like the one shown in Figure 1.5b [Frey and Kschischang 1996;Kschischang and Frey 1997; MacKay, McEliece and Cheng 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Turbo-decoding as an instance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 220
                            }
                        ],
                        "text": "Although this nonsystematic ITC code does not perform as well as a turbo-codewith the same K and N , it does perform signi cantly better than the best rate 1/2 low-density parity-check code published to date [MacKay and Neal 1996] with K = 32; 671 andN = 65; 389."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 281
                            }
                        ],
                        "text": "\u2026corresponding to a low-density parity-check code consists of a large number of simpleparity-check trellises, where each constraint ensures that one nonsystematic branch variablefrom each of a very small number of trellises are equal (two or three trellises are used in[MacKay and Neal 1996])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 108
                            }
                        ],
                        "text": "Interestingly, the standard iterative decoders for low-densityparity-check codes [Gallager 1963; MacKay and Neal 1996] process the soft decisions foreach parity-check equation by applying the forward-backward algorithm to a parity-checktrellis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 115
                            }
                        ],
                        "text": "Figure 5.10b shows the Bayesian network for a low-density parity-check code [Gallager1963; Tanner 1981; MacKay and Neal 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "However, it appears they do not comeas close to Shannon's limit as do turbo-codes for rates of 1/3 and 1/2 [MacKay and Neal1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 235
                            }
                        ],
                        "text": "\u2026not depend on ) to obtain a probabilityP (vjD) = Z P (vj )P ( jD)d : (3.51)For example, this integral can be approximated using Laplace's approximation [Spiegelhal-ter and Lauritzen 1990], Markov chain Monte Carlo methods [Neal 1993; Neal 1996] orvariational techniques [Jaakkola and Jordan 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesion Learning for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Springer-Verlag, New York NY."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 235
                            }
                        ],
                        "text": "There are a variety of practical algorithms for obtaining such an approximation, including Markov chain Monte Carlo methods [Geman and Geman 1984; Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld methods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan 1996], and inverse model methods [Hinton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 68
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower thana certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 18
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton 1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan 1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower than a certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 170
                            }
                        ],
                        "text": "\u2026Carlo methods [Geman and Geman 1984;Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld meth-ods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan1996], and inverse model methods [Hinton et al. 1995; Dayan et al. 1995] (see Section 4.5)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The mean eld theory in EM procedures for blind Markov random eld"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92127933"
                        ],
                        "name": "Igor I. Sheykhet",
                        "slug": "Igor-I.-Sheykhet",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Sheykhet",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Igor I. Sheykhet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50098763"
                        ],
                        "name": "B. Y. Simkin",
                        "slug": "B.-Y.-Simkin",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Simkin",
                            "middleNames": [
                                "Ya."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Y. Simkin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 188
                            }
                        ],
                        "text": "In order to perform classi cation, we would like to compute P (vj ) for a given visiblevector: P (vj ) =Xh P (v;hj ): (3.30)This problem can be viewed as a form of free energy estimation [Sheykhet and Simkin 1990;Neal 1993]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 67
                            }
                        ],
                        "text": "30) This problem can be viewed as a form of free energy estimation [Sheykhet and Simkin 1990; Neal 1993]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123171900,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "54fc1eda600346ee6e75619e868a227dfa8309c1",
            "isKey": true,
            "numCitedBy": 8,
            "numCiting": 136,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Monte-Carlo-method-in-the-theory-of-solutions-Sheykhet-Simkin",
            "title": {
                "fragments": [],
                "text": "Monte Carlo method in the theory of solutions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 235
                            }
                        ],
                        "text": "There are a variety of practical algorithms for obtaining such an approximation, including Markov chain Monte Carlo methods [Geman and Geman 1984; Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld methods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan 1996], and inverse model methods [Hinton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 19
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower thana certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Monte Carlo methods [Geman and Geman 1984;Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld meth-ods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan1996], and inverse model methods [Hinton et al. 1995; Dayan et al. 1995] (see\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 18
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton 1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan 1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower than a certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mean eld theory learning algorithm for neural"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 103
                            }
                        ],
                        "text": "Examples of graphi-cal models include Markov random elds [Kinderman and Snell 1980], Bayesian networks[Pearl 1988], and chain graphs [Lauritzen and Wermuth 1989]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 191
                            }
                        ],
                        "text": "Work done on continuous-valued Bayesian networkshas focussed mainly on Gaussian random variables that are linked linearly such that thejoint distribution over all variables is also Gaussian [Pearl 1988; Heckerman and Geiger1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 6
                            }
                        ],
                        "text": "(See [Pearl 1988] for an extensive discussion of dependency-separation.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 188
                            }
                        ],
                        "text": "\u2026of the variables of interest in h.The Gibbs sampling algorithm is the simplest of the Markov chain Monte Carlo methods,and has been successfully applied to Bayesian networks [Pearl 1987; Pearl 1988; Neal 1992]as well as other graphical models [Geman and Geman 1984; Hinton and Sejnowski 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pro habilistic Reasoning i n Intelligent Sgsterns. -Morgan Kauhann"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 639,
                                "start": 192
                            }
                        ],
                        "text": "Work done on continuous-vdued Bayesian networks has focussed mainly on Gaussian random variables that are linked linearly such that the joint distribution over d l variables is also Gaussian [Pearl 1988; Heckerman and Geiger 19951. Lauritzen et al. [1990] have included discrete random variables within the linear Gaussian kamework. They consider networks that are singly-connected, so that probability propagation can be used. Most work on continuous-valued Bayesian networks requires that al1 the conditional distributions represented by the network can be easily derived using information elicited kom experts. Hofmann and Tresp [1996] consider estimating continuous Bayesian networks that may be richly connected, but they assume that al1 variables are observed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 47
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower thana certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 184
                            }
                        ],
                        "text": "There are several proofs that each EM iteration is guaranteed to increasethe likelihood of the training data [Baum and Petrie 1966; Dempster, Laird and Rubin 1977;Meng and Rubin 1992; Neal and Hinton 1993]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 192
                            }
                        ],
                        "text": "Work done on continuous-vdued Bayesian networks has focussed mainly on Gaussian random variables that are linked linearly such that the joint distribution over d l variables is also Gaussian [Pearl 1988; Heckerman and Geiger 19951. Lauritzen et al. [1990] have included discrete random variables within the linear Gaussian kamework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new view of the EM aigorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 59
                            }
                        ],
                        "text": "In this section, I review a technique cailed dice sampling [Neal 1997; Frey 1997a], that can be used for drawing a value z from a univariate probability density p ( z ) - in the context of inference, p(z) is the conditional distribution"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 60
                            }
                        ],
                        "text": "In this section, I review a technique called slicesampling [Neal 1997; Frey 1997a], that can be used for drawing a value z from a univariateprobability density p(z) | in the context of inference, p(z) is the conditional distributionp(zkjfzj = z( 1)j gNj=1;j 6=k)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 70
                            }
                        ],
                        "text": "A new value is obtained for each hidden variable using slice sampling[Neal 1997] (see Section 2.2.4), based on the distribution for the variable conditioned on all\n92 Pattern Classi cationother variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Markov chain Mmte Car10 methods based on Wcing\" the density function"
            },
            "venue": {
                "fragments": [],
                "text": "preparation."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 68
                            }
                        ],
                        "text": "A common approach [Peterson and Anderson 1987; Neal and Hinton1993; Zhang 1993; Hinton et al. 1995; Dayan et al. 1995; Saul, Jaakkola and Jordan1996] is to minimize an upper bound on C, thus guaranteeing that the cost is lower thana certain value."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 170
                            }
                        ],
                        "text": "\u2026Carlo methods [Geman and Geman 1984;Hinton and Sejnowski 1986; Ripley 1987; Potamianos and Goutsias 1993], mean eld meth-ods [Chandler 1987; Peterson and Anderson 1987; Zhang 1993; Saul, Jaakkola and Jordan1996], and inverse model methods [Hinton et al. 1995; Dayan et al. 1995] (see Section 4.5)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The mean field theory in EM procedures for blind Markov randorn field Mage restoration"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing. 2 2 7 4 0 ."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "For example, the latent variables in a hidden Markov model [Rabiner1989] with a xed state space size can be integrated out in a way so that the encodingcomplexity is linear in the number of latent variables (number of time steps).4.2 Communicating extra information through the choice ofcodewordIn\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "X tutoriai on hidden hlarkov models and selected applications in speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE. ?7:257-286."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48738422"
                        ],
                        "name": "D. Rapaport",
                        "slug": "D.-Rapaport",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rapaport",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rapaport"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 78
                            }
                        ],
                        "text": "2 Monte Carlo inferenceThe Monte Carlo method [Hammersley and Handscomb 1964; Kalos and Whitlock 1986;Ripley 1987] makes use of pseudo-random numbers in order to perform computations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 47
                            }
                        ],
                        "text": "2 Monte Carlo inference The Monte Carlo method [Hammersley and Handscomb 1964; Kalos and Whitlock 1986; Ripley 1987] makes use of pseudo-random numbers in order to perform computations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123213522,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "718a77d3986a28a81733d264f599ee9acb9f92f9",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Book-review:Monte-Carlo-methods.-Volume-I:-Basics-Rapaport",
            "title": {
                "fragments": [],
                "text": "Book review:Monte Carlo methods. Volume I: Basics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3184408"
                        ],
                        "name": "N. Wermuth",
                        "slug": "N.-Wermuth",
                        "structuredName": {
                            "firstName": "Nanny",
                            "lastName": "Wermuth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wermuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 134
                            }
                        ],
                        "text": "Examples of graphi-cal models include Markov random elds [Kinderman and Snell 1980], Bayesian networks[Pearl 1988], and chain graphs [Lauritzen and Wermuth 1989]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 133
                            }
                        ],
                        "text": "Examples of graphical models include Markov random elds [Kinderman and Snell 1980], Bayesian networks [Pearl 1988], and chain graphs [Lauritzen and Wermuth 1989]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121833680,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "082f19517c2039668715b04f7619d7e4b95f3e0c",
            "isKey": false,
            "numCitedBy": 733,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Graphical-Models-for-Associations-between-some-of-Lauritzen-Wermuth",
            "title": {
                "fragments": [],
                "text": "Graphical Models for Associations between Variables, some of which are Qualitative and some Quantitative"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557361"
                        ],
                        "name": "C. Peterson",
                        "slug": "C.-Peterson",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110869996"
                        ],
                        "name": "James R. Anderson",
                        "slug": "James-R.-Anderson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Anderson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3851750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "607802a4067cb7738bac85d3ca3386f859e637b9",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Mean-Field-Theory-Learning-Algorithm-for-Neural-Peterson-Anderson",
            "title": {
                "fragments": [],
                "text": "A Mean Field Theory Learning Algorithm for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690704"
                        ],
                        "name": "Edward A. Lee",
                        "slug": "Edward-A.-Lee",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Lee",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward A. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815712"
                        ],
                        "name": "D. Messerschmitt",
                        "slug": "D.-Messerschmitt",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Messerschmitt",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Messerschmitt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5603493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84ac23f6e7fb643d41a7db7d06cdb42196ffb6d8",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Digital-communication-(2.-ed.)-Lee-Messerschmitt",
            "title": {
                "fragments": [],
                "text": "Digital communication (2. ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 202
                            }
                        ],
                        "text": "Probabilisticstructure has been most extensively developed in the arti cial intelligence literature, withapplications ranging from taxonomic hierarchies [Woods 1975; Schubert 1976] to medicaldiagnosis [Spiegelhalter 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 204
                            }
                        ],
                        "text": "Probabilistic structure has been most extensively developed in the arti cial intelligence literature, with applications ranging from taxonomic hierarchies [Woods 1975; Schubert 1976] to medical diagnosis [Spiegelhalter 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast algorithms for probabilistic reasoning in in uence diagrams"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast algorithm for probabilistic reasoning in influence diagrams. with applications in genetics and expert systems"
            },
            "venue": {
                "fragments": [],
                "text": "J . Q.. edi tors. Infiuence Diagrams"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 49
                            }
                        ],
                        "text": "There are many techniques for algebraic decoding [Lin and Costeilo 1983; Blahut 1990; Wicker 1995) and algebraic decoders usually take advantage of special structure that is built into the code to make decoding easier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 5
                            }
                        ],
                        "text": "See [Wicker 1995] for a textbook treatment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 85
                            }
                        ],
                        "text": "There are many techniques for algebraicdecoding [Lin and Costello 1983; Blahut 1990; Wicker 1995] and algebraic decoders usuallytake advantage of special structure that is built into the code to make decoding easier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E m r Control Systems for Digital Communications and Storage"
            },
            "venue": {
                "fragments": [],
                "text": "Prentice-Hall Inc.. Englewood Cliffs N J."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Codes and iterative decoding on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 55
                            }
                        ],
                        "text": "Recently proposed iterative decoders for productcodes [Lodge et al. 1993; Hagenauer, O er and Papke 1996] can be viewed as probabilitypropagation in the corresponding networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 55
                            }
                        ],
                        "text": "Recently proposed iterative decoders for product codes [Lodge et al. 1993; Hagenauer, O er and Papke 1996] can be viewed as probability propagation in the corresponding networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Separable MAP ` lters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recent extensions of the EM algorithm (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "(A similar early stopping techniquehas been used with regression models [Rasmussen 1996].)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eualuation of Gaussian Processes and Other Methods for Non- Linear Regression- De part ment of Cornputer Science, University of Toronto, Toronto Canada"
            },
            "venue": {
                "fragments": [],
                "text": "Doctord dissertation (ftp://ftp.cs .toronto.edu/pub/carl/thesis.ps.gz)."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 110
                            }
                        ],
                        "text": "There are several proofs that each EM iteration is guaranteed to increase the likelihood of the training data [Baum and Petrie 1966; Dempster, Laird and Rubin 1977; Meng and Rubin 1992; Neal and Hinton 1993]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 163
                            }
                        ],
                        "text": "There are several proofs that each EM iteration is guaranteed to increasethe likelihood of the training data [Baum and Petrie 1966; Dempster, Laird and Rubin 1977;Meng and Rubin 1992; Neal and Hinton 1993]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recent extensions of the EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Correlation and causat ion"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Agricultural Research. 20557-585Zemel. R. E. ( 1993). A minimum descnp tion Iength framework for unsuperuised learning. Department of Computer Science. University of Toronto. Toronto Canada. Doctoral dissertation."
            },
            "year": 1921
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Monte Carlo rnethod in the theory of solutions"
            },
            "venue": {
                "fragments": [],
                "text": "Sheykhet,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Turbo-decoding as an instance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the BJCR trellis for Iinear biock codes"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE \u00ef''kansactzons on Information Theory. 42."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arithmetic coding. IBM J o u ~ n a i"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 33,
            "methodology": 42,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 82,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/Bayesian-networks-for-pattern-classification,-data-Hinton-Frey/a4a34496869a2ef6dfd2ddb880ae5b5dc9cdf60f?sort=total-citations"
}