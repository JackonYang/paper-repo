{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "The hyper-parameter of Transformer used in our paper is same as [28] produced outputs of dimension dmodel = 512."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 118
                            }
                        ],
                        "text": "Given a sentence si = (c (i) 1 , . . . , c (i) T ), text embeddings of sentence si is defined as follows\nte (i) 1:T = TransformerEncoder ( c (i) 1:T ; \u0398tenc ) , (1)\nwhere c(i)1:T = [c (i) 1 , . . . , c (i) T ] T \u2208 RT\u00d7dmodel denotes the input sequence, c(i)t \u2208 Rdmodel represents a token embedding (e.g., Word2Vec) of each character c(i)t , dmodel is the dimension of the model, te(i)1:T = [te (i) 1 , . . . , te (i) T ]\nT \u2208 RT\u00d7dmodel denotes the output sequence, te(i)t \u2208 Rdmodel represents the encoder output of Transformer for the i-th character c(i)t , and \u0398tenc represents the encoder parameters of Transformer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Networks Setting In the encoder part, the text segments feature extractor is implemented by the encoder module of Transformer [28] yielding text embeddings and the image segments feature extractor is implemented by ResNet50 [29] generating image embeddings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "One branch of Encoder generates text embeddings using encoder of Transformer [28] for capturing local textual context."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 140
                            }
                        ],
                        "text": "B. Implementation Details\nNetworks Setting In the encoder part, the text segments feature extractor is implemented by the encoder module of Transformer [28] yielding text embeddings and the image segments feature extractor is implemented by ResNet50 [29] generating image embeddings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 78
                            }
                        ],
                        "text": "We also use dropout with a ratio of 0.1 on both BiLSTM and the encoder of the Transformer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "The overall architecture is shown in Figure 3, which contains 3 modules:\n\u2022 Encoder: This module encodes text segments using Transformer to get text embeddings and image segments using CNN to get image embeddings."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": true,
            "numCitedBy": 35148,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110965354"
                        ],
                        "name": "Xiaojing Liu",
                        "slug": "Xiaojing-Liu",
                        "structuredName": {
                            "firstName": "Xiaojing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112256"
                        ],
                        "name": "Feiyu Gao",
                        "slug": "Feiyu-Gao",
                        "structuredName": {
                            "firstName": "Feiyu",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feiyu Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112207346"
                        ],
                        "name": "Qiong Zhang",
                        "slug": "Qiong-Zhang",
                        "structuredName": {
                            "firstName": "Qiong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36530509"
                        ],
                        "name": "Huasha Zhao",
                        "slug": "Huasha-Zhao",
                        "structuredName": {
                            "firstName": "Huasha",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huasha Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Alternative approaches [8], [9] predefine a graph to combine textual and visual information by using graph convolutions"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "Although [9] use a fully connected graph that every node/text segments is connected, this operation leads to graph aggregate useless and redundancy node information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Meanwhile, due to [9] simply and roughly regards graph as fully connectivity no matter how complicated the documents are, graph convolution aggregates useless and redundancy information between nodes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "The most related works to our method are [8], [9], using graph module to capture non-local and multimodal features for extraction but still differ from ours in several aspects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "Second, [9] also does not use images features to improve the performance of extraction tasks without ambiguity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "In the literature of [8], [9], the relative importance of visual features and nonsequential information is debated and graph neural networks modeling on document brings well performance on extraction entity tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] directly define a fully connected graph then uses a self-attention mechanism to define convolution on fully connected nodes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 380,
                                "start": 377
                            }
                        ],
                        "text": ",vN ] \u2208 RN\u00d7dmodel of graph nodes, where vi \u2208 Rmodel is the i-th node of the graph and the initial value of V is equal to X0, Graph Module generate a soft adjacent matrix A that represents the pairwise relationship weight between two nodes firstly through graph learning operation, and extract features H for each node vi using a multi-layer perception (MLP) networks just like [9] on input V and corresponding relation embedding \u03b1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Different from the existing key information works [8], [9] that only use text segments or bounding boxes, one of our key contribution in this paper is that we also use image segments simultaneously containing morphology information to improve document representations performance which can be exploited to help key information extraction tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "Existing key information works [8], [9] using graph neural networks modelling global layout context and non-sequential information need prior knowledge to pre-define task-specific edge type and adjacent matrix of the graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 85528598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04df8c70257b5280b9d303502c9d7ddf946f181b",
            "isKey": true,
            "numCitedBy": 86,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model."
            },
            "slug": "Graph-Convolution-for-Multimodal-Information-from-Liu-Gao",
            "title": {
                "fragments": [],
                "text": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces a graph convolution based model to combine textual and visual information presented in VRDs and outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5606742"
                        ],
                        "name": "Yujie Qian",
                        "slug": "Yujie-Qian",
                        "structuredName": {
                            "firstName": "Yujie",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujie Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2628786"
                        ],
                        "name": "Enrico Santus",
                        "slug": "Enrico-Santus",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Santus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enrico Santus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8752221"
                        ],
                        "name": "Zhijing Jin",
                        "slug": "Zhijing-Jin",
                        "structuredName": {
                            "firstName": "Zhijing",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhijing Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144084849"
                        ],
                        "name": "Jiang Guo",
                        "slug": "Jiang-Guo",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 162
                            }
                        ],
                        "text": "Besides, PICK make full use of features of the documents including text, image, and position features by using graph convolution to get richer representation for KIE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Alternative approaches [8], [9] predefine a graph to combine textual and visual information by using graph convolutions"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "KIE is essential for a wide range of technologies such as efficient archiving, fast indexing, document analytics and so on, which has a pivotal role in many services and applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "The main contributions of this paper can be summarized as follows: \u2022 In this paper, we present a novel method for KIE, which is\nmore effective and robust in handling complex documents layout."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Most KIE systems simply regard extraction tasks as a sequence tagging problems and implemented by Named Entity Recognition (NER) [2] framework, processing the plain text as a linear sequence result in ignoring most of valuable visual and non-sequential information (e.g., text, position, layout, and image) of documents for KIE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "In summary, these results show that the benefits of using both visual features and layout structure in KIE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "But [8] needs prior knowledge and extensive human efforts to predefine task-specific edge type and adjacent matrix of the graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "The most related works to our method are [8], [9], using graph module to capture non-local and multimodal features for extraction but still differ from ours in several aspects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "In the literature of [8], [9], the relative importance of visual features and nonsequential information is debated and graph neural networks modeling on document brings well performance on extraction entity tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Nevertheless, KIE from documents as the downstream task of OCR, compared to typical OCR tasks, had been a largely under explored domain and is also a challenging task [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "This architecture has been extensively proved and demonstrated to be valid in previous work on KIE [8], [9]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "It shows superior performance in all the scenarios and shows the capacity of KIE from documents with variable or fixed layout."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "In this paper, we study the problem of how to improve KIE ability by automatically making full use of the textual and visual features within documents."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "First, [8] only use textual and position features where images are not used and need to predefine task-specific edge type and connectivity between nodes of the graph."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "The aim of KIE is to extract texts of a number of key fields from given documents, and save the texts to structured documents."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "Existing research recognizes the critical role played by making full use of both textual and visual features of the\ndocuments to improve the performance of KIE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Different from the existing key information works [8], [9] that only use text segments or bounding boxes, one of our key contribution in this paper is that we also use image segments simultaneously containing morphology information to improve document representations performance which can be exploited to help key information extraction tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "Although this method uses image features and position to pre-train model and performs well on downstream tasks for document image understanding such as KIE, it doesn\u2019t consider the latent relationship between two text segments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Existing key information works [8], [9] using graph neural networks modelling global layout context and non-sequential information need prior knowledge to pre-define task-specific edge type and adjacent matrix of the graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Recently, a few studies in the task of KIE have attempted to make full use of untapped features in complex documents."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "PICK combines a graph module with the encoder-decoder framework for KIE tasks as illustrated in Figure 2(d)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] define edge to horizontally or vertically connected nodes/text segments that are close to each other and specify four types of adjacent matrix (left-to-right, right-to-left, up-to-down, and down-toup)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Most modern methods considered KIE as a sequence taggers problem and solved by NER as shown in Figure 2(b)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 53109320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1da8e1ad1814d81f69433ac877ef70caa950e4e6",
            "isKey": true,
            "numCitedBy": 63,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks \u2014 namely textual, social media and visual information extraction \u2014 shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin."
            },
            "slug": "GraphIE:-A-Graph-Based-Framework-for-Information-Qian-Santus",
            "title": {
                "fragments": [],
                "text": "GraphIE: A Graph-Based Framework for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Evaluation on three different tasks shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin, and generates a richer representation that can be exploited to improve word-level predictions."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41016725"
                        ],
                        "name": "Thomas Kipf",
                        "slug": "Thomas-Kipf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kipf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Kipf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "2) Graph Convolution: Graph convolutional network (GCN) is applied to capture global visual information and layout of nodes from the graph."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "This result is somewhat counter-intuitive but this phenomenon illustrates a characteristic of the GCN [10] that the deeper the model (number of layers) is, the more it probably will be overfitting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "Overall, GCN methods can be split into spatial convolution and spectral convolution methods[19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "operation [10] as illustrated in Figure 2(c)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "[25] proposes a version of GCNs suited to model syntactic dependency graphs to encode sentences for semantic role labeling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "[26] proposed a lexicon-based GCN with global semantics to avoid word ambiguities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Spectral methods that generally define graph convolution operation based on the spectral representation of graphs[10], however, are not propitious to dynamic graph structures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "Besides, recent research using both textual and visual features to aid the extraction mainly depends on graph-based representations due to graph convolutional networks (GCN) [10] demonstrated huge success in unstructured data tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3144218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36eff562f65125511b5dfab68ce7f7a943c27478",
            "isKey": true,
            "numCitedBy": 11719,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin."
            },
            "slug": "Semi-Supervised-Classification-with-Graph-Networks-Kipf-Welling",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Classification with Graph Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs which outperforms related methods by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110655544"
                        ],
                        "name": "He Guo",
                        "slug": "He-Guo",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3343260"
                        ],
                        "name": "Xiameng Qin",
                        "slug": "Xiameng-Qin",
                        "structuredName": {
                            "firstName": "Xiameng",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiameng Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1962338347"
                        ],
                        "name": "Jiaming Liu",
                        "slug": "Jiaming-Liu",
                        "structuredName": {
                            "firstName": "Jiaming",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaming Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912505"
                        ],
                        "name": "Junyu Han",
                        "slug": "Junyu-Han",
                        "structuredName": {
                            "firstName": "Junyu",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyu Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272123"
                        ],
                        "name": "Jingtuo Liu",
                        "slug": "Jingtuo-Liu",
                        "structuredName": {
                            "firstName": "Jingtuo",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingtuo Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12081764"
                        ],
                        "name": "Errui Ding",
                        "slug": "Errui-Ding",
                        "structuredName": {
                            "firstName": "Errui",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Errui Ding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "n to the textual features, through various features extractors such as recurrent neural networks (RNNs) or convolutional neural networks (CNNs) on word- and character- level [2], [12], [13]. Although [14], [15] uses visual image features to process extraction, it only focuses on image features and does not take textual features into account. [6] attempts to use both textual and visual features for doc"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " text and out of position print font. For this dataset, 2,104 and 526 images are used for training and testing individually. Train Ticket contains 2k real images and 300k synthetic images proposed in [14]. Every train ticket has eight key text \ufb01elds including ticket number, starting station, train number, destination station, date, ticket rates, seat category, and name. This dataset mainly consists of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "medical invoice, train ticket, and SROIE scenario, in the condition of a variable number of appeared entity, mean entity recall (mER), mean entity precision (mEP), and mean entity F-1 (mEF) de\ufb01ned in [14] are used to benchmark performance of PICK. Label Generation For train ticket datasets, we annotated the bbox and label the pre-de\ufb01ned entity type for each bbox then use the OCR system to generate the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 202712641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5efdc2d4084e47f7b436c97bf81b60fef8f6bdd",
            "isKey": true,
            "numCitedBy": 14,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting Text of Interest (ToI) from images is a crucial part of many OCR applications, such as entity recognition of cards, invoices, and receipts. Most of the existing works employ complicated engineering pipeline, which contains OCR and structure information extraction, to fulfill this task. This paper proposes an Entity-aware Attention Text Extraction Network called EATEN, which is an end-to-end trainable system to extract the ToIs without any post-processing. In the proposed framework, each entity is parsed by its corresponding entity-aware decoder, respectively. Moreover, we innovatively introduce a state transition mechanism which further improves the robustness of visual ToI extraction. In consideration of the absence of public benchmarks, we construct a dataset of almost 0.6 million images in three real-world scenarios (train ticket, passport and business card), which is publicly available at https://github.com/beacandler/EATEN. To the best of our knowledge, EATEN is the first single shot method to extract entities from images. Extensive experiments on these benchmarks demonstrate the state-of-the-art performance of EATEN."
            },
            "slug": "EATEN:-Entity-Aware-Attention-for-Single-Shot-Text-Guo-Qin",
            "title": {
                "fragments": [],
                "text": "EATEN: Entity-Aware Attention for Single Shot Visual Text Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes an Entity-aware Attention Text Extraction Network called EATEN, which is an end-to-end trainable system to extract the ToIs without any post-processing, and is the first single shot method to extract entities from images."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152390773"
                        ],
                        "name": "Bo Jiang",
                        "slug": "Bo-Jiang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4927354"
                        ],
                        "name": "Ziyan Zhang",
                        "slug": "Ziyan-Zhang",
                        "structuredName": {
                            "firstName": "Ziyan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziyan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41021195"
                        ],
                        "name": "Doudou Lin",
                        "slug": "Doudou-Lin",
                        "structuredName": {
                            "firstName": "Doudou",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doudou Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37864689"
                        ],
                        "name": "Jin Tang",
                        "slug": "Jin-Tang",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151264175"
                        ],
                        "name": "B. Luo",
                        "slug": "B.-Luo",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Luo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "We use the modified loss function based on [11] to optimize the learnable weight vector wi as follows"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "In this way, we incorporate improved graph learningconvolutional network inspired by [11] into existing graph architecture to learn a soft adjacent matrix A to model the graph context for downstream tasks illustrated in the lower left corner of Figure 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "PICK incorporates graph learning module inspired by [11] into existing graph architecture to learn a soft adjacent matrix to effectively and efficiently refine the graph context structure indicating the relationship between nodes for downstream tasks instead of predefining edge type of the graph artificially."
                    },
                    "intents": []
                }
            ],
            "corpusId": 195497272,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "960a6293d068e479435b693df04b839b24ffb7d9",
            "isKey": true,
            "numCitedBy": 101,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Graph Convolutional Neural Networks (graph CNNs) have been widely used for graph data representation and semi-supervised learning tasks. However, existing graph CNNs generally use a fixed graph which may not be optimal for semi-supervised learning tasks. In this paper, we propose a novel Graph Learning-Convolutional Network (GLCN) for graph data representation and semi-supervised learning. The aim of GLCN is to learn an optimal graph structure that best serves graph CNNs for semi-supervised learning by integrating both graph learning and graph convolution in a unified network architecture. The main advantage is that in GLCN both given labels and the estimated labels are incorporated and thus can provide useful \u2018weakly\u2019 supervised information to refine (or learn) the graph construction and also to facilitate the graph convolution operation for unknown label estimation. Experimental results on seven benchmarks demonstrate that GLCN significantly outperforms the state-of-the-art traditional fixed structure based graph CNNs."
            },
            "slug": "Semi-Supervised-Learning-With-Graph-Networks-Jiang-Zhang",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning With Graph Learning-Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of GLCN is to learn an optimal graph structure that best serves graph CNNs for semi-supervised learning by integrating both graph learning and graph convolution in a unified network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830914"
                        ],
                        "name": "Guillaume Lample",
                        "slug": "Guillaume-Lample",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Lample",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Lample"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50324141"
                        ],
                        "name": "Sandeep Subramanian",
                        "slug": "Sandeep-Subramanian",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Subramanian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Subramanian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189948"
                        ],
                        "name": "Kazuya Kawakami",
                        "slug": "Kazuya-Kawakami",
                        "structuredName": {
                            "firstName": "Kazuya",
                            "lastName": "Kawakami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuya Kawakami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "Most KIE systems simply regard extraction tasks as a sequence tagging problems and implemented by Named Entity Recognition (NER) [2] framework, processing the plain text as a linear sequence result in ignoring most of valuable visual and non-sequential information (e.g., text, position, layout, and image) of documents for KIE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 220
                            }
                        ],
                        "text": "The majority of methods, however, pay attention to the textual features, through various features extractors such as recurrent neural networks (RNNs) or convolutional neural networks (CNNs) on word- and character- level [2], [12], [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "In comparison to the typical NER task, it is much more challenging to distinguish entity without ambiguity from complicated documents for a machine."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "Most modern methods considered KIE as a sequence taggers problem and solved by NER as shown in Figure 2(b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "sequence tagging problems and implemented by Named Entity Recognition (NER) [2] framework, processing the plain text as a linear sequence result in ignoring most of valuable visual and non-sequential information (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6042994,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "24158c9fc293c8a998ac552b1188404a877da292",
            "isKey": true,
            "numCitedBy": 2898,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016."
            },
            "slug": "Neural-Architectures-for-Named-Entity-Recognition-Lample-Ballesteros",
            "title": {
                "fragments": [],
                "text": "Neural Architectures for Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 of juny 2016."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "We implement the CNN using ResNet [29] and resize image under condition H \u00d7W = T then encode each image segments individually and we can get a document D image embeddings, defining it as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 27
                            }
                        ],
                        "text": "We implement the CNN using ResNet [29] and resize image under condition H \u00d7W = T then encode each image segments individually and we can get a document D image embeddings, defining it as\nIE = [ie(1); . . . ; ie(N)] \u2208 RN\u00d7T\u00d7dmodel ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 224
                            }
                        ],
                        "text": "Networks Setting In the encoder part, the text segments feature extractor is implemented by the encoder module of Transformer [28] yielding text embeddings and the image segments feature extractor is implemented by ResNet50 [29] generating image embeddings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 241
                            }
                        ],
                        "text": "B. Implementation Details\nNetworks Setting In the encoder part, the text segments feature extractor is implemented by the encoder module of Transformer [28] yielding text embeddings and the image segments feature extractor is implemented by ResNet50 [29] generating image embeddings."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 95318,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390799037"
                        ],
                        "name": "Zheng Huang",
                        "slug": "Zheng-Huang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153819461"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73730984"
                        ],
                        "name": "Jianhua He",
                        "slug": "Jianhua-He",
                        "structuredName": {
                            "firstName": "Jianhua",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianhua He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "SROIE [1] contains 626 receipts for training and 347 receipts for testing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Nevertheless, KIE from documents as the downstream task of OCR, compared to typical OCR tasks, had been a largely under explored domain and is also a challenging task [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 211026630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d00cbb0c05c1dc922126fe72c1078b773d01c688",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The ICDAR 2019 Challenge on \"Scanned receipts OCR and key information extraction\" (SROIE) covers important aspects related to the automated analysis of scanned receipts. The SROIE tasks play a key role in many document analysis systems and hold significant commercial potential. Although a lot of work has been published over the years on administrative document analysis, the community has advanced relatively slowly, as most datasets have been kept private. One of the key contributions of SROIE to the document analysis community is to offer a first, standardized dataset of 1000 whole scanned receipt images and annotations, as well as an evaluation procedure for such tasks. The Challenge is structured around three tasks, namely Scanned Receipt Text Localization (Task 1), Scanned Receipt OCR (Task 2) and Key Information Extraction from Scanned Receipts (Task 3). The competition opened on 10th February, 2019 and closed on 5th May, 2019. We received 29, 24 and 18 valid submissions received for the three competition tasks, respectively. This report presents the competition datasets, define the tasks and the evaluation protocols, offer detailed submission statistics, as well as an analysis of the submitted performance. While the tasks of text localization and recognition seem to be relatively easy to tackle, it is interesting to observe the variety of ideas and approaches proposed for the information extraction task. According to the submissions' performance we believe there is still margin for improving information extraction performance, although the current dataset would have to grow substantially in following editions. Given the success of the SROIE competition evidenced by the wide interest generated and the healthy number of submissions from academic, research institutes and industry over different countries, we consider that the SROIE competition can evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field."
            },
            "slug": "ICDAR2019-Competition-on-Scanned-Receipt-OCR-and-Huang-Chen",
            "title": {
                "fragments": [],
                "text": "ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The ICDAR 2019 Challenge on \"Scanned receipts OCR and key information extraction\" (SROIE) covers important aspects related to the automated analysis of scanned receipts, and is considered to evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032611"
                        ],
                        "name": "Yiheng Xu",
                        "slug": "Yiheng-Xu",
                        "structuredName": {
                            "firstName": "Yiheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123545597"
                        ],
                        "name": "Minghao Li",
                        "slug": "Minghao-Li",
                        "structuredName": {
                            "firstName": "Minghao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500855"
                        ],
                        "name": "Lei Cui",
                        "slug": "Lei-Cui",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110003"
                        ],
                        "name": "Shaohan Huang",
                        "slug": "Shaohan-Huang",
                        "structuredName": {
                            "firstName": "Shaohan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92660691"
                        ],
                        "name": "Ming Zhou",
                        "slug": "Ming-Zhou",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] proposed LayoutLM method, inspired by BERT [7], for document image understanding using pre-training of text and layout."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "Note that LayoutLM [6] also uses extra pre-training datasets and documents class supervised information to train the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] attempts to use both textual and visual features for document understanding and gets good performance on some documents, through pre-training of text and layout, but it doesn\u2019t consider the relationship between text within documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 209515395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3465c06c872d8c48d628c5fc2d484087719351b6",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm."
            },
            "slug": "LayoutLM:-Pre-training-of-Text-and-Layout-for-Image-Xu-Li",
            "title": {
                "fragments": [],
                "text": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The LayoutLM is proposed to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067331064"
                        ],
                        "name": "Tao Gui",
                        "slug": "Tao-Gui",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Gui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Gui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51192034"
                        ],
                        "name": "Yicheng Zou",
                        "slug": "Yicheng-Zou",
                        "structuredName": {
                            "firstName": "Yicheng",
                            "lastName": "Zou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yicheng Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49346854"
                        ],
                        "name": "Qi Zhang",
                        "slug": "Qi-Zhang",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24859244"
                        ],
                        "name": "Minlong Peng",
                        "slug": "Minlong-Peng",
                        "structuredName": {
                            "firstName": "Minlong",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minlong Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41037252"
                        ],
                        "name": "Jinlan Fu",
                        "slug": "Jinlan-Fu",
                        "structuredName": {
                            "firstName": "Jinlan",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinlan Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712533"
                        ],
                        "name": "Zhongyu Wei",
                        "slug": "Zhongyu-Wei",
                        "structuredName": {
                            "firstName": "Zhongyu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhongyu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790227"
                        ],
                        "name": "Xuanjing Huang",
                        "slug": "Xuanjing-Huang",
                        "structuredName": {
                            "firstName": "Xuanjing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuanjing Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[26] proposed a lexicon-based GCN with global semantics to avoid word ambiguities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202769823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd64405e42f09b55088d48f5f074880c71725fc3",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks (RNN) used for Chinese named entity recognition (NER) that sequentially track character and word information have achieved great success. However, the characteristic of chain structure and the lack of global semantics determine that RNN-based models are vulnerable to word ambiguities. In this work, we try to alleviate this problem by introducing a lexicon-based graph neural network with global semantics, in which lexicon knowledge is used to connect characters to capture the local composition, while a global relay node can capture global sentence semantics and long-range dependency. Based on the multiple graph-based interactions among characters, potential words, and the whole-sentence semantics, word ambiguities can be effectively tackled. Experiments on four NER datasets show that the proposed model achieves significant improvements against other baseline models."
            },
            "slug": "A-Lexicon-Based-Graph-Neural-Network-for-Chinese-Gui-Zou",
            "title": {
                "fragments": [],
                "text": "A Lexicon-Based Graph Neural Network for Chinese NER"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A lexicon-based graph neural network with global semantics is introduced, in which lexicon knowledge is used to connect characters to capture the local composition, while a global relay node can capture global sentence semantics and long-range dependency."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15144597"
                        ],
                        "name": "Franck Lebourgeois",
                        "slug": "Franck-Lebourgeois",
                        "structuredName": {
                            "firstName": "Franck",
                            "lastName": "Lebourgeois",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Franck Lebourgeois"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372426"
                        ],
                        "name": "Z. Bublinski",
                        "slug": "Z.-Bublinski",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Bublinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Bublinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739381"
                        ],
                        "name": "H. Emptoz",
                        "slug": "H.-Emptoz",
                        "structuredName": {
                            "firstName": "Hubert",
                            "lastName": "Emptoz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Emptoz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Besides, a handful of other methods [16], [17], [18] make full use of features to support extraction tasks based on human-designed features or task-specific knowledge, which are not extensible on other documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62602509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a2c3f55e7a84ad5fa5eaa577d8d58ee73e14e2b",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Outlines a fast and efficient method for extracting graphics and text paragraphs from printed documents. The method presented is based on bottom-up approach to document analysis and it achieves very good performance in most cases. During the preprocessing characters are linked together to form blocks. Created blocks are segmented, labelled and merged into paragraphs. Simultaneously, graphics are extracted from the image. Algorithms for each step of processing are presented. Also, the obtained experimental results are included.<<ETX>>"
            },
            "slug": "A-fast-and-efficient-method-for-extracting-text-and-Lebourgeois-Bublinski",
            "title": {
                "fragments": [],
                "text": "A fast and efficient method for extracting text paragraphs and graphics from unconstrained documents"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "Outlines a fast and efficient method for extracting graphics and text paragraphs from printed documents based on bottom-up approach to document analysis and achieves very good performance in most cases."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108336397"
                        ],
                        "name": "Si Zhang",
                        "slug": "Si-Zhang",
                        "structuredName": {
                            "firstName": "Si",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Si Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8163721"
                        ],
                        "name": "Hanghang Tong",
                        "slug": "Hanghang-Tong",
                        "structuredName": {
                            "firstName": "Hanghang",
                            "lastName": "Tong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanghang Tong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684988"
                        ],
                        "name": "Jiejun Xu",
                        "slug": "Jiejun-Xu",
                        "structuredName": {
                            "firstName": "Jiejun",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiejun Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786514"
                        ],
                        "name": "R. Maciejewski",
                        "slug": "R.-Maciejewski",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Maciejewski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Maciejewski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Overall, GCN methods can be split into spatial convolution and spectral convolution methods[19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54526291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47cccceb180dd93d5b098d02cba631c3998e8cf8",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Graph-structured data naturally appear in numerous application domains, ranging from social analysis, bioinformatics to computer vision. The unique capability of graphs enables capturing the structural relations among data, and thus allows to harvest more insights compared to analyzing data in isolation. However, graph mining is a challenging task due to the underlying complex and diverse connectivity patterns. A potential solution is to learn the representation of a graph in a low-dimensional Euclidean space via embedding techniques that preserve the graph properties. Although tremendous efforts have been made to address the graph representation learning problem, many of them still suffer from their shallow learning mechanisms. On the other hand, deep learning models on graphs have recently emerged in both machine learning and data mining areas and demonstrated superior performance for various problems. In this survey, we conduct a comprehensive review specifically on the emerging field of graph convolutional networks, which is one of the most prominent graph deep learning models. We first introduce two taxonomies to group the existing works based on the types of convolutions and the areas of applications, then highlight some graph convolutional network models in details. Finally, we present several challenges in this area and discuss potential directions for future research."
            },
            "slug": "Graph-Convolutional-Networks:-Algorithms,-and-Open-Zhang-Tong",
            "title": {
                "fragments": [],
                "text": "Graph Convolutional Networks: Algorithms, Applications and Open Challenges"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A comprehensive review of the emerging field of graph convolutional networks, which is one of the most prominent graph deep learning models, and introduces two taxonomies to group the existing works based on the types of convolutions and the areas of applications."
            },
            "venue": {
                "fragments": [],
                "text": "CSoNet"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "[6] proposed LayoutLM method, inspired by BERT [7], for document image understanding using pre-training of text and layout."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": false,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719068"
                        ],
                        "name": "Anoop R. Katti",
                        "slug": "Anoop-R.-Katti",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Katti",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop R. Katti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39387393"
                        ],
                        "name": "Cordula Guder",
                        "slug": "Cordula-Guder",
                        "structuredName": {
                            "firstName": "Cordula",
                            "lastName": "Guder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cordula Guder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14334250"
                        ],
                        "name": "Sebastian Brarda",
                        "slug": "Sebastian-Brarda",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Brarda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Brarda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704747"
                        ],
                        "name": "S. Bickel",
                        "slug": "S.-Bickel",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Bickel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216963"
                        ],
                        "name": "J. H\u00f6hne",
                        "slug": "J.-H\u00f6hne",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "H\u00f6hne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00f6hne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803968"
                        ],
                        "name": "J. Faddoul",
                        "slug": "J.-Faddoul",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Faddoul",
                            "middleNames": [
                                "Baptiste"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Faddoul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Although [14], [15] uses visual image features to process extraction, it only focuses on image features and does not take textual features into account."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52815006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15aae08159856cdbf0ce539357d473a04dcbb7f3",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images."
            },
            "slug": "Chargrid:-Towards-Understanding-2D-Documents-Katti-Reisswig",
            "title": {
                "fragments": [],
                "text": "Chargrid: Towards Understanding 2D Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel type of text representation is introduced that preserves the 2D layout of a document by encoding each document page as a two-dimensional grid of characters and it is shown that it significantly outperforms approaches based on sequential text or document images."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748796"
                        ],
                        "name": "Linfeng Song",
                        "slug": "Linfeng-Song",
                        "structuredName": {
                            "firstName": "Linfeng",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linfeng Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48378565"
                        ],
                        "name": "Yue Zhang",
                        "slug": "Yue-Zhang",
                        "structuredName": {
                            "firstName": "Yue",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yue Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40296541"
                        ],
                        "name": "Zhiguo Wang",
                        "slug": "Zhiguo-Wang",
                        "structuredName": {
                            "firstName": "Zhiguo",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiguo Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22], [23] proposed Graph LSTM, which enables a varied number of incoming dependencies at each memory cell."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52115592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "749c00144b6a005f0fbe3ee8cc07e0090a45f7b8",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Cross-sentence n-ary relation extraction detects relations among n entities across multiple sentences. Typical methods formulate an input as a document graph, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure. We propose a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature."
            },
            "slug": "N-ary-Relation-Extraction-using-Graph-State-LSTM-Song-Zhang",
            "title": {
                "fragments": [],
                "text": "N-ary Relation Extraction using Graph-State LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing, and speeds up computation by allowing more parallelization."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38258535"
                        ],
                        "name": "Shaolei Wang",
                        "slug": "Shaolei-Wang",
                        "structuredName": {
                            "firstName": "Shaolei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaolei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48378565"
                        ],
                        "name": "Yue Zhang",
                        "slug": "Yue-Zhang",
                        "structuredName": {
                            "firstName": "Yue",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yue Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256319"
                        ],
                        "name": "Wanxiang Che",
                        "slug": "Wanxiang-Che",
                        "structuredName": {
                            "firstName": "Wanxiang",
                            "lastName": "Che",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanxiang Che"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40282288"
                        ],
                        "name": "Ting Liu",
                        "slug": "Ting-Liu",
                        "structuredName": {
                            "firstName": "Ting",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ting Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[24] jointly extract entities and relations through designing a directed graph schema."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51605731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a252c7abc7286fab01d3f89b8655c8dbc2dbf512",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Both entity and relation extraction can benefit from being performed jointly, allowing each task to correct the errors of the other. Most existing neural joint methods extract entities and relations separately and achieve joint learning\u00a0 through parameter sharing, leading to a drawback that information between output entities and relations cannot be fully exploited. In this paper, we convert the joint task into a directed graph by designing a novel graph scheme and propose a transition-based approach to generate the directed graph incrementally, which can achieve joint learning through joint decoding. Our method can model underlying dependencies not only between entities and relations, but also between relations. Experiments on NewYork Times (NYT) corpora show that our approach outperforms the state-of-the-art methods.\u00a0"
            },
            "slug": "Joint-Extraction-of-Entities-and-Relations-Based-on-Wang-Zhang",
            "title": {
                "fragments": [],
                "text": "Joint Extraction of Entities and Relations Based on a Novel Graph Scheme"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper converts the joint task into a directed graph by designing a novel graph scheme and proposes a transition-based approach to generate the directed graph incrementally, which can achieve joint learning through joint decoding."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444569"
                        ],
                        "name": "Petar Velickovic",
                        "slug": "Petar-Velickovic",
                        "structuredName": {
                            "firstName": "Petar",
                            "lastName": "Velickovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petar Velickovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7153363"
                        ],
                        "name": "Guillem Cucurull",
                        "slug": "Guillem-Cucurull",
                        "structuredName": {
                            "firstName": "Guillem",
                            "lastName": "Cucurull",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillem Cucurull"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8742492"
                        ],
                        "name": "Arantxa Casanova",
                        "slug": "Arantxa-Casanova",
                        "structuredName": {
                            "firstName": "Arantxa",
                            "lastName": "Casanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arantxa Casanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144290131"
                        ],
                        "name": "Adriana Romero",
                        "slug": "Adriana-Romero",
                        "structuredName": {
                            "firstName": "Adriana",
                            "lastName": "Romero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adriana Romero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144269589"
                        ],
                        "name": "P. Lio\u2019",
                        "slug": "P.-Lio\u2019",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Lio\u2019",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lio\u2019"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "convolution category which generally defines graph convolution operation directly through defining an operation on node groups of neighbors [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3292002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33998aff64ce51df8dee45989cdca4b6b1329ec4",
            "isKey": false,
            "numCitedBy": 5524,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."
            },
            "slug": "Graph-Attention-Networks-Velickovic-Cucurull",
            "title": {
                "fragments": [],
                "text": "Graph Attention Networks"
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863322"
                        ],
                        "name": "Anik\u00f3 Simon",
                        "slug": "Anik\u00f3-Simon",
                        "structuredName": {
                            "firstName": "Anik\u00f3",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anik\u00f3 Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1907819"
                        ],
                        "name": "Jean-Christophe Pret",
                        "slug": "Jean-Christophe-Pret",
                        "structuredName": {
                            "firstName": "Jean-Christophe",
                            "lastName": "Pret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Christophe Pret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406054409"
                        ],
                        "name": "A. Johnson",
                        "slug": "A.-Johnson",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Johnson",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Johnson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "However, this solution [4], [5] only uses text and position information to extract entity and need a large amount of task-specific knowledge and human-designed rules, which does not extend to other types of documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29276706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7094063edf765c44dcce4aada3ed0ca725b74d96",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new bottom-up method for document layout analysis. The algorithm was implemented in the CLIDE (Chemical Literature Data Extraction) system, but the method described here is suitable for a broader range of documents. It is based on Kruskal's algorithm and uses a special distance-metric between the components to construct the physical page structure. The method has all the major advantages of bottom-up systems: independence from different text spacing and independence from different block alignments. The algorithms computational complexity is reduced to linear by using heuristics and path-compression."
            },
            "slug": "A-Fast-Algorithm-for-Bottom-Up-Document-Layout-Simon-Pret",
            "title": {
                "fragments": [],
                "text": "A Fast Algorithm for Bottom-Up Document Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new bottom-up method for document layout analysis based on Kruskal's algorithm and uses a special distance-metric between the components to construct the physical page structure."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3157053"
                        ],
                        "name": "Nanyun Peng",
                        "slug": "Nanyun-Peng",
                        "structuredName": {
                            "firstName": "Nanyun",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nanyun Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759772"
                        ],
                        "name": "Hoifung Poon",
                        "slug": "Hoifung-Poon",
                        "structuredName": {
                            "firstName": "Hoifung",
                            "lastName": "Poon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hoifung Poon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2596310"
                        ],
                        "name": "Chris Quirk",
                        "slug": "Chris-Quirk",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Quirk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Quirk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "[22], [23] proposed Graph LSTM, which enables a varied number of incoming dependencies at each memory cell."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2797612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54b8aadb7c2576665ce26caf59464b6449ac9ccf",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy."
            },
            "slug": "Cross-Sentence-N-ary-Relation-Extraction-with-Graph-Peng-Poon",
            "title": {
                "fragments": [],
                "text": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction is explored, demonstrating its effectiveness with both conventional supervised learning and distant supervision."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022957"
                        ],
                        "name": "Diego Marcheggiani",
                        "slug": "Diego-Marcheggiani",
                        "structuredName": {
                            "firstName": "Diego",
                            "lastName": "Marcheggiani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diego Marcheggiani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144889265"
                        ],
                        "name": "Ivan Titov",
                        "slug": "Ivan-Titov",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Titov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivan Titov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[25] proposes a version of GCNs suited to model syntactic dependency graphs to encode sentences for semantic role labeling."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16839291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3a3c163f25b9181f1fb7e71a32482a7393d2088",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English."
            },
            "slug": "Encoding-Sentences-with-Graph-Convolutional-for-Marcheggiani-Titov",
            "title": {
                "fragments": [],
                "text": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs, is proposed, observing that GCN layers are complementary to LSTM ones."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2500309"
                        ],
                        "name": "Federico Monti",
                        "slug": "Federico-Monti",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Monti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Federico Monti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804261"
                        ],
                        "name": "D. Boscaini",
                        "slug": "D.-Boscaini",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boscaini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boscaini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426718"
                        ],
                        "name": "Jonathan Masci",
                        "slug": "Jonathan-Masci",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Masci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Masci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796150"
                        ],
                        "name": "E. Rodol\u00e0",
                        "slug": "E.-Rodol\u00e0",
                        "structuredName": {
                            "firstName": "Emanuele",
                            "lastName": "Rodol\u00e0",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rodol\u00e0"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064928589"
                        ],
                        "name": "Jan Svoboda",
                        "slug": "Jan-Svoboda",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Svoboda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Svoboda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732570"
                        ],
                        "name": "M. Bronstein",
                        "slug": "M.-Bronstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bronstein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bronstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "convolution category which generally defines graph convolution operation directly through defining an operation on node groups of neighbors [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 301319,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f09f7888aa5aeaf88a2a44aea768d9a8747e97d2",
            "isKey": false,
            "numCitedBy": 1185,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph-and 3D shape analysis and show that it consistently outperforms previous approaches."
            },
            "slug": "Geometric-Deep-Learning-on-Graphs-and-Manifolds-Monti-Boscaini",
            "title": {
                "fragments": [],
                "text": "Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features and test the proposed method on standard tasks from the realms of image-, graph-and 3D shape analysis and show that it consistently outperforms previous approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378954"
                        ],
                        "name": "Xuezhe Ma",
                        "slug": "Xuezhe-Ma",
                        "structuredName": {
                            "firstName": "Xuezhe",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuezhe Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 231
                            }
                        ],
                        "text": "The majority of methods, however, pay attention to the textual features, through various features extractors such as recurrent neural networks (RNNs) or convolutional neural networks (CNNs) on word- and character- level [2], [12], [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10489017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4",
            "isKey": false,
            "numCitedBy": 1994,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
            },
            "slug": "End-to-end-Sequence-Labeling-via-Bi-directional-Ma-Hovy",
            "title": {
                "fragments": [],
                "text": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel neutral network architecture is introduced that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF, thus making it applicable to a wide range of sequence labeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052473561"
                        ],
                        "name": "Jason P. C. Chiu",
                        "slug": "Jason-P.-C.-Chiu",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Chiu",
                            "middleNames": [
                                "P.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason P. C. Chiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143662759"
                        ],
                        "name": "Eric Nichols",
                        "slug": "Eric-Nichols",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nichols",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Nichols"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 225
                            }
                        ],
                        "text": "The majority of methods, however, pay attention to the textual features, through various features extractors such as recurrent neural networks (RNNs) or convolutional neural networks (CNNs) on word- and character- level [2], [12], [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6300165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10a4db59e81d26b2e0e896d3186ef81b4458b93f",
            "isKey": false,
            "numCitedBy": 1335,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
            },
            "slug": "Named-Entity-Recognition-with-Bidirectional-Chiu-Nichols",
            "title": {
                "fragments": [],
                "text": "Named Entity Recognition with Bidirectional LSTM-CNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel neural network architecture is presented that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654957"
                        ],
                        "name": "Daniel Schuster",
                        "slug": "Daniel-Schuster",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2956206"
                        ],
                        "name": "Klemens Muthmann",
                        "slug": "Klemens-Muthmann",
                        "structuredName": {
                            "firstName": "Klemens",
                            "lastName": "Muthmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klemens Muthmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054613265"
                        ],
                        "name": "D. Esser",
                        "slug": "D.-Esser",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Esser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Esser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145417024"
                        ],
                        "name": "A. Schill",
                        "slug": "A.-Schill",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113458452"
                        ],
                        "name": "Michael Berger",
                        "slug": "Michael-Berger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2906706"
                        ],
                        "name": "C. Weidling",
                        "slug": "C.-Weidling",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Weidling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Weidling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652700"
                        ],
                        "name": "Kamil Aliyev",
                        "slug": "Kamil-Aliyev",
                        "structuredName": {
                            "firstName": "Kamil",
                            "lastName": "Aliyev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kamil Aliyev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942133"
                        ],
                        "name": "A. Hofmeier",
                        "slug": "A.-Hofmeier",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Hofmeier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hofmeier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "However, this solution [4], [5] only uses text and position information to extract entity and need a large amount of task-specific knowledge and human-designed rules, which does not extend to other types of documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18934920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87dee6b4a5fcbab541b45a967c24030df6cee29b",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic information extraction from scanned business documents is especially valuable in the application domain of document archiving. But current systems for automated document processing still require a lot of configuration work that can only be done by experienced users or administrators. We present an approach for information extraction which purely builds on end-user provided training examples and intentionally omits efficient known extraction techniques like rule based extraction that require intense training and/or information extraction expertise. Our evaluation on a large corpus of business documents shows competitive results of above 85% F1-measure on 10 commonly used fields like document type, sender, receiver and date. The system is deployed and used inside the commercial document management system DocuWare."
            },
            "slug": "Intellix-End-User-Trained-Information-Extraction-Schuster-Muthmann",
            "title": {
                "fragments": [],
                "text": "Intellix -- End-User Trained Information Extraction for Document Archiving"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents an approach for information extraction which purely builds on end-user provided training examples and intentionally omits efficient known extraction techniques like rule based extraction that require intense training and/or information extraction expertise."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143823474"
                        ],
                        "name": "Mar\u00e7al Rusi\u00f1ol",
                        "slug": "Mar\u00e7al-Rusi\u00f1ol",
                        "structuredName": {
                            "firstName": "Mar\u00e7al",
                            "lastName": "Rusi\u00f1ol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mar\u00e7al Rusi\u00f1ol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406403963"
                        ],
                        "name": "Tayeb Benkhelfallah",
                        "slug": "Tayeb-Benkhelfallah",
                        "structuredName": {
                            "firstName": "Tayeb",
                            "lastName": "Benkhelfallah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tayeb Benkhelfallah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401920887"
                        ],
                        "name": "V. P. d'Andecy",
                        "slug": "V.-P.-d'Andecy",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "d'Andecy",
                            "middleNames": [
                                "Poulain"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. P. d'Andecy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Besides, a handful of other methods [16], [17], [18] make full use of features to support extraction tasks based on human-designed features or task-specific knowledge, which are not extensible on other documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12155580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3170d280095b2198570073eaa068d6b2946334e3",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an incremental framework aimed at extracting field information from administrative document images in the context of a Digital Mail-room scenario. Given a single training sample in which the user has marked which fields have to be extracted from a particular document class, a document model representing structural relationships among words is built. This model is incrementally refined as the system processes more and more documents from the same class. A reformulation of the tf-idf statistic scheme allows to adjust the importance weights of the structural relationships among words. We report in the experimental section our results obtained with a large dataset of real invoices."
            },
            "slug": "Field-Extraction-from-Administrative-Documents-by-Rusi\u00f1ol-Benkhelfallah",
            "title": {
                "fragments": [],
                "text": "Field Extraction from Administrative Documents by Incremental Structural Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "An incremental framework aimed at extracting field information from administrative document images in the context of a Digital Mail-room scenario is presented and results obtained with a large dataset of real invoices are reported."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090792"
                        ],
                        "name": "Y. Aumann",
                        "slug": "Y.-Aumann",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Aumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145864794"
                        ],
                        "name": "Ronen Feldman",
                        "slug": "Ronen-Feldman",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronen Feldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2659434"
                        ],
                        "name": "Yair Liberzon",
                        "slug": "Yair-Liberzon",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Liberzon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Liberzon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47861681"
                        ],
                        "name": "Binyamin Rosenfeld",
                        "slug": "Binyamin-Rosenfeld",
                        "structuredName": {
                            "firstName": "Binyamin",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binyamin Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2012715"
                        ],
                        "name": "Jonathan Schler",
                        "slug": "Jonathan-Schler",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Schler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Schler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 286
                            }
                        ],
                        "text": "The main challenge faced by many researchers is how to fully and efficiently exploit both textual and visual features of documents to get a richer semantic representation that is crucial for extracting key information without ambiguity in many cases and the expansibility of the method [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9846921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37b5296d40fd7ad553a87932e2cc088b3970cce2",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Typographic and visual information is an integral part of textual documents. Most information extraction (IE) systems ignore most of this visual information, processing the text as a linear sequence of words. Thus, much valuable information is lost. In this paper, we show how to make use of this visual information for IE. We present an algorithm that allows to automatically extract specific fields of the document (such as the title, author, etc.) based exclusively on the visual formatting of the document, without any reference to the semantic content. The algorithm employs a machine learning approach, whereby the system is first provided with a set of training documents in which the target fields are manually tagged and automatically learns how to extract these fields in future documents. We implemented the algorithm in a system for automatic analysis of documents in PDF format. We present experimental results of applying the system on a set of financial documents, extracting nine different target fields. Overall, the system achieved a 90% accuracy."
            },
            "slug": "Visual-information-extraction-Aumann-Feldman",
            "title": {
                "fragments": [],
                "text": "Visual information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An algorithm is presented that allows to automatically extract specific fields of a document based exclusively on the visual formatting of the document, without any reference to the semantic content, based on a machine learning approach."
            },
            "venue": {
                "fragments": [],
                "text": "Knowledge and Information Systems"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055652031"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "The decoder shown in Figure 3 consists of Union layer, BiLSTM [30] layer and CRF [31] layer for key information extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "The decoder is composed of BiLSTM [30] and CRF [31] layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9594328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "685d42a668413422615519a52ac75d66fded4611",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we apply bidirectional training to a long short term memory (LSTM) network for the first time. We also present a modified, full gradient version of the LSTM learning algorithm. We discuss the significance of framewise phoneme classification to continuous speech recognition, and the validity of using bidirectional networks for online causal tasks. On the TIMIT speech database, we measure the framewise phoneme classification scores of bidirectional and unidirectional variants of both LSTM and conventional recurrent neural networks (RNNs). We find that bidirectional LSTM outperforms both RNNs and unidirectional LSTM."
            },
            "slug": "Framewise-phoneme-classification-with-bidirectional-Graves-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Framewise phoneme classification with bidirectional LSTM networks"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is found that bidirectional LSTM outperforms both RNNs and unidirectionalLSTM, and the significance of framewise phoneme classification to continuous speech recognition and the validity of usingbidirectional networks for online causal tasks is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041815"
                        ],
                        "name": "Kumutha Swampillai",
                        "slug": "Kumutha-Swampillai",
                        "structuredName": {
                            "firstName": "Kumutha",
                            "lastName": "Swampillai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kumutha Swampillai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144795097"
                        ],
                        "name": "Mark Stevenson",
                        "slug": "Mark-Stevenson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Stevenson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Besides, a handful of other methods [16], [17], [18] make full use of features to support extraction tasks based on human-designed features or task-specific knowledge, which are not extensible on other documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14846155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efbb63f3cb13cf7b6fc6ec5e6c4129be82c919cc",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on relation extraction has focussed on identifying relationships between entities that occur in the same sentence (intra-sentential relations) rather than between entities in different sentences (inter-sentential relations) despite previous research having shown that intersentential relations commonly occur in information extraction corpora. This paper describes a SVM-based approach to relation extraction that is applied to both types. Adapted features and techniques for counter-acting bias in SVM models are used to deal with specific issues that arise in the inter-sentential case. It was found that the structured features used for intrasentential relation extraction can be easily adapted for the inter-sentential case and provides comparable performance."
            },
            "slug": "Extracting-Relations-Within-and-Across-Sentences-Swampillai-Stevenson",
            "title": {
                "fragments": [],
                "text": "Extracting Relations Within and Across Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It was found that the structured features used for intrasentential relation extraction can be easily adapted for the inter-sentential case and provides comparable performance."
            },
            "venue": {
                "fragments": [],
                "text": "RANLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "The decoder shown in Figure 3 consists of Union layer, BiLSTM [30] layer and CRF [31] layer for key information extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "The decoder is composed of BiLSTM [30] and CRF [31] layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13409,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143667674"
                        ],
                        "name": "J. Veenstra",
                        "slug": "J.-Veenstra",
                        "structuredName": {
                            "firstName": "Jorn",
                            "lastName": "Veenstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veenstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "When we get bbox and the corresponding entity type and transcripts of bbox, we enumerate all transcripts of the bbox and convert it to IOB format [27] used to minimize CRF loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": ", y (i) T ) sequentially using the IOB (Inside, Outside, Begin) tagging scheme [27], where T is the length of sentence si."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1845735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "008a2291a257072f22764196a3acf0a394bf203a",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a \"convenient\" data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the the data representation choice has a minor influence on chunking performance. However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set."
            },
            "slug": "Representing-Text-Chunks-Sang-Veenstra",
            "title": {
                "fragments": [],
                "text": "Representing Text Chunks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the the data representation choice has a minor influence on chunking performance, however, equipped with the most suitable data representation, the memory-based learning chunker was able to improve the best published chunking results for a standard data set."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 2019 Competition on Scanned Receipt OCR and Information Extraction , \u201d in 2019 International Conference on Document Analysis and Recognition ( ICDAR )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 2019 Competition on Scanned Receipt OCR and Information Extraction , \u201d in 2019 International Conference on Document Analysis and Recognition ( ICDAR )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 14,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/PICK:-Processing-Key-Information-Extraction-from-Yu-Lu/ba5ed98c4546fada5c732bced4a1c1615f1a4c16?sort=total-citations"
}