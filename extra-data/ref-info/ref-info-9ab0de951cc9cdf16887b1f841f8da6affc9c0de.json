{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69539592"
                        ],
                        "name": "J. Stallkamp",
                        "slug": "J.-Stallkamp",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Stallkamp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stallkamp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2502317"
                        ],
                        "name": "Marc Schlipsing",
                        "slug": "Marc-Schlipsing",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Schlipsing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Schlipsing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2743486"
                        ],
                        "name": "J. Salmen",
                        "slug": "J.-Salmen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Salmen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Salmen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748824"
                        ],
                        "name": "C. Igel",
                        "slug": "C.-Igel",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Igel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Igel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Experiments conducted after phase 1 produced a new record of 99.17% by increasing the network capacity, and by using greyscale images instead of color."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15926837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22fe619996b59c09cb73be40103a123d2e328111",
            "isKey": true,
            "numCitedBy": 685,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The \u201cGerman Traffic Sign Recognition Benchmark\u201d is a multi-category classification competition held at IJCNN 2011. Automatic recognition of traffic signs is required in advanced driver assistance systems and constitutes a challenging real-world computer vision and pattern recognition problem. A comprehensive, lifelike dataset of more than 50,000 traffic sign images has been collected. It reflects the strong variations in visual appearance of signs due to distance, illumination, weather conditions, partial occlusions, and rotations. The images are complemented by several precomputed feature sets to allow for applying machine learning algorithms without background knowledge in image processing. The dataset comprises 43 classes with unbalanced class frequencies. Participants have to classify two test sets of more than 12,500 images each. Here, the results on the first of these sets, which was used in the first evaluation stage of the two-fold challenge, are reported. The methods employed by the participants who achieved the best results are briefly described and compared to human traffic sign recognition performance and baseline results."
            },
            "slug": "The-German-Traffic-Sign-Recognition-Benchmark:-A-Stallkamp-Schlipsing",
            "title": {
                "fragments": [],
                "text": "The German Traffic Sign Recognition Benchmark: A multi-class classification competition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The \u201cGerman Traffic Sign Recognition Benchmark\u201d is a multi-category classification competition held at IJCNN 2011, and a comprehensive, lifelike dataset of more than 50,000 traffic sign images has been collected."
            },
            "venue": {
                "fragments": [],
                "text": "The 2011 International Joint Conference on Neural Networks"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398957464"
                        ],
                        "name": "S. Lafuente-Arroyo",
                        "slug": "S.-Lafuente-Arroyo",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Lafuente-Arroyo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lafuente-Arroyo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398957456"
                        ],
                        "name": "P. Gil-Jim\u00e9nez",
                        "slug": "P.-Gil-Jim\u00e9nez",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Gil-Jim\u00e9nez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gil-Jim\u00e9nez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1448060325"
                        ],
                        "name": "R. Maldonado-Bascon",
                        "slug": "R.-Maldonado-Bascon",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Maldonado-Bascon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Maldonado-Bascon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398470120"
                        ],
                        "name": "F. L\u00f3pez-Ferreras",
                        "slug": "F.-L\u00f3pez-Ferreras",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "L\u00f3pez-Ferreras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. L\u00f3pez-Ferreras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398470117"
                        ],
                        "name": "S. Maldonado-Basc\u00f3n",
                        "slug": "S.-Maldonado-Basc\u00f3n",
                        "structuredName": {
                            "firstName": "Saturnino",
                            "lastName": "Maldonado-Basc\u00f3n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maldonado-Basc\u00f3n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17114200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47f5c8d63065a1d438232fa6fb7dc254a7690c7c",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with the detection and classification of traffic signs in outdoor environments. The information provided by traffic signs on roads is very important for the safety of drivers. However, in these situations the illumination conditions can not be predicted, the position and the orientation of signs in the scene are not known and other objects can block the vision of them. For these reasons we have developed an extensive test set which includes all kind of signs. In an artificial vision system, the key to recognize traffic signs is how to detect them and identify their geometric shapes. So, in this work we propose a method that uses a technique based on support vector machines (SVMs) for the classification. The patterns generated by the vectors represent the distances to borders (DtB) of the objects candidate to be traffic signs. Experimental results show the effectiveness of the proposed method."
            },
            "slug": "Traffic-sign-shape-classification-evaluation-I:-SVM-Lafuente-Arroyo-Gil-Jim\u00e9nez",
            "title": {
                "fragments": [],
                "text": "Traffic sign shape classification evaluation I: SVM using distance to borders"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work proposes a method that uses a technique based on support vector machines (SVMs) for the classification of traffic signs in outdoor environments and results show the effectiveness of the proposed method."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Proceedings. Intelligent Vehicles Symposium, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077257730"
                        ],
                        "name": "Kevin Jarrett",
                        "slug": "Kevin-Jarrett",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Jarrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Jarrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 148
                            }
                        ],
                        "text": "However more sophisticated non-linear modules have recently been shown to yield higher accuracy, particularly in the small training set size regime [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f88427d7aa8225e47f946ac41a0667d7b69ac52",
            "isKey": false,
            "numCitedBy": 2084,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%)."
            },
            "slug": "What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "What is the best multi-stage architecture for object recognition?"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks and that two stages of feature extraction yield better accuracy than one."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35242,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1897550"
                        ],
                        "name": "P. Pacl\u00edk",
                        "slug": "P.-Pacl\u00edk",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pacl\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pacl\u00edk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687265"
                        ],
                        "name": "J. Novovicov\u00e1",
                        "slug": "J.-Novovicov\u00e1",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Novovicov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Novovicov\u00e1"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "The poorest performing network (#27) uses the traditional single-scale (\u201css\u201d) feature architecture while other networks use multi-scale (\u201cms\u201d) features by feeding their first and second stage features to the classifier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17091348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d1f3078208fb101e66d54765f86aeb8d606678",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The robust and general method for the recognition of traffic devices like road signs in traffic scene images is necessary for the creation of Driver Support System. Color may be used as a useful attribute for the decomposition of classification problem into several apriori defined road sign groups/subproblems. In this paper, the colorless method for the road sign classification is presented working on gray-level images and allowing the same problem decomposition as its color-based counterpart. The method may be used in combination with the color-independent sign detection algorithms. The road sign recognition system then works entirely without the color which may be used as an alternative procedure when the input traffic scene images lacks good color information."
            },
            "slug": "Road-Sign-Classification-without-Color-Information-Pacl\u00edk-Novovicov\u00e1",
            "title": {
                "fragments": [],
                "text": "Road Sign Classification without Color Information"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The colorless method for the road sign classification is presented working on gray-level images and allowing the same problem decomposition as its color-based counterpart."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209503"
                        ],
                        "name": "Jialue Fan",
                        "slug": "Jialue-Fan",
                        "structuredName": {
                            "firstName": "Jialue",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jialue Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143836295"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50118130"
                        ],
                        "name": "Ying Wu",
                        "slug": "Ying-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Contrary to [12], we use the output of the first stage after pooling/subsampling rather than before."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10112350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b4b43f10c5779d67ccee15d8d0be10ed036971b",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we treat tracking as a learning problem of estimating the location and the scale of an object given its previous location, scale, as well as current and previous image frames. Given a set of examples, we train convolutional neural networks (CNNs) to perform the above estimation task. Different from other learning methods, the CNNs learn both spatial and temporal features jointly from image pairs of two adjacent frames. We introduce multiple path ways in CNN to better fuse local and global information. A creative shift-variant CNN architecture is designed so as to alleviate the drift problem when the distracting objects are similar to the target in cluttered environment. Furthermore, we employ CNNs to estimate the scale through the accurate localization of some key points. These techniques are object-independent so that the proposed method can be applied to track other types of object. The capability of the tracker of handling complex situations is demonstrated in many testing sequences."
            },
            "slug": "Human-Tracking-Using-Convolutional-Neural-Networks-Fan-Xu",
            "title": {
                "fragments": [],
                "text": "Human Tracking Using Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper treats tracking as a learning problem of estimating the location and the scale of an object given its previous location, scale, as well as current and previous image frames, and introduces multiple path ways in CNN to better fuse local and global information."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2912480"
                        ],
                        "name": "Y. Nguwi",
                        "slug": "Y.-Nguwi",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Nguwi",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nguwi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089801"
                        ],
                        "name": "A. Kouzani",
                        "slug": "A.-Kouzani",
                        "structuredName": {
                            "firstName": "Abbas",
                            "lastName": "Kouzani",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kouzani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22352552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "559302b64d41868ba7cca91eaedfdb3bfada8592",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic road sign recognition system first locates road signs within images captured by an imaging sensor on-board of a vehicle, and then identifies the detected road signs. This paper presents an automatic neural-network-based road sign recognition system. First, a study of the existing road sign recognition research is presented. In this study, the issues associated with automatic road sign recognition are described, the existing methods developed to tackle the road sign recognition problem are reviewed, and a comparison of the features of these methods is given. Second, the developed road sign recognition system is described. The system is capable of analysing live colour road scene images, detecting multiple road signs within each image, and classifying the type of road signs detected. The system consists of two modules: detection and classification. The detection module segments the input image in the hue-saturation-intensity colour space, and then detects road signs using a Multi-layer Perceptron neural-network. The classification module determines the type of detected road signs using a series of one to one architectural Multi-layer Perceptron neural networks. Two sets of classifiers are trained using the Resillient-Backpropagation and Scaled-Conjugate-Gradient algorithms. The two modules of the system are evaluated individually first. Then the system is tested as a whole. The experimental results demonstrate that the system is capable of achieving an average recognition hit-rate of 95.96% using the scaled-conjugate-gradient trained classifiers."
            },
            "slug": "Detection-and-classification-of-road-signs-in-Nguwi-Kouzani",
            "title": {
                "fragments": [],
                "text": "Detection and classification of road signs in natural environments"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The issues associated with automatic road sign recognition are described, the existing methods developed to tackle the road sign Recognition problem are reviewed, and a comparison of the features of these methods is given."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computing and Applications"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686515"
                        ],
                        "name": "A. Escalera",
                        "slug": "A.-Escalera",
                        "structuredName": {
                            "firstName": "Arturo",
                            "lastName": "Escalera",
                            "middleNames": [
                                "de",
                                "la"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Escalera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144403161"
                        ],
                        "name": "L. Moreno",
                        "slug": "L.-Moreno",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Moreno",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Moreno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1853452"
                        ],
                        "name": "M. Salichs",
                        "slug": "M.-Salichs",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Salichs",
                            "middleNames": [
                                "Angel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Salichs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49837693"
                        ],
                        "name": "J. M. Armingol",
                        "slug": "J.-M.-Armingol",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Armingol",
                            "middleNames": [
                                "Mar\u00eda"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Armingol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17840216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0a96911c706bd7ffdcb030424fa1453118d6cdf",
            "isKey": false,
            "numCitedBy": 552,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A vision-based vehicle guidance system for road vehicles can have three main roles: (1) road detection; (2) obstacle detection; and (3) sign recognition. The first two have been studied for many years and with many good results, but traffic sign recognition is a less-studied field. Traffic signs provide drivers with very valuable information about the road, in order to make driving safer and easier. The authors think that traffic signs most play the same role for autonomous vehicles. They are designed to be easily recognized by human drivers mainly because their color and shapes are very different from natural environments. The algorithm described in this paper takes advantage of these features. It has two main parts. The first one, for the detection, uses color thresholding to segment the image and shape analysis to detect the signs. The second one, for the classification, uses a neural network. Some results from natural scenes are shown."
            },
            "slug": "Road-traffic-sign-detection-and-classification-Escalera-Moreno",
            "title": {
                "fragments": [],
                "text": "Road traffic sign detection and classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The algorithm described in this paper takes advantage of color thresholding to segment the image and shape analysis to detect the signs and uses a neural network for the classification."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Ind. Electron."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143842692"
                        ],
                        "name": "Nick Barnes",
                        "slug": "Nick-Barnes",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Barnes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nick Barnes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143772180"
                        ],
                        "name": "A. Zelinsky",
                        "slug": "A.-Zelinsky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zelinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zelinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102983910"
                        ],
                        "name": "L. Fletcher",
                        "slug": "L.-Fletcher",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fletcher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12005659,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76b739826a1e3ad3300f58f806f0a78966e74fcc",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for classifying road signs have a high computational cost per pixel processed. A detection stage that has a lower computational cost can facilitate real-time processing. Various authors have used shape and color-based detectors. Shape-based detectors have an advantage under variable lighting conditions and sign deterioration that, although the apparent color may change, the shape is preserved. In this paper, we present the radial symmetry detector for detecting speed signs. We evaluate the detector itself in a system that is mounted within a road vehicle. We also evaluate its performance that is integrated with classification over a series of sequences from roads around Canberra and demonstrate it while running online in our road vehicle. We show that it can detect signs with high reliability in real time. We examine the internal parameters of the algorithm to adapt it to road sign detection. We demonstrate the stability of the system under the variation of these parameters and show computational speed gains through their tuning. The detector is demonstrated to work under a wide variety of visual conditions."
            },
            "slug": "Real-Time-Speed-Sign-Detection-Using-the-Radial-Barnes-Zelinsky",
            "title": {
                "fragments": [],
                "text": "Real-Time Speed Sign Detection Using the Radial Symmetry Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents the radial symmetry detector for detecting speed signs, and shows that it can detect signs with high reliability in real time and demonstrate the stability of the system under the variation of these parameters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Intelligent Transportation Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31778202"
                        ],
                        "name": "C. G. Keller",
                        "slug": "C.-G.-Keller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Keller",
                            "middleNames": [
                                "Gustav"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704594"
                        ],
                        "name": "C. Sprunk",
                        "slug": "C.-Sprunk",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Sprunk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sprunk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96619180"
                        ],
                        "name": "C. Bahlmann",
                        "slug": "C.-Bahlmann",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Bahlmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bahlmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4837721"
                        ],
                        "name": "J. Giebel",
                        "slug": "J.-Giebel",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Giebel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Giebel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800882"
                        ],
                        "name": "G. Baratoff",
                        "slug": "G.-Baratoff",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Baratoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baratoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15752127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e34083ee15dc10da522b17cd2d6a7bb5996c517",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a camera-based system for detection, tracking, and classification of U.S. speed signs is presented. The implemented application uses multiple connected stages and iteratively reduces the number of pixels to process for recognition. Possible sign locations are detected using a fast, shape-based interest operator. Remaining objects other than speed signs are discarded using a classifier similar to the Viola-Jones detector. Classification results from tracked candidates are utilized to improve recognition accuracy. On a standard PC the system reached a detection speed of 27 fps with an accuracy of 98.8%. Including classification, speed sign recognition rates of 96.3% were achieved with a frame rate of approximately 11 fps and one false alarm every 42 s."
            },
            "slug": "Real-time-recognition-of-U.S.-speed-signs-Keller-Sprunk",
            "title": {
                "fragments": [],
                "text": "Real-time recognition of U.S. speed signs"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A camera-based system for detection, tracking, and classification of U.S. speed signs using multiple connected stages and iteratively reduces the number of pixels to process for recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Intelligent Vehicles Symposium"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940285"
                        ],
                        "name": "B. Martini",
                        "slug": "B.-Martini",
                        "structuredName": {
                            "firstName": "Berin",
                            "lastName": "Martini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Martini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2447628"
                        ],
                        "name": "Polina Akselrod",
                        "slug": "Polina-Akselrod",
                        "structuredName": {
                            "firstName": "Polina",
                            "lastName": "Akselrod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Polina Akselrod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39576799"
                        ],
                        "name": "S. Talay",
                        "slug": "S.-Talay",
                        "structuredName": {
                            "firstName": "Sel\u00e7uk",
                            "lastName": "Talay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Talay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889774"
                        ],
                        "name": "E. Culurciello",
                        "slug": "E.-Culurciello",
                        "structuredName": {
                            "firstName": "Eugenio",
                            "lastName": "Culurciello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Culurciello"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 112
                            }
                        ],
                        "text": "Although signs are available as video sequences in the training set, temporal information is not in the test set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6542026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3c82b476162d2d006e02180530875a64af18154",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a scalable hardware architecture to implement large-scale convolutional neural networks and state-of-the-art multi-layered artificial vision systems. This system is fully digital and is a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images. We present a performance comparison between a software, FPGA and ASIC implementation that shows a speed up in custom hardware implementations."
            },
            "slug": "Hardware-accelerated-convolutional-neural-networks-Farabet-Martini",
            "title": {
                "fragments": [],
                "text": "Hardware accelerated convolutional neural networks for synthetic vision systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This system is fully digital and is a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30017846"
                        ],
                        "name": "N. Pinto",
                        "slug": "N.-Pinto",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "The local normalization operations are inspired by visual neuroscience models [13],\n[14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5955557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "688b6fbc3c5c06e254961f70de9d855d3d008d09",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, \u201cnatural\u201d images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled \u201cnatural\u201d images in guiding that progress. In particular, we show that a simple V1-like model\u2014a neuroscientist's \u201cnull\u201d model, which should perform poorly at real-world visual object recognition tasks\u2014outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a \u201csimpler\u201d recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition\u2014real-world image variation."
            },
            "slug": "Why-is-Real-World-Visual-Object-Recognition-Hard-Pinto-Cox",
            "title": {
                "fragments": [],
                "text": "Why is Real-World Visual Object Recognition Hard?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that a simple V1-like model\u2014a neuroscientist's \u201cnull\u201d model\u2014outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794837"
                        ],
                        "name": "Siwei Lyu",
                        "slug": "Siwei-Lyu",
                        "structuredName": {
                            "firstName": "Siwei",
                            "lastName": "Lyu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siwei Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "The local normalization operations are inspired by visual neuroscience models [13],\n[14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7415524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25d92fe0ab92ae57bd8fed87c95fa9a207ec61aa",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a nonlinear image representation based on divisive normalization that is designed to match the statistical properties of photographic images, as well as the perceptual sensitivity of biological visual systems. We decompose an image using a multi-scale oriented representation, and use studentpsilas t as a model of the dependencies within local clusters of coefficients. We then show that normalization of each coefficient by the square root of a linear combination of the amplitudes of the coefficients in the cluster reduces statistical dependencies. We further show that the resulting divisive normalization transform is invertible and provide an efficient iterative inversion algorithm. Finally, we probe the statistical and perceptual advantages of this image representation by examining its robustness to added noise, and using it to enhance image contrast."
            },
            "slug": "Nonlinear-image-representation-using-divisive-Lyu-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Nonlinear image representation using divisive normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper decomposes an image using a multi-scale oriented representation, and uses studentpsilas t as a model of the dependencies within local clusters of coefficients to show that normalization of each coefficient by the square root of a linear combination of the amplitudes of the coefficients in the cluster reduces statistical dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "It is interesting to note that the top 13 systems all use ConvNets with at least 98.10% accuracy and that human performance (98.81"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15064817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78e5bca056ffc6186400ba540a0c0f43df909a12",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Energy-based learning (EBL) is a general framework to describe supervised and unsupervised training methods for probabilistic and non-probabilistic factor graphs. An energy-based model associates a scalar energy to configurations of inputs, outputs, and latent variables. Learning machines can be constructed by assembling modules and loss functions. Gradient-based learning procedures are easily implemented through semi-automatic differentiation of complex models constructed by assembling predefined modules. We introduce an open-source and cross-platform C++ library called EBLearn to enable the construction of energy-based learning models. EBLearn is composed of two major components, libidx: an efficient and flexible multi-dimensional tensor library, and libeblearn: an object-oriented library of trainable modules and learning algorithms. The latter has facilities for such models as convolutional networks, as well as for image processing. It also provides graphical display functions."
            },
            "slug": "EBLearn:-Open-Source-Energy-Based-Learning-in-C++-Sermanet-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "EBLearn: Open-Source Energy-Based Learning in C++"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An open-source and cross-platform C++ library called EBLearn is introduced to enable the construction of energy-based learning models and is composed of two major components, libidx: an efficient and flexible multi-dimensional tensor library, and libeblearn: an object-oriented library of trainable modules and learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2009 21st IEEE International Conference on Tools with Artificial Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70019186"
                        ],
                        "name": "J. T\u00f8rresen",
                        "slug": "J.-T\u00f8rresen",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "T\u00f8rresen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. T\u00f8rresen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122154698"
                        ],
                        "name": "J. Bakke",
                        "slug": "J.-Bakke",
                        "structuredName": {
                            "firstName": "J.W.",
                            "lastName": "Bakke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bakke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682273"
                        ],
                        "name": "L. Sekanina",
                        "slug": "L.-Sekanina",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Sekanina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sekanina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6736889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "806965039d43a2512ad4dbf7cfcaec2272e40c9c",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic traffic sign detection system is important in a driver assistance system. An approach for detecting Norwegian speed limit signs is proposed. It consists of three major steps: color-based filtering, locating sign(s) in an image and detection of numbers on the sign. About 91% correct recognition is achieved for a selection of 198 images."
            },
            "slug": "Efficient-recognition-of-speed-limit-signs-T\u00f8rresen-Bakke",
            "title": {
                "fragments": [],
                "text": "Efficient recognition of speed limit signs"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An automatic traffic sign detection system is important in a driver assistance system and consists of three major steps: color-based filtering, locating sign(s) in an image and detection of numbers on the sign."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. The 7th International IEEE Conference on Intelligent Transportation Systems (IEEE Cat. No.04TH8749)"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "1) Validation: Traffic sign examples in the GTSRB dataset were extracted from 1-second video sequences, i.e. each real-world instance yields 30 samples with usually increasing resolution as the camera is approaching the sign."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adv. Neural Information Processing Systems (NIPS*10), Workshop on Deep Learning and Unsupervised Feature Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Neural Information Processing Systems (NIPS*10), Workshop on Deep Learning and Unsupervised Feature Learning"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Traffic-sign-recognition-with-multi-scale-Networks-Sermanet-LeCun/9ab0de951cc9cdf16887b1f841f8da6affc9c0de?sort=total-citations"
}