{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426224"
                        ],
                        "name": "E. Domany",
                        "slug": "E.-Domany",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Domany",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Domany"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69363504"
                        ],
                        "name": "J. Hammen",
                        "slug": "J.-Hammen",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hammen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hammen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144340430"
                        ],
                        "name": "K. Schulten",
                        "slug": "K.-Schulten",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schulten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schulten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60552860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f93748343ed57baa9ed93d1d88e18bb2845bc37",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This work aims to address the requirement for coverage of this multidisciplinary and rapidly changing field of research. It begins with an introduction to the central theme of the book, collective phenomena in neural networks, which is applied in subsequent chapters to the specific areas of dynamics and storage capacity of networks of formal neurons with symmetric or asymmetric couplings, learning algorithms, temporal association, structured data (software) and structural nets (hardware). This textbook on physics, computer science, artificial neuroscience, psychology, cognitive science and applied mathematics is intended for graduate students and researchers."
            },
            "slug": "Models-of-Neural-Networks-I-Domany-Hammen",
            "title": {
                "fragments": [],
                "text": "Models of Neural Networks I"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work begins with an introduction to the central theme of the book, collective phenomena in neural networks, which is applied in subsequent chapters to the specific areas of dynamics and storage capacity of networks of formal neurons with symmetric or asymmetric couplings."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197258"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153856933"
                        ],
                        "name": "M. Brown",
                        "slug": "M.-Brown",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863326"
                        ],
                        "name": "I. Mian",
                        "slug": "I.-Mian",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Mian",
                            "middleNames": [
                                "Saira"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5233893"
                        ],
                        "name": "K. Sj\u00f6lander",
                        "slug": "K.-Sj\u00f6lander",
                        "structuredName": {
                            "firstName": "Kimmen",
                            "lastName": "Sj\u00f6lander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sj\u00f6lander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2160404,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "5d28fc1a4027d23cc9e4ad8555361d48940e9be8",
            "isKey": false,
            "numCitedBy": 2003,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMMs) are applied to the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated on the globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the SWISS-PROT 22 database for other sequences that are members of the given protein family, or contain the given domain. The HMM produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate three-dimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appears to have a slight advantage over PROFILESEARCH in terms of lower rates of false negatives and false positives, even though the HMM is trained using only unaligned sequences, whereas PROFILESEARCH requires aligned training sequences. Our results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the alpha-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling. This region has been suggested to contain the functional domains that are typical or essential for all L-type calcium channels regardless of whether they couple to ryanodine receptors, conduct ions or both."
            },
            "slug": "Hidden-Markov-models-in-computational-biology.-to-Krogh-Brown",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models in computational biology. Applications to protein modeling."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the alpha-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708077"
                        ],
                        "name": "S. Eddy",
                        "slug": "S.-Eddy",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Eddy",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187514"
                        ],
                        "name": "R. Durbin",
                        "slug": "R.-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Durbin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15848882,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f1852e23a96bb49800543d686d97f64df07d5d2b",
            "isKey": false,
            "numCitedBy": 841,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a general approach to several RNA sequence analysis problems using probabilistic models that flexibly describe the secondary structure and primary sequence consensus of an RNA sequence family. We call these models 'covariance models'. A covariance model of tRNA sequences is an extremely sensitive and discriminative tool for searching for additional tRNAs and tRNA-related sequences in sequence databases. A model can be built automatically from an existing sequence alignment. We also describe an algorithm for learning a model and hence a consensus secondary structure from initially unaligned example sequences and no prior structural information. Models trained on unaligned tRNA examples correctly predict tRNA secondary structure and produce high-quality multiple alignments. The approach may be applied to any family of small RNA sequences."
            },
            "slug": "RNA-sequence-analysis-using-covariance-models.-Eddy-Durbin",
            "title": {
                "fragments": [],
                "text": "RNA sequence analysis using covariance models."
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work describes a general approach to several RNA sequence analysis problems using probabilistic models that flexibly describe the secondary structure and primary sequence consensus of an RNA sequence family, called 'covariance models'."
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic acids research"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49393083"
                        ],
                        "name": "B. Everitt",
                        "slug": "B.-Everitt",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Everitt",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Everitt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122093866,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "345d4daa63202fedd6311ae295b298e216291af3",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 General introduction.- 1.1 Introduction.- 1.2 Latent variables and latent variable models.- 1.3 The role of models.- 1.4 The general latent model.- 1.5 A simple latent variable model.- 1.6 Estimation and goodness-of-fit.- 1.7 Path diagrams.- 1.8 Summary.- 2 Factor analysis.- 2.1 Introduction.- 2.2 Explanatory and confirmatory factor analysis.- 2.3 The factor analysis model.- 2.4 Identifiability of the factor analysis model.- 2.5 Estimating the parameters in the factor analysis model.- 2.6 Goodness-of-fit tests.- 2.7 Rotation of factors.- 2.8 Numerical examples.- 2.9 Confirmatory factor analysis.- 2.10 Summary.- 3 The LISREL model.- 3.1 Introduction.- 3.2 The LISREL model.- 3.3 Identification.- 3.4 Estimating the parameters in the LISREL model.- 3.5 Instrumental variables.- 3.6 Numerical examples.- 3.7 Assessing goodness-of-fit.- 3.8 Multigroup analysis.- 3.9 Summary.- 4 Latent variable models for categorical data.- 4.1 Introduction.- 4.2 Factor analysis of binary variables.- 4.3 Latent structure models.- 4.4 Summary.- 5 Some final comments.- 5.1 Introduction.- 5.2 Assessing the fit of latent variable models by cross-validation procedures.- 5.3 Latent variables - fact or fiction?.- 5.4 Summary.- Appendix A Estimating the parameters in latent variable models a brief account of computational procedures.- Appendix B Computer programs for latent variable models.- Exercises.- References."
            },
            "slug": "An-Introduction-to-Latent-Variable-Models-Everitt",
            "title": {
                "fragments": [],
                "text": "An Introduction to Latent Variable Models"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Assessment of the fit of latent variable models by cross-validation procedures by estimating the parameters in latent variable model a brief account of computational procedures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 5,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Bayesian-neural-networks-and-density-networks-Mackay/f2861a5e59de4d61bcd8a9ae4785978ac11fc9c1?sort=total-citations"
}