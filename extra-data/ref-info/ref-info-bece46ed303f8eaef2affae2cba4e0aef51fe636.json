{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36840465"
                        ],
                        "name": "J. Yamron",
                        "slug": "J.-Yamron",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yamron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yamron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721219"
                        ],
                        "name": "I. Carp",
                        "slug": "I.-Carp",
                        "structuredName": {
                            "firstName": "Ira",
                            "lastName": "Carp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Carp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721830"
                        ],
                        "name": "L. Gillick",
                        "slug": "L.-Gillick",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Gillick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gillick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38332678"
                        ],
                        "name": "S. Lowe",
                        "slug": "S.-Lowe",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lowe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686679"
                        ],
                        "name": "P. V. Mulbregt",
                        "slug": "P.-V.-Mulbregt",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Mulbregt",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. V. Mulbregt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Hidden Markov models (HMMs) are a powerful tool for representing sequential data, and have been applied with significant success to many text-related tasks, including part-of-spe ech tagging (Kupiec, 1992), text segmentation and event tracking ( Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998 ), named entity recognition (Bikel, Schwartz, & Weischedel, 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18454274,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee276a682f0ee2e01a61265c5e92b8d8d89e4de2",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuing progress in the automatic transcription of broadcast speech via speech recognition has raised the possibility of applying information retrieval techniques to the resulting (errorful) text. For these techniques to be easily applicable, it is highly desirable that the transcripts be segmented into stories. This paper introduces a general methodology based on HMMs and on classical language modeling techniques for automatically inferring story boundaries and for retrieving stories relating to a specific event. In this preliminary work, we report some highly promising results on accurate text. Future work will apply these techniques to errorful transcripts."
            },
            "slug": "A-hidden-Markov-model-approach-to-text-segmentation-Yamron-Carp",
            "title": {
                "fragments": [],
                "text": "A hidden Markov model approach to text segmentation and event tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A general methodology based on HMMs and on classical language modeling techniques for automatically inferring story boundaries and for retrieving stories relating to a specific event is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 99
                            }
                        ],
                        "text": "HMMs have also been successful in similar naturallanguage tasks, including part-of-speech tagging (Kupiec, 1992), named-entity recognition (Bikel et al., 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 191
                            }
                        ],
                        "text": "Hidden Markov models (HMMs) are a powerful tool for representing sequential data, and have been applied with significant success to many text-related tasks, including part-of-spe ech tagging (Kupiec, 1992), text segmentation and event tracking (Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998), named entity recognition (Bikel, Schwartz, & Weischedel, 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "\u2026tool for representing sequential data, and have been applied with significant success to many text-related tasks, including part-of-speech tagging (Kupiec, 1992), text segmentation and event tracking (Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998), named entity recognition (Bikel, Schwartz, &\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62680996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "352dbd26580856ba4b9877d43aeba304343af66d",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-part-of-speech-tagging-using-a-hidden-Markov-Kupiec",
            "title": {
                "fragments": [],
                "text": "Robust part-of-speech tagging using a hidden Markov model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789644"
                        ],
                        "name": "T. Leek",
                        "slug": "T.-Leek",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Leek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 188
                            }
                        ],
                        "text": "HMMs have also been successful in similar naturallanguage tasks, including part-of-speech tagging (Kupiec, 1992), named-entity recognition (Bikel et al., 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 259
                            }
                        ],
                        "text": "\u2026to many text-related tasks, including part-of-speech tagging (Kupiec, 1992), text segmentation and event tracking (Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998), named entity recognition (Bikel, Schwartz, & Weischedel, 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59798638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c11fb8460b4e2e8cf76cc3abe1dc3eaf153b67d9",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "OF THE THESIS Information Extraction Using Hidden Markov Models by Timothy Robert Leek Master of Science in Computer Science University of California, San Diego, 1997 Professor Charles Peter Elkan, Chair This thesis shows how to design and tune a hidden Markov model to extract factual information from a corpus of machine-readable English prose. In particular, the thesis presents a HMM that classi es and parses natural language assertions about genes being located at particular positions on chromosomes. The facts extracted by this HMM can be inserted into biological databases. The HMM is trained on a small set of sentence fragments chosen from the collected scienti c abstracts in the OMIM (On-Line Mendelian Inheritance in Man) database and judged to contain the target binary relationship between gene names and gene locations. Given a novel sentence, all contiguous fragments are ranked by log-odds score, i.e. the log of the ratio of the probability of the fragment according to the target HMM to that according to a \\null\" HMM trained on all OMIM sentences. The most probable path through the HMM gives bindings for the annotations with precision as high as 80%. In contrast with traditional natural language processing methods, this stochastic approach makes no use either of part-of-speech taggers or dictionaries, instead employing non-emitting states to assemble modules roughly corresponding to noun, verb, and prepostional phrases. Algorithms for reestimating parameters for HMMs with non-emitting states are presented in detail. The ability to tolerate new words and recognize a wide variety of syntactic forms arises from the judicious use of \\gap\" states. v Chapter I Good Facts Are Hard to Find Finding facts in English prose is a task that humans are good at and computers are bad at. However, humans cannot stand to spend more than a few minutes at a time occupied with such drudgery. In this respect, nding facts is unlike a host of the other jobs computers are currently hopeless at, like telling a joke, riding a bike, and cooking a dinner. While there is no pressing need for computers to be good at those things, it is already of paramount importance that computers be pro cient at nding information with precision in the proliferating archives of electronic text available on the Internet and elsewhere. The state of the art in information retrieval technology is of limited use in this application. Standard boolean searching, vectorbased approaches and latent semantic indexing are geared more toward open-ended exploration than toward the targeted, detailed subsentence processing necessary for the fact nding or information extraction task. Since these approaches discard syntax, a large class of targets, in which the relationships between groups of words are important, must be fundamentally beyond them. The critical noun and verb groups of a fact can only be found by doing some kind of parsing. Information extraction is in most cases what people really want to do when they rst set about searching text, i.e. before they lower their sights to correspond to available tools. But this does not mean that nothing less than full-blown NLP (natural language processing) will satisfy. There are many real-world text searching 1 2 tasks that absolutely require syntactic information and yet are restricted enough to be tractable. An historian might want to locate passages in the Virginia colony records mentioning the \\event\" of a slave running away. The words slave, run, and away, all very common words, and their various synonyms used in an unconstrained search would return much dross. To nd this fact with precision we need to place constraints upon the arrangement of the words in the sentence; we need to limit the search with syntax. For instance, one might require that when two groups of words corresponding to slave and run appear in a sentence, that the slave is in fact the one doing the running. Similar examples of what we call fact searching are commonplace in most domains. A market analyst might want to scan the Wall Street Journal and pick out all mentions of corporate management changes. And a geneticist would be thrilled to be able to tease out of scienti c abstracts facts mapping genes to speci c locations on chromosomes. Historically, the eld of information extraction has employed discrete manipulations in order to process sentences into the critical noun and verb groups. An incoming sentence is tagged for part-of-speech and then handed o to a scaled-down parser or DFA (deterministic nite automaton) which uses local syntax to decide if the elements of a fact are present and to divide the sentence up into logical elements. Recent advances in statistical natural language processing have been applied to this problem but typically only in an ancillary role, e.g. in constructing dictionaries [17] and tagging words for part-of-speech [4]. The main processing engine remains combinatorial in avor. Systems like FASTUS [8] and CIRCUS [14] do surprisingly well, considering the di culty of the task, achieving precision and recall of better than 80%. But they require hand-built grammars or dictionaries of extraction patterns in order to attain this level of performance. A notable exception is the LIEP [9] system which learns to generalize extraction patterns from training examples. We have chosen to pursue a uni ed stochastic approach to the information extraction task, modeling sentence fragments containing the target fact with a hidden Markov model (HMM) which we use both to decide if a candidate sentence fragment 3 contains the fact and also to identify the important elements or slot llers in the fact. An HMM trained to recognize a small set of representative sentence fragments di ers radically from a DFA or discrete pattern matcher designed for the same task in that it outputs a probability. Unlike a DFA, an HMM will accept any sequence of words with non-zero probability. The probability it computes (after some corrections for sentence length and background frequencies of words) varies gracefully between the extremes of predicting extremely low probability for sequences that tend not to contain the fact to predicting high probability for ones that tend to contain it. There is no need, if we use an HMM to nd and process facts, to employ heuristics in order to rank and choose between competing explanations for a sentence; symbolic approaches often do so [9]. The probability the HMM computes is meaningful information we can use directly to reason about candidate facts in principled ways that submit to analysis. The HMM is a very compact and exible representation for the information extraction task which seems to be less reliant upon human engineering and prior knowledge than non-probabilistic approaches. This thesis will discuss our e orts to construct a model for a binary relationship between gene names and gene locations, as found in a variety of syntactic forms in scienti c abstracts. The model is structured hierarchically: at the top level states are collected into modules corresponding to noun or verb groups, whereas at the bottom level, in some cases, states function entirely deterministically, employing DFAs to recognize commonly occurring patterns. The HMM consists of only 64 states with an average of 3 transitions each, and explicitly mentions less than 150 words. When deploying the model to nd facts in novel sentences, no attempt is made to tag for part-of-speech. \\Gap\" states, which assign emission probability according to word frequency in the entire corpus, permit the HMM to recognize disconnected segments of a fact and tolerate new words. Unknown words, if they appear in the right local context, are accepted by the HMM essentially without penalty. So while the list of words likely to participate in forming a gene name or gene location is long and populated by words both common and rare to the corpus our approach is competent at correctly identifying even unknown words as 4 long as they appear anked by other words that serve to index the fact well. The accuracy of this HMM approach to information extraction, in the context of the gene name|location fact, is on par with symbolic approaches. This thesis is organized as follows. We begin with a description of the gene name|location information extraction task. Next, we present the modular HMM architecture constructed for this task, motivating our choice of null or background model and demonstrating the discriminatory power it adds to this approach. A brief technical discussion comes next, of the precise formulae used to reestimate parameters for an HMM with non-emitting states. Then we provide implementation and optimization details, followed by training and testing performance. We conclude with some remarks on the use of prior knowledge and ideas for future work. Chapter II Automatic Annotation Generation We consider the question of nding facts in unrestricted prose in the context of lling in slots in a database of facts about genes. The slots in the database correspond to biological entities. These are described by single words or simple phrases, three examples of which might be the name of a gene, some speci cation of its location, and some list of diseases in which it is known to be involved. An example pair of acceptable entries is SLOT ENTRY Gene Name: (The gene encoding BARK2) Gene Location: (mouse chromosome 5) which we might nd buried in a sentence like The gene encoding BARK2 mapped to mouse chromosome 5, whereas that encoding BARK1 was localized to mouse chromosome 19. This is valuable information that is available nowhere except in the published literature. Specialized databases like SwissProt and GenBank do not contain these kinds of associations. So there is interest in developing automated systems for lling in these slots. In order to populate these slots, we must locate and correctly analyze binary (or perhaps even ternary and higher) relations between likely ele"
            },
            "slug": "Information-Extraction-Using-Hidden-Markov-Models-Leek",
            "title": {
                "fragments": [],
                "text": "Information Extraction Using Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This thesis shows how to design and tune a hidden Markov model to extract factual information from a corpus of machine-readable English prose and presents a HMM that classifies and parses natural language assertions about genes being located at particular positions on chromosomes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "Ronald Rosenfeld",
                        "slug": "Ronald-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "\u2026have been applied with considerable success to many natural language tasks, including language modeling for speech recognition (Rosenfeld, 1994; Chen & Rosenfeld, 1999), segmentation of newswire stories (Beeferman et al., 1999), part-of-speech tagging, prepositional phrase attachment and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "These include the combination of traditional tri grams with \u201ctrigger word\u201d features (Rosenfeld, 1994) and the combination of arbitrary features of sentences with tri gram models ( Chen & Rosenfeld, 1999a )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 175
                            }
                        ],
                        "text": "These include the combination of traditional trigrams with \u201ctrigger word\u201d features (Rosenfeld, 1994) and the combination of arbitrary features of sentences with trigram models (Chen & Rosenfeld, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12070294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d8cb09f19c0afdc68d39cc55743104ec396d86e",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional maximum entropy models have been successfully applied to estimating language model probabilities of the form P(w|h), but are often to demanding computationally. Furthermore, the conditional framework does not lend itself to expressing global sentential phenomena. We have previously introduced a non-conditional maximum entropy language model which directly models the probability of an entire sentence or utterance. The model treats each utterance as a \"bag of features\", where features are arbitrary computable properties of the sentence. Using the model is computationally straightforward since it does not require normalization. Training the model requires efficient sampling of sentences from an exponential distribution. In this paper, we further develop the model and demonstrate its feasibility and power. We compare the efficiency of several sampling techniques. implement smoothing to accommodate rare features, and suggest an efficient algorithm for improving the convergence rate. We then present a novel procedure for feature selection, which exploits discrepancies between the existing model and the training corpus. We demonstrate our ideas by constructing and analyzing competitive modes in the Switchboard domain."
            },
            "slug": "Efficient-sampling-and-feature-selection-in-whole-Chen-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Efficient sampling and feature selection in whole sentence maximum entropy language models"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper develops a non-conditional maximum entropy language model which directly models the probability of an entire sentence or utterance, and presents a novel procedure for feature selection, which exploits discrepancies between the existing model and the training corpus."
            },
            "venue": {
                "fragments": [],
                "text": "1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 148
                            }
                        ],
                        "text": "\u2026by maximum entropy have been applied with considerable success to many natural language tasks, including language modeling for speech recognition (Rosenfeld, 1994; Chen & Rosenfeld, 1999), segmentation of newswire stories (Beeferman et al., 1999), part-of-speech tagging, prepositional phrase\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 173
                            }
                        ],
                        "text": "Exponential models derived by maximum entropy have been applied with considerable success to many natural language tasks, including language modeling for speech recognition (Rosenfeld, 1994; Chen & Rosenfeld, 1999), segmentation of newswire stories (Beeferman et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 82
                            }
                        ],
                        "text": "These include the combination of traditional trigrams with \u201ctrigger word\u201d features (Rosenfeld, 1994) and the combination of arbitrary features of sentences with trigram models (Chen & Rosenfeld, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1735632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b26fa1b848ed808a0511db34bce2426888f0b68",
            "isKey": false,
            "numCitedBy": 417,
            "numCiting": 120,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model's parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge. In this thesis, I view language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary). The goal of language modeling is then to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy. Most existing statistical language models exploit the immediate past only. To extract information from further back in the document's history, I use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from many sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, I apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the NE solution. Language modeling, Adaptive language modeling, Statistical language modeling, Maximum entropy, Speech recognition."
            },
            "slug": "Adaptive-Statistical-Language-Modeling;-A-Maximum-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Adaptive Statistical Language Modeling; A Maximum Entropy Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis views language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary), and applies the principle of Maximum Entropy to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145334214"
                        ],
                        "name": "M. Rahim",
                        "slug": "M.-Rahim",
                        "structuredName": {
                            "firstName": "Mazin",
                            "lastName": "Rahim",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rahim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14074975,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "442fff5c760643c1fd46bd97b0877de0bcb8ba2c",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate a probabilistic framework for automatic speech recognition based on the intrinsic geometric properties of curves. In particular, we analyze the setting in which two variables-one continuous (x), one discrete (s)-evolve jointly in time. We suppose that the vector x traces out a smooth multidimensional curve and that the variable s evolves stochastically as a function of the arc length traversed along this curve. Since arc length does not depend on the rate at which a curve is traversed, this gives rise to a family of Markov processes whose predictions, Pr[s|x], are invariant to nonlinear warpings of time. We describe the use of such models, known as Markov processes on curves (MPCs), for automatic speech recognition, where x are acoustic feature trajectories and s are phonetic transcriptions. On two tasks--recognizing New Jersey town names and connected alpha-digits--we find that MPCs yield lower word error rates than comparably trained hidden Markov models."
            },
            "slug": "Markov-Processes-on-Curves-for-Automatic-Speech-Saul-Rahim",
            "title": {
                "fragments": [],
                "text": "Markov Processes on Curves for Automatic Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "On two tasks--recognizing New Jersey town names and connected alpha-digits--the use of Markov processes on curves (MPCs) for automatic speech recognition is described, and it is found that MPCs yield lower word error rates than comparably trained hidden Markov models."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 281
                            }
                        ],
                        "text": "\u2026with considerable success to many natural language tasks, including language modeling for speech recognition (Rosenfeld, 1994; Chen & Rosenfeld, 1999), segmentation of newswire stories (Beeferman et al., 1999), part-of-speech tagging, prepositional phrase attachment and parsing (Ratnaparkhi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "The model closest to our proposal is the part-of-speech tagger of Ratnaparkhi (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 77
                            }
                        ],
                        "text": ", 1999), part-of-speech tagging, prepositional phrase attachment and parsing (Ratnaparkhi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2600845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy. \nWe discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages: \nState-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources. \nKnowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or \"knowledge-poor\", but yet succeed in approximating complex linguistic relationships. \nReusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis. \nThe experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models."
            },
            "slug": "Maximum-entropy-models-for-natural-language-Ratnaparkhi-Marcus",
            "title": {
                "fragments": [],
                "text": "Maximum entropy models for natural language ambiguity resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "\u2026have been applied with considerable success to many natural language tasks, including language modeling for speech recognition (Rosenfeld, 1994; Chen & Rosenfeld, 1999), segmentation of newswire stories (Beeferman et al., 1999), part-of-speech tagging, prepositional phrase attachment and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 175
                            }
                        ],
                        "text": "These include the combination of traditional trigrams with \u201ctrigger word\u201d features (Rosenfeld, 1994) and the combination of arbitrary features of sentences with trigram models (Chen & Rosenfeld, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17052790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance."
            },
            "slug": "A-Gaussian-Prior-for-Smoothing-Maximum-Entropy-Chen-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A Gaussian Prior for Smoothing Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Over a large number of data sets, it is found that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144628595"
                        ],
                        "name": "S. Argamon",
                        "slug": "S.-Argamon",
                        "structuredName": {
                            "firstName": "Shlomo",
                            "lastName": "Argamon",
                            "middleNames": [
                                "Engelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Argamon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349412"
                        ],
                        "name": "Yuval Krymolowski",
                        "slug": "Yuval-Krymolowski",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Krymolowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuval Krymolowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Non-probabilistic methods such as memory-based techniques ( Argamon, Dagan, & Krymolowski, 1998 ), transformation-based learning (Brill, 1995), and Winnow-based combinations of linear classifiers (Roth, 1998), do not give normalized scores to each decision that can be combined into overall scores for decision sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8074746,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7fad831935254c9c9ec39ffb03752a3f736c3f76",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction."
            },
            "slug": "A-Memory-Based-Approach-to-Learning-Shallow-Natural-Argamon-Dagan",
            "title": {
                "fragments": [],
                "text": "A Memory-Based Approach to Learning Shallow Natural Language Patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus that enables easy porting to new domains and to sub-language patterns for information extraction."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 272
                            }
                        ],
                        "text": "\u2026believe that a distributed state representation and nonfully connected topologies may facilitate applications to more demanding tasks, such as information extraction with large vocabulary and many features, as well as automatic feature generation and selection following Della Pietra et al. (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32183233"
                        ],
                        "name": "Andrew Borthwick",
                        "slug": "Andrew-Borthwick",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Borthwick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Borthwick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144360624"
                        ],
                        "name": "J. Sterling",
                        "slug": "J.-Sterling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sterling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sterling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The MENE named-entity recognizer ( Borthwick, Sterling, Agichtein, & Grishman, 1998 ) uses an exponential model to label each word with a label indicating the position of the word in a labeled-entity class (start, inside, end or singleton), but the conditioning information does not include the previous label, unlike our model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6118890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4df361d65a15ca9a7fc27c58c38b04d1f41e6f62",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel statistical namedentity (i.e. \"proper name\") recognition system built around a maximum entity framework. By working v,ithin the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published. 1 I N T R O D U C T I O N Named entity recognition is one of the simplest of the common message understanding tasks. The objective is to identify and categorize all members of certain categories of \"proper names\" from a given corpus. The specific test bed which will be the subject of this paper is that of the Seventh Message Understanding Conference (MUC-7), in which the task was to identify \"names\" falling into one of seven categories: person, organization, location, date, time, percentage, and monetary amount. This paper describes a new system called \"Maximum Entropy Named Entity\" or \"MENE\" (pronounced \"meanie\"). By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decision. These knowledge sources include capitalization features, lexical features, and features indicating the current section of text. It makes use of a broad array of dictionaries of useful single or multi-word terms such as first names, company names, and corporate suffixes, and automatically handles cases where words are in more than one dictionary. Our dictio152 naries required no manual editing and were either downloaded from the web or were simply \"obvious\" lists entered by hand. This system, built from off-the-shelf knowledge sources, contained no hand-generated pat terns and achieved a result which is comparable with that of the best statistical systems. Further experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thus-far reported by any system on a MUC evaluation. Given appropriate training data, we believe that this system is highly portable to other domains and languages and have already achieved good results on upper-case English. We also feel that there are plenty of avenues to explore in enhancing the system's performance on English-language newspaper"
            },
            "slug": "Exploiting-Diverse-Knowledge-Sources-via-Maximum-in-Borthwick-Sterling",
            "title": {
                "fragments": [],
                "text": "Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This paper describes a novel statistical namedentity recognition system built around a maximum entity framework using the framework of maximum entropy theory and utilizing a flexible object-based architecture to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@COLING/ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 170
                            }
                        ],
                        "text": "\u2026such as memory-based techniques (Argamon, Dagan, & Krymolowski, 1998), transformation-based learning (Brill, 1995), and Winnow-based combinations of linear classifiers (Roth, 1998), do not give normalized scores to each decision that can be combined into overall scores for decision sequences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 194
                            }
                        ],
                        "text": "Non-probabilistic methods such as memory-based techniques (Argamon, Dagan, & Krymolowski, 1998), transformation-based learning (Brill, 1995), and Winnow-based combinations of linear classifiers (Roth, 1998), do not give normalized scores to each decision that can be combined into overall scores for decision sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ed17a1114e2dc48597ab17cc8d5234006f525c9",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best."
            },
            "slug": "Learning-to-Resolve-Natural-Language-Ambiguities:-A-Roth",
            "title": {
                "fragments": [],
                "text": "Learning to Resolve Natural Language Ambiguities: A Unified Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An extensive experimental comparison of the approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging shows that it outperforms other methods tried for these tasks or performs comparably to the best."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40359484"
                        ],
                        "name": "K. Kanazawa",
                        "slug": "K.-Kanazawa",
                        "structuredName": {
                            "firstName": "Keiji",
                            "lastName": "Kanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For HMMs and related graphical models, tied parameters and factored state representations (Ghahramani & Jordan, 1996;  Kanazawa, Koller, & Russell, 1995 ) have been used to alleviate this difficulty."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 421074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "135d19fe9c3836d5ba5f6af7620e4d25f2fed710",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic simulation algorithms such as likelihood weighting often give fast, accurate approximations to posterior probabilities in probabilistic networks, and are the methods bf choice for very large networks. Unfortunately, the special characteristics of dynamic probabilistic networks (DPNs), which are used to represent stochastic temporal processes, mean that standard simulation algorithms perform very poorly. In essence, the simulation trials diverge further and further from reality as the process is observed over time. In this paper, we present simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality. The first algorithm, \"evidence reversal\" (ER) restructures each time slice of the DPN so that the evidence nodes for the slice become ancestors of the state variables. The second algorithm, called \"survival of the fittest\" sampling (SOF), \"repopulates\" the set of trials at each time step using a stochastic reproduction rate weighted by the likelihood of the evidence according to each trial. We compare the performance of each algorithm with likelihood weighting on the original network, and also investigate the benefits of combining the ER and SOF methods. The ER/SOF combination appears to maintain bounded error independent of the number of time steps in the simulation."
            },
            "slug": "Stochastic-simulation-algorithms-for-dynamic-Kanazawa-Koller",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation algorithms for dynamic probabilistic networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality are presented and the benefits of combining the ER and SOF methods are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30716906"
                        ],
                        "name": "M. Stoelinga",
                        "slug": "M.-Stoelinga",
                        "structuredName": {
                            "firstName": "Marielle",
                            "lastName": "Stoelinga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stoelinga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 74
                            }
                        ],
                        "text": "That is, the model is in the form of probabilistic finite-state acceptor (Paz, 1971), in which ! is the probability of the transition from state to state on input ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1207382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9956f29b45ec60463511cb24bafdb2380e6b1aad",
            "isKey": false,
            "numCitedBy": 481,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Introduction-to-Probabilistic-Automata-Stoelinga",
            "title": {
                "fragments": [],
                "text": "An Introduction to Probabilistic Automata"
            },
            "venue": {
                "fragments": [],
                "text": "Bull. EATCS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747150"
                        ],
                        "name": "R. Burke",
                        "slug": "R.-Burke",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Burke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Burke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2263239"
                        ],
                        "name": "K. Hammond",
                        "slug": "K.-Hammond",
                        "structuredName": {
                            "firstName": "Kristian",
                            "lastName": "Hammond",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hammond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2638449"
                        ],
                        "name": "V. Kulyukin",
                        "slug": "V.-Kulyukin",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kulyukin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kulyukin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779477"
                        ],
                        "name": "S. Lytinen",
                        "slug": "S.-Lytinen",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Lytinen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lytinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801234"
                        ],
                        "name": "Noriko Tomuro",
                        "slug": "Noriko-Tomuro",
                        "structuredName": {
                            "firstName": "Noriko",
                            "lastName": "Tomuro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noriko Tomuro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47236369"
                        ],
                        "name": "Scott Schoenberg",
                        "slug": "Scott-Schoenberg",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Schoenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Schoenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the case of FAQs, at least one such system, a question-answering system, has been described in the literature ( Burke, Hammond, Kulyukin, Lytinen, & Tomuro, 1997 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8601999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e78cabd2f0a8bc2487bf77fa5c62657ed589f64e",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This technical report describes FAQ Finder, a natural language question answering system that uses files of frequently asked questions as its knowledge base. Unlike AI question-answering systems that focus on the generation of new answers, FAQ Finder retrieves existing ones found in frequently-asked question files. Unlike information retrieval approaches that rely on a purely lexical metric of similarity between query and document, FAQ Finder uses a semantic knowledge base (WordNet) to improve its ability to match question and answer. We describe the design and the current implementation of the system and its support components, including results from an evaluation of the system''s performance against a corpus of user questions. An important finding was that a combination of semantic and statistical techniques works better than any single approach. We analyze failures of the system and discuss future research aimed at addressing them."
            },
            "slug": "Question-Answering-from-Frequently-Asked-Question-Burke-Hammond",
            "title": {
                "fragments": [],
                "text": "Question Answering from Frequently Asked Question Files: Experiences with the FAQ FINDER System"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This technical report describes FAQ Finder, a natural language question answering system that uses files of frequently asked questions as its knowledge base, and describes the design and the current implementation of the system and its support components."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 127
                            }
                        ],
                        "text": "Non-probabilistic methods such as memory-based techniques (Argamon, Dagan, & Krymolowski, 1998), transformation-based learning (Brill, 1995), and Winnow-based combinations of linear classifiers (Roth, 1998), do not give normalized scores to each decision that can be combined into overall scores for decision sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 128
                            }
                        ],
                        "text": "Non-probabilistic methods such as memory-based techniques (Argamon, Dagan, & Krymolowski, 1998), transformation-based learning (Brill, 1995), and Winnow-based combinations of linear classifiers (Roth, 1998), do not give normalized scores to each decision that can be combined into overall scores for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 134248,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "2b2eb4a9bb146e3ffaa0b025fba0ed14240c683f",
            "isKey": false,
            "numCitedBy": 1821,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of injection molding wherein a pair of separable mold plates are initially urged together and fluid plastic is injected into a mold cavity formed between the mold plates to form an article. The injection pressure of the fluid plastic is utilized to generate forces sufficient to overcome the internal forces urging the mold plates apart and thus hold the mold plates together until the material being molded solidifies either by cooling, chemical reaction or phase change."
            },
            "slug": "Transformation-Based-Error-Driven-Learning-and-A-in-Brill",
            "title": {
                "fragments": [],
                "text": "Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Injection molding wherein a pair of separable mold plates are initially urged together and fluid plastic is injected into a mold cavity formed between the mold plates to form an article."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 108
                            }
                        ],
                        "text": "A hidden Markov model (HMM) is a finite state automaton with stochastic state transitions and observations (Rabiner, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 28
                            }
                        ],
                        "text": "The \u201cthree classic problems\u201d (Rabiner, 1989) of HMMs can all be straightforwardly solved in this new model with new variants of the forward-backward, Viterbi and Baum-Welch algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Space limitations pre-\nvent a full description here of Viterbi and Baum-Welch; see Rabiner (1989) for an excellent tutorial."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": true,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The exponential models follow from a maximumentropyargument,and are trained by generalized iterative scaling (GIS) (Darroch & Ratcliff, 1972), which is similar in form and computational cost to the expectation-maximization (EM) algorithm( Dempster,Laird, &Rubin, 1977 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49223598"
                        ],
                        "name": "J. Darroch",
                        "slug": "J.-Darroch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darroch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darroch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12360582"
                        ],
                        "name": "D. Ratcliff",
                        "slug": "D.-Ratcliff",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "The remainder of the paper describes our alternative model in detail, explains how to fit the parameters using GIS, (for both known and unknown state sequences), and presents the variant of the forward-backward procedure, out of which solutions to the \u201cclassic problems\u201d follow naturally."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Generalized Iterative Scaling (GIS) ( Darroch & Ratcliff, 1972 ) is an iterative algorithm for finding the values that form the maximum entropy solution for each transition function (Eq 4)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "We also believe it would be worth investigating training with partially labeled data using the combination of Baum-Welch and GIS discussed earlier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Training of the parameters is performed with Generalized Iterative Scaling ( Darroch & Ratcliff, 1972 ), which is similar in form and computational cost to Expectation-Maximization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Start iteration 0 of GIS with some arbitrary parameter\nvalues, say > ( * ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "We then apply GIS using the feature statistics for the events assigned to each in order to induce the transition function \" $ for ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Note that GIS does not have to be run to convergence in each M-step; not doing so would make this an example of Generalized Expectation-Maximization (GEM), which is also guaranteed to converge to a local maximum."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "The application of GIS to learning the transition function \" $ for state consists of the following steps:\n1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 181
                            }
                        ],
                        "text": "Many thanks to John Lafferty for helpful discussions on training with unknown state and on associating observations with states instead of transitions, to Kamal Nigam for help with GIS, and to Michael Collins for guidance on related work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "The M-step uses the GIS procedure with feature frequencies based on the E-step state occupancies to compute new transition functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 5
                            }
                        ],
                        "text": "GIS (Darroch & Ratcliff, 1972) finds iteratively the > val-ues that form the maximum entropy solution for each transition function (Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 119
                            }
                        ],
                        "text": "The exponential models follow from a maximum entropy argument, and are trained by generalized iterative scaling (GIS) (Darroch & Ratcliff, 1972), which is similar in form and computational cost to the expectation-maximization (EM) algorithm (Dempster, Laird, & Rubin, 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120862597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37c931cbaa9217b829596dd196520a838562a109",
            "isKey": true,
            "numCitedBy": 1329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-Iterative-Scaling-for-Log-Linear-Models-Darroch-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Generalized Iterative Scaling for Log-Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "The remainder of the paper describes our alternative model in detail, explains how to fit the parameters using GIS, (for both known and unknown state sequences), and presents the variant of the forward-backward procedure, out of which solutions to the \u201cclassic problems\u201d follow naturally."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "We also believe it would be worth investigating training with partially labeled data using the combination of Baum-Welch and GIS discussed earlier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Start iteration 0 of GIS with some arbitrary parameter\nvalues, say > ( * ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "We then apply GIS using the feature statistics for the events assigned to each in order to induce the transition function \" $ for ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Note that GIS does not have to be run to convergence in each M-step; not doing so would make this an example of Generalized Expectation-Maximization (GEM), which is also guaranteed to converge to a local maximum."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "The application of GIS to learning the transition function \" $ for state consists of the following steps:\n1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 181
                            }
                        ],
                        "text": "Many thanks to John Lafferty for helpful discussions on training with unknown state and on associating observations with states instead of transitions, to Kamal Nigam for help with GIS, and to Michael Collins for guidance on related work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "The M-step uses the GIS procedure with feature frequencies based on the E-step state occupancies to compute new transition functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 5
                            }
                        ],
                        "text": "GIS (Darroch & Ratcliff, 1972) finds iteratively the > val-ues that form the maximum entropy solution for each transition function (Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 119
                            }
                        ],
                        "text": "The exponential models follow from a maximum entropy argument, and are trained by generalized iterative scaling (GIS) (Darroch & Ratcliff, 1972), which is similar in form and computational cost to the expectation-maximization (EM) algorithm (Dempster, Laird, & Rubin, 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generlized iterative scaling for log-linear models"
            },
            "venue": {
                "fragments": [],
                "text": "The Annals of Mathematical Statistics,"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 200
                            }
                        ],
                        "text": "HMMs have also been successful in similar naturallanguage tasks, including part-of-speech tagging (Kupiec, 1992), named-entity recognition (Bikel et al., 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 271
                            }
                        ],
                        "text": "\u2026to many text-related tasks, including part-of-speech tagging (Kupiec, 1992), text segmentation and event tracking (Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998), named entity recognition (Bikel, Schwartz, & Weischedel, 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information extraction using hmms and shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": "Papers from the AAAI99 Workshop on Machine Learning for Information Extration"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 200
                            }
                        ],
                        "text": "HMMs have also been successful in similar naturallanguage tasks, including part-of-speech tagging (Kupiec, 1992), named-entity recognition (Bikel et al., 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 271
                            }
                        ],
                        "text": "\u2026to many text-related tasks, including part-of-speech tagging (Kupiec, 1992), text segmentation and event tracking (Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998), named entity recognition (Bikel, Schwartz, & Weischedel, 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information extracti"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 99
                            }
                        ],
                        "text": "HMMs have also been successful in similar naturallanguage tasks, including part-of-speech tagging (Kupiec, 1992), named-entity recognition (Bikel et al., 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "\u2026tool for representing sequential data, and have been applied with significant success to many text-related tasks, including part-of-speech tagging (Kupiec, 1992), text segmentation and event tracking (Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998), named entity recognition (Bikel, Schwartz, &\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust partofspeech tagging using a hidden Markov model"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Speech and Language"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "\u2026have been applied with considerable success to many natural language tasks, including language modeling for speech recognition (Rosenfeld, 1994; Chen & Rosenfeld, 1999), segmentation of newswire stories (Beeferman et al., 1999), part-of-speech tagging, prepositional phrase attachment and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 175
                            }
                        ],
                        "text": "These include the combination of traditional trigrams with \u201ctrigger word\u201d features (Rosenfeld, 1994) and the combination of arbitrary features of sentences with trigram models (Chen & Rosenfeld, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient sampling and feat"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 128
                            }
                        ],
                        "text": "Non-probabilistic methods such as memory-based techniques (Argamon, Dagan, & Krymolowski, 1998), transformation-based learning (Brill, 1995), and Winnow-based combinations of linear classifiers (Roth, 1998), do not give normalized scores to each decision that can be combined into overall scores for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transformation - based errordriven learning and natural language processing : a case study in part of speech tagging"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "imum likelihood from incomplete data via the EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society , Series B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 127
                            }
                        ],
                        "text": "Non-probabilistic methods such as memory-based techniques (Argamon, Dagan, & Krymolowski, 1998), transformation-based learning (Brill, 1995), and Winnow-based combinations of linear classifiers (Roth, 1998), do not give normalized scores to each decision that can be combined into overall scores for decision sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 128
                            }
                        ],
                        "text": "Non-probabilistic methods such as memory-based techniques (Argamon, Dagan, & Krymolowski, 1998), transformation-based learning (Brill, 1995), and Winnow-based combinations of linear classifiers (Roth, 1998), do not give normalized scores to each decision that can be combined into overall scores for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transformation-based error-driven lear"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generlized iterativescaling for loglinear models"
            },
            "venue": {
                "fragments": [],
                "text": "The Annals of Mathematical Statistics"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 188
                            }
                        ],
                        "text": "HMMs have also been successful in similar naturallanguage tasks, including part-of-speech tagging (Kupiec, 1992), named-entity recognition (Bikel et al., 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 259
                            }
                        ],
                        "text": "\u2026to many text-related tasks, including part-of-speech tagging (Kupiec, 1992), text segmentation and event tracking (Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998), named entity recognition (Bikel, Schwartz, & Weischedel, 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information extraction using hidden Mar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 200
                            }
                        ],
                        "text": "HMMs have also been successful in similar naturallanguage tasks, including part-of-speech tagging (Kupiec, 1992), named-entity recognition (Bikel et al., 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 271
                            }
                        ],
                        "text": "\u2026to many text-related tasks, including part-of-speech tagging (Kupiec, 1992), text segmentation and event tracking (Yamron, Carp, Gillick, Lowe, & van Mulbregt, 1998), named entity recognition (Bikel, Schwartz, & Weischedel, 1999) and information extraction (Leek, 1997; Freitag & McCallum, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information extraction using hmms and shrinkage. In Papers from the AAAI-99 Workshop on Machine Learning for Information Extration"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 24
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 30,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Maximum-Entropy-Markov-Models-for-Information-and-McCallum-Freitag/bece46ed303f8eaef2affae2cba4e0aef51fe636?sort=total-citations"
}