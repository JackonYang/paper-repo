{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70011052"
                        ],
                        "name": "Weston J Stitson Mo",
                        "slug": "Weston-J-Stitson-Mo",
                        "structuredName": {
                            "firstName": "Weston",
                            "lastName": "Stitson Mo",
                            "middleNames": [
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weston J Stitson Mo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69902577"
                        ],
                        "name": "Sch\u00f6lkopf B Bottou L",
                        "slug": "Sch\u00f6lkopf-B-Bottou-L",
                        "structuredName": {
                            "firstName": "Sch\u00f6lkopf",
                            "lastName": "Bottou L",
                            "middleNames": [
                                "B"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sch\u00f6lkopf B Bottou L"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 43
                            }
                        ],
                        "text": "Some work on this method are, for example, (Osuna et al., 1997b; Joachims, 1998; Platt, 1998; Saunders et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60691110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d36f7543109a8c859d423ddb98bf6d2bd4e13d4d",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector Machine (SVM) is a new type of learning machine. The SVM is a general architecture that can be applied to pattern recognition, regression estimation and other problems. The following researchers were involved in the development of the SVM: The Support Vector Machine (SVM) program allows a user to carry out pattern recognition and regression estimation, using support vector techniques on some given data. If you have any questions not answered by the documentation, you can e-mail us at:"
            },
            "slug": "Support-Vector-Machine-Reference-Manual-Saunders-Mo",
            "title": {
                "fragments": [],
                "text": "Support Vector Machine Reference Manual"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The Support Vector Machine (SVM) program allows a user to carry out pattern recognition and regression estimation, using support vector techniques on some given data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13375961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44e0167ef3ff820917425839be0c145179c5a5a",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classification speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily finds a set of kernel basis functions of a specified maximum size (dmax) to approximate the SVM primal cost function well; (3) it is efficient and roughly scales as O(ndmax2) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors."
            },
            "slug": "Building-Support-Vector-Machines-with-Reduced-Keerthi-Chapelle",
            "title": {
                "fragments": [],
                "text": "Building Support Vector Machines with Reduced Classifier Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A primal method that decouples the idea of basis functions from the concept of support vectors and greedily finds a set of kernel basis functions of a specified maximum size to approximate the SVM primal cost function well."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771659"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15140283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c4749d9d3f1724aa01778d69a3774c732ca44c",
            "isKey": false,
            "numCitedBy": 844,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector Machine (SVM) is a new and very promising classification technique developed by Vapnik and his group at AT\\&T Bell Labs. This new learning algorithm can be seen as an alternative training technique for Polynomial, Radial Basis Function and Multi-Layer Perceptron classifiers. An interesting property of this approach is that it is an approximate implementation of the Structural Risk Minimization (SRM) induction principle. The derivation of Support Vector Machines, its relationship with SRM, and its geometrical insight, are discussed in this paper. Training a SVM is equivalent to solve a quadratic programming problem with linear and box constraints in a number of variables equal to the number of data points. When the number of data points exceeds few thousands the problem is very challenging, because the quadratic form is completely dense, so the memory needed to store the problem grows with the square of the number of data points. Therefore, training problems arising in some real applications with large data sets are impossible to load into memory, and cannot be solved using standard non-linear constrained optimization algorithms. We present a decomposition algorithm that can be used to train SVM''s over large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of, and also establish the stopping criteria for the algorithm. We present previous approaches, as well as results and important details of our implementation of the algorithm using a second-order variant of the Reduced Gradient Method as the solver of the sub-problems. As an application of SVM''s, we present preliminary results we obtained applying SVM to the problem of detecting frontal human faces in real images."
            },
            "slug": "Support-Vector-Machines:-Training-and-Applications-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "Support Vector Machines: Training and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Preliminary results are presented obtained applying SVM to the problem of detecting frontal human faces in real images, and the main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of, and also establish the stopping criteria for the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61116019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "isKey": false,
            "numCitedBy": 5454,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-SVM-learning-practical-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large scale SVM learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771659"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2845602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
            "isKey": false,
            "numCitedBy": 2841,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points."
            },
            "slug": "Training-support-vector-machines:-an-application-to-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "Training support vector machines: an application to face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets is presented, and the feasibility of the approach on a face detection problem that involves a data set of 50,000 data points is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110551642"
                        ],
                        "name": "Chih-Wei Hsu",
                        "slug": "Chih-Wei-Hsu",
                        "structuredName": {
                            "firstName": "Chih-Wei",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Wei Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The convergence of decomposition methods was first studied in (Chang et al., 2000) but algorithms discussed there do not coincide with existing implementations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, its result applies only to decomposition methods discussed in (Chang et al., 2000) but not LIBSVM or other existing software."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14412042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a53c92180026ae14baebde250284e7942aa5c5bf",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The support vector machine (SVM) is a new and promising technique for pattern recognition. It requires the solution of a large dense quadratic programming problem. Traditional optimization methods cannot be directly applied due to memory restrictions. Up to now, very few methods can handle the memory problem and an important one is the \"decomposition method.\" However, there is no convergence proof so far. In this paper, we connect this method to projected gradient methods and provide theoretical proofs for a version of decomposition methods. An extension to bound-constrained formulation of SVM is also provided.We then show that this convergence proof is valid for general decomposition methods if their working set selection meets a simple requirement."
            },
            "slug": "The-analysis-of-decomposition-methods-for-support-Chang-Hsu",
            "title": {
                "fragments": [],
                "text": "The analysis of decomposition methods for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper connects this method to projected gradient methods and provides theoretical proofs for a version of decomposition methods and shows that this convergence proof is valid for general decomposition Methods if their working set selection meets a simple requirement."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 31
                            }
                        ],
                        "text": "-support vector classi.cation \n[Sch\u00a8 olkopf et al. 2000] introduces a new parameter ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207673395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d73c0d0c92446102fdb6cc728b5d69674a1a387",
            "isKey": false,
            "numCitedBy": 2613,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of , and report experimental results."
            },
            "slug": "New-Support-Vector-Algorithms-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "New Support Vector Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new class of support vector algorithms for regression and classification that eliminates one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110551642"
                        ],
                        "name": "Chih-Wei Hsu",
                        "slug": "Chih-Wei-Hsu",
                        "structuredName": {
                            "firstName": "Chih-Wei",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Wei Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15874442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f755d620b57acf27a16ff95923c5677ff8198bb",
            "isKey": false,
            "numCitedBy": 6346,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such \"all-together\" methods. We then compare their performance with three methods based on binary classifications: \"one-against-all,\" \"one-against-one,\" and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the \"one-against-one\" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors."
            },
            "slug": "A-comparison-of-methods-for-multiclass-support-Hsu-Lin",
            "title": {
                "fragments": [],
                "text": "A comparison of methods for multiclass support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Decomposition implementations for two \"all-together\" multiclass SVM methods are given and it is shown that for large problems methods by considering all data at once in general need fewer support vectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": "The decision function is l t sgn yiaiK(xi,x) +b . i=1 It is shown that \neT a =. can be replaced by eT a =. [Crisp and Burges 2000; Chang and Lin 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In (Crisp and Burges, 2000; Chang and Lin, 2001), it has been shown that e\u03b1 \u2265 \u03bd can be replaced by e\u03b1 = \u03bd."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1778325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b27033b100dd92e34624c261ef80a6bd7f9aaaf1",
            "isKey": false,
            "numCitedBy": 359,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The -support vector machine (-SVM) for classification proposed by Schlkopf, Smola, Williamson, and Bartlett (2000) has the advantage of using a parameter on controlling the number of support vectors. In this article, we investigate the relation between -SVM and C-SVM in detail. We show that in general they are two different problems with the same optimal solution set. Hence, we may expect that many numerical aspects of solving them are similar. However, compared to regular C-SVM, the formulation of -SVM is more complicated, so up to now there have been no effective methods for solving large-scale -SVM. We propose a decomposition method for -SVM that is competitive with existing methods for C-SVM. We also discuss the behavior of -SVM by some numerical experiments."
            },
            "slug": "Training-v-Support-Vector-Classifiers:-Theory-and-Chang-Lin",
            "title": {
                "fragments": [],
                "text": "Training v-Support Vector Classifiers: Theory and Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A decomposition method for -SVM is proposed that is competitive with existing methods for C-SVM and shows that in general they are two different problems with the same optimal solution set."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110550856"
                        ],
                        "name": "Chih-Wei Hsu",
                        "slug": "Chih-Wei-Hsu",
                        "structuredName": {
                            "firstName": "Chih-Wei",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Wei Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 36777484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f0a4081740de82b27b85a4b02f5a1a41a858609",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The decomposition method is currently one of the major methods for solving support vector machines. An important issue of this method is the selection of working sets. In this paper through the design of decomposition methods for bound-constrained SVM formulations we demonstrate that the working set selection is not a trivial task. Then from the experimental analysis we propose a simple selection of the working set which leads to faster convergences for difficult cases. Numerical experiments on different types of problems are conducted to demonstrate the viability of the proposed method."
            },
            "slug": "A-Simple-Decomposition-Method-for-Support-Vector-Hsu-Lin",
            "title": {
                "fragments": [],
                "text": "A Simple Decomposition Method for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Through the design of decomposition methods for bound-constrained SVM formulations, it is demonstrated that the working set selection is not a trivial task and a simple selection is proposed which leads to faster convergences for difficult cases."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110551642"
                        ],
                        "name": "Chih-Wei Hsu",
                        "slug": "Chih-Wei-Hsu",
                        "structuredName": {
                            "firstName": "Chih-Wei",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Wei Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 154
                            }
                        ],
                        "text": "A Simple Example of Running LIBSVM While detailed instructions of using LIBSVM are available in the README file of the package and the practical guide by Hsu et al. [2003], here we give a simple example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 123
                            }
                        ],
                        "text": "For instructions of using LIBSVM, see the README file included in the package, the LIBSVM FAQ,3 and the practical guide by Hsu et al. [2003]. LIBSVM supports the following learning tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2443126,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "7abeda3a20c13bfee416d94efa313ff870656fec",
            "isKey": false,
            "numCitedBy": 7092,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machine (SVM) is a popular technique for classication. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signicant steps. In this guide, we propose a simple procedure, which usually gives reasonable results."
            },
            "slug": "A-Practical-Guide-to-Support-Vector-Classication-Hsu-Chang",
            "title": {
                "fragments": [],
                "text": "A Practical Guide to Support Vector Classication"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A simple procedure is proposed, which usually gives reasonable results and is suitable for beginners who are not familiar with SVM."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "It can be clearly seen that C-SVC and one-class SVM are already in the form of problem \n(11)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 111
                            }
                        ],
                        "text": "Then, according to the SVM formulation, svm train one calls a corresponding subroutine \nsuch as solve c svc for C-SVC and solve nu svc for ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "Note that b of C-SVC and E-SVR plays \nthe same role as -. in one-class SVM, so we de.ne ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 96
                            }
                        ],
                        "text": "In Section 2, we describe SVM formulations sup\u00adported in LIBSVM: C-Support Vector \nClassi.cation (C-SVC), ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "{1,-1}, C-SVC \n[Boser et al. 1992; Cortes and Vapnik 1995] solves 4LIBSVM Tools: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. \nthe following primal optimization problem: l t min 1 w T w +C .i (1) w,b,. 2 i=1 subject to yi(w T f(xi) \n+b) =1 -.i, .i =0,i =1,...,l, where f(xi)maps xi into a higher-dimensional space and C > 0 is the regularization \nparameter."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "CONCLUSIONS When we released the .rst version \nof LIBSVM in 2000, only two-class C-SVC was supported."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 15
                            }
                        ],
                        "text": "{1,-1}, C-SVC \n[Boser et al. 1992; Cortes and Vapnik 1995] solves 4LIBSVM Tools: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. \nthe following primal optimization problem: l t min 1 w T w +C .i (1) w,b,. 2 i=1 subject to yi(w T f(xi) \n+b) =1 -.i, .i =0,i =1,...,l, where f(xi)maps xi into a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 47
                            }
                        ],
                        "text": "Quadratic Problems with One Linear Constraint: C-SVC, -SVR, and One-Class SVM We consider \nthe following general form of C-SVC, E-SVR, and one-class SVM: min f (a) a subject to yT a =,, (11) 0 \n=at =C,t =1,...,l, where T f (a) = 1 aTQa +pa 2 and yt =\u00b11,t =1,...,l."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", l, in two classes, and an indicator vector y \u2208 R such that yi \u2208 {1,\u22121}, C-SVC (Boser et al., 1992; Cortes and Vapnik, 1995) solves the following primal optimization problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "The subroutine So, stoplve minimizes a general form of SVM 7The \ndefault solver is C-SVC using the RBF kernel (48) with C = 1and ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 47
                            }
                        ],
                        "text": "Algorithm 1 requires an initial feasible a.For C-SVC and E-SVR, because the zero vector is feasible, \nwe select it as the initial a."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": true,
            "numCitedBy": 10835,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849128"
                        ],
                        "name": "Rong-En Fan",
                        "slug": "Rong-En-Fan",
                        "structuredName": {
                            "firstName": "Rong-En",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong-En Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277171"
                        ],
                        "name": "Pai-Hsuen Chen",
                        "slug": "Pai-Hsuen-Chen",
                        "structuredName": {
                            "firstName": "Pai-Hsuen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pai-Hsuen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6256522,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faef8fb61fdc2ca13a34cfe9f991fd30b21bfe0f",
            "isKey": false,
            "numCitedBy": 1418,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using first order information."
            },
            "slug": "Working-Set-Selection-Using-Second-Order-for-Vector-Fan-Chen",
            "title": {
                "fragments": [],
                "text": "Working Set Selection Using Second Order Information for Training Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new technique for working set selection in SMO-type decomposition methods that uses second order information to achieve fast convergence andoretical properties such as linear convergence are established."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 140
                            }
                        ],
                        "text": "The decision function is l t sgn yiaiK(xi,x) +b . i=1 It is shown that \neT a =. can be replaced by eT a =. [Crisp and Burges 2000; Chang and Lin 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1627812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a36c11be9be934b33dbd8f0ec65abdc0ecd4cc7b",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The \u03bd-support vector machine (\u03bd-SVM) for classification proposed by Sch\u00f6lkopf, Smola, Williamson, and Bartlett (2000) has the advantage of using a parameter \u03bd on controlling the number of support vectors. In this article, we investigate the relation between \u03bd-SVM and C-SVM in detail. We show that in general they are two different problems with the same optimal solution set. Hence, we may expect that many numerical aspects of solving them are similar. However, compared to regular C-SVM, the formulation of \u03bd-SVM is more complicated, so up to now there have been no effective methods for solving large-scale \u03bd-SVM. We propose a decomposition method for \u03bd-SVM that is competitive with existing methods for C-SVM. We also discuss the behavior of \u03bd-SVM by some numerical experiments."
            },
            "slug": "Training-\u03bd-Support-Vector-Classifiers:-Theory-and-Chang-Lin",
            "title": {
                "fragments": [],
                "text": "Training \u03bd-Support Vector Classifiers: Theory and Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This article proposes a decomposition method for \u03bd-SVM that is competitive with existing methods for C-S VM and shows that in general they are two different problems with the same optimal solution set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": false,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7157087,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1226a673ce3f69738366be7b53eaf82dd33e290d",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The decomposition method is currently one of the major methods for solving support vector machines (SVM). Its convergence properties have not been fully understood. The general asymptotic convergence was first proposed by Chang et al. However, their working set selection does not coincide with existing implementation. A later breakthrough by Keerthi and Gilbert (2000, 2002) proved the convergence finite termination for practical cases while the size of the working set is restricted to two. In this paper, we prove the asymptotic convergence of the algorithm used by the software SVM(light) and other later implementation. The size of the working set can be any even number. Extensions to other SVM formulations are also discussed."
            },
            "slug": "On-the-convergence-of-the-decomposition-method-for-Lin",
            "title": {
                "fragments": [],
                "text": "On the convergence of the decomposition method for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The asymptotic convergence of the algorithm used by the software SVM(light) and other later implementation is proved and the size of the working set can be any even number."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1897534699"
                        ],
                        "name": "ChenPai-Hsuen",
                        "slug": "ChenPai-Hsuen",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "ChenPai-Hsuen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ChenPai-Hsuen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643819532"
                        ],
                        "name": "LinChih-Jen",
                        "slug": "LinChih-Jen",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "LinChih-Jen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "LinChih-Jen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643791277"
                        ],
                        "name": "Sch\u00f6lkopfBernhard",
                        "slug": "Sch\u00f6lkopfBernhard",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sch\u00f6lkopfBernhard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sch\u00f6lkopfBernhard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221223436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cef206029e2a925e0754d23651ab228d5904d481",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We briefly describe the main ideas of statistical learning theory, support vector machines (SVMs), and kernel feature spaces. We place particular emphasis on a description of the so-called -SVM, in..."
            },
            "slug": "A-tutorial-on-support-vector-machines-ChenPai-Hsuen-LinChih-Jen",
            "title": {
                "fragments": [],
                "text": "A tutorial on -support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "The main ideas of statistical learning theory, support vector machines (SVMs), and kernel feature spaces are described, with particular emphasis on a description of the so-called -SVM."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756710"
                        ],
                        "name": "T. Glasmachers",
                        "slug": "T.-Glasmachers",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Glasmachers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Glasmachers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748824"
                        ],
                        "name": "C. Igel",
                        "slug": "C.-Igel",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Igel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Igel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 543374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89ebae27c20b0c440c778d10878027436c91ccd9",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is significantly faster."
            },
            "slug": "Maximum-Gain-Working-Set-Selection-for-SVMs-Glasmachers-Igel",
            "title": {
                "fragments": [],
                "text": "Maximum-Gain Working Set Selection for SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection is proved, which is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "and regression can be seen in (Chang and Lin, 2001, Section 4) and ( Chang and Lin, 2002 ),"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39821366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66162798799890489b1cc0f2c00e8d0d8f71e9e2",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the relation between-support vector regression (-SVR) and v-support vector regression (v-SVR). In particular, we focus on properties that are different from those of C-support vector classification (C-SVC) andv-support vector classification (v-SVC). We then discuss some issues that do not occur in the case of classification: the possible range of and the scaling of target values. A practical decomposition method forv-SVR is implemented, and computational experiments are conducted. We show some interesting numerical observations specific to regression."
            },
            "slug": "Training-v-Support-Vector-Regression:-Theory-and-Chang-Lin",
            "title": {
                "fragments": [],
                "text": "Training v-Support Vector Regression: Theory and Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work discusses the relation between-support vector regression (-SVR) and v- support vector regression (v-SVR), and focuses on properties that are different from those of C- Support vector classification (C-SVC) andv-supportvector classification (v -SVC)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 91
                            }
                        ],
                        "text": "An extreme case of the decomposition methods is the Sequential Minimal Optimization (SMO) [Platt 1998], \nwhich restricts B to have only two elements."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5546,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 34
                            }
                        ],
                        "text": "{1,-1}, C-SVC \n[Boser et al. 1992; Cortes and Vapnik 1995] solves 4LIBSVM Tools: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. \nthe following primal optimization problem: l t min 1 w T w +C .i (1) w,b,. 2 i=1 subject to yi(w T f(xi) \n+b) =1 -.i, .i =0,i =1,...,l, where f(xi)maps xi into a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52874011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "isKey": false,
            "numCitedBy": 33423,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "slug": "Support-Vector-Networks-Cortes-Vapnik",
            "title": {
                "fragments": [],
                "text": "Support-Vector Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684722"
                        ],
                        "name": "S. Fine",
                        "slug": "S.-Fine",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Fine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005127"
                        ],
                        "name": "K. Scheinberg",
                        "slug": "K.-Scheinberg",
                        "structuredName": {
                            "firstName": "Katya",
                            "lastName": "Scheinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13899309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db7af02be7cbadc029f9104a8c784d02de42df7",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SVM training is a convex optimization problem which scales with the training set size rather than the feature space dimension. While this is usually considered to be a desired quality, in large scale problems it may cause training to be impractical. The common techniques to handle this difficulty basically build a solution by solving a sequence of small scale subproblems. Our current effort is concentrated on the rank of the kernel matrix as a source for further enhancement of the training procedure. We first show that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity. We then suggest an efficient use of a known factorization technique to approximate a given kernel matrix by a low rank matrix, which in turn will be used to feed the optimizer. Finally, we derive an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors). This bound is general in the sense that it holds regardless of the approximation method."
            },
            "slug": "Efficient-SVM-Training-Using-Low-Rank-Kernel-Fine-Scheinberg",
            "title": {
                "fragments": [],
                "text": "Efficient SVM Training Using Low-Rank Kernel Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity and derives an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors)."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40762207"
                        ],
                        "name": "L. Palagi",
                        "slug": "L.-Palagi",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Palagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Palagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818909"
                        ],
                        "name": "M. Sciandrone",
                        "slug": "M.-Sciandrone",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Sciandrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sciandrone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8826126,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1339cb7d8bdc38ad60ef7537e67abd9f898dc7d0",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we consider the convex quadratic programming problem arising in support vector machine (SVM), which is a technique designed to solve a variety of learning and pattern recognition problems. Since the Hessian matrix is dense and real applications lead to large-scale problems, several decomposition methods have been proposed, which split the original problem into a sequence of smaller subproblems. SVM light algorithm is a commonly used decomposition method for SVM, and its convergence has been proved only recently under a suitable block-wise convexity assumption on the objective function. In SVM light algorithm, the size q of the working set, i.e. the dimension of the subproblem, can be any even number. In the present paper, we propose a decomposition method on the basis of a proximal point modification of the subproblem and the basis of a working set selection rule that includes, as a particular case, the one used by the SVM light algorithm. We establish the asymptotic convergence of the method, for any size q\u2009\u2265\u20092 of the working set, and without requiring any further block-wise convexity assumption on the objective function. Furthermore, we show that the algorithm satisfies in a finite number of iterations a stopping criterion based on the violation of the optimality conditions."
            },
            "slug": "On-the-convergence-of-a-modified-version-of-SVM-Palagi-Sciandrone",
            "title": {
                "fragments": [],
                "text": "On the convergence of a modified version of SVM light algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work considers the convex quadratic programming problem arising in support vector machine (SVM), which is a technique designed to solve a variety of learning and pattern recognition problems, and proposes a decomposition method on the basis of a proximal point modification of the subproblem and a working set selection rule."
            },
            "venue": {
                "fragments": [],
                "text": "Optim. Methods Softw."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705762"
                        ],
                        "name": "N. List",
                        "slug": "N.-List",
                        "structuredName": {
                            "firstName": "Nikolas",
                            "lastName": "List",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. List"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13581795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "487c39672393b17f789b84e3f24527c274f345e7",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider (a subclass of) convex quadratic optimization problems and analyze decomposition algorithms that perform, at least approximately, steepest-descent exact line search. We show that these algorithms, when implemented properly, are within \u01eb of optimality after O(log 1/\u01eb) iterations for strictly convex cost functions, and after O(1/\u01eb) iterations in the general case. Our analysis is general enough to cover the algorithms that are used in software packages like SVMTorch and (first or second order) LibSVM. To the best of our knowledge, this is the first paper coming up with a convergence rate for these algorithms without introducing unnecessarily restrictive assumptions."
            },
            "slug": "SVM-Optimization-and-Steepest-Descent-Line-Search-Simon-List",
            "title": {
                "fragments": [],
                "text": "SVM-Optimization and Steepest-Descent Line Search"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This is the first paper coming up with a convergence rate for these algorithms without introducing unnecessarily restrictive assumptions, and it is general enough to cover the algorithms that are used in software packages like SVMTorch and (first or second order) LibSVM."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2003575"
                        ],
                        "name": "E. Gilbert",
                        "slug": "E.-Gilbert",
                        "structuredName": {
                            "firstName": "Elmer",
                            "lastName": "Gilbert",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gilbert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9246935,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "e62fb4d2027d46d8d39c4fbda74aff89dcf5574e",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Convergence of a generalized version of the modified SMO algorithms given by Keerthi et al. for SVM classifier design is proved. The convergence results are also extended to modified SMO algorithms for solving \u03bd-SVM classifier problems."
            },
            "slug": "Convergence-of-a-Generalized-SMO-Algorithm-for-SVM-Keerthi-Gilbert",
            "title": {
                "fragments": [],
                "text": "Convergence of a Generalized SMO Algorithm for SVM Classifier Design"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Convergence of a generalized version of the modified SMO algorithms given by Keerthi et al. for SVM classifier design is proved and the results are extended to modifiedSMO algorithms for solving \u03bd-SVM classifiers problems."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277171"
                        ],
                        "name": "Pai-Hsuen Chen",
                        "slug": "Pai-Hsuen-Chen",
                        "structuredName": {
                            "firstName": "Pai-Hsuen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pai-Hsuen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849128"
                        ],
                        "name": "Rong-En Fan",
                        "slug": "Rong-En-Fan",
                        "structuredName": {
                            "firstName": "Rong-En",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong-En Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9417532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57a18d3dbef4d79de48a2ffedfb7d5be2dfec5ac",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Decomposition methods are currently one of the major methods for training support vector machines. They vary mainly according to different working set selections. Existing implementations and analysis usually consider some specific selection rules. This paper studies sequential minimal optimization type decomposition methods under a general and flexible way of choosing the two-element working set. The main results include: 1) a simple asymptotic convergence proof, 2) a general explanation of the shrinking and caching techniques, and 3) the linear convergence of the methods. Extensions to some support vector machine variants are also discussed."
            },
            "slug": "A-study-on-SMO-type-decomposition-methods-for-Chen-Fan",
            "title": {
                "fragments": [],
                "text": "A study on SMO-type decomposition methods for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The main results include a simple asymptotic convergence proof, a general explanation of the shrinking and caching techniques, and the linear convergence of the methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772326"
                        ],
                        "name": "S. Shevade",
                        "slug": "S.-Shevade",
                        "structuredName": {
                            "firstName": "Shirish",
                            "lastName": "Shevade",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shevade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145880755"
                        ],
                        "name": "C. Bhattacharyya",
                        "slug": "C.-Bhattacharyya",
                        "structuredName": {
                            "firstName": "Chiranjib",
                            "lastName": "Bhattacharyya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bhattacharyya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38445965"
                        ],
                        "name": "K. Murthy",
                        "slug": "K.-Murthy",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Murthy",
                            "middleNames": [
                                "R.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1536643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d95b96d71669f3f4edfcc95cacd428b62b3fcde",
            "isKey": false,
            "numCitedBy": 1804,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This article points out an important source of inefficiency in Platt's sequential minimal optimization (SMO) algorithm that is caused by the use of a single threshold value. Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO. These modified algorithms perform significantly faster than the original SMO on all benchmark data sets tried."
            },
            "slug": "Improvements-to-Platt's-SMO-Algorithm-for-SVM-Keerthi-Shevade",
            "title": {
                "fragments": [],
                "text": "Improvements to Platt's SMO Algorithm for SVM Classifier Design"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO that perform significantly faster than the original SMO on all benchmark data sets tried."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9094007"
                        ],
                        "name": "Yuh-Jye Lee",
                        "slug": "Yuh-Jye-Lee",
                        "structuredName": {
                            "firstName": "Yuh-Jye",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuh-Jye Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14152802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de4d36bc28e13917297d619f585cd8f7860848f0",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An algorithm is proposed which generates a nonlinear kernel-based separating surface that requires as little as 1% of a large dataset for its explicit evaluation. To generate this nonlinear surface, the entire dataset is used as a constraint in an optimization problem with very few variables corresponding to the 1% of the data kept. The remainder of the data can be thrown away after solving the optimization problem. This is achieved by making use of a rectangular m\u00d7m kernel K(A, \u0100) that greatly reduces the size of the quadratic program to be solved and simplifies the characterization of the nonlinear separating surface. Here, the m rows of A represent the original m data points while the m rows of \u0100 represent a greatly reduced m data points. Computational results indicate that test set correctness for the reduced support vector machine (RSVM), with a nonlinear separating surface that depends on a small randomly selected portion of the dataset, is better than that of a conventional support vector machine (SVM) with a nonlinear surface that explicitly depends on the entire dataset, and much better than a conventional SVM using a small random sample of the data. Computational times, as well as memory usage, are much smaller for RSVM than that of a conventional SVM using the entire dataset."
            },
            "slug": "RSVM:-Reduced-Support-Vector-Machines-Lee-Mangasarian",
            "title": {
                "fragments": [],
                "text": "RSVM: Reduced Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Computational results indicate that test set correctness for the reduced support vector machine (RSVM), with a nonlinear separating surface that depends on a small randomly selected portion of the dataset, is better than that of a conventional support vectors machine (SVM) with aNonlinear surface that explicitly depends on the entire dataset, and much better than a conventional SVM using a small random sample of the data."
            },
            "venue": {
                "fragments": [],
                "text": "SDM"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Weston and Watkins, 1998; Platt et al., 2000)) have shown that it does not perform as good as \u201cone-against-one\u201d In addition, though we have to train as many as k(k \u2212 1)/2 classifiers, as each problem is smaller (only data from two classes), the total training time may not be more than the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1204938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32484f6d111bf21f1395a34a087991a9041dd0ae",
            "isKey": false,
            "numCitedBy": 1876,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new learning architecture: the Decision Directed Acyclic Graph (DDAG), which is used to combine many two-class classifiers into a multiclass classifier. For an N-class problem, the DDAG contains N(N - 1)/2 classifiers, one for each pair of classes. We present a VC analysis of the case when the node classifiers are hyperplanes; the resulting bound on the test error depends on N and on the margin achieved at the nodes, but not on the dimension of the space. This motivates an algorithm, DAGSVM, which operates in a kernel-induced feature space and uses two-class maximal margin hyperplanes at each decision-node of the DDAG. The DAGSVM is substantially faster to train and evaluate than either the standard algorithm or Max Wins, while maintaining comparable accuracy to both of these algorithms."
            },
            "slug": "Large-Margin-DAGs-for-Multiclass-Classification-Platt-Cristianini",
            "title": {
                "fragments": [],
                "text": "Large Margin DAGs for Multiclass Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm, DAGSVM, is presented, which operates in a kernel-induced feature space and uses two-class maximal margin hyperplanes at each decision-node of the DDAG, which is substantially faster to train and evaluate than either the standard algorithm or Max Wins, while maintaining comparable accuracy to both of these algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754328"
                        ],
                        "name": "D. Hush",
                        "slug": "D.-Hush",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Hush",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790221"
                        ],
                        "name": "C. Scovel",
                        "slug": "C.-Scovel",
                        "structuredName": {
                            "firstName": "Clint",
                            "lastName": "Scovel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Scovel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "iterations. An earlier work is in ( Hush and Scovel, 2003 )."
                    },
                    "intents": []
                }
            ],
            "corpusId": 24895645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06b0b55d9844d8a4b56414c4f26a419cf8805f6e",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the convergence properties of a general class of decomposition algorithms for support vector machines (SVMs). We provide a model algorithm for decomposition, and prove necessary and sufficient conditions for stepwise improvement of this algorithm. We introduce a simple \u201crate certifying\u201d condition and prove a polynomial-time bound on the rate of convergence of the model algorithm when it satisfies this condition. Although it is not clear that existing SVM algorithms satisfy this condition, we provide a version of the model algorithm that does. For this algorithm we show that when the slack multiplier C satisfies \u221a1/2 \u2264 C \u2264 mL, where m is the number of samples and L is a matrix norm, then it takes no more than 4LC2m4/\u2208 iterations to drive the criterion to within \u2208 of its optimum."
            },
            "slug": "Polynomial-Time-Decomposition-Algorithms-for-Vector-Hush-Scovel",
            "title": {
                "fragments": [],
                "text": "Polynomial-Time Decomposition Algorithms for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper provides a model algorithm for decomposition, and shows that when the slack multiplier C satisfies \u221a1/2 \u2264 C \u2264 mL, then it takes no more than 4LC2m4/\u2208 iterations to drive the criterion to within \u2208 of its optimum."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11180693,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "341a7c06340f4f8cc9dfbf160d9054c30f104da4",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently the asymptotic convergence of some commonly used decomposition methods for support vector machines has been established. However, their local convergence rates are still unknown. In this paper, under the assumptions that the kernel matrix is positive definite and the problem is non-degenerate, we prove the linear convergence of a popular decomposition method."
            },
            "slug": "Linear-Convergence-of-a-Decomposition-Method-for-Lin",
            "title": {
                "fragments": [],
                "text": "Linear Convergence of a Decomposition Method for Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798966"
                        ],
                        "name": "Hsuan-Tien Lin",
                        "slug": "Hsuan-Tien-Lin",
                        "structuredName": {
                            "firstName": "Hsuan-Tien",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsuan-Tien Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 3
                            }
                        ],
                        "text": "In (Lin and Lin, 2003), we have studied this issue in detail and proposed the following modification:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7727271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47e3fa7eeb0a88d64a9510c33ae256a35616edac",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The sigmoid kernel was quite popular for support vector machines due to its origin from neural networks. Although it is known that the kernel matrix may not be positive semi-definite (PSD), other properties are not fully studied. In this paper, we discuss such non-PSD kernels through the viewpoint of separability. Results help to validate the possible use of non-PSD kernels. One example shows that the sigmoid kernel matrix is conditionally positive definite (CPD) in certain parameters and thus are valid kernels there. However, we also explain that the sigmoid kernel is not better than the RBF kernel in general. Experiments are given to illustrate our analysis. Finally, we discuss how to solve the non-convex dual problems by SMO-type decomposition methods. Suitable modifications for any symmetric non-PSD kernel matrices are proposed with convergence proofs."
            },
            "slug": "A-Study-on-Sigmoid-Kernels-for-SVM-and-the-Training-Lin",
            "title": {
                "fragments": [],
                "text": "A Study on Sigmoid Kernels for SVM and the Training of non-PSD Kernels by SMO-type Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper discusses non-PSD kernels through the viewpoint of separability, and shows that the sigmoid kernel matrix is conditionally positive definite (CPD) in certain parameters and thus are valid kernels there."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47429813"
                        ],
                        "name": "N. Segata",
                        "slug": "N.-Segata",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Segata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Segata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2930903"
                        ],
                        "name": "E. Blanzieri",
                        "slug": "E.-Blanzieri",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Blanzieri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Blanzieri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 86
                            }
                        ],
                        "text": "Under given parameters C > 0and E> 0, the standard form of support vector regression [Vapnik \n1998] is ll tt 1 T min w w + C .i + C .i * w,b,.,. * 2 i=1 i=1 subject to w T f(xi) + b- zi = E + .i, \nzi - w T f(xi) - b = E + .i * , * .i,.i = 0,i = 1,...,l."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14323205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd2e5b8f07a4b4675f4b5ab8273ab10eb80704a9",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "A computationally efficient approach to local learning with kernel methods is presented. The Fast Local Kernel Support Vector Machine (FaLK-SVM) trains a set of local SVMs on redundant neighbourhoods in the training set and an appropriate model for each query point is selected at testing time according to a proximity strategy. Supported by a recent result by Zakai and Ritov (2009) relating consistency and localizability, our approach achieves high classification accuracies by dividing the separation function in local optimisation problems that can be handled very efficiently from the computational viewpoint. The introduction of a fast local model selection further speeds-up the learning process. Learning and complexity bounds are derived for FaLK-SVM, and the empirical evaluation of the approach (with data sets up to 3 million points) showed that it is much faster and more accurate and scalable than state-of-the-art accurate and approximated SVM solvers at least for non high-dimensional data sets. More generally, we show that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability."
            },
            "slug": "Fast-and-Scalable-Local-Kernel-Machines-Segata-Blanzieri",
            "title": {
                "fragments": [],
                "text": "Fast and Scalable Local Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798966"
                        ],
                        "name": "Hsuan-Tien Lin",
                        "slug": "Hsuan-Tien-Lin",
                        "structuredName": {
                            "firstName": "Hsuan-Tien",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsuan-Tien Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2567246"
                        ],
                        "name": "R. C. Weng",
                        "slug": "R.-C.-Weng",
                        "structuredName": {
                            "firstName": "Ruby",
                            "lastName": "Weng",
                            "middleNames": [
                                "Chiu-Hsing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Weng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6445796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a76a95c8766550c84be48efb70dc7391e84553df",
            "isKey": false,
            "numCitedBy": 896,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nPlatt\u2019s probabilistic outputs for Support Vector Machines (Platt, J. in Smola, A., et al. (eds.) Advances in large margin classifiers. Cambridge, 2000) has been popular for applications that require posterior class probabilities. In this note, we propose an improved algorithm that theoretically converges and avoids numerical difficulties. A simple and ready-to-use pseudo code is included.\n"
            },
            "slug": "A-note-on-Platt\u2019s-probabilistic-outputs-for-support-Lin-Lin",
            "title": {
                "fragments": [],
                "text": "A note on Platt\u2019s probabilistic outputs for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An improved algorithm that theoretically converges and avoids numerical difficulties is proposed for Platt\u2019s probabilistic outputs for Support Vector Machines."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30772939"
                        ],
                        "name": "D. Crisp",
                        "slug": "D.-Crisp",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crisp",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Crisp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17714220,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e864149ebd56f331b63c82cec0f2017b337b152c",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that the recently proposed variant of the Support Vector machine (SVM) algorithm, known as \u03bd-SVM, can be interpreted as a maximal separation between subsets of the convex hulls of the data, which we call soft convex hulls. The soft convex hulls are controlled by choice of the parameter \u03bd. If the intersection of the convex hulls is empty, the hyperplane is positioned halfway between them such that the distance between convex hulls, measured along the normal, is maximized; and if it is not, the hyperplane's normal is similarly determined by the soft convex hulls, but its position (perpendicular distance from the origin) is adjusted to minimize the error sum. The proposed geometric interpretation of \u03bd-SVM also leads to necessary and sufficient conditions for the existence of a choice of \u03bd for which the \u03bd-SVM solution is nontrivial."
            },
            "slug": "A-Geometric-Interpretation-of-SVM-Classifiers-Crisp-Burges",
            "title": {
                "fragments": [],
                "text": "A Geometric Interpretation of ?-SVM Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is shown that the recently proposed variant of the Support Vector machine (SVM) algorithm, known as \u03bd-SVM, can be interpreted as a maximal separation between subsets of the convex hulls of the data, which are called soft convex Hulls."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2000"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2181729"
                        ],
                        "name": "Shuo-Peng Liao",
                        "slug": "Shuo-Peng-Liao",
                        "structuredName": {
                            "firstName": "Shuo-Peng",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuo-Peng Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798966"
                        ],
                        "name": "Hsuan-Tien Lin",
                        "slug": "Hsuan-Tien-Lin",
                        "structuredName": {
                            "firstName": "Hsuan-Tien",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsuan-Tien Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 8
                            }
                        ],
                        "text": "Then in (Liao et al., 2002) we show that even if we expand the working set and sub-problem to have four variables, in most decomposition iterations, only the two originally selected variables are changed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12726489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62bdd5086872dced1b9df4e2e9c7b4e1c37b9098",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The dual formulation of support vector regression involves two closely related sets of variables. When the decomposition method is used, many existing approaches use pairs of indices from these two sets as the working set. Basically, they select a base set first and then expand it so all indices are pairs. This makes the implementation different from that for support vector classification. In addition, a larger optimization subproblem has to be solved in each iteration. We provide theoretical proofs and conduct experiments to show that using the base set as the working set leads to similar convergence (number of iterations). Therefore, by using a smaller working set while keeping a similar number of iterations, the program can be simpler and more efficient."
            },
            "slug": "A-Note-on-the-Decomposition-Methods-for-Support-Liao-Lin",
            "title": {
                "fragments": [],
                "text": "A Note on the Decomposition Methods for Support Vector Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work provides theoretical proofs and conduct experiments to show that using the base set as the working set leads to similar convergence (number of iterations) and the program can be simpler and more efficient."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2567246"
                        ],
                        "name": "R. C. Weng",
                        "slug": "R.-C.-Weng",
                        "structuredName": {
                            "firstName": "Ruby",
                            "lastName": "Weng",
                            "middleNames": [
                                "Chiu-Hsing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Weng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13546412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4929d112891770e9795f3e24cc87ca3c63f1f556",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector regression (SVR) has been popular in the past decade, but it provides only an estimated target value instead of predictive probability intervals. Many work have addressed this issue but sometimes the SVR formula must be modified. This paper presents a rather simple and direct approach to construct such intervals. We assume that the conditional distribution of the target value depends on its input only through the predicted value, and propose to model this distribution by simple functions. Experiments show that the proposed approach gives predictive intervals with competitive coverages with Bayesian SVR methods. I. I NTRODUCTION In the past decade support vector regression (SVR) [15], [12] has been popular for regression problems. SVR provides only an estimated target value; however, the statement that the future value falls in an interval with a specified probability is more informative. This paper aims to construct predictive intervals for the future values. For conventional linear regression, the prediction interval has been well developed; for example, see [16] for Gaussian noise case and [3], [14] for non-Gaussian case. SVR differs from conventional regression in that it maps input data into a high dimensional reproducing kernel Hilbert space and uses an -insensitive loss function. As a result, SVR has a sparse representation of solutions, and hence is relatively fast in training/testing. However, due to these differences, the existing methods for constructing prediction intervals can not be applied. Recently Bayesian interpretations of SVR have been developed [6], [4], [2] along the ways of Bayesian techniques for Neural Networks [8] and for SVM classification [13], [11]. Using a Bayesian framework, one can determine parameters in SVR by maximizing an evidence function, and at the same time derive an error bar for prediction. Some of these Bayesian approaches perform well, but in several situations they cannot be applied. For example, they may modify the SVR formulation, so it is more difficult to use existing SVR software. In addition, some may prefer using other methods (e.g., cross validation) for parameter selection. As the best parameters are not from"
            },
            "slug": "Simple-Probabilistic-Predictions-for-Support-Vector-Lin-Weng",
            "title": {
                "fragments": [],
                "text": "Simple Probabilistic Predictions for Support Vector Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a rather simple and direct approach to construct predictive intervals for the future values by assuming that the conditional distribution of the target value depends on its input only through the predicted value, and proposing to model this distribution by simple functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "proposed in Section 3.2 only variables corresponding to a small set are still allowed to move ( Lin, 2002b,  Theorem II.3):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30603357,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d2eb5d46bdcd64a0a23ec3b9119fdfbe3ff75251",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In a previous paper, the author (2001) proved the convergence of a commonly used decomposition method for support vector machines (SVMs). However, there is no theoretical justification about its stopping criterion, which is based on the gap of the violation of the optimality condition. It is essential to have the gap asymptotically approach zero, so we are sure that existing implementations stop in a finite number of iterations after reaching a specified tolerance. Here, we prove this result and illustrate it by two extensions: /spl nu/-SVM and a multiclass SVM by Crammer and Singer (2001). A further result shows that, in final iterations of the decomposition method, only a particular set of variables are still being modified. This supports the use of the shrinking and caching techniques in some existing implementations. Finally, we prove the asymptotic convergence of a decomposition method for this multiclass SVM. Discussions on the difference between this convergence proof and the one in another paper by Lin are also included."
            },
            "slug": "A-formal-analysis-of-stopping-criteria-of-methods-Lin",
            "title": {
                "fragments": [],
                "text": "A formal analysis of stopping criteria of decomposition methods for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A result shows that, in final iterations of the decomposition method, only a particular set of variables are still being modified, which supports the use of the shrinking and caching techniques in some existing implementations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "Domain Computer vision Natural language processing Neuroimaging Bioinformatics Representative \nworks LIBPMK [Grauman and Darrell 2005] Maltparser [Nivre et al. 2007] PyMVPA [Hanke et al. 2009] BDVal \n[Dorff et al. 2010] A typical use of LIBSVM involves two steps: .rst, training a dataset to obtain\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13036203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "625bce34ec80d29242340400d916e799d2975430",
            "isKey": false,
            "numCitedBy": 1593,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches"
            },
            "slug": "The-pyramid-match-kernel:-discriminative-with-sets-Grauman-Darrell",
            "title": {
                "fragments": [],
                "text": "The pyramid match kernel: discriminative classification with sets of image features"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new fast kernel function is presented which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space and is shown to be positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2110475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc912ae25797e5f7c0d73300d3968ad8339b411",
            "isKey": false,
            "numCitedBy": 4686,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a simple subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data."
            },
            "slug": "Estimating-the-Support-of-a-High-Dimensional-Sch\u00f6lkopf-Platt",
            "title": {
                "fragments": [],
                "text": "Estimating the Support of a High-Dimensional Distribution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data by carrying out sequential optimization over pairs of input patterns and providing a theoretical analysis of the statistical performance of the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399006281"
                        ],
                        "name": "F. Fogelman-Souli\u00e9",
                        "slug": "F.-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fogelman-Souli\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195609364,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d703fbc34b55a829a3c703bfc268406dfce0cc0a",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This volume contains the collected papers of the NATO Advanced Research Workshop on Neurocomputing, held in February 1989. Various fields in neurocomputing were covered, including new or improved neural network algorithms, product units, recurrent multilayer networks, Boltzmann machines, Kohonen maps, growth algorithms, VLSI circuits, VLSI neurons, dedicated processors, coding, links with hidden Markov Models (HMMs), recognition of digits, wind maps, mammographic images, cortical columns, posture and movement co-ordination."
            },
            "slug": "Neurocomputing-:-algorithms,-architectures-and-Fogelman-Souli\u00e9-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Neurocomputing : algorithms, architectures and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This volume contains the collected papers of the NATO Advanced Research Workshop on Neurocomputing, held in February 1989, and covered new or improved neural network algorithms, product units, recurrent multilayer networks, Boltzmann machines, Kohonen maps, growth algorithms and dedicated processors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144086182"
                        ],
                        "name": "I. Song",
                        "slug": "I.-Song",
                        "structuredName": {
                            "firstName": "Iickho",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2775410"
                        ],
                        "name": "Hwang-Ki Min",
                        "slug": "Hwang-Ki-Min",
                        "structuredName": {
                            "firstName": "Hwang-Ki",
                            "lastName": "Min",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hwang-Ki Min"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9383401"
                        ],
                        "name": "C. Park",
                        "slug": "C.-Park",
                        "structuredName": {
                            "firstName": "Cheol",
                            "lastName": "Park",
                            "middleNames": [
                                "Hoon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Park"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16237677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bddad4dc0dfa8efa402aa5d18c29304a5760f12",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "1 2 (w \u00b7 w) + \u03b3 2 (w * \u00b7 w *) + C n i=1 \u03be * i s.t. y i ((w \u00b7 z i) + b) \u2265 1 \u2212 \u03be The dual optimization problem of (29) is minimize \u03b1 \u2212 n i=1 \u03b1 i + 1 2 n i, j =1 \u03b1 i \u03b1 j y i y j (z i \u00b7 z j) + 1 2\u03b3 n i, j =1 (\u03b1 i + \u03b2 i \u2212 C)(\u03b1 j + \u03b2 j \u2212 C)(z * i \u00b7 z * j) s.t. ACKNOWLEDGMENT The authors would like to thank the anonymous reviewers for their helpful technical feedback. REFERENCES [1] B. Bakker and T. Heskes, \" Task clustering and gating for Bayesian multitask learning, \" A framework for learning predictive structures from multiple tasks and unlabeled data, \" [20] J. Platt, \" Using sparseness and analytic QP to speed training of support vector machines, \" in Multi-task learning for classification with Dirichlet process priors, \" Abstract\u2014 Owing to the singularity of the within-class scatter, linear discriminant analysis (LDA) becomes ill-posed for small sample size (SSS) problems. Null-space-based LDA (NLDA), which is an extension of LDA, provides good discriminant performances for SSS problems. Yet, as the original scheme for the feature extractor (FE) of NLDA suffers from a complexity burden, a few modified schemes have since been proposed for complexity reduction. In this brief, by transforming the problem of finding the FE of NLDA into a linear equation problem, a novel scheme is derived, offering a further reduction of the complexity."
            },
            "slug": "Working-Set-Selection-Using-Second-Order-for-Svm,-\"-Song-Min",
            "title": {
                "fragments": [],
                "text": "Working Set Selection Using Second Order Information for Training Svm, \" Complexity-reduced Scheme for Feature Extraction with Linear Discriminant Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 30,
                "text": "By transforming the problem of finding the FE of NLDA into a linear equation problem, a novel scheme is derived, offering a further reduction of the complexity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 86
                            }
                        ],
                        "text": "Under given parameters C > 0and E> 0, the standard form of support vector regression [Vapnik \n1998] is ll tt 1 T min w w + C .i + C .i * w,b,.,. * 2 i=1 i=1 subject to w T f(xi) + b- zi = E + .i, \nzi - w T f(xi) - b = E + .i * , * .i,.i = 0,i = 1,...,l."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "It can be clearly seen that C-SVC and one-class SVM are already in the form of problem \n(11)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 70
                            }
                        ],
                        "text": ", l, in two classes, and a vector y \u2208 Rl such that yi \u2208 {1,\u22121}, C-SVC (Cortes and Vapnik, 1995; Vapnik, 1998) solves the following primal problem:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 111
                            }
                        ],
                        "text": "Then, according to the SVM formulation, svm train one calls a corresponding subroutine \nsuch as solve c svc for C-SVC and solve nu svc for ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "Note that b of C-SVC and E-SVR plays \nthe same role as -. in one-class SVM, so we de.ne ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 96
                            }
                        ],
                        "text": "In Section 2, we describe SVM formulations sup\u00adported in LIBSVM: C-Support Vector \nClassi.cation (C-SVC), ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "{1,-1}, C-SVC \n[Boser et al. 1992; Cortes and Vapnik 1995] solves 4LIBSVM Tools: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. \nthe following primal optimization problem: l t min 1 w T w +C .i (1) w,b,. 2 i=1 subject to yi(w T f(xi) \n+b) =1 -.i, .i =0,i =1,...,l, where f(xi)maps xi into a higher-dimensional space and C > 0 is the regularization \nparameter."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "CONCLUSIONS When we released the .rst version \nof LIBSVM in 2000, only two-class C-SVC was supported."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 47
                            }
                        ],
                        "text": "Quadratic Problems with One Linear Constraint: C-SVC, -SVR, and One-Class SVM We consider \nthe following general form of C-SVC, E-SVR, and one-class SVM: min f (a) a subject to yT a =,, (11) 0 \n=at =C,t =1,...,l, where T f (a) = 1 aTQa +pa 2 and yt =\u00b11,t =1,...,l."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 122
                            }
                        ],
                        "text": ", (xl, zl)}, such that xi \u2208 Rn is an input and zi \u2208 R1 is a target output, the standard form of support vector regression (Vapnik, 1998) is:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "The subroutine So, stoplve minimizes a general form of SVM 7The \ndefault solver is C-SVC using the RBF kernel (48) with C = 1and ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 47
                            }
                        ],
                        "text": "Algorithm 1 requires an initial feasible a.For C-SVC and E-SVR, because the zero vector is feasible, \nwe select it as the initial a."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": true,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15058655"
                        ],
                        "name": "S. Knerr",
                        "slug": "S.-Knerr",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Knerr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Knerr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078169"
                        ],
                        "name": "L. Personnaz",
                        "slug": "L.-Personnaz",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Personnaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Personnaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097910"
                        ],
                        "name": "G. Dreyfus",
                        "slug": "G.-Dreyfus",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Dreyfus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dreyfus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59718844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26b96b404827222040d42b25067dba60749d9ddc",
            "isKey": false,
            "numCitedBy": 918,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A stepwise procedure for building and training a neural network intended to perform classification tasks, based on single layer learning rules, is presented. This procedure breaks up the classification task into subtasks of increasing complexity in order to make learning easier. The network structure is not fixed in advance: it is subject to a growth process during learning. Therefore, after training, the architecture of the network is guaranteed to be well adapted for the classification problem."
            },
            "slug": "Single-layer-learning-revisited:-a-stepwise-for-and-Knerr-Personnaz",
            "title": {
                "fragments": [],
                "text": "Single-layer learning revisited: a stepwise procedure for building and training a neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A stepwise procedure for building and training a neural network intended to perform classification tasks, based on single layer learning rules, is presented, which breaks up the classification task into subtasks of increasing complexity in order to make learning easier."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3182415"
                        ],
                        "name": "Tingyao Wu",
                        "slug": "Tingyao-Wu",
                        "structuredName": {
                            "firstName": "Tingyao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tingyao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2567246"
                        ],
                        "name": "R. C. Weng",
                        "slug": "R.-C.-Weng",
                        "structuredName": {
                            "firstName": "Ruby",
                            "lastName": "Weng",
                            "middleNames": [
                                "Chiu-Hsing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Weng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 51
                            }
                        ],
                        "text": "Detailed implementation notes are in Appendix C of (Wu et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 20
                            }
                        ],
                        "text": "More details are in (Wu et al., 2004) for classification and in (Lin and Weng, 2004) for regression."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 28
                            }
                        ],
                        "text": "Then the second approach in (Wu et al., 2004) is used to obtain pi from all these rij \u2019s."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7664224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d02083f5e9206ff88ed1e284da8665d4bee81ba",
            "isKey": false,
            "numCitedBy": 1891,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Pairwise coupling is a popular multi-class classification method that combines all comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods: voting and the method by Hastie and Tibshirani (1998)"
            },
            "slug": "Probability-Estimates-for-Multi-class-by-Pairwise-Wu-Lin",
            "title": {
                "fragments": [],
                "text": "Probability Estimates for Multi-class Classification by Pairwise Coupling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Two approaches for obtaining class probabilities can be reduced to linear systems and are easy to implement and shown conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705762"
                        ],
                        "name": "N. List",
                        "slug": "N.-List",
                        "structuredName": {
                            "firstName": "Nikolas",
                            "lastName": "List",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. List"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9982978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "549483a142ba392bbe253eadfdd3eeabf1fdc429",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efficiently approaches the optimal solution. The number of iterations required to be within e of optimality grows linearly with 1/e and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k 0 equality constraints for some fixed constant k 0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results of Hush and Scovel [8] in several ways. First our result holds for Convex Quadratic Optimization whereas the results of Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler."
            },
            "slug": "General-Polynomial-Time-Decomposition-Algorithms-List-Simon",
            "title": {
                "fragments": [],
                "text": "General Polynomial Time Decomposition Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efficiently approaches the optimal solution."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5658362,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fb88874bd831c5e469fdf14b4ac5089eb995e1f6",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The asymptotic convergence of C.-J. Lin (2001) can be applied to a modified SMO (sequential minimal optimization) algorithm by S.S. Keerthi et al. (2001) with some assumptions. The author shows that for this algorithm those assumptions are not necessary."
            },
            "slug": "Asymptotic-convergence-of-an-SMO-algorithm-without-Lin",
            "title": {
                "fragments": [],
                "text": "Asymptotic convergence of an SMO algorithm without any assumptions"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2773229"
                        ],
                        "name": "Kevin C. Dorff",
                        "slug": "Kevin-C.-Dorff",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Dorff",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin C. Dorff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49333106"
                        ],
                        "name": "N. Chambwe",
                        "slug": "N.-Chambwe",
                        "structuredName": {
                            "firstName": "Nyasha",
                            "lastName": "Chambwe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chambwe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100313"
                        ],
                        "name": "Marko Srdanovic",
                        "slug": "Marko-Srdanovic",
                        "structuredName": {
                            "firstName": "Marko",
                            "lastName": "Srdanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marko Srdanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47823776"
                        ],
                        "name": "F. Campagne",
                        "slug": "F.-Campagne",
                        "structuredName": {
                            "firstName": "Fabien",
                            "lastName": "Campagne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Campagne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 192
                            }
                        ],
                        "text": "Domain Computer vision Natural language processing Neuroimaging Bioinformatics Representative \nworks LIBPMK [Grauman and Darrell 2005] Maltparser [Nivre et al. 2007] PyMVPA [Hanke et al. 2009] BDVal \n[Dorff et al. 2010] A typical use of LIBSVM involves two steps: .rst, training a dataset to obtain a \nmodel and second, using the model to predict information of a testing dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Neuroimaging Bioinformatics Representative \nworks LIBPMK [Grauman and Darrell 2005] Maltparser [Nivre et al. 2007] PyMVPA [Hanke et al. 2009] BDVal \n[Dorff et al. 2010] A typical use of LIBSVM involves two steps: .rst, training a dataset to obtain a \nmodel and second, using the model to predict\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "BDVal: reproducible large-scale predictive model development \nand validation in high-throughput datasets."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10212158,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "63f8529c7669d35b5196417c5e208ca09e305851",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "UNLABELLED\nHigh-throughput data can be used in conjunction with clinical information to develop predictive models. Automating the process of developing, evaluating and testing such predictive models on different datasets would minimize operator errors and facilitate the comparison of different modeling approaches on the same dataset. Complete automation would also yield unambiguous documentation of the process followed to develop each model. We present the BDVal suite of programs that fully automate the construction of predictive classification models from high-throughput data and generate detailed reports about the model construction process. We have used BDVal to construct models from microarray and proteomics data, as well as from DNA-methylation datasets. The programs are designed for scalability and support the construction of thousands of alternative models from a given dataset and prediction task.\n\n\nAVAILABILITY AND IMPLEMENTATION\nThe BDVal programs are implemented in Java, provided under the GNU General Public License and freely available at http://bdval.campagnelab.org."
            },
            "slug": "BDVal:-reproducible-large-scale-predictive-model-in-Dorff-Chambwe",
            "title": {
                "fragments": [],
                "text": "BDVal: reproducible large-scale predictive model development and validation in high-throughput datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The BDVal suite of programs are presented, that fully automate the construction of predictive classification models from high-throughput data and generate detailed reports about the model construction process."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761354"
                        ],
                        "name": "Michael Hanke",
                        "slug": "Michael-Hanke",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hanke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Hanke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722413"
                        ],
                        "name": "Y. Halchenko",
                        "slug": "Y.-Halchenko",
                        "structuredName": {
                            "firstName": "Yaroslav",
                            "lastName": "Halchenko",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Halchenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2196760"
                        ],
                        "name": "P. Sederberg",
                        "slug": "P.-Sederberg",
                        "structuredName": {
                            "firstName": "Per",
                            "lastName": "Sederberg",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sederberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2327323"
                        ],
                        "name": "J. Haxby",
                        "slug": "J.-Haxby",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Haxby",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Haxby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872018"
                        ],
                        "name": "S. Pollmann",
                        "slug": "S.-Pollmann",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Pollmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pollmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 28
                            }
                        ],
                        "text": ", 2007) Neuroimaging PyMVPA (Hanke et al., 2009) Bioinformatics BDVal (Dorff et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 143
                            }
                        ],
                        "text": "\u2026language processing Neuroimaging Bioinformatics Representative \nworks LIBPMK [Grauman and Darrell 2005] Maltparser [Nivre et al. 2007] PyMVPA [Hanke et al. 2009] BDVal \n[Dorff et al. 2010] A typical use of LIBSVM involves two steps: .rst, training a dataset to obtain a \nmodel and second,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 165
                            }
                        ],
                        "text": "Domain Computer vision Natural language processing Neuroimaging Bioinformatics Representative \nworks LIBPMK [Grauman and Darrell 2005] Maltparser [Nivre et al. 2007] PyMVPA [Hanke et al. 2009] BDVal \n[Dorff et al. 2010] A typical use of LIBSVM involves two steps: .rst, training a dataset to obtain a \nmodel and second, using the model to predict information of a testing dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "PyMVPA: A Python toolbox for multivariate pattern analysis of fMRI \ndata."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 9
                            }
                        ],
                        "text": "(1997a); Joachims (1998); Platt (1998); Keerthi et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 125
                            }
                        ],
                        "text": "For instructions of using LIBSVM, see the README file included in the package, the LIBSVM FAQ,(2) and the practical guide by Hsu et al. (2003). An earlier version of this article was published in Chang and Lin (2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2845142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2094bfcda9b544223a2de9ea3f57dc643bcbd1ba",
            "isKey": true,
            "numCitedBy": 420,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Decoding patterns of neural activity onto cognitive states is one of the central goals of functional brain imaging. Standard univariate fMRI analysis methods, which correlate cognitive and perceptual function with the blood oxygenation-level dependent (BOLD) signal, have proven successful in identifying anatomical regions based on signal increases during cognitive and perceptual tasks. Recently, researchers have begun to explore new multivariate techniques that have proven to be more flexible, more reliable, and more sensitive than standard univariate analysis. Drawing on the field of statistical learning theory, these new classifier-based analysis techniques possess explanatory power that could provide new insights into the functional properties of the brain. However, unlike the wealth of software packages for univariate analyses, there are few packages that facilitate multivariate pattern classification analyses of fMRI data. Here we introduce a Python-based, cross-platform, and open-source software toolbox, called PyMVPA, for the application of classifier-based analysis techniques to fMRI datasets. PyMVPA makes use of Python\u2019s ability to access libraries written in a large variety of programming languages and computing environments to interface with the wealth of existing machine learning packages. We present the framework in this paper and provide illustrative examples on its usage, features, and programmability."
            },
            "slug": "PyMVPA:-a-Python-Toolbox-for-Multivariate-Pattern-Hanke-Halchenko",
            "title": {
                "fragments": [],
                "text": "PyMVPA: a Python Toolbox for Multivariate Pattern Analysis of fMRI Data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A Python-based, cross-platform, and open-source software toolbox, called PyMVPA, for the application of classifier-based analysis techniques to fMRI datasets, which makes use of Python\u2019s ability to access libraries written in a large variety of programming languages and computing environments to interface with the wealth of existing machine learning packages."
            },
            "venue": {
                "fragments": [],
                "text": "Neuroinformatics"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681502"
                        ],
                        "name": "Yuji Matsumoto",
                        "slug": "Yuji-Matsumoto",
                        "structuredName": {
                            "firstName": "Yuji",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuji Matsumoto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7833213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0171832161d0c6dc61fc0dd22e47ce58b240b2f5",
            "isKey": false,
            "numCitedBy": 520,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Parsing unrestricted text is useful for many language technology applications but requires parsing methods that are both robust and efficient. MaltParser is a language-independent system for data-driven dependency parsing that can be used to induce a parser for a new language from a treebank sample in a simple yet flexible manner. Experimental evaluation confirms that MaltParser can achieve robust, efficient and accurate parsing for a wide range of languages without language-specific enhancements and with rather limited amounts of training data."
            },
            "slug": "MaltParser:-A-language-independent-system-for-Matsumoto",
            "title": {
                "fragments": [],
                "text": "MaltParser: A language-independent system for data-driven dependency parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental evaluation confirms that MaltParser can achieve robust, efficient and accurate parsing for a wide range of languages without language-specific enhancements and with rather limited amounts of training data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720988"
                        ],
                        "name": "Joakim Nivre",
                        "slug": "Joakim-Nivre",
                        "structuredName": {
                            "firstName": "Joakim",
                            "lastName": "Nivre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joakim Nivre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712813"
                        ],
                        "name": "Johan Hall",
                        "slug": "Johan-Hall",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Hall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johan Hall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145446170"
                        ],
                        "name": "Jens Nilsson",
                        "slug": "Jens-Nilsson",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Nilsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jens Nilsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649192"
                        ],
                        "name": "Atanas Chanev",
                        "slug": "Atanas-Chanev",
                        "structuredName": {
                            "firstName": "Atanas",
                            "lastName": "Chanev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Atanas Chanev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3348151"
                        ],
                        "name": "G. Eryigit",
                        "slug": "G.-Eryigit",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Eryigit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Eryigit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804668"
                        ],
                        "name": "Sandra K\u00fcbler",
                        "slug": "Sandra-K\u00fcbler",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "K\u00fcbler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandra K\u00fcbler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1915339"
                        ],
                        "name": "S. Marinov",
                        "slug": "S.-Marinov",
                        "structuredName": {
                            "firstName": "Svetoslav",
                            "lastName": "Marinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748039"
                        ],
                        "name": "E. Marsi",
                        "slug": "E.-Marsi",
                        "structuredName": {
                            "firstName": "Erwin",
                            "lastName": "Marsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Marsi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Domain Representative works Computer vision LIBPMK (Grauman and Darrell, 2005) Natural language processing Maltparser (Nivre et al., 2007) Neuroimaging PyMVPA (Hanke et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9743340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4546c7b4d2103523b104daddea6c7eaa1925cf12",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Parsing unrestricted text is useful for many language technology applications but requires parsing methods that are both robust and efficient. MaltParser is a language-independent system for data-driven dependency parsing that can be used to induce a parser for a new language from a treebank sample in a simple yet flexible manner. Experimental evaluation confirms that MaltParser can achieve robust, efficient and accurate parsing for a wide range of languages without language-specific enhancements and with rather limited amounts of training data."
            },
            "slug": "MaltParser:-A-Language-Independent-System-for-Nivre-Hall",
            "title": {
                "fragments": [],
                "text": "MaltParser: A Language-Independent System for Data-Driven Dependency Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental evaluation confirms that MaltParser can achieve robust, efficient and accurate parsing for a wide range of languages without language-specific enhancements and with rather limited amounts of training data."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277171"
                        ],
                        "name": "Pai-Hsuen Chen",
                        "slug": "Pai-Hsuen-Chen",
                        "structuredName": {
                            "firstName": "Pai-Hsuen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pai-Hsuen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491001514"
                        ],
                        "name": "Bernhard Sch lkopf",
                        "slug": "Bernhard-Sch-lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "lkopf",
                            "middleNames": [
                                "Sch"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Sch lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60673449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6eaecfcb51a3edd5fb7b25c3d7a20f555b9e7b76",
            "isKey": false,
            "numCitedBy": 453,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Disclosed are apparatus (10) for making a complexly patterned extrudate. The apparatus (10) includes a food cooker extruder (12) for providing at least one extrudable food product, at least one food color supply (18), a pattern forming die (20) for mixing the food color and extrudate food product to form a complexly patterned food product, a reducing passageway for reducing the cross sectional area of the patterned dough from an inlet end (34) to outlet end (36) of at least 50:1 at an average convergence angle of </=45 DEG and an extruder exit port at the discharge end of the reducing passageway. The methods comprise the steps of: providing a plastic extrudable food mass; providing at least one color; mixing the food mass and color to form an organized complexly patterned food dough having an initial cross sectional area; reducing the initial cross sectional area by a factor of at least 50:1 with an average convergence angle of </=45 DEG while maintaining the cross sectional pattern to form a reduced cross sectional patterned dough, and extruding the reduced cross sectional dough through a die port having an opening of equal to the reduced cross sectional area to form a complexly patterned extrudate."
            },
            "slug": "A-tutorial-on-support-vector-machines-Chen-Lin",
            "title": {
                "fragments": [],
                "text": "A tutorial on?-support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The methods comprise the steps of providing a plastic extrudable food mass; providing at least one color; mixing the food mass and color to form an organized complexly patterned food dough having an initial cross sectional area."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14461054,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d700e611ee7ffdf54873684a9e8883d3da0bcd7",
            "isKey": false,
            "numCitedBy": 1188,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Among other things, we prove that multiquadric surface interpolation is always solvable, thereby settling a conjecture of R. Franke."
            },
            "slug": "Interpolation-of-scattered-data:-Distance-matrices-Micchelli",
            "title": {
                "fragments": [],
                "text": "Interpolation of scattered data: Distance matrices and conditionally positive definite functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "REFERENCES BOSER,B.E., GUYON,I., AND VAPNIK, V. 1992."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "AND VAPNIK, V. 1995."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "VAPNIK, V. 1998."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik . Supportvector network"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 81
                            }
                        ],
                        "text": ", l, in two classes, and an indicator vector y \u2208 Rl such that yi \u2208 {1,\u22121}, C-SVC [Boser et al. 1992; Cortes and Vapnik 1995] solves"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "It can be clearly seen that C-SVC and one-class SVM are already in the form of problem \n(11)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 111
                            }
                        ],
                        "text": "Then, according to the SVM formulation, svm train one calls a corresponding subroutine \nsuch as solve c svc for C-SVC and solve nu svc for ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "Note that b of C-SVC and E-SVR plays \nthe same role as -. in one-class SVM, so we de.ne ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 96
                            }
                        ],
                        "text": "In Section 2, we describe SVM formulations sup\u00adported in LIBSVM: C-Support Vector \nClassi.cation (C-SVC), ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "{1,-1}, C-SVC \n[Boser et al. 1992; Cortes and Vapnik 1995] solves 4LIBSVM Tools: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. \nthe following primal optimization problem: l t min 1 w T w +C .i (1) w,b,. 2 i=1 subject to yi(w T f(xi) \n+b) =1 -.i, .i =0,i =1,...,l, where f(xi)maps xi into a higher-dimensional space and C > 0 is the regularization \nparameter."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "CONCLUSIONS When we released the .rst version \nof LIBSVM in 2000, only two-class C-SVC was supported."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 34
                            }
                        ],
                        "text": "{1,-1}, C-SVC \n[Boser et al. 1992; Cortes and Vapnik 1995] solves 4LIBSVM Tools: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. \nthe following primal optimization problem: l t min 1 w T w +C .i (1) w,b,. 2 i=1 subject to yi(w T f(xi) \n+b) =1 -.i, .i =0,i =1,...,l, where f(xi)maps xi into a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 47
                            }
                        ],
                        "text": "Quadratic Problems with One Linear Constraint: C-SVC, -SVR, and One-Class SVM We consider \nthe following general form of C-SVC, E-SVR, and one-class SVM: min f (a) a subject to yT a =,, (11) 0 \n=at =C,t =1,...,l, where T f (a) = 1 aTQa +pa 2 and yt =\u00b11,t =1,...,l."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "The subroutine So, stoplve minimizes a general form of SVM 7The \ndefault solver is C-SVC using the RBF kernel (48) with C = 1and ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 47
                            }
                        ],
                        "text": "Algorithm 1 requires an initial feasible a.For C-SVC and E-SVR, because the zero vector is feasible, \nwe select it as the initial a."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support-vector network"
            },
            "venue": {
                "fragments": [],
                "text": "Mach. Learn. 20, 273\u2013297."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 17
                            }
                        ],
                        "text": "1 (Theorem IV in (Fan et al., 2005)) Assume Q is positive semi-definite."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 49
                            }
                        ],
                        "text": "Algorithm 1 (An SMO-type Decomposition method in (Fan et al., 2005)) 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 62
                            }
                        ],
                        "text": "Here we consider an SMO-type decomposition method proposed in (Fan et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Working set selection using the second order information for training SVM"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Machine Learning Research,"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "139044657"
                        ],
                        "name": "U.H.-G. Kressel",
                        "slug": "U.H.-G.-Kressel",
                        "structuredName": {
                            "firstName": "U.H.-G.",
                            "lastName": "Kressel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U.H.-G. Kressel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 96
                            }
                        ],
                        "text": "Several works have studied the number of iterations of decomposition methods; see, for example, List and Simon (2007). However, algorithms studied in these works are slightly different from LIBSVM, so there is no theoretical result yet on LIBSVM\u2019s number of iterations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 74
                            }
                        ],
                        "text": "Some earlier works on decomposition methods for SVM include, for example, Osuna et al. (1997a); Joachims (1998); Platt (1998); Keerthi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57961414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6de3c8cc48ce8b0e11802a1abce6b7a890dc8c5",
            "isKey": false,
            "numCitedBy": 1140,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pairwise-classification-and-support-vector-machines-Kressel",
            "title": {
                "fragments": [],
                "text": "Pairwise classification and support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Weston and Watkins, 1998; Platt et al., 2000)) have shown that it does not perform as good as \u201cone-against-one\u201d In addition, though we have to train as many as k(k \u2212 1)/2 classifiers, as each problem is smaller (only data from two classes), the total training time may not be more than the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7359186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89a37349688b49bbfc9fd643db5a41b9071f9ca2",
            "isKey": false,
            "numCitedBy": 1309,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multi-Class-Support-Vector-Machines-Weston-Watkins",
            "title": {
                "fragments": [],
                "text": "Multi-Class Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 236168999,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training \u03bd-Support Vector Regression: Theory and Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061462269"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56563878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "384bb3944abe9441dcd2cede5e7cd7353e9ee5f7",
            "isKey": false,
            "numCitedBy": 5100,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Outputs-for-Support-vector-Machines-Platt",
            "title": {
                "fragments": [],
                "text": "Probabilistic Outputs for Support vector Machines and Comparisons to Regularized Likelihood Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 45
                            }
                        ],
                        "text": "{1,-1}, C-SVC \n[Boser et al. 1992; Cortes and Vapnik 1995] solves 4LIBSVM Tools: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. \nthe following primal optimization problem: l t min 1 w T w +C .i (1) w,b,. 2 i=1 subject to yi(w T f(xi) \n+b) =1 -.i, .i =0,i =1,...,l, where f(xi)maps xi into a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support - vector network"
            },
            "venue": {
                "fragments": [],
                "text": "Mach . Learn ."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support vector machines: Training and applications. AI Memo 1602"
            },
            "venue": {
                "fragments": [],
                "text": "Support vector machines: Training and applications. AI Memo 1602"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LIBSVM: a library for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Transactions on Intelligent Systems and Technology,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum-Gain working set selection for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support-vector network"
            },
            "venue": {
                "fragments": [],
                "text": "Mach. Learn"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A study on SMOtype decomposition methods for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . Neural Netw ."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kressel . Pairwise classification and support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Another approach to polychotomous classification"
            },
            "venue": {
                "fragments": [],
                "text": "Another approach to polychotomous classification"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum - Gain working set selection for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "J . Mach . Learn . Res ."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 45
                            }
                        ],
                        "text": "The first use of this strategy on SVM was in (Friedman, 1996; Kre\u00dfel, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Another approach to polychotomous classification"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Department of Statistics, Stanford University,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 62
                            }
                        ],
                        "text": "The convergence of decomposition methods was first studied in (Chang et al., 2000) but algorithms discussed there do not coincide with existing implementations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "However, its result applies only to decomposition methods discussed in (Chang et al., 2000) but not LIBSVM or other existing software."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The analysis of decomposition methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A geometric interpretation of \u03bd-SVM"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mangasarian . RSVM : Reduced support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LIBSVM : A library for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans . Intell . Syst . Technol ."
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum - Gain working set selection for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "J . Mach . Learn . Res ."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contour plot of running the parameter selection tool in LIBSVM. The dataset heart scale (included in the package) is used. The x-axis is log 2 C and the y-axis is log 2 \u03b3"
            },
            "venue": {
                "fragments": [],
                "text": "Contour plot of running the parameter selection tool in LIBSVM. The dataset heart scale (included in the package) is used. The x-axis is log 2 C and the y-axis is log 2 \u03b3"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the convergence of a modified version of SVM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Optimization Methods and Software,"
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 77,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/LIBSVM:-A-library-for-support-vector-machines-Chang-Lin/273dfbcb68080251f5e9ff38b4413d7bd84b10a1?sort=total-citations"
}