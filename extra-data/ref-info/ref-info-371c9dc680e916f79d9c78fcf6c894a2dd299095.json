{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 20
                            }
                        ],
                        "text": "Learning with hints [1, 4, 6, 16] constructs a neural network with n output units, one for each function Ik (k = 1,2, ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18522085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9464d15f4f8d578f93332db4aa1c9c182fd51735",
            "isKey": false,
            "numCitedBy": 734,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multitask-Learning:-A-Knowledge-Based-Source-of-Caruana",
            "title": {
                "fragments": [],
                "text": "Multitask Learning: A Knowledge-Based Source of Inductive Bias"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "See [17] for a more detailed description of EBNN and further references."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "The last method described here uses the explanation-based neural network learning algorithm (EBNN), which was originally proposed in the context of reinforcement learning [8, 17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46398994,
            "fieldsOfStudy": [
                "Computer Science",
                "Education",
                "Psychology"
            ],
            "id": "75e50717070e82cdf3945265a75def6960b55a9d",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nLifelong learning addresses situations in which a learner faces a series of different learning tasks providing the opportunity for synergy among them. Explanation-based neural network learning (EBNN) is a machine learning algorithm that transfers knowledge across multiple learning tasks. When faced with a new learning task, EBNN exploits domain knowledge accumulated in previous learning tasks to guide generalization in the new one. As a result, EBNN generalizes more accurately from less data than comparable methods. Explanation-Based Neural Network Learning: A Lifelong Learning Approach describes the basic EBNN paradigm and investigates it in the context of supervised learning, reinforcement learning, robotics, and chess."
            },
            "slug": "Explanation-based-neural-network-learning-a-Thrun",
            "title": {
                "fragments": [],
                "text": "Explanation-based neural network learning a lifelong learning approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Learning with hints [1, 4, 6, 16] constructs a neural network with n output units, one for each function fk (k = 1; 2; : : :; n)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6211302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a24508e65e599b5b20c33af96dbe7017d5caca37",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Probably the most important problem in machine learning is the preliminary biasing of a learner's hypothesis space so that it is small enough to ensure good generalisation from reasonable training sets, yet large enough that it contains a good solution to the problem being learnt. In this paper a mechanism for {\\em automatically} learning or biasing the learner's hypothesis space is introduced. It works by first learning an appropriate {\\em internal representation} for a learning environment and then using that representation to bias the learner's hypothesis space for the learning of future tasks drawn from the same environment. \nAn internal representation must be learnt by sampling from {\\em many similar tasks}, not just a single task as occurs in ordinary machine learning. It is proved that the number of examples $m$ {\\em per task} required to ensure good generalisation from a representation learner obeys $m = O(a+b/n)$ where $n$ is the number of tasks being learnt and $a$ and $b$ are constants. If the tasks are learnt independently ({\\em i.e.} without a common representation) then $m=O(a+b)$. It is argued that for learning environments such as speech and character recognition $b\\gg a$ and hence representation learning in these environments can potentially yield a drastic reduction in the number of examples required per task. It is also proved that if $n = O(b)$ (with $m=O(a+b/n)$) then the representation learnt will be good for learning novel tasks from the same environment, and that the number of examples required to generalise well on a novel task will be reduced to $O(a)$ (as opposed to $O(a+b)$ if no representation is used). \nIt is shown that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results."
            },
            "slug": "Learning-internal-representations-Baxter",
            "title": {
                "fragments": [],
                "text": "Learning internal representations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is proved that the number of examples required to ensure good generalisation from a representation learner obeys and that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The last method described here uses the explanation-based neural network learning algorithm (EBNN), which was originally proposed in the context of reinforcement learning [8,  17 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One of the approaches here, EBNN, has extensively been studied in the context of robot perception [11], reinforcement learning for robot control, and chess [ 17 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "See [ 17 ] for a more detailed description of EBNN and further references."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60488206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939d7319b058d8f4d413daa7a153e25e798b9372",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter introduces the major learning approach studied in this book: the explanation-based neural network learning algorithm (EBNN). EBNN approaches the meta-level learning problem by learning a theory of the domain. This domain theory is domain-specific. It characterizes, for example, the relevance of individual features, their cross-dependencies, or certain invariant properties of the domain that apply to all learning tasks within the domain. Obviously, when the learner has a model of such regularities, there is an opportunity to generalize more accurately or, alternatively, learn from less training data. This is because without knowledge about these regularities the learner has to learn them from scratch, which necessarily requires more training data. EBNN transfers previously learned knowledge by explaining and analyzing training examples in terms of the domain theory. The result of this analysis is a set of domain-specific shape constraints for the function to be learned at the base-level. Thus, these constraints guide the base-level learning of new functions in a knowledgeable, domain-specific way."
            },
            "slug": "Explanation-based-neural-network-learning-Thrun",
            "title": {
                "fragments": [],
                "text": "Explanation-based neural network learning"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This chapter introduces the major learning approach studied in this book: the explanation-based neural network learning algorithm (EBNN), which approaches the meta-level learning problem by learning a theory of the domain that characterizes the relevance of individual features, their cross-dependencies, or certain invariant properties of the Domain."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More specifically, training examples in EBNN are of the sort hx; fn(x);rxfn(x)i, which are fit using the Tangent-Prop algorithm [14]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2184474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff32cebbdb8a436ccd8ae797647428615ae32d74",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera). \n \nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform."
            },
            "slug": "Tangent-Prop-A-Formalism-for-Specifying-Selected-in-Simard-Victorri",
            "title": {
                "fragments": [],
                "text": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A scheme is implemented that allows a network to learn the derivative of its outputs with respect to distortion operators of their choosing, which not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the authors wish the network to perform."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113373342"
                        ],
                        "name": "Daniel N. Hill",
                        "slug": "Daniel-N.-Hill",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Hill",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel N. Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152465217"
                        ],
                        "name": "Michael P. Johnson",
                        "slug": "Michael-P.-Johnson",
                        "structuredName": {
                            "firstName": "Michael P.",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael P. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "An alternative way for exploiting support sets to improve memory-based learning is to learn a distance function [3, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18813809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6556d9f911b5565420c1854b2b4235a20d6fb5a0",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The generalization error of a function approximator, feature set or smoother can be estimated directly by the leave-one-out cross-validation error. For memory-based methods, this is computationally feasible. We describe an initial version of a general memory-based learning system (GMBL): a large collection of learners brought into a widely applicable machine-learning family. We present ongoing investigations into search algorithms which, given a dataset, nd the family members and features that generalize best. We also describe GMBL's application to two noisy, di cult problems|predicting car engine emissions from pressure waves, and controlling a robot billiards player with redundant state variables."
            },
            "slug": "An-Empirical-Investigation-of-Brute-Force-to-choose-Moore-Hill",
            "title": {
                "fragments": [],
                "text": "An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An initial version of a general memory-based learning system (GMBL) is described: a large collection of learners brought into a widely applicable machine-learning family, and ongoing investigations into search algorithms which, given a dataset, are the family members and features that generalize best."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3192218"
                        ],
                        "name": "W. Ahn",
                        "slug": "W.-Ahn",
                        "structuredName": {
                            "firstName": "Woo-kyoung",
                            "lastName": "Ahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31200689"
                        ],
                        "name": "W. Brewer",
                        "slug": "W.-Brewer",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Brewer",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Brewer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They are often able to generalize correctly even from a single training example [2, 10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The results are also consistent with observations made about human learning [2, 10], namely that previously learned knowledge plays an important role in generalization, particularly when training data is scarce."
                    },
                    "intents": []
                }
            ],
            "corpusId": 141309203,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ee66a0d2c61dbccdaaa7111b0ccf08067b8c5507",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reviews traditional psychological theories of concept acquisition. Almost all of these theories assume that concepts are learned through the repeated occurrence of similar instances. This broad framework will be referred to as repeated similarity\u2014based learning (RSBL). We contrast these psychological theories with the recent work in artificial intelligence on explanation\u2014based learning (EBL) that makes strong use of the learner\u2019s background knowledge. In addition, we report the results from a series of experiments which show that humans carry out EBL of new information when exposed to single instances of material from knowledge\u2014rich domains."
            },
            "slug": "Psychological-Studies-of-Explanation\u2014Based-Learning-Ahn-Brewer",
            "title": {
                "fragments": [],
                "text": "Psychological Studies of Explanation\u2014Based Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Learning with hints [1, 4, 6, 16] constructs a neural network with n output units, one for each function fk (k = 1; 2; : : :; n)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 319536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3cd36c092abd65d6ac8e648f3468eeee90ee1fc",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-from-hints-in-neural-networks-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "Learning from hints in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "An alternative way for exploiting support sets to improve memory-based learning is to learn a distance function [3, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53765548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "547b014cb7ed593e661b9c2847dc305bde77833b",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of locally weighted regression in memory-based robot learning is explored. A local model is formed to answer each query, using a weighted regression in which close points (similar experiences) are weighted more than distant points (less relevant experiences). This approach implements a philosophy of modeling a complex function with many simple local models. The author explains how an appropriate distance metric or measure of similarity can be found, and how the distance metric is used. How irrelevant input variables and terms in the local model are detected is also explained. An example from the control of a robot arm is used to compare this approach with other robot control and learning techniques.<<ETX>>"
            },
            "slug": "Using-locally-weighted-regression-for-robot-Atkeson",
            "title": {
                "fragments": [],
                "text": "Using locally weighted regression for robot learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The author explains how an appropriate distance metric or measure of similarity can be found, and how the distance metric is used in the use of locally weighted regression in memory-based robot learning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE International Conference on Robotics and Automation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 171
                            }
                        ],
                        "text": "The last method described here uses the explanation-based neural network learning algorithm (EBNN), which was originally proposed in the context of reinforcement learning [8, 17]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2351863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e42cf7b98c0ea5294a3bca10acaa95d0714d44f3",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "How can artificial neural nets generalize better from fewer examples? In order to generalize successfully, neural network learning methods typically require large training data sets. We introduce a neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks. For example, in robot control learning tasks reported here, previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions. For each observed training example of the target function (e.g. the robot control policy), the learner explains the observed example in terms of its prior knowledge, then analyzes this explanation to infer additional information about the shape, or slope, of the target function. This shape knowledge is used to bias generalization when learning the target function. Results are presented applying this approach to a simulated robot task based on reinforcement learning."
            },
            "slug": "Explanation-Based-Neural-Network-Learning-for-Robot-Mitchell-Thrun",
            "title": {
                "fragments": [],
                "text": "Explanation-Based Neural Network Learning for Robot Control"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks that is used to bias generalization when learning the target function."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5746940"
                        ],
                        "name": "M. Lando",
                        "slug": "M.-Lando",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Lando",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "An example of the above is the recognition of faces [5,  7 ]. When learning to recognize the -th person, say Bob, the learner is given a set of positive and negative example of face images of this person."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12558836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa1eea18e569a13bb262e3e6b266509b36bf6bb4",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abtraet. We describe a computational model of face recognition, which generalizes from single views of faces by taking advantage of prior experience with other faces. seen under a wider range of viewing conditions. The model represents face images by veclo~s of activities of graded overlapping receptive fields (m). It relies on high-spatial-frequency information to estimate the~viewing conditions, which are then used to normalize (via a h\u2019ansfonnation specific for faces), and identify, the low-spatial-frequency representation of the input. The class-specific msformatian approach allows the model to replicate a series of psychophysical findings on face recognition and constitutes an advance over cmnt face-recognition methods, which are incapable of generalization from a single example."
            },
            "slug": "Receptive-field-spaces-and-class-based-from-a-view-Lando-Edelman",
            "title": {
                "fragments": [],
                "text": "Receptive field spaces and class-based generalization from a single view in face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A computational model of face recognition, which generalizes from single views of faces by taking advantage of prior experience with other faces, constitutes an advance over cmnt face-recognition methods, which are incapable of generalization from a single example."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957934"
                        ],
                        "name": "Y. Moses",
                        "slug": "Y.-Moses",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Moses",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Moses"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8411547,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "13f752e5ec3b6187014e97ae5d719500f648f41d",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "An image of a face depends not only on its shape, but also on the viewpoint, illumination conditions, and facial expression. A face recognition system must overcome the changes in face appearance induced by these factors. Two related questions were investigated: the capacity of the human visual system to generalize the recognition of faces to novel images, and the level at which this generalization occurs. This problem was approached by comparing the identification and generalization capacity for upright and inverted faces. For upright faces, remarkably good generalization to novel conditions was found. For inverted faces, the generalization to novel views was significantly worse for both new illumination and viewpoint, although the performance on the training images was similar to that on the upright condition. The results indicate that at least some of the processes that support generalization across viewpoint and illumination are neither universal (because subjects did not generalize as easily for inverted faces as for upright ones) nor strictly object specific (because in upright faces nearly perfect generalization was possible from a single view, by itself insufficient for building a complete object-specific model). It is proposed that generalization in face recognition occurs at an intermediate level that is applicable to a class of objects, and that at this level upright and inverted faces initially constitute distinct object classes."
            },
            "slug": "Generalization-to-Novel-Images-in-Upright-and-Faces-Moses-Ullman",
            "title": {
                "fragments": [],
                "text": "Generalization to Novel Images in Upright and Inverted Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is proposed that generalization in face recognition occurs at an intermediate level that is applicable to a class of objects, and that at this level upright and inverted faces initially constitute distinct object classes."
            },
            "venue": {
                "fragments": [],
                "text": "Perception"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980953"
                        ],
                        "name": "S. Suddarth",
                        "slug": "S.-Suddarth",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Suddarth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Suddarth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24934793"
                        ],
                        "name": "A. Holden",
                        "slug": "A.-Holden",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Holden",
                            "middleNames": [
                                "D.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Holden"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Learning with hints [1, 4, 6, 16] constructs a neural network with n output units, one for each function fk (k = 1; 2; : : :; n)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41080002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfe7dbbd6e8d5d3720f58c2c9a0b9bec040a8ef8",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Symbolic-Neural-Systems-and-the-Use-of-Hints-for-Suddarth-Holden",
            "title": {
                "fragments": [],
                "text": "Symbolic-Neural Systems and the Use of Hints for Developing Complex Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Man Mach. Stud."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740300"
                        ],
                        "name": "D. Beymer",
                        "slug": "D.-Beymer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beymer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beymer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14814282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66505cb708b098a93331471f079965f6ded4ea7f",
            "isKey": false,
            "numCitedBy": 449,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "To create a pose-invariant face recognizer, one strategy is the view-based approach, which uses a set of real example views at different poses. But what if we only have one real view available, such as a scanned passport photo-can we still recognize faces under different poses? Given one real view at a known pose, it is still possible to use the view-based approach by exploiting prior knowledge of faces to generate virtual views, or views of the face as seen from different poses. To represent prior knowledge, we use 2D example views of prototype faces under different rotations. We develop example-based techniques for applying the rotation seen in the prototypes to essentially \"rotate\" the single real view which is available. Next, the combined set of one real and multiple virtual views is used as example views for a view-based, pose-invariant face recognizer. Oar experiments suggest that among the techniques for expressing prior knowledge of faces, 2D example-based approaches should be considered alongside the more standard 3D modeling techniques.<<ETX>>"
            },
            "slug": "Face-recognition-from-one-example-view-Beymer-Poggio",
            "title": {
                "fragments": [],
                "text": "Face recognition from one example view"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Oar experiments suggest that among the techniques for expressing prior knowledge of faces, 2D example-based approaches should be considered alongside the more standard 3D modeling techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "Notice that the new representation, g, is obtained through the support sets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105387808"
                        ],
                        "name": "D. Shepard",
                        "slug": "D.-Shepard",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Shepard",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shepard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "Another commonly used method, which is due to Shepard [13], averages the output values\nof all training examples but weights each example according to the inverse distance to the query point x.\ns(x) :=\n0 @ X\nhxi;yii2X\nyi jjx xijj+ \"\n1 A 0 @ X\nhxi;yii2X\n1 jjx xijj+ \"\n1 A 1\n(1)\nHere \" > 0 is a small\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42723195,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "dd03f2ba2e166143a173aa4dba432df302ae898f",
            "isKey": false,
            "numCitedBy": 4387,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In many fields using empirical areal data there arises a need for interpolating from irregularly-spaced data to produce a continuous surface. These irregularly-spaced locations, hence referred to as \u201cdata points,\u201d may have diverse meanings: in meterology, weather observation stations; in geography, surveyed locations; in city and regional planning, centers of data-collection zones; in biology, observation locations. It is assumed that a unique number (such as rainfall in meteorology, or altitude in geography) is associated with each data point. In order to display these data in some type of contour map or perspective view, to compare them with data for the same region based on other data points, or to analyze them for extremes, gradients, or other purposes, it is extremely useful, if not essential, to define a continuous function fitting the given values exactly. Interpolated values over a fine grid may then be evaluated. In using such a function it is assumed that the original data are without error, or that compensation for error will be made after interpolation."
            },
            "slug": "A-two-dimensional-interpolation-function-for-data-Shepard",
            "title": {
                "fragments": [],
                "text": "A two-dimensional interpolation function for irregularly-spaced data"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "In many fields using empirical areal data there arises a need for interpolating from irregularly-spaced data to produce a continuous surface, and it is extremely useful, if not essential, to define a continuous function fitting the given values exactly."
            },
            "venue": {
                "fragments": [],
                "text": "ACM National Conference"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928927"
                        ],
                        "name": "C. Stanfill",
                        "slug": "C.-Stanfill",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Stanfill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stanfill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16624499,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "123a726f6feb2bce29708b68ab2db5cdf9fcdaf4",
            "isKey": false,
            "numCitedBy": 1436,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "slug": "Toward-memory-based-reasoning-Stanfill-Waltz",
            "title": {
                "fragments": [],
                "text": "Toward memory-based reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "Since this additional data is desired to support learning fn, Xk is called a support set for the training set X."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalizing from a single view in face recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Generalizing from a single view in face recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "They are often able to generalize correctly even from a single training example [2, 10]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization across changes in illumination and viewing position in upright and inverted faces"
            },
            "venue": {
                "fragments": [],
                "text": "Generalization across changes in illumination and viewing position in upright and inverted faces"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 52
                            }
                        ],
                        "text": "An example of the above is the recognition of faces [5, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalizing from a single view in face recognition. TechnicalReport CS-TR 95-02"
            },
            "venue": {
                "fragments": [],
                "text": "Department of Applied Mathematics and Computer Science,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "Since this additional data is desired to support learning fn, Xk is called a support set for the training set X."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition from one model view"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "One of the approaches here, EBNN, has extensively been studied in the context of robot perception [11], reinforcement learning for robot control, and chess [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "See [17] for a more detailed description of EBNN and further references."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 171
                            }
                        ],
                        "text": "The last method described here uses the explanation-based neural network learning algorithm (EBNN), which was originally proposed in the context of reinforcement learning [8, 17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Explanation-BasedNeural Network Learning: A Lifelong LearningApproach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "One of the approaches here, EBNN, has extensively been studied in the context of robot perception [11], reinforcement learning for robot control, and chess [17]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Explanation-based neural network learning from mobile robot perception"
            },
            "venue": {
                "fragments": [],
                "text": "Symbolic Visual Learning"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Is-Learning-The-n-th-Thing-Any-Easier-Than-Learning-Thrun/371c9dc680e916f79d9c78fcf6c894a2dd299095?sort=total-citations"
}