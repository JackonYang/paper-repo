{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6789514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces some generalizations of Vapnik's (1982) method of structural risk minimization (SRM). As well as making explicit some of the details on SRM, it provides a result that allows one to trade off errors on the training sample against improved generalization performance. It then considers the more general case when the hierarchy of classes is chosen in response to the data. A result is presented on the generalization performance of classifiers with a \"large margin\". This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines). The paper concludes with a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets. Four examples are given of such functions, including the Vapnik-Chervonenkis (1971) dimension measured on the sample."
            },
            "slug": "Structural-Risk-Minimization-Over-Data-Dependent-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization Over Data-Dependent Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A result is presented that allows one to trade off errors on the training sample against improved generalization performance, and a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3234984"
                        ],
                        "name": "R. Herbrich",
                        "slug": "R.-Herbrich",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Herbrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Herbrich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61776378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23d2d8b687d31b11573473a7c7792b7ec08d0745",
            "isKey": false,
            "numCitedBy": 473,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nLinear classifiers in kernel spaces have emerged as a major topic within the field of machine learning. The kernel technique takes the linear classifier--a limited, but well-established and comprehensively studied model--and extends its applicability to a wide range of nonlinear pattern-recognition tasks such as natural language processing, machine vision, and biological sequence analysis. This book provides the first comprehensive overview of both the theory and algorithms of kernel classifiers, including the most recent developments. It begins by describing the major algorithmic advances: kernel perceptron learning, kernel Fisher discriminants, support vector machines, relevance vector machines, Gaussian processes, and Bayes point machines. Then follows a detailed introduction to learning theory, including VC and PAC-Bayesian theory, data-dependent structural risk minimization, and compression bounds. Throughout, the book emphasizes the interaction between theory and algorithms: how learning algorithms work and why. The book includes many examples, complete pseudo code of the algorithms presented, and an extensive source code library."
            },
            "slug": "Learning-Kernel-Classifiers-Theory-and-Algorithms-Herbrich",
            "title": {
                "fragments": [],
                "text": "Learning Kernel Classifiers - Theory and Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book provides the first comprehensive overview of both the theory and algorithms of kernel classifiers, including the most recent developments, and a detailed introduction to learning theory, including VC and PAC-Bayesian theory, data-dependent structural risk minimization, and compression bounds."
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive computation and machine learning"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50263663"
                        ],
                        "name": "D. Boswell",
                        "slug": "D.-Boswell",
                        "structuredName": {
                            "firstName": "Dustin",
                            "lastName": "Boswell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boswell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "On Kernel Target AlignmentNello Cristianini nello s.berkeley.eduBIOwulf Te hnologies, BerkeleyandUniversity of California, BerkeleyComputer S ien e Department329 Soda Hall Berkeley, CA 94720-1776, USAJaz Kandola jaz s.rhul.a .ukRoyal Holloway CollegeUniversity of LondonEgham, Surrey TW20 0EX,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18986102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36",
            "isKey": false,
            "numCitedBy": 2113,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines (SVM\u2019s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM\u2019s introduce the notion of a \u201ckernel induced feature space\u201d which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM\u2019s is that the higher-dimensional space doesn\u2019t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system\u2019s likelihood to perform well on unseen data) of SVM\u2019s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM\u2019s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM\u2019s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than \u201cyes/no\u201d classification)."
            },
            "slug": "Introduction-to-Support-Vector-Machines-Boswell",
            "title": {
                "fragments": [],
                "text": "Introduction to Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Support Vector Machines (SVM\u2019s) are intuitive, theoretically wellfounded, and have shown to be practically successful."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14727192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53",
            "isKey": false,
            "numCitedBy": 13353,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software."
            },
            "slug": "An-Introduction-to-Support-Vector-Machines-and-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory, and will guide practitioners to updated literature, new applications, and on-line software."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271549"
                        ],
                        "name": "M. Berry",
                        "slug": "M.-Berry",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berry",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Berry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143660088"
                        ],
                        "name": "B. Hendrickson",
                        "slug": "B.-Hendrickson",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Hendrickson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hendrickson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145503401"
                        ],
                        "name": "P. Raghavan",
                        "slug": "P.-Raghavan",
                        "structuredName": {
                            "firstName": "Prabhakar",
                            "lastName": "Raghavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Raghavan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59711557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56b151ec7f2f559cb233b75e4a6360dcd7e0386d",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Many approaches for retrieving documents from electronic databases depend on the literal matching of words in user\u2019s query to the keywords defining database objects. Since there is great diversity in the words people use to describe the same object, literalor lexicalbased methods can often retrieve irrelevant documents. Another approach to exploit the implicit higher-order structure in the association of terms with text objects is to compute the singular value decomposition (SVD) of large sparse term by text-object matrices. Latent Semantic Indexing (LSI) is a conceptual indexing method which employs the SVD to represent terms and objects by dominant singular subspaces ST that user queries can be matched in a iower-rank semantic space. This paper considers a third, intermediate approach to facilitate the immediate d+= \u2018:Lon ,of aocu.ment (or term) ciuscers. We demo~strate both traditiena! sparse r-. arrix reordering schemes (e. g., Reverse Cuthill-McKee) and spectral-based a~proaches (e.g., Correspondence Analysis or Fiedler vector-based spectral bisxtion) that can be used to permute original term by document (hypertext) .=. atrices to a narrow-banded form suitable for the detection of document (or Ierm] clusters. Although thk approach would not exploit the higher-order semantic structure in the database, it can be used to develo<p browsing tools for E>-pertext and on-line information at a reduced computational cost."
            },
            "slug": "Sparse-matrix-reordering-schemes-for-browsing-Berry-Hendrickson",
            "title": {
                "fragments": [],
                "text": "Sparse matrix reordering schemes for browsing hypertext"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A third, intermediate approach to facilitate the immediate d+= \u2018:Lon ,of aocu.ment (or term) ciuscers is considered, which would not exploit the higher-order semantic structure in the database, but can be used to develop browsing tools for E-pertext and on-line information at a reduced computational cost."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5544,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144551309"
                        ],
                        "name": "P. Pavlidis",
                        "slug": "P.-Pavlidis",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pavlidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646057"
                        ],
                        "name": "Jinsong Cai",
                        "slug": "Jinsong-Cai",
                        "structuredName": {
                            "firstName": "Jinsong",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinsong Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2361327"
                        ],
                        "name": "W. Grundy",
                        "slug": "W.-Grundy",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Grundy",
                            "middleNames": [
                                "Noble"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grundy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 149
                            }
                        ],
                        "text": "\u2026hypertext ( ombining linkand words information (Joa hims et al., 2001)) and bioinformati s ( ombining gene expres-sion and philogeneti information (Pavlidis et al., 2001)).29\nCristianini, Kandola, Elisseeff, Shawe-TaylorWe gave also riteria for adapting kernels to a given target, both by tting a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1044641,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "36708bbc473ec0b07752782de8a4d00f903eec3d",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "In our attempts to understand cellular function at the molecular level, we must be able to synthesize information from disparate types of genomic data. We consider the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons. We demonstrate the application of the support vector machine (SVM) learning algorithm to this functional inference task. Our results suggest the importance of exploiting prior information about the heterogeneity of the data. In particular, we propose an SVM kernel function that is explicitly heterogeneous. We also show how to use knowledge about heterogeneity to aid in feature selection."
            },
            "slug": "Gene-functional-classification-from-heterogeneous-Pavlidis-Weston",
            "title": {
                "fragments": [],
                "text": "Gene functional classification from heterogeneous data"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work considers the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons and proposes an SVM kernel function that is explicitly heterogeneous."
            },
            "venue": {
                "fragments": [],
                "text": "RECOMB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14705843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "646b3c7184df0d4617463dc3f428ff8a11d1c2a9",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper uses off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. It is shown, loosely speaking, that for any two algorithms A and B, there are as many targets (or priors over targets) for which A has lower expected OTS error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is \u201canti-cross-validation\u201d (choose the generalizer with largest cross-validation error). On the other hand, for loss functions other than zero-one (e.g., quadratic loss), there are a priori distinctions between algorithms. However even for such loss functions, any algorithm is equivalent on average to its \u201crandomized\u201d version, and in this still has no first principles justification in terms of average error. Nonetheless, it may be that (for example) cross-validation has better minimax properties than anti-cross-validation, even for zero-one loss. This paper also analyzes averages over hypotheses rather than targets. Such analyses hold for all possible priors. Accordingly they prove, as a particular example, that cross-validation can not be justified as a Bayesian procedure. In fact, for a very natural restriction of the class of learning algorithms, one should use anti-crossvalidation rather than cross-validation (!). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the VC dimension of your generalizer is small; and the training set is large, then with high probability your OTS error is small. Other implications for \u201cmembership queries\u201d algorithms and \u201cpunting\u201d algorithms are also discussed. \u201cEven after the observation of the frequent conjunction of objects, we have no reason to draw any inference concerning any object beyond those of which we have had experience.\u201d David Hume, in A Treatise of Human Nature, Book I, part 3, Section 12."
            },
            "slug": "OFF-TRAINING-SET-ERROR-AND-A-PRIORI-DISTINCTIONS-Wolpert",
            "title": {
                "fragments": [],
                "text": "OFF-TRAINING SET ERROR AND A PRIORI DISTINCTIONS BETWEEN LEARNING ALGORITHMS"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown, loosely speaking, that for any two algorithms A and B, there are as many targets (or priors over targets) for which A has lower expected OTS error than B as vice-versa, for loss functions like zero-one loss."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13901468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "041c670177d16bb2b2af80f5835b29de92666764",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper pr oves that it is impossible to justify a corre latio n between rep roducti on of a training set and generali zation err or off of the training set using only a pr iori reasoning. As a resu lt , the use in the real world of any genera lizer that fits a hypothesis functi on to a training set (e.g., the use of back-propagation ) is implicitl y pr edic ated on an ass umpt ion abo ut the physical universe. This pap er shows how this ass umpt ion can be expressed in te rms of a non-Euclidean inn er product between two vectors, one represent ing the physical universe and one representing the generalizer. In deriving this result , a novel formalism for address ing mac hine learni ng is developed . T his new formalism can be viewed as an exte nsion of the conventional \"Bayesian\" formalism, to (among other things). allow one to address the case in which one 's assumed \"priors\" are not exactly correct . The most impor tant feature of this new formalism is that it uses an ext remely lowlevel event space, consis ting of triples of {target function , hypothesis fun cti on , train ing set }. Partly as a resu lt of this feature, most other form alisms that have been constructed to address machine learn ing (e.g., PAC , the Bayesian formalism , and th e \"sta tist ical mechanics\" formalism ) are sp ecial cases of the form alism presented in this paper. Consequent ly such formalisms are capable of addressing only a subset of the issues addressed in this pap er. In fact , the formalism of this paper can be used to address all generalization issues of which the author is aware: over-t ra in ing , the need to restrict the number of free parameters in the hypothesis funct ion , th e problems associated wit h a \"non-representa tive\" training set , whether and when cross-validat ion work s, whether and when stacked genera lizat ion work s, whether and when a particu lar regu lari zer will work , and so forth. A summary of som e of the more important resu lt s of this pap er conce rn ing these and related topics can be found in the conclusion . *Current address: The Sant a Fe Institute, 1660 Old Pecos Trail, Suite A, Santa Fe, NM, 87501. Electronic mail address: dh\\/~sfi . santafe. edu 48 David H. Wolp ert"
            },
            "slug": "On-the-Connection-between-In-sample-Testing-and-Wolpert",
            "title": {
                "fragments": [],
                "text": "On the Connection between In-sample Testing and Generalization Error"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "It is impossible to justify a corre latio n between rep roducti on of a training set and generali zation err or off of the training set using only a pr iori reasoning, and a novel formalism for address ing mac hine learni ng is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738441"
                        ],
                        "name": "P. Drineas",
                        "slug": "P.-Drineas",
                        "structuredName": {
                            "firstName": "Petros",
                            "lastName": "Drineas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Drineas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709874"
                        ],
                        "name": "A. Frieze",
                        "slug": "A.-Frieze",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Frieze",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Frieze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144632403"
                        ],
                        "name": "R. Kannan",
                        "slug": "R.-Kannan",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kannan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kannan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144310126"
                        ],
                        "name": "V. Vinay",
                        "slug": "V.-Vinay",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Vinay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vinay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 73
                            }
                        ],
                        "text": "Spe tral Clustering algorithms have been re ently dis ussed and analyzed(Drineas et al., 1999), while Kernel PCA S h-olkopf et al. (1999) and Latent Semanti Kernels Cristianini et al. (2000) also are strongly related to the labeling methods dis ussedhere."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5329008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22ca4af08b52e7051c59dc692d80d8d7a727c3d8",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of dividing a set of m points in Euclidean n-space into k clusters (m, n are variable while k is fixed), so as to minimize the sum of distances squared of each point to its \u201ccluster center\u201d. This formulation differs in two ways from the most frequently considered clustering problems in the literature, namely, here we have k fixed and m, n variable, and we use the sum of squared distances as our measure; we will argue that our problem is natural in many contexts. We consider a relaxation of the discrete problem: find the k-dimensional subspace V so that the sum of distances squared to V (of the m points) is minimized. We show: (i) The relaxation can be solved by the Singular Value Decomposition (SVD) of Linear Algebra. (ii) The solution of the relaxation can be used to get a 2-approximation algorithm for the original problem. More importantly, (iii) we argue that in fact the relaxation provides a generalized clustering which is useful in its own right. Finally, (iv) we show that the SVD of a randomly chosen submatrix (according to a suitable probability distribution) of the matrix provides an approximation to the SVD of the whole matrix, thus yielding a very fast randomized algorithm. This cam be applied to problems of very large size which typically arise in modern applications."
            },
            "slug": "Clustering-in-large-graphs-and-matrices-Drineas-Frieze",
            "title": {
                "fragments": [],
                "text": "Clustering in large graphs and matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is argued that in fact the relaxation provides a generalized clustering which is useful in its own right and can be applied to problems of very large size which typically arise in modern applications."
            },
            "venue": {
                "fragments": [],
                "text": "SODA '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15917152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c5e562437ee94fb6e4d60ec559386dd0a433513",
            "isKey": false,
            "numCitedBy": 796,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A single linear programming formulation is proposed which generates a plane that of minimizes an average sum of misclassified points belonging to two disjoint points sets in n-dimensional real space. When the convex hulls of the two sets are also disjoint, the plane completely separates the two sets. When the convex hulls intersect, our linear program, unlike all previously proposed linear programs, is guaranteed to generate some error-minimizing plane, without the imposition of extraneous normalization constraints that inevitably fail to handle certain cases. The effectiveness of the proposed linear program has been demonstrated by successfully testing it on a number of databases. In addition, it has been used in conjunction with the multisurface method of piecewise-linear separation to train a feed-forward neural network with a single hidden layer."
            },
            "slug": "Robust-linear-programming-discrimination-of-two-Bennett-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Robust linear programming discrimination of two linearly inseparable sets"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A single linear programming formulation is proposed which generates a plane that of minimizes an average sum of misclassified points belonging to two disjoint points sets in n-dimensional real space, without the imposition of extraneous normalization constraints that inevitably fail to handle certain cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144500070"
                        ],
                        "name": "Shuchi Chawla",
                        "slug": "Shuchi-Chawla",
                        "structuredName": {
                            "firstName": "Shuchi",
                            "lastName": "Chawla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuchi Chawla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "Other algorithms for lustering and transdu tion inspired by graph theory have alsobeen re ently put forward Blum and Chawla (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5892518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eedbab3ae55fd6a4e7bbc75fcc261293384f883",
            "isKey": false,
            "numCitedBy": 1057,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data."
            },
            "slug": "Learning-from-Labeled-and-Unlabeled-Data-using-Blum-Chawla",
            "title": {
                "fragments": [],
                "text": "Learning from Labeled and Unlabeled Data using Graph Mincuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data is considered."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "The concentration of j is considered in [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116929976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fcdee6c6d885ac2bd32e122dbf282f93720c22",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 557,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface * Introduction * The Bayes Error * Inequalities and alternatedistance measures * Linear discrimination * Nearest neighbor rules *Consistency * Slow rates of convergence Error estimation * The regularhistogram rule * Kernel rules Consistency of the k-nearest neighborrule * Vapnik-Chervonenkis theory * Combinatorial aspects of Vapnik-Chervonenkis theory * Lower bounds for empirical classifier selection* The maximum likelihood principle * Parametric classification *Generalized linear discrimination * Complexity regularization *Condensed and edited nearest neighbor rules * Tree classifiers * Data-dependent partitioning * Splitting the data * The resubstitutionestimate * Deleted estimates of the error probability * Automatickernel rules * Automatic nearest neighbor rules * Hypercubes anddiscrete spaces * Epsilon entropy and totally bounded sets * Uniformlaws of large numbers * Neural networks * Other error estimates *Feature extraction * Appendix * Notation * References * Index"
            },
            "slug": "A-Probabilistic-Theory-of-Pattern-Recognition-Devroye-Gy\u00f6rfi",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Theory of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Bayes Error and Vapnik-Chervonenkis theory are applied as guide for empirical classifier selection on the basis of explicit specification and explicit enforcement of the maximum likelihood principle."
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Modelling and Applied Probability"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145518576"
                        ],
                        "name": "K. Liou",
                        "slug": "K.-Liou",
                        "structuredName": {
                            "firstName": "K",
                            "lastName": "Liou",
                            "middleNames": [
                                "P"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Liou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753507"
                        ],
                        "name": "A. Pothen",
                        "slug": "A.-Pothen",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pothen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pothen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8978853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c802c8d48580ebba10a0c65d021758e7238dada3",
            "isKey": false,
            "numCitedBy": 1807,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem ofcomputing a small vertex separator in a graph arises in the context ofcomputing a good ordering for the parallel factorization of sparse, symmetric matrices. An algebraic approach for computing vertex separators is considered in this paper. It is shown that lower bounds on separator sizes can be obtained in terms of the eigenvalues of the Laplacian matrix associated with a graph. The Laplacian eigenvectors of grid graphs can be computed from Kronecker products involving the eigenvectors ofpath graphs, and these eigenvectors can be used to compute good separators in grid graphs. A heuristic algorithm is designed to compute a vertex separator in a general graph by first computing an edge separator in the graph from an eigenvector of the Laplacian matrix, and then using a maximum matching in a subgraph to compute the vertex separator. Results on the quality of the separators computed by the spectral algorithm are presented, and these are compared with separators obtained from other algorithms for computing separators. Finally, the time required to compute the Laplacian eigenvector is reported, and the accuracy with which the eigenvector must be computed to obtain good separators is considered. The spectral algorithm has the advantage that it can be implemented on a mediumsize multiprocessor in a straightforward manner. Key words, graph partitioning, graph spectra, Laplacian matrix, ordering algorithms, parallel orderings, sparse matrix, vertex separator AMS(MOS) subject classifications. 65F50, 65F05, 65F15, 68R10"
            },
            "slug": "PARTITIONING-SPARSE-MATRICES-WITH-EIGENVECTORS-OF-Liou-Pothen",
            "title": {
                "fragments": [],
                "text": "PARTITIONING SPARSE MATRICES WITH EIGENVECTORS OF GRAPHS*"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that lower bounds on separator sizes can be obtained in terms of the eigenvalues of the Laplacian matrix associated with a graph, which can be used to compute good separators in grid graphs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2839642"
                        ],
                        "name": "B. Carl",
                        "slug": "B.-Carl",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Carl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Carl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101264613"
                        ],
                        "name": "Irmtraud Stephani",
                        "slug": "Irmtraud-Stephani",
                        "structuredName": {
                            "firstName": "Irmtraud",
                            "lastName": "Stephani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irmtraud Stephani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119490731,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "53bab7b5073443420d39d6622016a403d3223b40",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Entropy quantities 2. Approximation quantities 3. Inequalities of Bernstein-Jackson type 3. Inequalities of Berstein-Jackson type 4. A refined Riesz theory 5. Operators with values in C(X) 6. Operator theoretical methods in the local theory of Banach spaces."
            },
            "slug": "Entropy,-Compactness-and-the-Approximation-of-Carl-Stephani",
            "title": {
                "fragments": [],
                "text": "Entropy, Compactness and the Approximation of Operators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Note that a recombination of these rank 1 kernels was made in so-called latent semantic kernels [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 56
                            }
                        ],
                        "text": "Also, it ispossible to relate it to the average margin (Cristianini et al., 2000), proving that an optimalalignment orresponds to a maximal margin separation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "I mC-ml f(x) I IlE [2] < V (x,y) y lEs [~L:#j Yiyjk(xi,xj)] + ~"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 165
                            }
                        ],
                        "text": "Spe tral Clustering algorithms have been re ently dis ussed and analyzed(Drineas et al., 1999), while Kernel PCA S h-olkopf et al. (1999) and Latent Semanti Kernels Cristianini et al. (2000) also are strongly related to the labeling methods dis ussedhere."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Latent semantic kernels for feature selection"
            },
            "venue": {
                "fragments": [],
                "text": "Latent semantic kernels for feature selection"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102993460"
                        ],
                        "name": "\u4e2d\u6fa4 \u771f",
                        "slug": "\u4e2d\u6fa4-\u771f",
                        "structuredName": {
                            "firstName": "\u4e2d\u6fa4",
                            "lastName": "\u771f",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u4e2d\u6fa4 \u771f"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117329261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9495e37bbaacdf30390b46b9402a77e7d0a33312",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Devroye,-L.,-Gyorfi,-L.-and-Lugosi,-G.-:-A-Theory-\u4e2d\u6fa4",
            "title": {
                "fragments": [],
                "text": "Devroye, L., Gyorfi, L. and Lugosi, G. : A Probabilistic Theory of Pattern Recognition, Springer (1996)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679035"
                        ],
                        "name": "C. McDiarmid",
                        "slug": "C.-McDiarmid",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "McDiarmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. McDiarmid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116663483,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6bbb79cc026ecca4264d95c4551fc58205b09533",
            "isKey": false,
            "numCitedBy": 1711,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Surveys-in-Combinatorics,-1989:-On-the-method-of-McDiarmid",
            "title": {
                "fragments": [],
                "text": "Surveys in Combinatorics, 1989: On the method of bounded differences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2734323"
                        ],
                        "name": "Y. Sawano",
                        "slug": "Y.-Sawano",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Sawano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sawano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087748"
                        ],
                        "name": "S. Saitoh",
                        "slug": "S.-Saitoh",
                        "structuredName": {
                            "firstName": "Saburou",
                            "lastName": "Saitoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Saitoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117252811,
            "fieldsOfStudy": [],
            "id": "636b46471adea4916ec1b2e38c8e8265218f6952",
            "isKey": false,
            "numCitedBy": 616,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Reproducing-Kernels-and-Its-Applications-Sawano-Saitoh",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels and Its Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 62
                            }
                        ],
                        "text": "Also, very re ent work fromFoster et al, and Ben-David et al (Forster et al., 2001, Ben-David et al., 2000), explores theproblem of embedding a ertain matrix into an Eu lidean spa e in order to derive inherentlimitations of kernel based representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 128
                            }
                        ],
                        "text": "Note thatBen-David and Simon have also studied a related on ept in assessing the realisability offun tion lasses using kernels (Ben-David et al., 2000)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 23264507,
            "fieldsOfStudy": [],
            "id": "59922a45dc7b6943a2ce2f864c752897d1a3ab8c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating the Optimal Margins of Embeddings in Euclidean Half Spaces"
            },
            "venue": {
                "fragments": [],
                "text": "COLT/EuroCOLT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41680909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ded923a192ffbf13e4466c6b7d2ede55724b716",
            "isKey": false,
            "numCitedBy": 726,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-Greedy-Matrix-Approximation-for-Machine-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Sparse Greedy Matrix Approximation for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12874699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "010403afd8f3369d53e675e47a38025d2b42f26d",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Composite-Kernels-for-Hypertext-Categorisation-Joachims-Cristianini",
            "title": {
                "fragments": [],
                "text": "Composite Kernels for Hypertext Categorisation"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Latent semanti kernels"
            },
            "venue": {
                "fragments": [],
                "text": "InternationalConferen e of Ma hine Learning ( ICML )"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A onservation law for generalization performan e"
            },
            "venue": {
                "fragments": [],
                "text": "International Conferen eon Ma hine Learning"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Th eory of Pattern Recognition . Number 31 in Applications of mathematics"
            },
            "venue": {
                "fragments": [],
                "text": "A Probabilistic Th eory of Pattern Recognition . Number 31 in Applications of mathematics"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stru tural risk minimizationover data dependent hierar hies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the method of bounded di eren es"
            },
            "venue": {
                "fragments": [],
                "text": "Surveys in ombinatori s"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Partitioning sparse matri es with eigenve tors of graphs"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J . Matrix Anal"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gene fun tional lassi ation from heterogeneous data"
            },
            "venue": {
                "fragments": [],
                "text": "Pro eedings of the Fifth International Conferen e on ComputationalMole ular Biology"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/On-Kernel-Target-Alignment-Cristianini-Shawe-Taylor/36aa0d0936b2cf128c646c36a1981807b5a27aaf?sort=total-citations"
}