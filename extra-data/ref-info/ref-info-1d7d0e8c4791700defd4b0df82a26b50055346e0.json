{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14295333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcf989bf87f026d2f0fe7e3d08812f364f50b67a",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A new learning algorithm is derived which performs online stochastic gradient ascent in the mutual information between outputs and inputs of a network. In the absence of a priori knowledge about the 'signal' and 'noise' components of the input, propagation of information depends on calibrating network non-linearities to the detailed higher-order moments of the input density functions. By incidentally minimising mutual information between outputs, as well as maximising their individual entropies, the network 'factorises' the input into independent components. As an example application, we have achieved near-perfect separation of ten digitally mixed speech signals. Our simulations lead us to believe that our network performs better at blind separation than the Herault-Jutten network, reflecting the fact that it is derived rigorously from the mutual information objective."
            },
            "slug": "A-Non-linear-Information-Maximisation-Algorithm-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "A Non-linear Information Maximisation Algorithm that Performs Blind Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new learning algorithm is derived which performs online stochastic gradient ascent in the mutual information between outputs and inputs of a network and performs better at blind separation than the Herault-Jutten network, reflecting the fact that it is derived rigorously from the Mutual information objective."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059566454"
                        ],
                        "name": "G. Deco",
                        "slug": "G.-Deco",
                        "structuredName": {
                            "firstName": "Gustavo",
                            "lastName": "Deco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Deco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799693"
                        ],
                        "name": "W. Brauer",
                        "slug": "W.-Brauer",
                        "structuredName": {
                            "firstName": "Wilfried",
                            "lastName": "Brauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Brauer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Deco & Brauer (1995) use cumulant expansions to approximate mutual information between outputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35721030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39b444c1f01077a652073d8965dfdfb06825efd1",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-higher-order-statistical-decorrelation-by-Deco-Brauer",
            "title": {
                "fragments": [],
                "text": "Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115302789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "698aedd44c51da829228e2c7d243960345efeb94",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focusing on the case of nonlinear transfer functions. We assume that both receptive fields (synaptic efficacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow. We also show that this result is valid for linear and, more generally, unbounded, transfer functions, provided optimization is performed under an additive constraint, i.e. which can be written as a sum of terms, each one being specific to one output neuron. Finally, we study the effect of a non-zero input noise. We find that, to first order in the input noise, assumed to be small in comparison with th..."
            },
            "slug": "Nonlinear-neurons-in-the-low-noise-limit:-a-code-5-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer Network 5"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15816289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15e5ac0a7c3542ce58a67c43fa0e81398be2e5a7",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focussing on the case of non linear transfer functions. We assume that both receptive elds (synaptic eecacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker'sinfomax principle) leads to a factorial code-hence to the same solution as required by the redundancy reduction principle of Barlow. We show also that this result is valid for linear, more generally unbounded, transfer functions, provided optimization is performed under an additive constraint, that is which can be written as a sum of terms, each one being speciic to one output neuron. Finally we study the eeect of a non zero input noise. We nd that, at rst order in the input noise, assumed to be small as compared to the-small-output noise, the above results are still valid, provided the output noise is uncorrelated from one neuron to the other. P.A.C.S. 87.30 Biophysics of neurophysiological processes Short title: Information maximization with non linear neurons To appear in NETWORK INDEX: nadalparga.infomaxredred.ps.Z nadal@physique.ens.fr 19 pages Infomax applied to non linear neurons, in the low noise limit, leads to redundancy reduction."
            },
            "slug": "Non-linear-neurons-in-the-low-noise-limit-:-a-code-Nadal",
            "title": {
                "fragments": [],
                "text": "Non linear neurons in the low noise limit : a factorial code maximizes information transferJean"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker'sinfomax principle) leads to a factorial code-hence to the same solution as required by the redundancy reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797581"
                        ],
                        "name": "D. Yellin",
                        "slug": "D.-Yellin",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Yellin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Yellin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659357"
                        ],
                        "name": "E. Weinstein",
                        "slug": "E.-Weinstein",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Weinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Weinstein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 91
                            }
                        ],
                        "text": "A similar proposal which combines separation with deconvolution is to be found in Yellin & Weinstein (1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 23171783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce846112026bea49e22310e1001bfbaf0f084756",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem in which we want to separate two (or more) signals that are coupled to each other through an unknown multiple-input-multiple-output linear system (channel). We prove that the signals can be decoupled, or separated, using only the condition that they are statistically independent, and find even weaker sufficient conditions involving their cross-polyspectra. By imposing these conditions on the reconstructed signals, we obtain a class of criteria for signal separation. These criteria are universal in the sense that they do not require any prior knowledge or information concerning The nature of the source signals. They may be communication signals, or speech signals, or any other 1-D or multidimensional signals (e.g., images). Computationally efficient algorithms for implementing the proposed criteria, that only involve the iterative solution to a linear least squares problem, are presented. >"
            },
            "slug": "Criteria-for-multichannel-signal-separation-Yellin-Weinstein",
            "title": {
                "fragments": [],
                "text": "Criteria for multichannel signal separation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that the signals can be decoupled, or separated, using only the condition that they are statistically independent, and even weaker sufficient conditions involving their cross-polyspectra are found."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61021083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddfc137d23d645e48cb0bc061063c91aafc9027e",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part I. Starting from the properties of networks with backward lateral inhibitions, we define an algorithm for adaptive spatial sampling of line\u2010structured images. Applications to character recognition are straightforward.Part II. Let be an array of n sensors, each sensitive to an unknown linear combination of n sources. This is a classical problem in Signal Processing. But what is less classical is to extract each source signal without any knowledge either about those signals or about their combination in the sensors outputs. The only assumption is that the sources are independent.This problem emerged from recent studies on neural networks where any message appears as an unknown mixing of primary entities which are to be \u2018\u2018discovered\u2019\u2019. According to the model of neural networks, we propose an algorithm based on: i \u2212 a network of fully interconnected processors (like neurons in a small volume of the Central Nervous System). ii \u2212 A law which controls the weights of the interconnections, derived from the He..."
            },
            "slug": "Space-or-time-adaptive-signal-processing-by-neural-H\u00e9rault-Jutten",
            "title": {
                "fragments": [],
                "text": "Space or time adaptive signal processing by neural network models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An algorithm for adaptive spatial sampling of line\u2010structured images using a model of neural networks based on a network of fully interconnected processors, like neurons in a small volume of the Central Nervous System, which controls the weights of the interconnections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153510402"
                        ],
                        "name": "L. Parra",
                        "slug": "L.-Parra",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Parra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Parra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059566454"
                        ],
                        "name": "G. Deco",
                        "slug": "G.-Deco",
                        "structuredName": {
                            "firstName": "Gustavo",
                            "lastName": "Deco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Deco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3475636"
                        ],
                        "name": "S. Miesbach",
                        "slug": "S.-Miesbach",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Miesbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miesbach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14093234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7920674d9c9bbdf9893194ff4fc9c3b28f4cc1ef",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "According to Barlow (1989), feature extraction can be understood as finding a statistically independent representation of the probability distribution underlying the measured signals. The search for a statistically independent representation can be formulated by the criterion of minimal mutual information, which reduces to decorrelation in the case of gaussian distributions. If nongaussian distributions are to be considered, minimal mutual information is the appropriate generalization of decorrelation as used in linear Principal Component Analyses (PCA). We also generalize to nonlinear transformations by only demanding perfect transmission of information. This leads to a general class of nonlinear transformations, namely symplectic maps. Conservation of information allows us to consider only the statistics of single coordinates. The resulting factorial representation of the joint probability distribution gives a density estimation. We apply this concept to the real world problem of electrical motor fault detection treated as a novelty detection task."
            },
            "slug": "Statistical-Independence-and-Novelty-Detection-with-Parra-Deco",
            "title": {
                "fragments": [],
                "text": "Statistical Independence and Novelty Detection with Information Preserving Nonlinear Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work applies the criterion of minimal mutual information to the real world problem of electrical motor fault detection treated as a novelty detection task, and generalizes to nonlinear transformations by only demanding perfect transmission of information."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18127428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efd2c547114d5296b38fbbdd5bff0acd324132f0",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of determining the weights for a set of linear filters (model \"cells\") so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors. The quantity that is maximized is the Shannon information rate, or equivalently the average mutual information between input and output. Several models for the role of processing noise are analyzed, and the biological motivation for considering them is described. For simple models in which nearby input signal values (in space or time) are correlated, the cells resulting from this optimization process include center-surround cells and cells sensitive to temporal variations in input signal."
            },
            "slug": "An-Application-of-the-Principle-of-Maximum-to-Linsker",
            "title": {
                "fragments": [],
                "text": "An Application of the Principle of Maximum Information Preservation to Linear Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper addresses the problem of determining the weights for a set of linear filters so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 80
                            }
                        ],
                        "text": "In contrast with these results, our experience with tests on the H-J network of Jutten & Herault (1991) has been that it occasionally fails to converge for two sources and only rarely converges for three, on the same speech and music signals we used for separating ten sources."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 119
                            }
                        ],
                        "text": "But it could only nd a symmetric decorrelation matrix, which would not su ce if the mixing matrix, A, were asymmetric (Jutten & Herault 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 127
                            }
                        ],
                        "text": "It bears a resemblance to the rule proposed by Platt & Faggin (1992) for adjustable time delays in the network architecture of Jutten & Herault (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 17
                            }
                        ],
                        "text": "The explanation (Jutten & Herault 1991) for the success of the H-J network is that the Taylor series expansion of g(u)h(u)T in (40) yields odd cross moments, such that the weights stop changing when:\nX i;j bijpqhu 2p+1 i u 2q+1 j i = 0 (41)\nfor all output unit pairs i 6= j, for p; q = 0; 1; 2; 3 :\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": true,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62698467,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "740dad27f20f6d835976a30a755568315902734b",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A procedure for building a blind equalizer, motivated by neural network theory, is described. The procedure treats the blind equalization problem as a self-organized process. The network consists of an input layer, a single hidden layer, and a single output unit. The learning process proceeds in two stages. In stage I the nonlinear transformation for the input layer to the hidden layer is computed in a self-organized manner, which is frozen once steady-state conditions are reached. Stage II, building on stage I, resembles a conventional Bussgang algorithm except for the fact that the output nonlinearity is adapted alongside the linear weights connected to the output unit.<<ETX>>"
            },
            "slug": "Blind-equalization-formulated-as-a-self-organized-Haykin",
            "title": {
                "fragments": [],
                "text": "Blind equalization formulated as a self-organized learning process"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A procedure for building a blind equalizer, motivated by neural network theory, is described, which treats the blind equalization problem as a self-organized process."
            },
            "venue": {
                "fragments": [],
                "text": "[1992] Conference Record of the Twenty-Sixth Asilomar Conference on Signals, Systems & Computers"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704975"
                        ],
                        "name": "G. Burel",
                        "slug": "G.-Burel",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Burel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Burel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "This argument is well-supported by the analysis of Nadal & Parga (1995), who independently reached the conclusion that in the low-noise limit, information maximisation yields factorial codes when both the non-linear function, g(u), and the weights, w, can be optimised."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Parra & Deco (1995) use symplectic transforms to train non-linear information-preserving mappings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 18
                            }
                        ],
                        "text": "This rule holds regardless of the architecture in which the network is embedded, and it is local, unlike the Aw rule in 2.16. It bears a resemblance to the rule proposed by Platt and Faggin (1992) for adjustable time delays in the network architecture of Jutten and Herault (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "In this case, H(Y jX) = H(N) (Nadal & Parga 1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 29
                            }
                        ],
                        "text": "may be found in Nadal and Parga (1995) and Schuster (1992), and it may be possible to define learning rules for such cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 18
                            }
                        ],
                        "text": "This rule holds regardless of the architecture in which the network is embedded, and it is local, unlike the Aw rule in 2.16. It bears a resemblance to the rule proposed by Platt and Faggin (1992) for adjustable time delays in the network architecture of Jutten and Herault (1991). The rule has an intuitive interpretation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The beginning of such an analysis may be found in Nadal & Parga (1995), and Schuster (1992) and it may be possible to de ne learning rules for such cases."
                    },
                    "intents": []
                }
            ],
            "corpusId": 23408621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d60694bb4f0f8736aff49f0513a4dca1303526ae",
            "isKey": true,
            "numCitedBy": 247,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources:-A-nonlinear-neural-Burel",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources: A nonlinear neural algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "The algorithm does not assume any knowledge of the input distributions, and is de ned here for the zero-noise limit."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "The rst, the development of information theoretic unsupervised learning rules for neural networks has been pioneered by Linsker 1992, Becker & Hinton 1992, Atick & Redlich 1993, Plumbley & Fallside 1988 and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Linsker (1992) derived a learning algorithm for maximising the mutual information between two layers of a network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42871496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "026e9b04bab73d3a34cd37f8b290f0c8f6da5f4e",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A network that develops to maximize the mutual information between its output and the signal portion of its input (which is admixed with noise) is useful for extracting salient input features, and may provide a model for aspects of biological neural network function. I describe a local synaptic Learning rule that performs stochastic gradient ascent in this information-theoretic quantity, for the case in which the input-output mapping is linear and the input signal and noise are multivariate gaussian. Feedforward connection strengths are modified by a Hebbian rule during a \"learning\" phase in which examples of input signal plus noise are presented to the network, and by an anti-Hebbian rule during an \"unlearning\" phase in which examples of noise alone are presented. Each recurrent lateral connection has two values of connection strength, one for each phase; these values are updated by an anti-Hebbian rule."
            },
            "slug": "Local-Synaptic-Learning-Rules-Suffice-to-Maximize-a-Linsker",
            "title": {
                "fragments": [],
                "text": "Local Synaptic Learning Rules Suffice to Maximize Mutual Information in a Linear Network"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A local synaptic Learning rule is described that performs stochastic gradient ascent in this information-theoretic quantity, for the case in which the input-output mapping is linear and the input signal and noise are multivariate gaussian."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153510402"
                        ],
                        "name": "L. Parra",
                        "slug": "L.-Parra",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Parra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Parra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059566454"
                        ],
                        "name": "G. Deco",
                        "slug": "G.-Deco",
                        "structuredName": {
                            "firstName": "Gustavo",
                            "lastName": "Deco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Deco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3475636"
                        ],
                        "name": "S. Miesbach",
                        "slug": "S.-Miesbach",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Miesbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miesbach"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18226869,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8b3ec1172d67f9380e2b2d5e472bb83570616ecf",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The basic idea of linear principal component analysis (PCA) involves decorrelating coordinates by an orthogonal linear transformation. In this paper we generalize this idea to the nonlinear case. Simultaneously we shall drop the usual restriction to Gaussian distributions. The linearity and orthogonality condition of linear PCA is replaced by the condition of volume conservation in order to avoid spurious information generated by the nonlinear transformation. This leads us to another very general class of nonlinear transformations, called symplectic maps. Later, instead of minimizing the correlation, we minimize the redundancy measured at the output coordinates. This generalizes second-order statistics, being only valid for Gaussian output distributions, to higher-order statistics. The proposed paradigm implements Barlow's redundancy-reduction principle for unsupervised feature extraction. The resulting factorial representation of the joint probability distribution presumably facilitates density estimatio..."
            },
            "slug": "Redundancy-reduction-with-information-preserving-Parra-Deco",
            "title": {
                "fragments": [],
                "text": "Redundancy reduction with information-preserving nonlinear maps"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11862601,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "5adc947858e3743320ff7eb535d8fbd0e4f6902f",
            "isKey": false,
            "numCitedBy": 419,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-II:-Problems-Comon-Jutten",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part II: Problems statement"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50341837"
                        ],
                        "name": "Shaolin Li",
                        "slug": "Shaolin-Li",
                        "structuredName": {
                            "firstName": "Shaolin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaolin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 100
                            }
                        ],
                        "text": "Under these conditions, information maximisation has extra properties not found in the linear case (Linsker 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2703680,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "24971f01cd9150c09d279e60bfd88ff4eddbc772",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The Herault-Jutten network has been used to separate independent sound sources that have been linearly mixed. The problem of separating a mixture of several independent signals in free-field conditions or a signal and echoes in confined spaces is compounded by propagation time delays between the source(s) and the microphones because the conventional Herault-Jutten network cannot tolerate time delays. In this paper, we combine a symmetrically balanced beamforming array with the conventional Herault-Jutten network. The resulting system can adaptively separate signals that include delays introduced by the propagation medium. The proposed algorithm has been simulated in digital communication multipath channels where intersymbol interference exists. The simulation results show two clear advantages of the proposed method over the conventional adaptive equalization: (1) there is no penalty for very long impulse responses caused by long delays, and (2) no training signals are needed for equalization. The design of a multibeamformer to handle the source separation of multiple broad-band signals is also presented. >"
            },
            "slug": "Adaptive-separation-of-mixed-broadband-sound-with-a-Li-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Adaptive separation of mixed broadband sound sources with delays by a beamforming Herault-Jutten network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, another group of information-theoretic algorithms has been proposed by  Becker and Hinton (1992) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "The rst, the development of information theoretic unsupervised learning rules for neural networks has been pioneered by Linsker 1992, Becker & Hinton 1992, Atick & Redlich 1993, Plumbley & Fallside 1988 and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The first, the development of information-theoretic unsupervised learning rules for neural networks, has been pioneered by Linsker (1992),  Becker and Hinton (1992) , Atick and Redlich (1993), Plumbley and Fallside (19881, and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 81
                            }
                        ],
                        "text": "Finally, another group of information theoretic algorithms have been proposed by Becker & Hinton (1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4332326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c85b7fe70dda0adbbd7630e2a341a904c74fbd2",
            "isKey": true,
            "numCitedBy": 407,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "THE standard form of back-propagation learning1 is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other (Fig. la). The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces."
            },
            "slug": "Self-organizing-neural-network-that-discovers-in-Becker-Hinton",
            "title": {
                "fragments": [],
                "text": "Self-organizing neural network that discovers surfaces in random-dot stereograms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors' simulations show that when the learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31727807,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "4c9e13d6d34da1cefa1be368d7fa7bf95cf4ad74",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Representation-and-separation-of-signals-using-PCA-Karhunen-Joutsensalo",
            "title": {
                "fragments": [],
                "text": "Representation and separation of signals using nonlinear PCA type learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150165387"
                        ],
                        "name": "J. Urgen Schmidhuber",
                        "slug": "J.-Urgen-Schmidhuber",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Urgen Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urgen Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 103
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "From this we may write WHJ = W 1 = W 1 W W 1 (39) so that our learning rule, (14), forms part of a rule for the recurrent H-J network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2142508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "675d381653da0d2825ae37ab06069a1525fafb79",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose a novel general principle for unsupervised learning of distributed non-redundant internal representations of input patterns. The principle is based on two opposing forces. For each represen-tational unit there is an adaptive predictor which tries to predict the unit from the remaining units. In turn, each unit tries to react to the environment such that it minimizes its predictability. This encourages each unit to lter\u00e0bstract concepts' out of the environmental input such that these concepts are statistically independent of those upon which the other units focus. I discuss various simple yet potentially powerful implementations of the principle which aim at nding binary factorial codes (Bar-low et al., 1989), i.e. codes where the probability of the occurrence of a particular input is simply the product of the probabilities of the corresponding code symbols. Such codes are potentially relevant for (1) segmentation tasks, (2) speeding up supervised learning, (3) novelty detection. Methods for nding factorial codes automatically implement Occam's razor for nding codes using a minimal number of units. Unlike previous methods the novel principle has a potential for removing not only linear but also non-linear output redundancy. Illustrative experiments show that algorithms based on the principle of predictability minimization are practically feasible. The nal part of this paper describes an entirely local algorithm that has a potential for learning unique representations of extended input sequences."
            },
            "slug": "Learning-Factorial-Codes-by-Predictability-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Factorial Codes by Predictability Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An entirely local algorithm is described that has a potential for learning unique representations of extended input sequences that are potentially relevant for segmentation tasks, speeding up supervised learning, and novelty detection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 103
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42023620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac91740ae76ed9dbd853bddd6d1d9dc43bc55179",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose a novel general principle for unsupervised learning of distributed nonredundant internal representations of input patterns. The principle is based on two opposing forces. For each representational unit there is an adaptive predictor, which tries to predict the unit from the remaining units. In turn, each unit tries to react to the environment such that it minimizes its predictability. This encourages each unit to filter \"abstract concepts\" out of the environmental input such that these concepts are statistically independent of those on which the other units focus. I discuss various simple yet potentially powerful implementations of the principle that aim at finding binary factorial codes (Barlow et al. 1989), i.e., codes where the probability of the occurrence of a particular input is simply the product of the probabilities of the corresponding code symbols. Such codes are potentially relevant for (1) segmentation tasks, (2) speeding up supervised learning, and (3) novelty detection. Methods for finding factorial codes automatically implement Occam's razor for finding codes using a minimal number of units. Unlike previous methods the novel principle has a potential for removing not only linear but also nonlinear output redundancy. Illustrative experiments show that algorithms based on the principle of predictability minimization are practically feasible. The final part of this paper describes an entirely local algorithm that has a potential for learning unique representations of extended input sequences."
            },
            "slug": "Learning-Factorial-Codes-by-Predictability-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Factorial Codes by Predictability Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A novel general principle for unsupervised learning of distributed nonredundant internal representations of input patterns based on two opposing forces that has a potential for removing not only linear but also nonlinear output redundancy."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797605"
                        ],
                        "name": "F. Faggin",
                        "slug": "F.-Faggin",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Faggin",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Faggin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14561566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4aefa67521edb6de90ccc70e26355bb572ef92da",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We have created new networks to unmix signals which have been mixed either with time delays or via filtering. We first show that a subset of the Herault-Jutten learning rules fulfills a principle of minimum output power. We then apply this principle to extensions of the Herault-Jutten network which have delays in the feedback path. Our networks perform well on real speech and music signals that have been mixed using time delays or filtering."
            },
            "slug": "Networks-for-the-Separation-of-Sources-that-Are-and-Platt-Faggin",
            "title": {
                "fragments": [],
                "text": "Networks for the Separation of Sources that Are Superimposed and Delayed"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "New networks to unmix signals which have been mixed either with time delays or via filtering are created and it is shown that a subset of the Herault-Jutten learning rules fulfills a principle of minimum output power."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207599521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1482c67fc0c96dbd1d190e5040ab113a53e544",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised developmental algorithm for linear maps is derived which reduces the pixel-entropy (using the measure introduced in previous work) at every update and thus removes pairwise correlations between pixels. Since the measure of pixel-entropy has only a global minimum the algorithm is guaranteed to converge to the minimum entropy map. Such optimal maps have recently been shown to possess cognitively desirable properties and are likely to be used by the nervous system to organize sensory information. The algorithm derived here turns out to be one proposed by Goodall for pairwise decorrelation. It is biologically plausible since in a neural network implementation it requires only data available locally to a neuron. In training over ensembles of two-dimensional input signals with the same spatial power spectrum as natural scenes, networks develop output neurons with center-surround receptive fields similar to those of ganglion cells in the retina. Some technical issues pertinent to developmental algorithms of this sort, such as symmetry fixing, are also discussed."
            },
            "slug": "Convergent-Algorithm-for-Sensory-Receptive-Field-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "Convergent Algorithm for Sensory Receptive Field Development"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An unsupervised developmental algorithm for linear maps is derived which reduces the pixel-entropy at every update and thus removes pairwise correlations between pixels, and is biologically plausible since in a neural network implementation it requires only data available locally to a neuron."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3361799"
                        ],
                        "name": "E. Sorouchyari",
                        "slug": "E.-Sorouchyari",
                        "structuredName": {
                            "firstName": "Esfandiar",
                            "lastName": "Sorouchyari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sorouchyari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "These observations may help to explain the existence of spurious solutions for H-J, as revealed, for example, in the stability analysis of Sorouchyari (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37484086,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f72708fb94c1f097216bca5431c325e50f69037e",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-III:-Stability-Sorouchyari",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part III: Stability analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29937259"
                        ],
                        "name": "Molgedey",
                        "slug": "Molgedey",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Molgedey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Molgedey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91758492"
                        ],
                        "name": "Schuster",
                        "slug": "Schuster",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Schuster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 13
                            }
                        ],
                        "text": "In addition, Molgedey & Schuster (1994) have proposed a novel technique that uses time delayed correlations to constrain the solution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15276512,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "904cffd541e574901add9c0885b2377c9a2e3268",
            "isKey": false,
            "numCitedBy": 782,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of separating n linearly superimposed uncorrelated signals and determining their mixing coefficients is reduced to an eigenvalue problem which involves the simultaneous diagonalization of two symmetric matrices whose elements are measureable time delayed correlation functions. The diagonalization matrix can be determined from a cost function whose number of minima is equal to the number of degenerate solutions. Our approach offers the possibility to separate also nonlinear mixtures of signals."
            },
            "slug": "Separation-of-a-mixture-of-independent-signals-time-Molgedey-Schuster",
            "title": {
                "fragments": [],
                "text": "Separation of a mixture of independent signals using time delayed correlations."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem of separating n linearly superimposed uncorrelated signals and determining their mixing coefficients is reduced to an eigenvalue problem which involves the simultaneous diagonalization of two symmetric matrices whose elements are measureable time delayed correlation functions."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "The principles may also be applied to other sensory modalities such as vision, where Field (1994) has recently argued that phase-insensitive information maximisation (using only second order statistics) is unable to predict local (non-Fourier) receptive elds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1650980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1152582155acaa0e9d0ccbc900a4641504256d",
            "isKey": false,
            "numCitedBy": 1344,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a compact coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal compact code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of sparse distributed coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling wavelet transforms are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented."
            },
            "slug": "What-Is-the-Goal-of-Sensory-Coding-Field",
            "title": {
                "fragments": [],
                "text": "What Is the Goal of Sensory Coding?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway and suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103005420"
                        ],
                        "name": "Schuster Hg",
                        "slug": "Schuster-Hg",
                        "structuredName": {
                            "firstName": "Schuster",
                            "lastName": "Hg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Schuster Hg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "may be found in Nadal and Parga (1995) and  Schuster (1992) , and it may be possible to define learning rules for such cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120854056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495d5dd4ec696fc338ae4186404efe5155f872bb",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The transmission of information through a nonlinear noisy neuron has been computed with the following results. The mutual information between input and output signals is, in the large-noise limit, rigorously given by the mean-squared variance of the fluctuations of the output of the nonlinear neuron. The changes of synaptic strengths that tend to maximize the mutual information are qualitatively similar to those obtained by Hebbian learning of the nonlinear neuron"
            },
            "slug": "Learning-by-maximizing-the-information-transfer-and-Hg",
            "title": {
                "fragments": [],
                "text": "Learning by maximizing the information transfer through nonlinear noisy neurons and \"noise breakdown"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The transmission of information through a nonlinear noisy neuron has been computed and the changes of synaptic strengths that tend to maximize the mutual information are qualitatively similar to those obtained by Hebbian learning of the nonlinear neuron."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762240"
                        ],
                        "name": "W. Bialek",
                        "slug": "W.-Bialek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bialek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bialek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829021"
                        ],
                        "name": "D. Ruderman",
                        "slug": "D.-Ruderman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ruderman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143860433"
                        ],
                        "name": "A. Zee",
                        "slug": "A.-Zee",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Zee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15218126,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "aa5601be49c9450528dcbdbfc1a9fa05b55155fd",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate the problem of optimizing the sampling of natural images using an array of linear filters. Optimization of information capacity is constrained by the noise levels of the individual channels and by a penalty for the construction of long-range interconnections in the array. At low signal-to-noise ratios the optimal filter characteristics correspond to bound states of a Schrodinger equation in which the signal spectrum plays the role of the potential. The resulting optimal filters are remarkably similar to those observed in the mammalian visual cortex and the retinal ganglion cells of lower vertebrates. The observed scale invariance of natural images plays an essential role in this construction."
            },
            "slug": "Optimal-Sampling-of-Natural-Images:-A-Design-for-Bialek-Ruderman",
            "title": {
                "fragments": [],
                "text": "Optimal Sampling of Natural Images: A Design Principle for the Visual System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The resulting optimal filters are remarkably similar to those observed in the mammalian visual cortex and the retinal ganglion cells of lower vertebrates."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 227
                            }
                        ],
                        "text": "More relevant, however, is the fact that the weighting, or relative importance, bijp, of the moments in (45) is determined by the information theoretic objective function in conjunction with the non-linear function g, while in (41), the bijpq values are accidents of the particular non-linear functions, g and h, that we choose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 233
                            }
                        ],
                        "text": "The explanation (Jutten & Herault 1991) for the success of the H-J network is that the Taylor series expansion of g(u)h(u)T in (40) yields odd cross moments, such that the weights stop changing when: Xi;j bijpqhu2p+1 i u2q+1 j i = 0 (41) for all output unit pairs i 6= j, for p; q = 0; 1; 2; 3 : : :, and for the coe cients bijpq coming from the Taylor series expansion of g and h."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "The convergence criterion (45) involves fewer cross moments than that of (41) and in this sense, may be viewed as a less restrictive condition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7441579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23e470ae5a6ac69133dcdd6bd3caa8cb09490c85",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Although the detection of invariant structure in a given set of input patterns is vital to many recognition tasks, connectionist learning rules tend to focus on directions of high variance (principal components). The prediction paradigm is often used to reconcile this dichotomy; here we suggest a more direct approach to invariant learning based on an anti-Hebbian learning rule. An unsupervised two-layer network implementing this method in a competitive setting learns to extract coherent depth information from random-dot stereograms."
            },
            "slug": "Competitive-Anti-Hebbian-Learning-of-Invariants-Schraudolph-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Competitive Anti-Hebbian Learning of Invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A more direct approach to invariant learning based on an anti-Hebbian learning rule is suggested, an unsupervised two-layer network implementing this method in a competitive setting learns to extract coherent depth information from random-dot stereograms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700994"
                        ],
                        "name": "R. Battiti",
                        "slug": "R.-Battiti",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Battiti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Battiti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 112
                            }
                        ],
                        "text": "We expect to address this issue in future work, and employ useful heuristic or explicit second-order techniques (Battiti 1992) to speed convergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "We expect to address this issue in future work, and employ useful heuristic or explicit 2nd order techniques (Battiti 1992) to speed convergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27960650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf6f07e699587c8d52faf829a289f8cbc7f11a5",
            "isKey": false,
            "numCitedBy": 1216,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "On-line first-order backpropagation is sufficiently fast and effective for many large-scale classification problems but for very high precision mappings, batch processing may be the method of choice. This paper reviews first- and second-order optimization methods for learning in feedforward neural networks. The viewpoint is that of optimization: many methods can be cast in the language of optimization techniques, allowing the transfer to neural nets of detailed results about computational complexity and safety procedures to ensure convergence and to avoid numerical problems. The review is not intended to deliver detailed prescriptions for the most appropriate methods in specific applications, but to illustrate the main characteristics of the different methods and their mutual relations."
            },
            "slug": "First-and-Second-Order-Methods-for-Learning:-and-Battiti",
            "title": {
                "fragments": [],
                "text": "First- and Second-Order Methods for Learning: Between Steepest Descent and Newton's Method"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "First- and second-order optimization methods for learning in feedforward neural networks are reviewed to illustrate the main characteristics of the different methods and their mutual relations."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21243149,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3f19a131970681eebbb003ac6e9b50c7a4db352b",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Animals that are primarily dependent on olfaction must obtain a description of the spatial location and the individual odor quality of environmental odor sources through olfaction alone. The variable nature of turbulent air flow makes such a remote sensing problem solvable if the animal can make use of the information conveyed by the fluctuation with time of the mixture of odor sources. Behavioral evidence suggests that such analysis takes place. An adaptive network can solve the essential problem, isolating the quality and intensity of the components within a mixture of several individual unknown odor sources. The network structure is an idealization of olfactory bulb circuitry. The dynamics of synapse change is essential to the computation. The synaptic variables themselves contain information needed by higher processing centers. The use of the same axons to convey intensity information and quality information requires time-coding of information. Covariation defines an individual odor source (object), and this may have a parallel in vision."
            },
            "slug": "Olfactory-computation-and-object-perception.-Hopfield",
            "title": {
                "fragments": [],
                "text": "Olfactory computation and object perception."
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An adaptive network can solve the essential problem, isolating the quality and intensity of the components within a mixture of several individual unknown odor sources."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27850855"
                        ],
                        "name": "M. Cohen",
                        "slug": "M.-Cohen",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Cohen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730857"
                        ],
                        "name": "A. Andreou",
                        "slug": "A.-Andreou",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Andreou",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Andreou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 16
                            }
                        ],
                        "text": "In addition, in Cohen & Andreou (1995), they report results with mixed sine waves and noise in 5x5 networks, but no separation results for more than two speakers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60858644,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "1483ad8f59ebd5f7cf4f97f859a50b544b842078",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we explore a combination of above-and-subthreshold CMOS circuit techniques for the implementation of analog neuromorphic network processing. The implemented network embodies a blind signal separation algorithm performing an independent component analysis. It is essentially a continuous time recursive linear adaptive filter that uses a non-linear adaptation rule. Analog I/O interface, weight coefficients and adaptation blocks are all integrated on the chip. A test 2-neuron-2-synapse network as well as a small 5-neuron-20-synapse network were fabricated in a 2 micron n-well double-polysilicon, double-metal CMOS process. Circuit designs at the transistor level yield area efficient implementations for synapses and the adaptation blocks. We present experimental results from testing the systems with sinusoidal signals and noise and report on its performance as a blind separator of mixed speech signals. >"
            },
            "slug": "Analog-CMOS-integration-and-experimentation-with-an-Cohen-Andreou",
            "title": {
                "fragments": [],
                "text": "Analog CMOS integration and experimentation with an autoadaptive independent component analyzer"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A combination of above-and-subthreshold CMOS circuit techniques for the implementation of analog neuromorphic network processing and its performance as a blind separator of mixed speech signals is reported on."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679715"
                        ],
                        "name": "Y. Baram",
                        "slug": "Y.-Baram",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Baram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Baram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17949792"
                        ],
                        "name": "Z. Roth",
                        "slug": "Z.-Roth",
                        "structuredName": {
                            "firstName": "Ze'ew",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Roth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "Most notably, Baram & Roth (1994) perform substantially the same analysis as ours, but apply their networks to probability density estimation and time series forecasting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "An example of such a function is the asymmetric generalised logistic function (see also Baram & Roth 1994) described by the di erential equation:\ny0 = dy\ndu = yp(1 y)r (32)\nwhere p and r are positive real numbers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56578755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3869270688a56c165d29e8702e437e8e4e197bd8",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An estimate of the probability density function of a random vector is obtained by maximizing the output entropy of a feed-forward network of sigmoidal units with respect to the input weights. particularly suitable for \u201creal time\u201d prediction. A triangular connectivity between the neurons and the input, which is naturally suggested by the statistical setting, reduces the number of parameters."
            },
            "slug": "Multi-Dimensional-Density-Shaping-by-Sigmoidal-Baram-Roth",
            "title": {
                "fragments": [],
                "text": "Multi-Dimensional Density Shaping by Sigmoidal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An estimate of the probability density function of a random vector is obtained by maximizing the output entropy of a feed-forward network of sigmoidal units with respect to the input weights, suitable for \u201creal time\u201d prediction."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICNN'95 - International Conference on Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42792,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790868"
                        ],
                        "name": "E. Vittoz",
                        "slug": "E.-Vittoz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Vittoz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vittoz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46650224"
                        ],
                        "name": "X. Arreguit",
                        "slug": "X.-Arreguit",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Arreguit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Arreguit"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117702374,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "49bbf103eba9b28d7f922499b70727d59c1ea861",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Let us consider an array of n unknown independent sources Xi(t) which, at any time, are only observable indirectly through the n signals Ei(t) obtained by an unknown linear combination of the Xi (Fig.1)."
            },
            "slug": "CMOS-Integration-of-Herault-Jutten-Cells-for-of-Vittoz-Arreguit",
            "title": {
                "fragments": [],
                "text": "CMOS Integration of Herault-Jutten Cells for Separation of Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Let us consider an array of n unknown independent sources Xi(t), which, at any time, are only observable indirectly through the n signals Ei(t) obtained by an unknown linear combination of the Xi."
            },
            "venue": {
                "fragments": [],
                "text": "Analog VLSI Implementation of Neural Systems"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161090"
                        ],
                        "name": "S. Laughlin",
                        "slug": "S.-Laughlin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Laughlin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Laughlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 148
                            }
                        ],
                        "text": "Various other authors have considered unsupervised learning rules for non-linear units, without justifying them in terms of information theory (see Karhunen & Joutsensalo 1994, and references therein)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8991866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "420c02bdc487338dbda64feb7491dcf9fb412f17",
            "isKey": false,
            "numCitedBy": 904,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The contrast-response function of a class of first order intemeurons in the fly's compound eye approximates to the cumulative probability distribution of contrast levels in natural scenes. Elementary information theory shows that this matching enables the neurons to encode contrast fluctuations most efficiently."
            },
            "slug": "A-Simple-Coding-Procedure-Enhances-a-Neuron's-Laughlin",
            "title": {
                "fragments": [],
                "text": "A Simple Coding Procedure Enhances a Neuron's Information Capacity"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The contrast-response function of a class of first order intemeurons in the fly's compound eye approximates to the cumulative probability distribution of contrast levels in natural scenes, showing that this matching enables the neurons to encode contrast fluctuations most efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "Zeitschrift fur Naturforschung. Section C, Biosciences"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27850855"
                        ],
                        "name": "M. Cohen",
                        "slug": "M.-Cohen",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Cohen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730857"
                        ],
                        "name": "A. Andreou",
                        "slug": "A.-Andreou",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Andreou",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Andreou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1135,
                                "start": 30
                            }
                        ],
                        "text": "The task is to recover the original sources by finding a square matrix, W, which is a permutation and rescaling of the inverse of the unknown matrix, A. The problem has also been called the \"cocktail-party\" p r ~ b l e m . ~ In blinddeconvolution, described in Haykin (1991,1994a) and illustrated in Figure 3b, a single unknown signal s ( t ) is convolved with an unknown tapped delay-line filter a l , . . . .aK, giving a corrupted signal x ( t ) = a ( t ) * s ( t ) where a ( t ) is the impulse response of the filter. The task is to recover s ( t ) by convolving x ( t ) with a learnt filter wl. . . . wL, which reverses the effect of the filter a( t ) . There are many similarities between the two problems. In one, sources are corrupted by the superposition of other sources. In the other, a source is corrupted by time-delayed versions of itself. In both cases, unsupervised learning must be used because no error signals are available. In both cases, second-order statistics are inadequate to solve the problem. For example, for blind separation, a second-order decorrelation technique such as that of Barlow and Foldiak (1989) would find uncorrelated, or linearly independent, projections, y, of the input data, x."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1198,
                                "start": 37
                            }
                        ],
                        "text": ", and for the coefficients b,,, coming from the Taylor series expansion of the tanh function. The convergence criterion 6.7 involves fewer cross-moments than that of 6.3 and in this sense may be viewed as a less restrictive condition. More relevant, however, is the fact that the weighting, or relative importance, b,,,, of the moments in 6.7 is determined by the information-theoretic objective function in conjunction with the nonlinear function g, while in 6.3, the b,,,, values are accidents of the particular nonlinear functions, g and k, that we choose. These observations may help to explain the existence of spurious solutions for H-J, as revealed, for example, in the stability analysis of Sorouchyari (1991). Several other approaches to blind separation exist. Comon (1994) expands the mutual information in terms of cumulants up to order 4, amounting to a truncation of the constraints in 5.7. A similar proposal that combines separation with deconvolution is to be found in Yellin and Weinstein (1994). Such cumulant-based methods seem to work, though they are complex. It is not clear how the truncation of the expansion affects the solution. In addition, Molgedey and Schuster (1994) proposed a novel technique that uses time-delayed correlations to constrain the solution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 784,
                                "start": 37
                            }
                        ],
                        "text": ", and for the coefficients b,,, coming from the Taylor series expansion of the tanh function. The convergence criterion 6.7 involves fewer cross-moments than that of 6.3 and in this sense may be viewed as a less restrictive condition. More relevant, however, is the fact that the weighting, or relative importance, b,,,, of the moments in 6.7 is determined by the information-theoretic objective function in conjunction with the nonlinear function g, while in 6.3, the b,,,, values are accidents of the particular nonlinear functions, g and k, that we choose. These observations may help to explain the existence of spurious solutions for H-J, as revealed, for example, in the stability analysis of Sorouchyari (1991). Several other approaches to blind separation exist. Comon (1994) expands the mutual information in terms of cumulants up to order 4, amounting to a truncation of the constraints in 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1014,
                                "start": 37
                            }
                        ],
                        "text": ", and for the coefficients b,,, coming from the Taylor series expansion of the tanh function. The convergence criterion 6.7 involves fewer cross-moments than that of 6.3 and in this sense may be viewed as a less restrictive condition. More relevant, however, is the fact that the weighting, or relative importance, b,,,, of the moments in 6.7 is determined by the information-theoretic objective function in conjunction with the nonlinear function g, while in 6.3, the b,,,, values are accidents of the particular nonlinear functions, g and k, that we choose. These observations may help to explain the existence of spurious solutions for H-J, as revealed, for example, in the stability analysis of Sorouchyari (1991). Several other approaches to blind separation exist. Comon (1994) expands the mutual information in terms of cumulants up to order 4, amounting to a truncation of the constraints in 5.7. A similar proposal that combines separation with deconvolution is to be found in Yellin and Weinstein (1994). Such cumulant-based methods seem to work, though they are complex."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1313,
                                "start": 37
                            }
                        ],
                        "text": ", and for the coefficients b,,, coming from the Taylor series expansion of the tanh function. The convergence criterion 6.7 involves fewer cross-moments than that of 6.3 and in this sense may be viewed as a less restrictive condition. More relevant, however, is the fact that the weighting, or relative importance, b,,,, of the moments in 6.7 is determined by the information-theoretic objective function in conjunction with the nonlinear function g, while in 6.3, the b,,,, values are accidents of the particular nonlinear functions, g and k, that we choose. These observations may help to explain the existence of spurious solutions for H-J, as revealed, for example, in the stability analysis of Sorouchyari (1991). Several other approaches to blind separation exist. Comon (1994) expands the mutual information in terms of cumulants up to order 4, amounting to a truncation of the constraints in 5.7. A similar proposal that combines separation with deconvolution is to be found in Yellin and Weinstein (1994). Such cumulant-based methods seem to work, though they are complex. It is not clear how the truncation of the expansion affects the solution. In addition, Molgedey and Schuster (1994) proposed a novel technique that uses time-delayed correlations to constrain the solution. Finally, Hopfield (1991) has applied a variant of the H-J architecture to odor separation in a model of the olfactory bulb."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 718,
                                "start": 37
                            }
                        ],
                        "text": ", and for the coefficients b,,, coming from the Taylor series expansion of the tanh function. The convergence criterion 6.7 involves fewer cross-moments than that of 6.3 and in this sense may be viewed as a less restrictive condition. More relevant, however, is the fact that the weighting, or relative importance, b,,,, of the moments in 6.7 is determined by the information-theoretic objective function in conjunction with the nonlinear function g, while in 6.3, the b,,,, values are accidents of the particular nonlinear functions, g and k, that we choose. These observations may help to explain the existence of spurious solutions for H-J, as revealed, for example, in the stability analysis of Sorouchyari (1991). Several other approaches to blind separation exist."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61291174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "522c7de4287c85cdb894ef680f930b39d0a56fb2",
            "isKey": true,
            "numCitedBy": 58,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors explore translinear circuits in subthreshold MOS technology and current-mode design techniques for the implementation of neuromorphic analog network processing. The architecture, also known as the Herault-Jutten network, performs an independent component analysis and is essentially a continuous-time recursive linear adaptive filter. Analog I/O interface, weight coefficients, and adaptation blocks are all integrated on the chip. A small network with six neurons and 30 synapses was fabricated in a 2- mu m double-polysilicon, double-metal n-well CMOS process. Circuit designs at the transistor level yield area-efficient implementations for neurons, synapses, and the adaptation blocks. The authors discuss the design methodology and constraints as well as test results from the fabricated chips. >"
            },
            "slug": "Current-mode-subthreshold-MOS-implementation-of-the-Cohen-Andreou",
            "title": {
                "fragments": [],
                "text": "Current-mode subthreshold MOS implementation of the Herault-Jutten autoadaptive network"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The authors explore translinear circuits in subthreshold MOS technology and current-mode design techniques for the implementation of neuromorphic analog network processing and discuss the design methodology and constraints as well as test results from the fabricated chips."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055575683"
                        ],
                        "name": "Steve Rogers",
                        "slug": "Steve-Rogers",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Rogers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Rogers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The problem has also been called the `cocktail-party' problem4\nIn blind deconvolution, described in (Haykin 1991, 1994a) and illustrated in Fig.3b, a single unknown signal s(t) is convolved with an unknown tapped delay-line lter a1; : : : ; aK, giving a corrupted signal x(t) = a(t) s(t) where a(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "lution\nIn the case of blind deconvolution, our approach most resembles the `Bussgang' family of techniques (Bellini 1994, Haykin 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 279
                            }
                        ],
                        "text": "The output, u, is considered to be a noisy version of the original signal, s. Models of the pdf's of the original signal and this noise are then constructed, and Bayesian reasoning yields a non-linear conditional estimator of s from u, which can be quite complex [see (20.39) in Haykin 1991]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "Haykin does note, though, that in the limit of high convolutional noise, g, can be well approximated by the tanh sigmoid non-linearity [(20.44) in Haykin 1991], exactly the non-linearity we have been using."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 109523942,
            "fieldsOfStudy": [
                "Engineering",
                "Biology"
            ],
            "id": "6b2abc90dffc1a434f49881b1468aaf896f7923a",
            "isKey": true,
            "numCitedBy": 2038,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Filter-Theory-Rogers",
            "title": {
                "fragments": [],
                "text": "Adaptive Filter Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "3 Combining separation and deconvolution The blind separation rules in (14) and (15) and the blind deconvolution rules in (23) and (24) can be easily combined."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 113
                            }
                        ],
                        "text": "The literature displays a diversity of approaches and justi cations|for historical reviews see (Comon 1994) and (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "This gives the blind deconvolution rule for a tapped delay weight at time t [compare with (24)]:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "The above equation can be di erentiated as follows, with respect\n1see the discussion in Haykin 1994b, chapter 11, also Cover & Thomas, chapter 9.\nto a parameter, w, involved in the mapping from X to Y :\n@\n@w I(Y"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "The only di erence is that (24) contains the term tanh(u) where (47) has the term u tanh(u), but as can be easily veri ed, these terms are of the same sign at all times, so the algorithms should behave similarly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "If g(u) = tanh(u) then this rule is very similar to (24)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "This yields exactly the learning rule one would expect: the leading weights in the lters follow the blind separation rules and all the others follow a decorrelation rule similar to (24) except that now there are tapped weights wikj between an input xj(t k) and an output yi(t)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "(5)The learning rate is de ned as the proportionality constant in (14)-(15) and (23)-(24)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 324
                            }
                        ],
                        "text": "Could it be that the success of the Bussgang approaches using Bayesian conditional estimators are due less to the exact form of the conditional estimator than to the general goal of squeezing as much information as possible through a sigmoid function? As noted, a similarity exists between the information maximisation rule (24), derived without any Bayesian modelling, and the Bussgang rule (47) when convolutional noise levels are high."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "They do not have su cient information to estimate the phase of the corrupting lter, a(t), only its amplitude (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "2 Blind deconvolution results Speech signals were convolved with various lters and the learning rules in (23) and (24) were used to perform blind deconvolution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The problem has also been called the `cocktail-party' problem4\nIn blind deconvolution, described in (Haykin 1991, 1994a) and illustrated in Fig.3b, a single unknown signal s(t) is convolved with an unknown tapped delay-line lter a1; : : : ; aK, giving a corrupted signal x(t) = a(t) s(t) where a(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": true,
            "numCitedBy": 9896,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The problem has also been called the `cocktail-party' problem4\nIn blind deconvolution, described in (Haykin 1991, 1994a) and illustrated in Fig.3b, a single unknown signal s(t) is convolved with an unknown tapped delay-line lter a1; : : : ; aK, giving a corrupted signal x(t) = a(t) s(t) where a(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "lution\nIn the case of blind deconvolution, our approach most resembles the `Bussgang' family of techniques (Bellini 1994, Haykin 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 279
                            }
                        ],
                        "text": "The output, u, is considered to be a noisy version of the original signal, s. Models of the pdf's of the original signal and this noise are then constructed, and Bayesian reasoning yields a non-linear conditional estimator of s from u, which can be quite complex [see (20.39) in Haykin 1991]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "Haykin does note, though, that in the limit of high convolutional noise, g, can be well approximated by the tanh sigmoid non-linearity [(20.44) in Haykin 1991], exactly the non-linearity we have been using."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62605959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cca4ac6c18cf415f4b0a9734311d644e9c557e8",
            "isKey": true,
            "numCitedBy": 2406,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-filter-theory-(2nd-ed.)-Haykin",
            "title": {
                "fragments": [],
                "text": "Adaptive filter theory (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Figure 2: The generalised logistic sigmoid (top row) of (32), and its slope, y, (bottom row), for (a) p = r = 1, (b) p = r = 5 and (c) p = r = 0:2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "The rst, the development of information theoretic unsupervised learning rules for neural networks has been pioneered by Linsker 1992, Becker & Hinton 1992, Atick & Redlich 1993, Plumbley & Fallside 1988 and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Linsker (1992) derived a learning algorithm for maximising the mutual information between two layers of a network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "An example of such a function is the asymmetric generalised logistic function (see also Baram & Roth 1994) described by the di erential equation: y = dy du = y(1 y) (32) where p and r are positive real numbers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "First, a non-linearity such as that de ned by (32) is optimised to approximate the cumulative distributions, (31), of known independent components (sources)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local synaptic learning rules su ce to maximise mutual information in a linear network, Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "Haykin does note, though, that in the limit of high convolutional noise, g, can be well approximated by the tanh sigmoid non-linearity [(20.44) in Haykin 1991], exactly the non-linearity we have been using."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "lution\nIn the case of blind deconvolution, our approach most resembles the `Bussgang' family of techniques (Bellini 1994, Haykin 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "The Jacobian in (20) is written as follows: J = det \" @y(ti) @x(tj)#ij = (detW ) M Y t=1 y0(t) (21) and may be decomposed into the determinant of the weight matrix (19), and the product of the slopes of the squashing function, y0(t) = @y(t)=@u(t), for all times t [see Appendix (53)]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 279
                            }
                        ],
                        "text": "The output, u, is considered to be a noisy version of the original signal, s. Models of the pdf's of the original signal and this noise are then constructed, and Bayesian reasoning yields a non-linear conditional estimator of s from u, which can be quite complex [see (20.39) in Haykin 1991]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The problem has also been called the `cocktail-party' problem4\nIn blind deconvolution, described in (Haykin 1991, 1994a) and illustrated in Fig.3b, a single unknown signal s(t) is convolved with an unknown tapped delay-line lter a1; : : : ; aK, giving a corrupted signal x(t) = a(t) s(t) where a(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive lter theory, 2nd ed., Prentice-Hall"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 133
                            }
                        ],
                        "text": "We are much indebted to Nicol Schraudolph, who not only supplied the original idea in Fig.1 and shared his unpublished calculations (schraudolph et al. 1991), but also provided detailed criticism at every stage of the work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Schraudolph et al (1991), in work that inspired this approach, considered it as a method for initialising weights in a neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "We have performed many simulations in which the H-J net failed to converge, but because there is substantial freedom in the choice of g and h in (40), we cannot be sure that our choices were good ones."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "It remains to conduct a detailed performance comparison between (40) and the algorithm presented here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "The explanation (Jutten & Herault 1991) for the success of the H-J network is that the Taylor series expansion of g(u)h(u) in (40) yields odd cross moments, such that the weights stop changing when: X"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal information ow in sigmoidal neurons, unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 219
                            }
                        ],
                        "text": "For example, if the input pdf fx(x) were gaussian, then the w0-rule would centre the steepest part of the sigmoid curve on the peak of fx(x), matching input density to output slope, in a manner suggested intuitively by (3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Analogously to (3), the multivariate probability density function of y can be written (Papoulis, eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "An example of such a function is the asymmetric generalised logistic function (see also Baram & Roth 1994) described by the di erential equation:\ny0 = dy\ndu = yp(1 y)r (32)\nwhere p and r are positive real numbers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Substituting (3) into (4) gives H(y) = E \"ln @y @x # E [ln fx(x)] (5) The second term on the right (the entropy of x) may be considered to be una ected by alterations in a parameter w determining g(x)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "Most notably, Baram & Roth (1994) perform substantially the same analysis as ours, but apply their networks to probability density estimation and time series forecasting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "5-5): fy(y) = fx(x) j@y=@xj (3) where the bars denote absolute value."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-dimensional density shaping by sig-  moidal networks with application to classi cation, estimation and fore-  casting, CIS report"
            },
            "venue": {
                "fragments": [],
                "text": "Centre for Intelligent systems,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The problem has also been called the `cocktail-party' problem4\nIn blind deconvolution, described in (Haykin 1991, 1994a) and illustrated in Fig.3b, a single unknown signal s(t) is convolved with an unknown tapped delay-line lter a1; : : : ; aK, giving a corrupted signal x(t) = a(t) s(t) where a(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "lution\nIn the case of blind deconvolution, our approach most resembles the `Bussgang' family of techniques (Bellini 1994, Haykin 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 279
                            }
                        ],
                        "text": "The output, u, is considered to be a noisy version of the original signal, s. Models of the pdf's of the original signal and this noise are then constructed, and Bayesian reasoning yields a non-linear conditional estimator of s from u, which can be quite complex [see (20.39) in Haykin 1991]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "Haykin does note, though, that in the limit of high convolutional noise, g, can be well approximated by the tanh sigmoid non-linearity [(20.44) in Haykin 1991], exactly the non-linearity we have been using."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adnptiz~c Filter Tl~cory"
            },
            "venue": {
                "fragments": [],
                "text": "Adnptiz~c Filter Tl~cory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "Haykin does note, though, that in the limit of high convolutional noise, g, can be well approximated by the tanh sigmoid non-linearity [(20.44) in Haykin 1991], exactly the non-linearity we have been using."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "lution\nIn the case of blind deconvolution, our approach most resembles the `Bussgang' family of techniques (Bellini 1994, Haykin 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 279
                            }
                        ],
                        "text": "The output, u, is considered to be a noisy version of the original signal, s. Models of the pdf's of the original signal and this noise are then constructed, and Bayesian reasoning yields a non-linear conditional estimator of s from u, which can be quite complex [see (20.39) in Haykin 1991]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "Numerical integration of this equation produces sigmoids suitable for very peaked (as p; r > 1, see Fig.2b) and at, unit-like (as p; r < 1, see Fig.2c) input distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The problem has also been called the `cocktail-party' problem4\nIn blind deconvolution, described in (Haykin 1991, 1994a) and illustrated in Fig.3b, a single unknown signal s(t) is convolved with an unknown tapped delay-line lter a1; : : : ; aK, giving a corrupted signal x(t) = a(t) s(t) where a(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive lter theory"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive lter theory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "5The learning rate is de ned as the proportionality constant in (14)-(15) and (23)-(24)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Secondly, ifA is almost singular, then any unmixingW must also be almost singular, making the learning in (14) quite unstable in the vicinity of a solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "For now, however, the network learning rule in (14) remains unbiological."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "3 Combining separation and deconvolution The blind separation rules in (14) and (15) and the blind deconvolution rules in (23) and (24) can be easily combined."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "The method of training was stochastic gradient ascent, but because of the costly matrix inversion in (14), weights were usually adjusted based on the summed W's of small `batches' of length B, where 5 B 300."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "A deterministic linear network can increase its information throughput without bound, as the [WT ] 1 term in (14) suggests."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Appendix | proof of learning rule (14) Consider a network with an input vector x, a weight matrixW, a bias vector w0 and a non-linearly transformed output vector y = g(u), u = Wx + w0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "3a and the algorithm in (14) and (15) was su cient to perform blind separation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "For sigmoidal units, y = g(u), u = Wx + w0, with g being the logistic function: g(u) = (1 + e u) 1, the resulting learning rules are familiar in form (proof given in the Appendix): W / hWT i 1 + (1 2y)xT (14) w0 / 1 2y (15) except that now x, y, w0 and 1 are vectors (1 is a vector of ones), W is a matrix, and the anti-Hebbian term has become an outer product."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "From this we may write WHJ = W 1 = W 1 W W 1 (39) so that our learning rule, (14), forms part of a rule for the recurrent H-J network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analog CMOS integration and exper-  imentation with an autoadaptive independent component analyzer, IEEE  Transactions on Circuits and Systems-II: analog and digital signal"
            },
            "venue": {
                "fragments": [],
                "text": "pro-  cessing,"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858054"
                        ],
                        "name": "P. F\u00f6ldi\u00e1k",
                        "slug": "P.-F\u00f6ldi\u00e1k",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "F\u00f6ldi\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. F\u00f6ldi\u00e1k"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 211
                            }
                        ],
                        "text": "This can be done by considering the `training set' of x's to approximate the density fx(x), and deriving an `online', stochastic gradient ascent learning rule: w / @H @w = @ @w ln @y @x ! = @y @x! 1 @ @w @y @x! (6) In the case of the logistic transfer function: y = 1 1 + e u ; u = wx+ w0 (7) 5"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 252
                            }
                        ],
                        "text": "4 For weights with time delays Consider a weight, w, with a time delay, d, and a sigmoidal non-linearity, g, so that: y(t) = g[wx(t d)] (25) We can maximise the entropy of y with respect to the time delay, again by maximising the log slope of y [as in (6)] : d / @H @d = @ @d (ln jy0j) (26) The crucial step in this derivation is to realise that @ @dx(t d) = @ @tx(t d): (27) Calling this quantity simply _ x, we may then write: @y @d = w _ xy0 (28) Our general rule is therefore given as follows: @ @d (ln jy0j) = 1 y0 @y0 @y @y @d = w _ x@y0 @y (29) When g is the tanh function, for example, this yields the following rule for adapting the time delay: d / 2w _ xy: (30) This rule holds regardless of the architecture in which the network is embedded, and it is local, unlike the w rule in (16)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 276
                            }
                        ],
                        "text": "in which the input is multiplied by a weight w and added to a bias-weight w0, the terms above evaluate as: @y @x = wy(1 y) (8) @ @w @y @x! = y(1 y)(1 + wx(1 2y)) (9) Dividing (9) by (8) gives the learning rule for the logistic function, as calculated from the general rule of (6): w / 1 w + x(1 2y) (10) Similar reasoning leads to the rule for the bias-weight: w0 / 1 2y (11) The e ect of these two rules can be seen in Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 125
                            }
                        ],
                        "text": "maximisation\nMany authors have formulated optimality criteria similar to ours, for both neural networks and sensory systems (Barlow 1989, Atick 1992, Bialek, Ruderman & Zee 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60513567,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7947b19f7d5488b8d76664c8b2d70daff350babd",
            "isKey": true,
            "numCitedBy": 339,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptation-and-decorrelation-in-the-cortex-Barlow-F\u00f6ldi\u00e1k",
            "title": {
                "fragments": [],
                "text": "Adaptation and decorrelation in the cortex"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "The calculation of this dependency proceeds as in the one unit case of (8) and (9)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "in which the input is multiplied by a weight w and added to a bias-weight w0, the terms above evaluate as: @y @x = wy(1 y) (8) @ @w @y @x! = y(1 y)(1 + wx(1 2y)) (9) Dividing (9) by (8) gives the learning rule for the logistic function, as calculated from the general rule of (6): w / 1 w + x(1 2y) (10) Similar reasoning leads to the rule for the bias-weight: w0 / 1 2y (11) The e ect of these two rules can be seen in Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "The rst, the development of information theoretic unsupervised learning rules for neural networks has been pioneered by Linsker 1992, Becker & Hinton 1992, Atick & Redlich 1993, Plumbley & Fallside 1988 and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 81
                            }
                        ],
                        "text": "Finally, another group of information theoretic algorithms have been proposed by Becker & Hinton (1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A self-organising neural network that dis-  covers surfaces in random-dot stereograms, Nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Both ICA and the `whitening' approach to deconvolution are examples of minimising I(y1; y2) for all pairs y1 and y2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 164
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 143
                            }
                        ],
                        "text": "The hope, in general, is that learning rules containing such terms will be sensitive to the right higher-order statistics necessary to perform ICA or whitening."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 96
                            }
                        ],
                        "text": "The literature displays a diversity of approaches and justi cations|for historical reviews see (Comon 1994) and (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 4
                            }
                        ],
                        "text": "See Comon (1994) and Hatzinakos & Nikias (1994) for the application of this approach to separation and deconvolution respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "It is this latter process, also\ncalled Independent Component Analysis (ICA), which enables the network to solve the blind separation task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 104
                            }
                        ],
                        "text": "The former process, the learning of W, is called the problem of Independent Component Analysis, or ICA (Comon 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "Section 4 discusses the conditions under which the information maximisation process can nd factorial codes (perform ICA), and therefore solve the separation and deconvolution problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 244
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Henceforth, we use the term redundancy reduction when we mean either ICA or the whitening of a time series."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Comon (1994) expands the mutual information in terms of cumulants up to order 4, amounting to a truncation of the constraints in (45)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Sipml Pro- CCSS"
            },
            "venue": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Sipml Pro- CCSS"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 183
                            }
                        ],
                        "text": "When we have chosen values for p and r, perhaps by some optimisation process, the rules for changing a single input-output weight, w, and a bias, w0, are subtly altered from (10) and (11), but clearly the same when p = r = 1:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 108
                            }
                        ],
                        "text": "lution\nIn the case of blind deconvolution, our approach most resembles the `Bussgang' family of techniques (Bellini 1994, Haykin 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 162
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 187
                            }
                        ],
                        "text": "Such reasoning has been used to justify both the Herault-Jutten (or `H-J') approach to blind separation (Comon et al 1991) and the so-called `Bussgang' approaches to blind deconvolution (Bellini 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bussgang techniques for blind deconvolution and equalisation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Both ICA and the `whitening' approach to deconvolution are examples of minimising I(y1; y2) for all pairs y1 and y2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 164
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 143
                            }
                        ],
                        "text": "The hope, in general, is that learning rules containing such terms will be sensitive to the right higher-order statistics necessary to perform ICA or whitening."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 96
                            }
                        ],
                        "text": "The literature displays a diversity of approaches and justi cations|for historical reviews see (Comon 1994) and (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 4
                            }
                        ],
                        "text": "See Comon (1994) and Hatzinakos & Nikias (1994) for the application of this approach to separation and deconvolution respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "It is this latter process, also\ncalled Independent Component Analysis (ICA), which enables the network to solve the blind separation task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 104
                            }
                        ],
                        "text": "The former process, the learning of W, is called the problem of Independent Component Analysis, or ICA (Comon 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "Section 4 discusses the conditions under which the information maximisation process can nd factorial codes (perform ICA), and therefore solve the separation and deconvolution problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 244
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Henceforth, we use the term redundancy reduction when we mean either ICA or the whitening of a time series."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Comon (1994) expands the mutual information in terms of cumulants up to order 4, amounting to a truncation of the constraints in (45)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Signal Process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 108
                            }
                        ],
                        "text": "lution\nIn the case of blind deconvolution, our approach most resembles the `Bussgang' family of techniques (Bellini 1994, Haykin 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 375,
                                "start": 371
                            }
                        ],
                        "text": "in which the input is multiplied by a weight w and added to a bias-weight w0, the terms above evaluate as: @y @x = wy(1 y) (8) @ @w @y @x! = y(1 y)(1 + wx(1 2y)) (9) Dividing (9) by (8) gives the learning rule for the logistic function, as calculated from the general rule of (6): w / 1 w + x(1 2y) (10) Similar reasoning leads to the rule for the bias-weight: w0 / 1 2y (11) The e ect of these two rules can be seen in Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 183
                            }
                        ],
                        "text": "When we have chosen values for p and r, perhaps by some optimisation process, the rules for changing a single input-output weight, w, and a bias, w0, are subtly altered from (10) and (11), but clearly the same when p = r = 1: w / 1 w + x(p(1 y) ry) (33) w0 / p(1 y) ry (34) The importance of being able to train a general function of this type will be explained in section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 162
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 187
                            }
                        ],
                        "text": "Such reasoning has been used to justify both the Herault-Jutten (or `H-J') approach to blind separation (Comon et al 1991) and the so-called `Bussgang' approaches to blind deconvolution (Bellini 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bussgang techniques for blind deconvolution and equali-  sation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Both ICA and the `whitening' approach to deconvolution are examples of minimising I(y1; y2) for all pairs y1 and y2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 164
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 143
                            }
                        ],
                        "text": "The hope, in general, is that learning rules containing such terms will be sensitive to the right higher-order statistics necessary to perform ICA or whitening."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 96
                            }
                        ],
                        "text": "The literature displays a diversity of approaches and justi cations|for historical reviews see (Comon 1994) and (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 4
                            }
                        ],
                        "text": "See Comon (1994) and Hatzinakos & Nikias (1994) for the application of this approach to separation and deconvolution respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "It is this latter process, also\ncalled Independent Component Analysis (ICA), which enables the network to solve the blind separation task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 104
                            }
                        ],
                        "text": "The former process, the learning of W, is called the problem of Independent Component Analysis, or ICA (Comon 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 795,
                                "start": 791
                            }
                        ],
                        "text": "4 For weights with time delays Consider a weight, w, with a time delay, d, and a sigmoidal non-linearity, g, so that: y(t) = g[wx(t d)] (25) We can maximise the entropy of y with respect to the time delay, again by maximising the log slope of y [as in (6)] : d / @H @d = @ @d (ln jy0j) (26) The crucial step in this derivation is to realise that @ @dx(t d) = @ @tx(t d): (27) Calling this quantity simply _ x, we may then write: @y @d = w _ xy0 (28) Our general rule is therefore given as follows: @ @d (ln jy0j) = 1 y0 @y0 @y @y @d = w _ x@y0 @y (29) When g is the tanh function, for example, this yields the following rule for adapting the time delay: d / 2w _ xy: (30) This rule holds regardless of the architecture in which the network is embedded, and it is local, unlike the w rule in (16)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "in equation (16) is decidedly non-local."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "For an individual weight, wij, this rule amounts to: wij / cof wij detW + xj(1 2yi) (16) where cof wij, the cofactor of wij , is ( 1)i+j times the determinant of the matrix obtained by removing the ith row and the jth column from W."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "Section 4 discusses the conditions under which the information maximisation process can nd factorial codes (perform ICA), and therefore solve the separation and deconvolution problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 244
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Henceforth, we use the term redundancy reduction when we mean either ICA or the whitening of a time series."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Comon (1994) expands the mutual information in terms of cumulants up to order 4, amounting to a truncation of the constraints in (45)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Signal  processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "The rst, the development of information theoretic unsupervised learning rules for neural networks has been pioneered by Linsker 1992, Becker & Hinton 1992, Atick & Redlich 1993, Plumbley & Fallside 1988 and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Dividing (9) by (8) gives the learning rule for the logistic function, as calculated from the general rule of (6):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 81
                            }
                        ],
                        "text": "Finally, another group of information theoretic algorithms have been proposed by Becker & Hinton (1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "in which the input is multiplied by a weight w and added to a bias-weight w0, the terms above evaluate as: @y @x = wy(1 y) (8)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "The calculation of this dependency proceeds as in the one unit case of (8) and (9)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A self-organising neural network that discovers surfaces in random-dot stereograms, Nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 80
                            }
                        ],
                        "text": "In contrast with these results, our experience with tests on the H-J network of Jutten & Herault (1991) has been that it occasionally fails to converge for two sources and only rarely converges for three, on the same speech and music signals we used for separating ten sources."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 119
                            }
                        ],
                        "text": "But it could only nd a symmetric decorrelation matrix, which would not su ce if the mixing matrix, A, were asymmetric (Jutten & Herault 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 127
                            }
                        ],
                        "text": "It bears a resemblance to the rule proposed by Platt & Faggin (1992) for adjustable time delays in the network architecture of Jutten & Herault (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 375,
                                "start": 371
                            }
                        ],
                        "text": "4 For weights with time delays Consider a weight, w, with a time delay, d, and a sigmoidal non-linearity, g, so that: y(t) = g[wx(t d)] (25) We can maximise the entropy of y with respect to the time delay, again by maximising the log slope of y [as in (6)] : d / @H @d = @ @d (ln jy0j) (26) The crucial step in this derivation is to realise that @ @dx(t d) = @ @tx(t d): (27) Calling this quantity simply _ x, we may then write: @y @d = w _ xy0 (28) Our general rule is therefore given as follows: @ @d (ln jy0j) = 1 y0 @y0 @y @y @d = w _ x@y0 @y (29) When g is the tanh function, for example, this yields the following rule for adapting the time delay: d / 2w _ xy: (30) This rule holds regardless of the architecture in which the network is embedded, and it is local, unlike the w rule in (16)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 17
                            }
                        ],
                        "text": "The explanation (Jutten & Herault 1991) for the success of the H-J network is that the Taylor series expansion of g(u)h(u)T in (40) yields odd cross moments, such that the weights stop changing when:\nX i;j bijpqhu 2p+1 i u 2q+1 j i = 0 (41)\nfor all output unit pairs i 6= j, for p; q = 0; 1; 2; 3 :\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: an adap-  tive algorithm based on neuromimetic architecture, Signal processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 21
                            }
                        ],
                        "text": "See Comon (1994) and Hatzinakos & Nikias (1994) for the application of this approach to separation and deconvolution respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "The Jacobian in (20) is written as follows: J = det \" @y(ti) @x(tj)#ij = (detW ) M Y t=1 y0(t) (21) and may be decomposed into the determinant of the weight matrix (19), and the product of the slopes of the squashing function, y0(t) = @y(t)=@u(t), for all times t [see Appendix (53)]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 256
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "At this point, we take the liberty of imagining there is an ensemble of such time series, so that we can write, fY (Y ) = fX(X) jJ j (20) where again, jJ j is the Jacobian of the transformation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind equalisation based on higher-  order statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The problem has also been called the `cocktail-party' problem4\nIn blind deconvolution, described in (Haykin 1991, 1994a) and illustrated in Fig.3b, a single unknown signal s(t) is convolved with an unknown tapped delay-line lter a1; : : : ; aK, giving a corrupted signal x(t) = a(t) s(t) where a(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "The above equation can be di erentiated as follows, with respect\n1see the discussion in Haykin 1994b, chapter 11, also Cover & Thomas, chapter 9.\nto a parameter, w, involved in the mapping from X to Y :\n@\n@w I(Y"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "They do not have su cient information to estimate the phase of the corrupting lter, a(t), only its amplitude (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 113
                            }
                        ],
                        "text": "The literature displays a diversity of approaches and justi cations|for historical reviews see (Comon 1994) and (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind Deconvolutioiz"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The problem has also been called the `cocktail-party' problem4\nIn blind deconvolution, described in (Haykin 1991, 1994a) and illustrated in Fig.3b, a single unknown signal s(t) is convolved with an unknown tapped delay-line lter a1; : : : ; aK, giving a corrupted signal x(t) = a(t) s(t) where a(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "The above equation can be di erentiated as follows, with respect\n1see the discussion in Haykin 1994b, chapter 11, also Cover & Thomas, chapter 9.\nto a parameter, w, involved in the mapping from X to Y :\n@\n@w I(Y"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "They do not have su cient information to estimate the phase of the corrupting lter, a(t), only its amplitude (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 113
                            }
                        ],
                        "text": "The literature displays a diversity of approaches and justi cations|for historical reviews see (Comon 1994) and (Haykin 1994a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks: a c omprehensive foundation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural networks: a c omprehensive foundation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91758492"
                        ],
                        "name": "Schuster",
                        "slug": "Schuster",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Schuster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 76
                            }
                        ],
                        "text": "The beginning of such an analysis may be found in Nadal & Parga (1995), and Schuster (1992) and it may be possible to de ne learning rules for such cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9782907,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "35229f5013e4c5f4291b7cbac5689e4e22b330a7",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-by-maximizing-the-information-transfer-and-Schuster",
            "title": {
                "fragments": [],
                "text": "Learning by maximizing the information transfer through nonlinear noisy neurons and \"noise breakdown\""
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. A, Atomic, molecular, and optical physics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1991940"
                        ],
                        "name": "J. Proakis",
                        "slug": "J.-Proakis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Proakis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Proakis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "15-93): H(y1; y2) = H(y1) +H(y2) I(y1; y2): (35) Maximising this joint entropy consists of maximising the individual entropies while minimising the mutual information, I(y1; y2), shared between the two."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "The algorithm presented in section 2 is a stochastic gradient ascent algorithmwhich maximises the joint entropy in (35)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2072334,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7eeb730524a8e980f91c923b8e1d026b17883e38",
            "isKey": false,
            "numCitedBy": 7906,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability,-random-variables-and-stochastic-Proakis",
            "title": {
                "fragments": [],
                "text": "Probability, random variables and stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 202
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "When we have chosen values for p and r, perhaps by some optimisation process, the rules for changing a single input-output weight, w, and a bias, w0, are subtly altered from (10) and (11), but clearly the same when p = r = 1:\nw / 1\nw + x(p(1 y) ry) (33)\nw0 / p(1 y) ry (34)\nThe importance of being\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Possible principles underlying the transformation of sensory messages, in Sensory Communication, R o s e n blith"
            },
            "venue": {
                "fragments": [],
                "text": "Possible principles underlying the transformation of sensory messages, in Sensory Communication, R o s e n blith"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Schraudolph et al (1991), in work that inspired this approach, considered it as a method for initialising weights in a neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 143
                            }
                        ],
                        "text": "= H(Y ) H(Y jX) (1)\nwhere H(Y ) is the entropy of the output, while H(Y jX) is whatever entropy the output has which didn't come from the input."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 133
                            }
                        ],
                        "text": "We are much indebted to Nicol Schraudolph, who not only supplied the original idea in Fig.1 and shared his unpublished calculations (schraudolph et al. 1991), but also provided detailed criticism at every stage of the work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal information ow in sigmoidal neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Optimal information ow in sigmoidal neurons"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A self-nrganising neural network that discovers surfaces in random-dot stereograms"
            },
            "venue": {
                "fragments": [],
                "text": "Nnture ( Loi ~ don )"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Redundancy reduction with information-preserving maps"
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Competitive anti-Hebbian learn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Redundancy reduction with informa tion-preserving maps"
            },
            "venue": {
                "fragments": [],
                "text": "Net work"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "6-63): fy(y) = fx(x) jJ j (12) where jJ j is the absolute value of the Jacobian of the transformation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources: a non-linear neural algorithm,  Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information-theoretic approach to unsupervised connectionist models"
            },
            "venue": {
                "fragments": [],
                "text": "In"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 105
                            }
                        ],
                        "text": "Such reasoning has been used to justify both the Herault-Jutten (or `H-J') approach to blind separation (Comon et al 1991) and the so-called `Bussgang' approaches to blind deconvolution (Bellini 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part 11: Problems statement. S i p a l Process"
            },
            "venue": {
                "fragments": [],
                "text": "Blind separation of sources, part 11: Problems statement. S i p a l Process"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 50
                            }
                        ],
                        "text": "A brief report of this research appears in Bell & Sejnowski (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A nonlinear information maximization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elementsoflnformation Theory"
            },
            "venue": {
                "fragments": [],
                "text": "John Wiley, New York."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 7
                            }
                        ],
                        "text": "Deco & Brauer (1995) use cumulant expansions to approximate mutual information between outputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-linear higher-order statistical decorrelation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "Then networks using this non-linearity are trained using the full weight matrix and bias vector generalisation of (33) and (34):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "w / 1 w + x(p(1 y) ry) (33) w0 / p(1 y) ry (34)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-linear neurons in the low noise limit: a factorial code maximises information"
            },
            "venue": {
                "fragments": [],
                "text": "transfer. Network,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning factorial coclcs by predictability rninimiz<?tion"
            },
            "venue": {
                "fragments": [],
                "text": "Ncrirwl Corrrp"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 47
                            }
                        ],
                        "text": "It bears a resemblance to the rule proposed by Platt & Faggin (1992) for adjustable time delays in the network architecture of Jutten & Herault (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Networks for the separation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 105
                            }
                        ],
                        "text": "Such reasoning has been used to justify both the Herault-Jutten (or `H-J') approach to blind separation (Comon et al 1991) and the so-called `Bussgang' approaches to blind deconvolution (Bellini 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part 11: Problems statement. Signal Process"
            },
            "venue": {
                "fragments": [],
                "text": "Blind separation of sources, part 11: Problems statement. Signal Process"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "Most notably, Baram & Roth (1994) perform substantially the same analysis as ours, but apply their networks to probability density estimation and time series forecasting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "An example of such a function is the asymmetric generalised logistic function (see also Baram & Roth 1994) described by the di erential equation:\ny0 = dy\ndu = yp(1 y)r (32)\nwhere p and r are positive real numbers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-dimensional density shaping by s i g moidal networks with application to classiication, estimation and forecasting , CIS report no"
            },
            "venue": {
                "fragments": [],
                "text": "Multi-dimensional density shaping by s i g moidal networks with application to classiication, estimation and forecasting , CIS report no"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 65
                            }
                        ],
                        "text": "The algorithm does not assume any knowledge of the input distributions, and is de ned here for the zero-noise limit."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "The rst, the development of information theoretic unsupervised learning rules for neural networks has been pioneered by Linsker 1992, Becker & Hinton 1992, Atick & Redlich 1993, Plumbley & Fallside 1988 and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 81
                            }
                        ],
                        "text": "Finally, another group of information theoretic algorithms have been proposed by Becker & Hinton (1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A self-organising neural network that discovers surfaces in random-dot stereograms"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 24
                            }
                        ],
                        "text": "In addition, in Cohen & Andreou (1995), they report results with mixed sine waves and noise in 5x5 networks, but no separation results for more than two speakers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analog CMOS integration and exper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 21
                            }
                        ],
                        "text": "See Comon (1994) and Hatzinakos & Nikias (1994) for the application of this approach to separation and deconvolution respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "The Jacobian in (20) is written as follows: J = det \" @y(ti) @x(tj) #"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 256
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind equalisation based on higherorder statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 13
                            }
                        ],
                        "text": "In addition, Molgedey & Schuster (1994) have proposed a novel technique that uses time delayed correlations to constrain the solution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Separation of independent signals using time-delayed correlations, Phys. Rev. Letts Non-linear neurons in the low noise limit: a factorial code maximises information transfer"
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 125
                            }
                        ],
                        "text": "maximisation\nMany authors have formulated optimality criteria similar to ours, for both neural networks and sensory systems (Barlow 1989, Atick 1992, Bialek, Ruderman & Zee 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning. Neural Coinp"
            },
            "venue": {
                "fragments": [],
                "text": "Unsupervised learning. Neural Coinp"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 202
                            }
                        ],
                        "text": "This process is variously known as factorial code learning (Barlow 1989), predictability minimisation (Schmidhuber 1992) as well as independent component analysis (Comon 1994) and redundancy reduction (Barlow 1961, Atick 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "The entropy of the output, H(y), is given by: H(y) = E [ln fy(y)] = Z 1 1 fy(y) ln fy(y)dy (4) where E[:] denotes expected value."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "Substituting (3) into (4) gives H(y) = E \"ln @y @x # E [ln fx(x)] (5) The second term on the right (the entropy of x) may be considered to be una ected by alterations in a parameter w determining g(x)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Possible principles underlying the transformation of sensory messages, in Sensory Communication, Rosenblith W.A"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "These observations may help to explain the existence of spurious solutions for H-J, as revealed, for example, in the stability analysis of Sorouchyari (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "For the tanh non-linearity, we derive: W / hWT i 1 2yxT (42) This converges in the mean when (ignoring bias weights and assuming x to be zero mean): hWT i 1 = 2htanh(Wx)xTi: (43) This condition can be readily rewritten (multiplying in by WT and using u =Wx) as: I = 2htanh(u)uT i: (44) Since tanh is an odd function, its series expansion is of the form tanh(u) = Pj bju2p+1, the bj being coe cients, and thus the convergence criterion (44) amounts to the condition Xi;j bijphu2p+1 i uji = 0 (45) for all output unit pairs i 6= j, for p = 0; 1; 2; 3 : : :, and for the coe cients bijp coming from the Taylor series expansion of the tanh function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part III: stability anal-  ysis, Signal processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 82
                            }
                        ],
                        "text": "A similar proposal which combines separation with deconvolution is to be found in Yellin & Weinstein (1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Criteria for multichannel signal separation. l E E E firms. Sigml Process"
            },
            "venue": {
                "fragments": [],
                "text": "Criteria for multichannel signal separation. l E E E firms. Sigml Process"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Redundancy reduction with infor"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ncuri71 Netzcwks: A Corizprt.l~cr~.siz~c Forrrr~latior~"
            },
            "venue": {
                "fragments": [],
                "text": "Ncuri71 Netzcwks: A Corizprt.l~cr~.siz~c Forrrr~latior~"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Separation of independent signals using time - delayed correlations"
            },
            "venue": {
                "fragments": [],
                "text": "Phys . Rev . Lett ."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Redundancy reduction with information - preserving maps"
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "First- and second-order methods for learning: Between steepest"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind equalisation based on higher-order"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 178
                            }
                        ],
                        "text": "The rst, the development of information theoretic unsupervised learning rules for neural networks has been pioneered by Linsker 1992, Becker & Hinton 1992, Atick & Redlich 1993, Plumbley & Fallside 1988 and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "50 0:01 0:04 377777775 (38) As can be seen, only one substantial entry (boxed) exists in each row and column."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information-theoretic approach to unsupervised connectionist models"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1988 Connectionist Models Summer School,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "y(t) = g(u(t)) = g(w(t) x(t)) (17) Y = g(U) = g(WX) (18)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Deco & Brauer (1995) use cumulant expansions to approximate mutual information between outputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-linear higher-order statistical decorrelation by volume-conserving neural architectures, Neural Networks, in press"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elei?zents of lnforinntion Theor!"
            },
            "venue": {
                "fragments": [],
                "text": "Elei?zents of lnforinntion Theor!"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "The rst, the development of information theoretic unsupervised learning rules for neural networks has been pioneered by Linsker 1992, Becker & Hinton 1992, Atick & Redlich 1993, Plumbley & Fallside 1988 and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 81
                            }
                        ],
                        "text": "Finally, another group of information theoretic algorithms have been proposed by Becker & Hinton (1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A self-nrganising neural network that discovers surfaces in random-dot stereograms"
            },
            "venue": {
                "fragments": [],
                "text": "Nnture (Loi~don)"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convergent algorithm tor sensory receptive"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A nonlinear information maximization algorithm that performs blind separation. I11 Adzlances in Neurnl In~ormntion Processing S y s"
            },
            "venue": {
                "fragments": [],
                "text": "A nonlinear information maximization algorithm that performs blind separation. I11 Adzlances in Neurnl In~ormntion Processing S y s"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 98
                            }
                        ],
                        "text": "A previous approach has been implemented in analog VLSI circuitry for real-time source separation (Vittoz and Arreguit 1989; Cohen and Andreou 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CMOS integration of Herault-lutten cells"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received October"
            },
            "venue": {
                "fragments": [],
                "text": "Received October"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 135
                            }
                        ],
                        "text": "We are much indebted to Nicol Schraudolph, who not only supplied the original idea in Figure 1 and shared his unpublished calculations (Schraudolph et al. 1991), but also provided detailed criticism at every stage of the work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Schraudolph et al (1991), in work that inspired this approach, considered it as a method for initialising weights in a neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 133
                            }
                        ],
                        "text": "We are much indebted to Nicol Schraudolph, who not only supplied the original idea in Fig.1 and shared his unpublished calculations (schraudolph et al. 1991), but also provided detailed criticism at every stage of the work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal information flow in sigmoidal neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Unpublished manuscript."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "y(t) = g(u(t)) = g(w(t) x(t)) (17) Y = g(U) = g(WX) (18)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elements of information theory, John Wiley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Space or time adaptive signal processing by n,eural network models"
            },
            "venue": {
                "fragments": [],
                "text": "In"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "In blind separation, as introduced by Herault & Jutten (1986), and illustrated in Fig.3a, a set of sources, s1(t); : : : ; sN (t), (di erent people speaking, music etc) are mixed together linearly by a matrix A."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Space or time adaptive signal processing by neural network models Olfactory computation and object perception"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Computing: AIP Conference Proceedings 151"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 34
                            }
                        ],
                        "text": "See Comon (1994) and Hatzinakos & Nikias (1994) for the application of this approach to separation and deconvolution respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 269
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind equalisation based"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "In blind separation, as introduced by Herault & Jutten (1986), and illustrated in Fig.3a, a set of sources, s1(t); : : : ; sN (t), (di erent people speaking, music etc) are mixed together linearly by a matrix A."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "4 For weights with time delays Consider a weight, w, with a time delay, d, and a sigmoidal non-linearity, g, so that: y(t) = g[wx(t d)] (25) We can maximise the entropy of y with respect to the time delay, again by maximising the log slope of y [as in (6)] : d / @H @d = @ @d (ln jy0j) (26) The crucial step in this derivation is to realise that @ @dx(t d) = @ @tx(t d): (27) Calling this quantity simply _ x, we may then write: @y @d = w _ xy0 (28) Our general rule is therefore given as follows: @ @d (ln jy0j) = 1 y0 @y0 @y @y @d = w _ x@y0 @y (29) When g is the tanh function, for example, this yields the following rule for adapting the time delay: d / 2w _ xy: (30) This rule holds regardless of the architecture in which the network is embedded, and it is local, unlike the w rule in (16)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Space or time adaptive signal processing by  neural network models, in Denker J.S. (ed), Neural networks for comput-  ing: AIP conference proceedings 151"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bliri~f Dccorrz~olrrtioi~"
            },
            "venue": {
                "fragments": [],
                "text": "Bliri~f Dccorrz~olrrtioi~"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information-Maximization 1159"
            },
            "venue": {
                "fragments": [],
                "text": "Information-Maximization 1159"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 148
                            }
                        ],
                        "text": "Various other authors have considered unsupervised learning rules for non-linear units, without justifying them in terms of information theory (see Karhunen & Joutsensalo 1994, and references therein)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 449,
                                "start": 445
                            }
                        ],
                        "text": "4 For weights with time delays Consider a weight, w, with a time delay, d, and a sigmoidal non-linearity, g, so that: y(t) = g[wx(t d)] (25) We can maximise the entropy of y with respect to the time delay, again by maximising the log slope of y [as in (6)] : d / @H @d = @ @d (ln jy0j) (26) The crucial step in this derivation is to realise that @ @dx(t d) = @ @tx(t d): (27) Calling this quantity simply _ x, we may then write: @y @d = w _ xy0 (28) Our general rule is therefore given as follows: @ @d (ln jy0j) = 1 y0 @y0 @y @y @d = w _ x@y0 @y (29) When g is the tanh function, for example, this yields the following rule for adapting the time delay: d / 2w _ xy: (30) This rule holds regardless of the architecture in which the network is embedded, and it is local, unlike the w rule in (16)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representation and separation of signals using non-linear PCA type learning, Neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Separation of independent signals using"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "4 For weights with time delays Consider a weight, w, with a time delay, d, and a sigmoidal non-linearity, g, so that: y(t) = g[wx(t d)] (25) We can maximise the entropy of y with respect to the time delay, again by maximising the log slope of y [as in (6)] :"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "In blind separation, as introduced by Herault & Jutten (1986), and illustrated in Fig.3a, a set of sources, s1(t); : : : ; sN (t), (di erent people speaking, music etc) are mixed together linearly by a matrix A."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Space or time adaptive signal processing by neural network models, in Denker J.S. (ed), Neural networks for computing: AIP conference proceedings 151"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elei?zents of lnforinntion Theor!"
            },
            "venue": {
                "fragments": [],
                "text": "John Wiley, New York."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 21
                            }
                        ],
                        "text": "See Comon (1994) and Hatzinakos & Nikias (1994) for the application of this approach to separation and deconvolution respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 256
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind equalisation based on higher-order Anthony J. Bell and Terrence J. Sejnowski statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Blind equalisation based on higher-order Anthony J. Bell and Terrence J. Sejnowski statistics"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "4 For weights with time delays Consider a weight, w, with a time delay, d, and a sigmoidal non-linearity, g, so that: y(t) = g[wx(t d)] (25) We can maximise the entropy of y with respect to the time delay, again by maximising the log slope of y [as in (6)] :"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "In blind separation, as introduced by Herault & Jutten (1986), and illustrated in Fig.3a, a set of sources, s1(t); : : : ; sN (t), (di erent people speaking, music etc) are mixed together linearly by a matrix A."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Space or time adaptive signal processing by neural network models, in Denker J.S. (ed), Neural networks for computing: AIP conference proceedings 151"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probability, Rnrzdom Variables and Stoclzastic Processes"
            },
            "venue": {
                "fragments": [],
                "text": "Probability, Rnrzdom Variables and Stoclzastic Processes"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 70
                            }
                        ],
                        "text": "An approach to this problem using `beamforming' may be found in (Li & Sejnowski 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive separation of mixed broadband"
            },
            "venue": {
                "fragments": [],
                "text": "Z. Naturforsch"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 21
                            }
                        ],
                        "text": "See Comon (1994) and Hatzinakos & Nikias (1994) for the application of this approach to separation and deconvolution respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 256
                            }
                        ],
                        "text": "tion\nAs indicated in section 3, approaches to blind separation and blind deconvolution have divided into those using non-linear functions (Jutten & Herault 1991, Bellini 1994) and those using explicit calculations of cumulants and polyspectra (Comon 1994, Hatzinakos & Nikias 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind equalisation based 011 higher-order Anthony J. Bell and Terrence J. Sejnowski statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Blind equalisation based 011 higher-order Anthony J. Bell and Terrence J. Sejnowski statistics"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "We can write this system either as a convolution or as a matrix equation: y(t) = g(u(t)) = g(w(t) x(t)) (17) Y = g(U) = g(WX) (18) in which Y , X and U are vectors of the whole time series, and W is a M M matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Deco & Brauer (1995) use cumulant expansions to approximate mutual information between outputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-linear higher-order statistical decorrela-  tion by volume-conserving neural architectures, Neural Networks, in press"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal intormition flow in sig~noidai"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning factorial coclcs by predictability rninimiz < ? tion"
            },
            "venue": {
                "fragments": [],
                "text": "Ncrirwl Corrrp ."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 13
                            }
                        ],
                        "text": "In addition, Molgedey & Schuster (1994) have proposed a novel technique that uses time delayed correlations to constrain the solution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "Then networks using this non-linearity are trained using the full weight matrix and bias vector generalisation of (33) and (34):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "w / 1 w + x(p(1 y) ry) (33) w0 / p(1 y) ry (34)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Separation of independent signals using time-delayed correlations"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. Letts. 72,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 671,
                                "start": 667
                            }
                        ],
                        "text": "4 For weights with time delays Consider a weight, w, with a time delay, d, and a sigmoidal non-linearity, g, so that: y(t) = g[wx(t d)] (25) We can maximise the entropy of y with respect to the time delay, again by maximising the log slope of y [as in (6)] : d / @H @d = @ @d (ln jy0j) (26) The crucial step in this derivation is to realise that @ @dx(t d) = @ @tx(t d): (27) Calling this quantity simply _ x, we may then write: @y @d = w _ xy0 (28) Our general rule is therefore given as follows: @ @d (ln jy0j) = 1 y0 @y0 @y @y @d = w _ x@y0 @y (29) When g is the tanh function, for example, this yields the following rule for adapting the time delay: d / 2w _ xy: (30) This rule holds regardless of the architecture in which the network is embedded, and it is local, unlike the w rule in (16)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 100
                            }
                        ],
                        "text": "Under these conditions, information maximisation has extra properties not found in the linear case (Linsker 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An application of the principle of maximum informa-  tion preservation to linear systems, in Advances in Neural Information  Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 56,
            "methodology": 51,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 121,
        "totalPages": 13
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski/1d7d0e8c4791700defd4b0df82a26b50055346e0?sort=total-citations"
}