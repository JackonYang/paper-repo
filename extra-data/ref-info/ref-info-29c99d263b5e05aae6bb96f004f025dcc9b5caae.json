{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8125917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd7cee21074ea6b346011d7463f7387ad9bfcc2a",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Because the World Wide Web consists primarily of text, information extraction is central to any effort that would use the Web as a resource for knowledge discovery. We show how information extraction can be cast as a standard machine learning problem, and argue for the suitability of relational learning in solving it. The implementation of a general-purpose relational learner for information extraction, SRV, is described. In contrast with earlier learning systems for information extraction, SRV makes no assumptions about document structure and the kinds of information available for use in learning extraction patterns. Instead, structural and other information is supplied as input in the form of an extensible token-oriented feature set. We demonstrate the effectiveness of this approach by adapting SRV for use in learning extraction rules for a domain consisting of university course and research project pages sampled from the Web. Making SRV Web-ready only involves adding several simple HTML-specific features to its basic feature set."
            },
            "slug": "Information-Extraction-from-HTML:-Application-of-a-Freitag",
            "title": {
                "fragments": [],
                "text": "Information Extraction from HTML: Application of a General Machine Learning Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work shows how information extraction can be cast as a standard machine learning problem, and argues for the suitability of relational learning in solving it, and the implementation of a general-purpose relational learner for information extraction, SRV."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16747313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f80e289b2f52b558d388aba7df2d1689513a928b",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The eld of information extraction (IE) is concerned with applying natural language processing (NLP) and information retrieval (IR) techniques to the automatic extraction of essential details from text documents. We are exploring the use of machine learning methods for IE. While the most promising methods we have developed perform well for problems deened over a collection of electronic seminar announcements, they are imprecise in their identiication of the boundaries of relevant text fragments (elds). Here, we entertain the idea of using grammatical inference (GI) methods to learn the appropriate form of a eld. We describe one method for translating raw text into an abstract alphabet suitable for GI, and show that, by combining one IE learning method with the resulting inferred grammars, large improvements in precision can be realized for some elds."
            },
            "slug": "Using-grammatical-inference-to-improve-precision-in-Freitag",
            "title": {
                "fragments": [],
                "text": "Using grammatical inference to improve precision in information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "One method for translating raw text into an abstract alphabet suitable for GI is described, and it is shown that, by combining one IE learning method with the resulting inferred grammars, large improvements in precision can be realized for some elds."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967815"
                        ],
                        "name": "M. E. Califf",
                        "slug": "M.-E.-Califf",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Califf",
                            "middleNames": [
                                "Elaine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Califf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "\u2026to nd the title of the group responsible for the attack the instrument of the attack and the victim s name from home pages we might seek to extract the owner s name home address and university a liation\nThere are many possible uses for a successful IE sys tem As a front end an IE system can\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 489775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16bd1fbe3694173eda4ad4338a85f8288d19bf02",
            "isKey": false,
            "numCitedBy": 700,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present a system, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains."
            },
            "slug": "Relational-Learning-of-Pattern-Match-Rules-for-Califf-Mooney",
            "title": {
                "fragments": [],
                "text": "Relational Learning of Pattern-Match Rules for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 150
                            }
                        ],
                        "text": "\u2026the object might be to nd the title of the group responsible for the attack the instrument of the attack and the victim s name from home pages we might seek to extract the owner s name home address and university a liation\nThere are many possible uses for a successful IE sys tem As a front end an\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10566644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "130cbc5e907cccbd0fcd4f9138bc9886dc3217d7",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a wealth of information to be mined from narrative text on the World Wide Web. Unfortunately, standard natural language processing (NLP) extraction techniques expect full, grammatical sentences, and perform poorly on the choppy sentence fragments that are often found on web pages. \n \nThis paper1 introduces Webfoot, a preprocessor that parses web pages into logically coherent segments based on page layout cues. Output from Webfoot is then passed on to CRYSTAL, an NLP system that learns text extraction rules from example. Webfoot and CRYSTAL transform the text into a formal representation that is equivalent to relational database entries. This is a necessary first step for knowledge discovery and other automated analysis of free text."
            },
            "slug": "Learning-to-Extract-Text-Based-Information-from-the-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Text-Based Information from the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Webfoot, a preprocessor that parses web pages into logically coherent segments based on page layout cues, is introduced and passed on to CRYSTAL, an NLP system that learns text extraction rules from example."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60585486,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca198cc81878fd036c7b97ee10441f1d09839f65",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "An enormous amount of knowledge is needed to infer the meaning of unrestricted natural language. The problem can be reduced to a manageable size by restricting attention to a specific {\\em domain}, which is a corpus of texts together with a predefined set of {\\em concepts} that are of interest to that domain. Two widely different domains are used to illustrate this domain-specific approach. One domain is a collection of Wall Street Journal articles in which the target concept is management succession events: identifying persons moving into corporate management positions or moving out. A second domain is a collection of hospital discharge summaries in which the target concepts are various classes of diagnosis or symptom. The goal of an information extraction system is to identify references to the concept of interest for a particular domain. A key knowledge source for this purpose is a set of text analysis rules based on the vocabulary, semantic classes, and writing style peculiar to the domain. This thesis presents CRYSTAL, an implemented system that automatically induces domain-specific text analysis rules from training examples. CRYSTAL learns rules that approach the performance of hand-coded rules, are robust in the face of noise and inadequate features, and require only a modest amount of training data. CRYSTAL belongs to a class of machine learning algorithms called covering algorithms, and presents a novel control strategy with time and space complexities that are independent of the number of features. CRYSTAL navigates efficiently through an extremely large space of possible rules. CRYSTAL also demonstrates that expressive rule representation is essential for high performance, robust text analysis rules. While simple rules are adequate to capture the most salient regularities in the training data, high performance can only be achieved when rules are expressive enough to reflect the subtlety and variability of unrestricted natural language."
            },
            "slug": "Learning-text-analysis-rules-for-domain-specific-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning text analysis rules for domain-specific natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis presents CRYSTAL, an implemented system that automatically induces domain-specific text analysis rules from training examples that approach the performance of hand-coded rules, are robust in the face of noise and inadequate features, and require only a modest amount of training data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 148
                            }
                        ],
                        "text": "\u2026group responsible for the attack the instrument of the attack and the victim s name from home pages we might seek to extract the owner s name home address and university a liation\nThere are many possible uses for a successful IE sys tem As a front end an IE system can enable database mining and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5119155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e7402ad740b73cc0bb64178f86df3478c3aaf5",
            "isKey": false,
            "numCitedBy": 1283,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Internet information resources present relational data|telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is di cult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge."
            },
            "slug": "Wrapper-Induction-for-Information-Extraction-Kushmerick-Weld",
            "title": {
                "fragments": [],
                "text": "Wrapper Induction for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces wrapper induction, a method for automatically constructing wrappers, and identifies hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60458454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69859be3ea6cb8eb38434c80fef5d4997eaec2dc",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation introduces a new theoretical model for text classification systems, including systems for document retrieval, automated indexing, electronic mail filtering, and similar tasks. The Concept Learning model emphasizes the role of manual and automated feature selection and classifier formation in text classification. It enables drawing on results from statistics and machine learning in explaining the effectiveness of alternate representations of text, and specifies desirable characteristics of text representations. \nThe use of syntactic parsing to produce indexing phrases has been widely investigated as a possible route to better text representations. Experiments with syntactic phrase indexing, however, have never yielded significant improvements in text retrieval performance. The Concept Learning model suggests that the poor statistical characteristics of a syntactic indexing phrase representation negate its desirable semantic characteristics. The application of term clustering to this representation to improve its statistical properties while retaining its desirable meaning properties is proposed. \nStandard term clustering strategies from information retrieval (IR), based on cooccurrence of indexing terms in documents or groups of documents, were tested on a syntactic indexing phrase representation. In experiments using a standard text retrieval test collection, small effectiveness improvements were obtained. \nAs a means of evaluating representation quality, a text retrieval test collection introduces a number of confounding factors. In contrast, the text categorization task allows much cleaner determination of text representation properties. In preparation for the use of text categorization to study text representation, a more effective and theoretically well-founded probabilistic text categorization algorithm was developed, building on work by Maron, Fuhr, and others. \nText categorization experiments supported a number of predictions of the Concept Learning model about properties of phrasal representations, including dimensionality properties not previously measured for text representations. However, in carefully controlled experiments using syntactic phrases produced by Church's stochastic bracketer, in conjunction with reciprocal nearest neighbor clustering, term clustering was found to produce essentially no improvement in the properties of the phrasal representation. New cluster analysis approaches are proposed to remedy the problems found in traditional term clustering methods."
            },
            "slug": "Representation-and-Learning-in-Information-Lewis",
            "title": {
                "fragments": [],
                "text": "Representation and Learning in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new theoretical model for text classification systems, including systems for document retrieval, automated indexing, electronic mail filtering, and similar tasks, is introduced, suggesting that the poor statistical characteristics of a syntactic indexing phrase representation negate its desirable semantic characteristics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 147
                            }
                        ],
                        "text": "\u2026of information and aspects of a problem than for others A statistical learner like Naive Bayes for example is useful for problems in which each fea ture contributes some evidence toward the determina tion of class membership and in which violations of the independence assumption do not\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12628510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea89276d99997e047bb3c960a084e43d54cbce0a",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Several well-developed approaches to inductive learning low exist, but each has specific limitations that are hard to overcome. Multi-strategy learning attempts to tackle this problem combining multiple methods in one algorithm. This article describes a unification of two widely-used empirical approaches: rule induction and instance-based learning. In the new algorithm, instances are treated as maximally specific rules, and classification is oerformed using a best-match strategy. Rules are learned by gradually generalizing instances until no improvement in apparent accuracy is obtained. Theoretical analysis shows this approach to be efficient. It is implemented in the RISE 3.1 system. In an extensive empirical study, RISE consistently achieves higher accuracies than state-of-the-art representatives of both its parent approaches (PEBLS and CN2), as well as a decision tree learner (C4.5). Lesion studies show that eachoof RISE's components is essential to this performance. Most significantly, in 14 of the 30 domains studied, RISE is more accurate than the best of PEBLS and CN2, showing that a significant synergy can be obtained by combining multiple empirical methods."
            },
            "slug": "Unifying-instance-based-and-rule-based-induction-Domingos",
            "title": {
                "fragments": [],
                "text": "Unifying instance-based and rule-based induction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In an extensive empirical study, RISE consistently achieves higher accuracies than state-of-the-art representatives of both its parent approaches, as well as a decision tree learner (C4.5)."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 147
                            }
                        ],
                        "text": "\u2026map directly from con dence to probability of correctness The third which we will call CBayes uses Bayes Rule to make combination decisions\nREGRESSION TO ESTIMATE CORRECTNESS\nIf a learner s con dence numbers are meaningful then the probability that a prediction is correct will increase with\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1504,
                                "start": 1498
                            }
                        ],
                        "text": "Within the constraint that all learners assign a con dence to any predictions they make any fragments they accept a wide range of behaviors is possible In particular for a number of reasons we cannot as sume that the con dences bear any resemblance to true probability of correctness or even that they are comparable across learners Bayes s con dences are large negative log probabilities for example\nWe do assume however that probability of correctness increases with increasing con dence for all learners The basic idea therefore is to attempt to compute a mapping for each learner from con dence to probabil ity of correctness Figure shows this in outline The speci c steps involved are\nValidate performance on a hold out set Re serve a part of the training set for validation After training each learner store its predictions with con dences on the hold out set\nUse regression to map con dences to prob abilities Based on the learner s performance on the hold out set attempt to model how its perfor mance varies with con dence What is modeled and the kind of regression used depends on the combination method\nUse the regression models and calculated probabilities to make the best choice on the test set\nWe experimented with three basic methods of combi nation The rst two which we will callMax and Prob both attempt to work with regression models that map directly from con dence to probability of correctness The third which we will call CBayes uses Bayes Rule to make combination decisions\nREGRESSION TO ESTIMATE CORRECTNESS\nIf a learner s con dence numbers are meaningful then the probability that a prediction is correct will increase with increasing con dence We use linear regression to model the rate at which this probability increases For each prediction made we create a datapoint x y where x is the prediction con dence and y is if the prediction was correct the corresponding fragment was a eld instance else\nThe result is a line equation which we use directly to map from learner con dence to probability of suc cess Both Max and Prob use the resulting estimates to arbitrate among multiple learners predictions for a document Estimates are computed for each learner s predictions and the prediction with the highest esti mate is chosen as the top combined prediction The two methods di er only in how they handle the case in which multiple learners o er predictions for the same text fragment In such an event Max simply takes the larger estimate as the probability that the fragment is a eld instance\nWe believe however that the fact that two or more learners agree on a prediction provides more informa tion than either prediction alone Indeed if we as sume that two probability estimates of an event Pa and Pb are independent then the combined probabil ity is the probability that they are not both wrong i e Pa Pb Prob s estimate is based on this assumption Given a set of probability estimates Pi its estimate for the combined probability is Q i Pi\nBAYESIAN PREDICTION COMBINATION\nAlthough Prob may exploit the availability of predic tions from multiple learners better than Max it still leaves something to be desired In particular it ig nores some of the available information such as the frequency with which a learner tends to predict at a given con dence level and any notion of prior proba bilities\nFor our nal combinationmethod we attempt to apply Bayes Rule which tells us how to maintain our prob ability estimates in response to incoming data Using Bayes Rule o ers two advantages over Prob It allows us to incorporate priors into our estimates and it tells us how to maintain our hypothesis space so that the resulting estimates are closer to true probabilities an advantage in terms of the accuracy coverage trade o\nHere a hypothesis Hi takes the form the fragment at this place in the document is a eld instance Let Pai C be the event Learner A predicted fragment i is a eld instance with con dence C For each frag ment i chosen by any of the learners we maintain two hypotheses explicitly Hi and Hi Individual learner predictions Pai C are treated as events which cause us to update hypotheses We want therefore to model Pr Pai CjHi and Pr Pai Cj Hi It is more con venient however to model the event Pai C i e the probability of a prediction with con dence at least C Modeling the cumulative probability yields better statistics and allows us to avoid the arbitrary decisions inherent in binning\nWe use exponential regression to model these two probabilities i e we perform linear regression on pairs of the form x log y where x is a con dence level and y is the cumulative probability of seeing a predic tion for a fragment given that it either is or is not a eld instance As an example consider the problem of creating the positive model Pr Pai CjHi for some learner A Let F be the total number of eld instances in the validation set and let Ga C be the number of eld instances identi ed by Learner A with predictions having con dence equal to or greater than C For every prediction made by Learner A we add a regression datapoint x log y where x is the con dence of the prediction and y Ga x F The neg ative model Pr Pai cj Hi is constructed in the same way except over non eld instance fragments any fragment in the validation set identi ed by any of the learners We settled on exponential regression em pirically but it is easy to see why it works better than\nTable Accuracy Coverage Results for the Seminar Announcement Domain\nspeaker location\nAcc Cov Acc Cov\nRote Bayes SRV\nMax Prob CBayes\nstime etime\nRote Bayes SRV\nMax Prob CBayes\nlinear regression Low con dence predictions tend to be more frequent than high con dence ones obeying something like Zipf s Law\nWith each prediction we use the two models associ ated with a learner to adjust the posterior probabilities of the two mutually exclusive hypotheses regarding the a ected fragment always normalizing so they sum to\nEXPERIMENTS\nWe experimented with data from two IE domains One consists of postings to electronic bulletin boards which describe upcoming seminars in a university en vironment The earliest of these announcements dates to October the most recent was posted in Au gust We manually tagged these announcements for four elds speaker location stime start time and etime end time The other domain is a collec tion of newswire articles on corporate acquisitions from the Reuters data set Lewis We de ned nine elds for this domain and manually annotated the collection to identify all instances of them We selected ve of the elds for these experiments acquired the o cial name of the company or resource that is being purchased purchaser acqabr the short name for acquired used in the body of the article purchabr and dlramt the price paid\nThe performance numbers we report here are the re sult of ve fold experiments in each domain In each iteration the datasets were randomly divided into two partitions of equal size One partition was used for training the other for testing\nTable Accuracy Coverage Results for the Acquisi tion Domain\nacquired purchaser\nAcc Cov Acc Cov\nRote Bayes SRV\nMax Prob CBayes\nacqabr purchabr\nRote Bayes SRV\nMax Prob CBayes\ndlramt\nRote Bayes SRV Max Prob CBayes\nTable F Scores Two scores are shown for each result Full the F score for the accuracy coverage re sults reported in Tables and and Peak the highest F score along the full accuracy coverage curve\nspeaker location stime\nFull Peak Full Peak Full Peak\nRote Bayes SRV\nMax Prob CBayes\netime acquired purchaser\nRote Bayes SRV\nMax Prob CBayes\nacqabr purchabr dlramt\nRote Bayes SRV\nMax Prob CBayes\nA third of the training set randomly selected was set aside for validation Each learner was trained on the remaining two thirds and tested on the validation set Following this validation step each learner was again trained on the entire training set and tested on the test set The goal of the combining methods was to use performance results on the validation set to arbitrate among predictions on the test set\nThe performance of all methods is summarized in Ta ble for the seminar announcement elds and Ta ble for the acquisition elds The unit of measure ment here as elsewhere in this paper is a document When assessing a learner s performance for a single document we can distinguish among four basic out comes no prediction from the learner prediction on a document lacking a eld instance spurious top pre diction is incorrect wrong and top prediction is cor rect correct The coverage column Cov shows for what fraction of those documents containing a eld instance a learner actually made a prediction The number in the accuracy column Acc shows the frac tion of correct predictions over documents for which the learner made a prediction and which contained a eld instance i e it ignores spurious predictions Note that if any single learner makes a spurious pre diction all combining methods also make one since they are limited to ordering the predictions made by actual learners Thus counting spurious predictions as errors while generally appropriate tends to obscure the di erences between the learners and the combining methods\nBoth the accuracy and coverage values should be considered together There are cases for example where the accuracy number makes Rote look like the strongest extraction method Its accuracy however is usually measured over a much smaller number of documents While it can typically recognize a fraction of eld instances with reasonable accuracy especially locations it does not stand up well to overall com parison with the other learners For convenience in comparing systems it is common in information re trieval and information extraction to combine preci sion and recall into a single summary number called the F measure\nF PR\nP R\nThe parameter determines how much to favor recall over precision Researchers in information extraction frequently report the F score of a system which weights precision and recall equally We can do the same with our accuracy coverage results Table\n0 0.2\n0.4\n0.6\n0.8\n1\n0 0.2 0.4 0.6 0.8 1\nA cc\nur ac\ny\nCoverage\nRote Bayes\nSRV Max Prob CBayes\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.2 0.4 0.6 0.8 1\nA cc\nur ac\ny\nCoverage\nRote Bayes\nSRV Max Prob CBayes\nSpeaker\nPurchaser\nFigure Plots of accuracy vs coverage for all meth ods on two elds speaker and purchaser\nshows the F scores for all learners and elds\nFor the purchabr eld there is clear statistical separa tion between the best individual learner SRV and the top two combining methods Prob and CBayes Note as Table makes clear that even in the cases where the di erence is less apparent the combining meth ods tend to outperform the best individual method at higher coverage levels Among the three combining methods there is not one case of statistical separation but across all elds a clear picture emerges in which Prob and CBayes are better than Max Note that even in cases where a combining method performs only as well as the best individual learner it has served a valu able purpose that of relieving us of the requirement of choosing a single learner If a combining method can do this in most cases while providing added value in a few we account it a clear success\nPerhaps more interesting than summary statistics are\nTable Overlap in Learner Behavior for the Speaker Field Numbers are the probability that column learner predicted correctly given that the row learner predicted correctly\nRote Bayes SRV Max Prob CBayes Rote Bayes SRV\nMax Prob CBayes\naccuracy coverage similar to precision recall graphs Each point x along the horizontal axis represents the x most con dent predictions The vertical value at this point is the accuracy of these predictions If the accuracy coverage curve declines monotonically it sug gests that the learner s con dence correlates well with actual accuracy\nFigure shows the accuracy coverage curves for all methods on two of the elds The speaker and purchaser elds are the ones for which CBayes does best These graphs make clear what the summary statistics cannot That combining learners allows us to make better accuracy coverage judgments than we can with a single learner The anomalous high con dence behavior of Prob and Max in the purchaser curve may be due to an over reliance on Rote which has similar behavior Note that the high con dence low coverage end of the curve is the part with the least statistical certainty Also although CBayes appears better than any individual learner an examination of the graphs for all elds does not support a preference of it over Prob or vice versa There are cases where CBayes has high con dence di culties similar to those shown here for Prob and Max We believe that better regression models will mitigate some of these phenom ena\nThe strength of a meta learning approach depends on the mutual independence of the constituent learners Table shows where some of the power of combining learners comes from on the speaker eld a relatively challenging task In this table we ask the question given that Learner A has predicted correctly on some document what is the probability that Learner B will also predict correctly The number in entry i j is the fraction of all documents correctly handled by method i which method j also correctly handled Based on this table it is evident that Rote and Bayes are more closely related to each other than either to SRV\nThe column for a combining method allows us to infer which learners it depends on most for its performance It appears from this that all three methods rely more on Rote than on Bayes We would hope to see this based on Figure since the few Rote predictions that are available for this eld tend to have higher accuracy than most Bayes predictions It is also gratifying that all methods appear to rely heavily on SRV since it is the best individual learner in this case\nCONCLUSION\nThe experimental results presented here show that multistrategy learning can be useful for the problem of information extraction We present one form of multi strategy learning in which the component learners are treated as black boxes and only their reliability as a function of con dence is modeled Nothing in the ba sic framework requires the information extraction set ting or makes any assumptions about the number or structure of component learners It is only necessary that learners be instrumented to associate a con dence with any prediction they make something which is al ready part of the design of many learners and which can be readily added to others\nWe do not claim that the multistrategy results re ported here are the best that can be achieved Many details remain to be lled in such as how best to con duct validation and which statistical assumptions are appropriate We have experimented with two kinds of regression to model learner reliability but would not be surprised if other methods which we have not tried such as logistic regression or a simple neural network might a ord increased accuracy We regard this as future work\nIt also remains to be seen how these results might be t into a more traditional information extraction set ting in which slot lling is performed as part of a larger system and as one of several interacting tasks Still the approaches described here are immediately applicable to a number of unconventional information extraction problems And we can begin to see how information extraction from ungrammatical text and other natural problems admitting multiple abstract representations can be addressed with machine learn ing methods"
                    },
                    "intents": []
                }
            ],
            "corpusId": 11708947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6df333b6e4dc95a19fb5dcfa49dbd3ac11db967b",
            "isKey": true,
            "numCitedBy": 293,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This article surveys the use of empirical, machine-learning methods for a particular natural language-understanding task-information extraction. The author presents a generic architecture for information-extraction systems and then surveys the learning algorithms that have been developed to address the problems of accuracy, portability, and knowledge acquisition for each component of the architecture."
            },
            "slug": "Empirical-Methods-in-Information-Extraction-Cardie",
            "title": {
                "fragments": [],
                "text": "Empirical Methods in Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The author presents a generic architecture for information-extraction systems and then surveys the learning algorithms that have been developed to address the problems of accuracy, portability, and knowledge acquisition for each component of the architecture."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144316325"
                        ],
                        "name": "P. Chan",
                        "slug": "P.-Chan",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Chan",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807433"
                        ],
                        "name": "S. Stolfo",
                        "slug": "S.-Stolfo",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Stolfo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stolfo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "\u2026assumption is directly violated Symbolic learners on the other hand work quite well for problems with elaborate feature sets especially for those classes ex pressible in logical terms using a small subset of fea tures These considerations suggest at multistrategy approach\nMultistrategy\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14326883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c14ea670799f2a5d5c1b1d951bf1131c9f672b3b",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose meta-leaming as a general technique to combine the results of multiple learning algorithms, each applied to a set of training data. We detail several metalearning strategies for combining independently learned classifiers, each computed by different algorithms, to improve overall prediction accuracy. The overall resulting classifier is composed of the classifiers generated by the different learning algorithms and a meta-classifier generated by a meta-learning strategy. The strategies described here are independent of the learning algorithms used. Preliminm-y experiments using different strategies and learning algorithms on two molecular biology sequence analysis data sets demonstrate encouraging results. Machine learning techniques are central to automated knowledge discovery systems and hence our approach can enhance the effectiveness of such systems."
            },
            "slug": "Experiments-on-multistrategy-learning-by-Chan-Stolfo",
            "title": {
                "fragments": [],
                "text": "Experiments on multistrategy learning by meta-learning"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "Meta-leaming is proposed as a general technique to combine the results of multiple learning algorithms, each applied to a set of training data, to improve overall prediction accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144864352"
                        ],
                        "name": "M. Maron",
                        "slug": "M.-Maron",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Maron",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6692916,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c390dbf06af49d3691bc7b906f5fd9b909c2f89b",
            "isKey": false,
            "numCitedBy": 519,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This inquiry examines a technique for automatically classifying (indexing) documents according to their subject content. The task, in essence, is to have a computing machine read a document and on the basis of the occurrence of selected clue words decide to which of many subject categories the document in question belongs. This paper describes the design, execution and evaluation of a modest experimental study aimed at testing empirically one statistical technique for automatic indexing."
            },
            "slug": "Automatic-Indexing:-An-Experimental-Inquiry-Maron",
            "title": {
                "fragments": [],
                "text": "Automatic Indexing: An Experimental Inquiry"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The design, execution and evaluation of a modest experimental study aimed at testing empirically one statistical technique for automatic indexed documents according to their subject content are described."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning logical deenitions from relations"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 146
                            }
                        ],
                        "text": "\u2026document and the di culty of adequately representing this informa tion for learning we surmise that no individual learn\ning approach is best for all IE problems An individual learner embodies biases that make it more suitable for some kinds of information and aspects of a problem than for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine Learning: A Multistrategy Approach"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning: A Multistrategy Approach"
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 13,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Multistrategy-Learning-for-Information-Extraction-Freitag/29c99d263b5e05aae6bb96f004f025dcc9b5caae?sort=total-citations"
}