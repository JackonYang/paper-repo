{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 198
                            }
                        ],
                        "text": "\u2026been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349519"
                        ],
                        "name": "Joel Ratsaby",
                        "slug": "Joel-Ratsaby",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Ratsaby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Ratsaby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144694846"
                        ],
                        "name": "S. Venkatesh",
                        "slug": "S.-Venkatesh",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Venkatesh",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Venkatesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "As a result, there has been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17561403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53dcb8199cda481d67663efd29f0d80f6f29bf32",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "1 INTRODUCTION We investigate the tradeoff between labeled The classical problem of learning a classification rule and unlabeled sample complexities in learning can be stated as follows: patterns from classes \" 1 \" and a classification rule for a parametric two-class \" 2 \" (or \" states of nature \") appear with probabilities problem. In the problem considered, a sam-P 1 = P and P2 = 1 \u2013 p, respectively; the pattern classes ple of m labeled examples and n unlabeled ex-are represented by feature vectors x in a common N-amples generated from a two-class, N-variate dimensional Euclidean space R N, the patterns of class Gaussian mixture is provided together with \" i \" distributed according to the class-conditional prob-side information specifying the parametric form ability density fi (x) (i = 1, 2), Labeled pairs (x, y) E of the probability densities. The class means RN x {1,2} are assumed generated according to the fol-and a priori class probabilities are, however, lowing mechanism: a pattern class (or \" label \") y e {1,2} unknown parameters. In this framework we is first drawn randomly according to the distribution of use the maximum likelihood estimation method classes {p 1, P2}; a corresponding random feature vector to estimate the unknown parameters and uti-x c RN is then drawn according to the class-conditional lize rates of convergence of uniform strong laws density fv. In the supervised learning scenario, a labeled to determine the tradeoff between error rate m-sample { (xj, y j), 1 < j < m } is acquired by inde-and sample complexity. In particular, we show pendent sampling from the distribution of pairs (x, y). that for the algorithm used, the misclassifi-Using the sample, the objective is to construct a deci-mation probability deviates from the minimal sion rule which when presented with a random pattern Bayes error rate by Cl(N3/5n-' /5) + Cl(e-cm) x drawn from the mixture density where N is the dimension of the feature space, f(x) = plfl (x) + P2f2(x) m is the number of labeled examples, n is the number of unlabeled examples, and c is a pos-produces a label which disagrees with the true class it ive constant. of origin by a probability P~,,O, close to the minimal pB~yw error rate. Formally this learning problem can be formulated in the framework of the Probably Approximately Correct (PAC) learning model (cf. [9, 10]) as follows: Given e \u2026"
            },
            "slug": "Learning-from-a-mixture-of-labeled-and-unlabeled-Ratsaby-Venkatesh",
            "title": {
                "fragments": [],
                "text": "Learning from a mixture of labeled and unlabeled examples with parametric side information"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The tradeoff between labeled and unlabeled sample complexities in learning is investigated and pendent sampling from the distribution of pairs (x, y) is shown."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7635678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8198e70878c907e1bd05e7a3fa4280d8c338df60",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach."
            },
            "slug": "Semi-Supervised-Support-Vector-Machines-Bennett-Demiriz",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A general S3VM model is proposed that minimizes both the misclassification error and the function capacity based on all the available data that can be converted to a mixed-integer program and then solved exactly using integer programming."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 281
                            }
                        ],
                        "text": "\u2026been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 292
                            }
                        ],
                        "text": "As a result, there has been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 150
                            }
                        ],
                        "text": "As a result, there has been a good deal of work in recent years on how unlabeled data can be usefully employed in order to produce better predictions (Ratsaby & Venkatesh, 1995; Castelli & Cover, 1996; Nigam et al., 1998; Blum & Mitchell, 1998; Bennett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles, 2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16962006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec503071363e3884821f9bf90ad5439b2d6c6c8c",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new approach to model selection that performs better than the standard complexity-penalization and hold-out error estimation techniques in many cases. The basic idea is to exploit the intrinsic metric structure of a hypothesis space, as determined by the natural distribution of unlabeled training patterns, and use this metric as a reference to detect whether the empirical error estimates derived from a small (labeled) training sample can be trusted in the region around an empirically optimal hypothesis. Using simple metric intuitions we develop new geometric strategies for detecting overfitting and performing robust yet responsive model selection in spaces of candidate functions. These new metric-based strategies dramatically outperform previous approaches in experimental studies of classical polynomial curve fitting. Moreover, the technique is simple, efficient, and can be applied to most function learning tasks. The only requirement is access to an auxiliary collection of unlabeled training data."
            },
            "slug": "A-New-Metric-Based-Approach-to-Model-Selection-Schuurmans",
            "title": {
                "fragments": [],
                "text": "A New Metric-Based Approach to Model Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A new approach to model selection is introduced that performs better than the standard complexity-penalization and hold-out error estimation techniques in many cases and dramatically outperform previous approaches in experimental studies of classical polynomial curve fitting."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150358370"
                        ],
                        "name": "Zhenyu Wu",
                        "slug": "Zhenyu-Wu",
                        "structuredName": {
                            "firstName": "Zhenyu",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenyu Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9274265"
                        ],
                        "name": "R. Leahy",
                        "slug": "R.-Leahy",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Leahy",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Leahy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2595046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "250748b4494cec56abd55ae049bdd38f4d42e5c8",
            "isKey": false,
            "numCitedBy": 1265,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel graph theoretic approach for data clustering is presented and its application to the image segmentation problem is demonstrated. The data to be clustered are represented by an undirected adjacency graph G with arc capacities assigned to reflect the similarity between the linked vertices. Clustering is achieved by removing arcs of G to form mutually exclusive subgraphs such that the largest inter-subgraph maximum flow is minimized. For graphs of moderate size ( approximately 2000 vertices), the optimal solution is obtained through partitioning a flow and cut equivalent tree of G, which can be efficiently constructed using the Gomory-Hu algorithm (1961). However for larger graphs this approach is impractical. New theorems for subgraph condensation are derived and are then used to develop a fast algorithm which hierarchically constructs and partitions a partially equivalent tree of much reduced size. This algorithm results in an optimal solution equivalent to that obtained by partitioning the complete equivalent tree and is able to handle very large graphs with several hundred thousand vertices. The new clustering algorithm is applied to the image segmentation problem. The segmentation is achieved by effectively searching for closed contours of edge elements (equivalent to minimum cuts in G), which consist mostly of strong edges, while rejecting contours containing isolated strong edges. This method is able to accurately locate region boundaries and at the same time guarantees the formation of closed edge contours. >"
            },
            "slug": "An-Optimal-Graph-Theoretic-Approach-to-Data-Theory-Wu-Leahy",
            "title": {
                "fragments": [],
                "text": "An Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A novel graph theoretic approach for data clustering is presented and its application to the image segmentation problem is demonstrated, resulting in an optimal solution equivalent to that obtained by partitioning the complete equivalent tree and is able to handle very large graphs with several hundred thousand vertices."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 38
                            }
                        ],
                        "text": "Theinsight of Greig et al. (1989) and Boykov et al. (1998)is that this energy function can be minimized by ap-plying a graph mincut algorithm.1In this paper, we show that this method can be ap-plied to the machine learning problem of combininglabeled and unlabeled data as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 52
                            }
                        ],
                        "text": "D pixel images (Greig et al., 1989; Roy& Cox, 1998; Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 70
                            }
                        ],
                        "text": "This is not surprising given its use for reduction ofnoise in images (Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6436838,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b543d70d3e4fe6673a2e39832f005fa7ebc79cec",
            "isKey": false,
            "numCitedBy": 528,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov Random Fields (MRFs) can be used for a wide variety of vision problems. In this paper we focus on MRFs with two-valued clique potentials, which form a generalized Potts model. We show that the maximum a posteriori estimate of such an MRF can be obtained by solving a multiway minimum cut problem on a graph. We develop efficient algorithms for computing good approximations to the minimum multiway, cut. The visual correspondence problem can be formulated as an MRF in our framework; this yields quite promising results on real data with ground truth. We also apply our techniques to MRFs with linear clique potentials."
            },
            "slug": "Markov-random-fields-with-efficient-approximations-Boykov-Veksler",
            "title": {
                "fragments": [],
                "text": "Markov random fields with efficient approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper shows that the maximum a posteriori estimate of such an MRF can be obtained by solving a multiway minimum cut problem on a graph, and develops efficient algorithms for computing good approximations to the minimum multiway, cut."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Shi and Malik (1997) give a moresophisticated approach based on normalized cuts.\nesting are the objective functions this approach canrepresent?"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Shi and Malik (1997) address thisproblem in the context of image segmentation by us-ing a normalized version of the mincut algorithm thatattempts to equalize the sizes of the partitions (it isNP-hard to nd the best 50/50 split in a graph, buttheir approach at least encourages some balance)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14848918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b94c7ff9532ab26c3aedbee3988ec4c7a237c173",
            "isKey": false,
            "numCitedBy": 12819,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very encouraging."
            },
            "slug": "Normalized-cuts-and-image-segmentation-Shi-Malik",
            "title": {
                "fragments": [],
                "text": "Normalized cuts and image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work treats image segmentation as a graph partitioning problem and proposes a novel global criterion, the normalized cut, for segmenting the graph, which measures both the total dissimilarity between the different groups as well as the total similarity within the groups."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Shi and Malik (1997) give a moresophisticated approach based on normalized cuts.\nesting are the objective functions this approach canrepresent?"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Shi and Malik (1997) address thisproblem in the context of image segmentation by us-ing a normalized version of the mincut algorithm thatattempts to equalize the sizes of the partitions (it isNP-hard to nd the best 50/50 split in a graph, buttheir approach at least encourages some balance)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Shi and Malik (1997) address this problem in the context of image segmentation by using a normalized version of the mincut algorithm that attempts to equalize the sizes of the partitions (it is NP-hard to nd the best 50/50 split in a graph, but their approach at least encourages some balance)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123466699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7a8e3850d7833777aa708e45ff4d0bc47ab1e05",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this project, we propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images as well as motion sequences and found that it gives very good results."
            },
            "slug": "Normalized-Cut-and-Image-Segmentation-Shi-Malik",
            "title": {
                "fragments": [],
                "text": "Normalized Cut and Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This project treats image segmentation as a graph partitioning problem and proposes a novel global criterion, the normalized cut, for segmenting the graph, which measures both the total dissimilarity between the different groups as well as the total similarity within the groups."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144626092"
                        ],
                        "name": "D. Snow",
                        "slug": "D.-Snow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Snow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Snow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144953417"
                        ],
                        "name": "P. Viola",
                        "slug": "P.-Viola",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Viola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 70
                            }
                        ],
                        "text": "This is not surprising given its use for reduction of noise in images (Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 73
                            }
                        ],
                        "text": "D pixel images (Greig et al., 1989; Roy& Cox, 1998; Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 91
                            }
                        ],
                        "text": "This is not surprising given its use for reduction ofnoise in images (Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 133
                            }
                        ],
                        "text": "Recently, a method based on graph mincuts has been proposed in the vision literature for the problem of cleaning up 3-D pixel images (Greig et al., 1989; Roy & Cox, 1998; Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17207598,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cfe5315080aa0b092dda19a127f9a7a2b61508bd",
            "isKey": true,
            "numCitedBy": 228,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Voxel occupancy is one approach for reconstructing the 3-dimensional shape of an object from multiple views. In voxel occupancy, the task is to produce a binary labeling of a set of voxels, that determines which voxels are filled and which are empty. In this paper, we give an energy minimization formulation of the voxel occupancy problem. The global minimum of this energy can be rapidly computed with a single graph cut, using a result due to D. Greig et al. (1989). The energy function we minimize contains a data term and a smoothness term. The data term is a sum over the individual voxels, where the penalty for a voxel is based on the observed intensities of the pixels that intersect it. The smoothness term is the number of empty voxels adjacent to filled ones. Our formulation can be viewed as a generalization of silhouette intersection, with two advantages: we do not compute silhouettes, which are a major source of errors; and we can naturally incorporate spatial smoothness. We give experimental results showing reconstructions from both real and synthetic imagery. Reconstruction using this smoothed energy function is not much more time consuming than simple silhouette intersection; it takes about 10 seconds to reconstruct a one million voxel volume."
            },
            "slug": "Exact-voxel-occupancy-with-graph-cuts-Snow-Viola",
            "title": {
                "fragments": [],
                "text": "Exact voxel occupancy with graph cuts"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": false,
            "numCitedBy": 13981,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879453"
                        ],
                        "name": "V. Castelli",
                        "slug": "V.-Castelli",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Castelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Castelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 155
                            }
                        ],
                        "text": "\u2026been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1389637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22834aa74138de7f4da42fb9dfb480cef4e7b177",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We observe a training set Q composed of l labeled samples {(X/sub 1/,/spl theta//sub 1/),...,(X/sub l/, /spl theta//sub l/)} and u unlabeled samples {X/sub 1/',...,X/sub u/'}. The labels /spl theta//sub i/ are independent random variables satisfying Pr{/spl theta//sub i/=1}=/spl eta/, Pr{/spl theta//sub i/=2}=1-/spl eta/. The labeled observations X/sub i/ are independently distributed with conditional density f/sub /spl theta/i/(/spl middot/) given /spl theta//sub i/. Let (X/sub 0/,/spl theta//sub 0/) be a new sample, independently distributed as the samples in the training set. We observe X/sub 0/ and we wish to infer the classification /spl theta//sub 0/. In this paper we first assume that the distributions f/sub 1/(/spl middot/) and f/sub 2/(/spl middot/) are given and that the mixing parameter is unknown. We show that the relative value of labeled and unlabeled samples in reducing the risk of optimal classifiers is the ratio of the Fisher informations they carry about the parameter /spl eta/. We then assume that two densities g/sub 1/(/spl middot/) and g/sub 2/(/spl middot/) are given, but we do not know whether g/sub 1/(/spl middot/)=f/sub 1/(/spl middot/) and g/sub 2/(/spl middot/)=f/sub 2/(/spl middot/) or if the opposite holds, nor do we know /spl eta/. Thus the learning problem consists of both estimating the optimum partition of the observation space and assigning the classifications to the decision regions. Here, we show that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples."
            },
            "slug": "The-relative-value-of-labeled-and-unlabeled-samples-Castelli-Cover",
            "title": {
                "fragments": [],
                "text": "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094778056"
                        ],
                        "name": "V. De Silva",
                        "slug": "V.-De-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "De Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. De Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 225
                            }
                        ],
                        "text": "\u2026withhigh probability.3We could add noise in the labels at this point, but forsimplicity we leave that out.4We could generalize this to assuming regions are D-dimensional manifolds lying in some higher dimensionalspace as in (Tenenbaum et al., 2000; Roweis & Saul, 2000)without a ecting the results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 55
                            }
                        ],
                        "text": "The following analysis borrows some nice proof ideasof Tenenbaum et al. (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221338160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "isKey": false,
            "numCitedBy": 12182,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
            },
            "slug": "A-global-geometric-framework-for-nonlinear-Tenenbaum-Silva",
            "title": {
                "fragments": [],
                "text": "A global geometric framework for nonlinear dimensionality reduction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set and efficiently computes a globally optimal solution, and is guaranteed to converge asymptotically to the true structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144520793"
                        ],
                        "name": "S\u00e9bastien Roy",
                        "slug": "S\u00e9bastien-Roy",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00e9bastien Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716933"
                        ],
                        "name": "I. Cox",
                        "slug": "I.-Cox",
                        "structuredName": {
                            "firstName": "Ingemar",
                            "lastName": "Cox",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16612668,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "006328c8add2ce30c186048c89097560d2661c27",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new algorithm for solving the N-camera stereo correspondence problem by transforming it into a maximum-flow problem. Once solved, the minimum-cut associated to the maximum-flow yields a disparity surface for the whole image at once. This global approach to stereo analysis provides a more accurate and coherent depth map than the traditional line-by-line stereo. Moreover, the optimality of the depth surface is guaranteed and can be shown to be a generalization of the dynamic programming approach that is widely used in standard stereo. Results show improved depth estimation as well as better handling of depth discontinuities. While the worst case running time is O(n/sup 2/d/sup 2/log(nd)), the observed average running time is O(n/sup 1.2/ d/sup 1.3/) for an image size of n pixels and depth resolution d."
            },
            "slug": "A-maximum-flow-formulation-of-the-N-camera-stereo-Roy-Cox",
            "title": {
                "fragments": [],
                "text": "A maximum-flow formulation of the N-camera stereo correspondence problem"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new algorithm for solving the N-camera stereo correspondence problem by transforming it into a maximum-flow problem that provides a more accurate and coherent depth map than the traditional line-by-line stereo."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2639,
                                "start": 56
                            }
                        ],
                        "text": ", 1998; Blum & Mitchell, 1998; Bennett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles, 2000; Schuurmans, 1997). Recently, a method based on graph mincuts has been proposed in the vision literature for the problem of cleaning up 3-D pixel images (Greig et al., 1989; Roy & Cox, 1998; Boykov et al., 1998; Snow et al., 2000). Given an initial noisy image created from a stereo camera, the goal is to improve the image by minimizing an appropriate \\energy function.\" This energy function combines a term for each pair of neighboring pixels that are at di erent depths (encouraging the algorithm to \\smooth\" the image) and a term for the number of pixels changed from the original image (encouraging the algorithm not to change too many pixels). The insight of Greig et al. (1989) and Boykov et al. (1998) is that this energy function can be minimized by applying a graph mincut algorithm.1 In this paper, we show that this method can be applied to the machine learning problem of combining labeled and unlabeled data as well. Given a dataset of labeled and unlabeled examples, we construct a graph on the examples such that the minimum cut on this graph yields an \\optimal\" binary labeling of the unlabeled data according to certain optimization functions. Our approach is inspired by the work of Kleinberg and Tardos (2000) who connect the work in vision to a more general classi cation setting they call the \\metric labeling problem\". In fact, we will be converting the learning problems into a technically simpler version of their setting (a binary rather than multi-way classi cation) that can be solved exactly rather than just approximated. Thus, our focus will be on how to construct an appropriate graph rather than developing new algorithms for solving the graph problem as in Kleinberg and Tardos (2000). As with most other approaches to combining labeled and unlabeled data, the high level idea of this method is to assign values to the unlabeled examples in order to optimize an associated objective function. For the mincut approach, the kinds of functions that can be optimized are limited to depend only on pairwise relationships among examples. What makes this approach especially appealing, however, is that for the functions we can handle, graph mincuts give a polynomial-time algorithm to nd the true global optimum. Thus we trade o the generality of an approach such as EM or hill-climbing/gradient-descent (which can be applied almost anywhere) for con dence in nding the exact optimum. The natural question then is: how inter1A similar technique is used by Wu and Leahy (1993) for image partitioning. Shi and Malik (1997) give a more sophisticated approach based on normalized cuts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 110
                            }
                        ],
                        "text": "In a sense, the graph mincut approach relates to nearest-neighbor style algorithms much like transductive SVM (Bennett & Demiriz, 1998; Hofmann, 1999) relates to the standard SVM algorithm: its goal is to assign labels to the unlabeled data in such a way as to make the underlying learning algorithm \\happiest\"."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1320,
                                "start": 56
                            }
                        ],
                        "text": ", 1998; Blum & Mitchell, 1998; Bennett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles, 2000; Schuurmans, 1997). Recently, a method based on graph mincuts has been proposed in the vision literature for the problem of cleaning up 3-D pixel images (Greig et al., 1989; Roy & Cox, 1998; Boykov et al., 1998; Snow et al., 2000). Given an initial noisy image created from a stereo camera, the goal is to improve the image by minimizing an appropriate \\energy function.\" This energy function combines a term for each pair of neighboring pixels that are at di erent depths (encouraging the algorithm to \\smooth\" the image) and a term for the number of pixels changed from the original image (encouraging the algorithm not to change too many pixels). The insight of Greig et al. (1989) and Boykov et al. (1998) is that this energy function can be minimized by applying a graph mincut algorithm.1 In this paper, we show that this method can be applied to the machine learning problem of combining labeled and unlabeled data as well. Given a dataset of labeled and unlabeled examples, we construct a graph on the examples such that the minimum cut on this graph yields an \\optimal\" binary labeling of the unlabeled data according to certain optimization functions. Our approach is inspired by the work of Kleinberg and Tardos (2000) who connect the work in vision to a more general classi cation setting they call the \\metric labeling problem\"."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 247
                            }
                        ],
                        "text": "\u2026been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 139
                            }
                        ],
                        "text": "In a sense, the graph min-cut approach relates to nearest-neighbor style algo-rithms much like transductive SVM (Bennett & Demi-riz, 1998; Hofmann, 1999) relates to the standard SVMalgorithm: its goal is to assign labels to the unlabeleddata in such a way as to make the underlying learn-ing\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "We would also like to compare theperformance our algorithm with that of TransductiveSVM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 800,
                                "start": 56
                            }
                        ],
                        "text": ", 1998; Blum & Mitchell, 1998; Bennett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles, 2000; Schuurmans, 1997). Recently, a method based on graph mincuts has been proposed in the vision literature for the problem of cleaning up 3-D pixel images (Greig et al., 1989; Roy & Cox, 1998; Boykov et al., 1998; Snow et al., 2000). Given an initial noisy image created from a stereo camera, the goal is to improve the image by minimizing an appropriate \\energy function.\" This energy function combines a term for each pair of neighboring pixels that are at di erent depths (encouraging the algorithm to \\smooth\" the image) and a term for the number of pixels changed from the original image (encouraging the algorithm not to change too many pixels). The insight of Greig et al. (1989) and Boykov et al. (1998) is that this energy function can be minimized by applying a graph mincut algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "In a sense, the graph min-cut approach relates to nearest-neighbor style algo-rithms much like transductive SVM (Bennett & Demi-riz, 1998; Hofmann, 1999) relates to the standard SVMalgorithm: its goal is to assign labels to the unlabeleddata in such a way as to make the underlying learn-ing algorithm \\happiest\"."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 150
                            }
                        ],
                        "text": "As a result, there has been a good deal of work in recent years on how unlabeled data can be usefully employed in order to produce better predictions (Ratsaby & Venkatesh, 1995; Castelli & Cover, 1996; Nigam et al., 1998; Blum & Mitchell, 1998; Bennett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles, 2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1809,
                                "start": 56
                            }
                        ],
                        "text": ", 1998; Blum & Mitchell, 1998; Bennett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles, 2000; Schuurmans, 1997). Recently, a method based on graph mincuts has been proposed in the vision literature for the problem of cleaning up 3-D pixel images (Greig et al., 1989; Roy & Cox, 1998; Boykov et al., 1998; Snow et al., 2000). Given an initial noisy image created from a stereo camera, the goal is to improve the image by minimizing an appropriate \\energy function.\" This energy function combines a term for each pair of neighboring pixels that are at di erent depths (encouraging the algorithm to \\smooth\" the image) and a term for the number of pixels changed from the original image (encouraging the algorithm not to change too many pixels). The insight of Greig et al. (1989) and Boykov et al. (1998) is that this energy function can be minimized by applying a graph mincut algorithm.1 In this paper, we show that this method can be applied to the machine learning problem of combining labeled and unlabeled data as well. Given a dataset of labeled and unlabeled examples, we construct a graph on the examples such that the minimum cut on this graph yields an \\optimal\" binary labeling of the unlabeled data according to certain optimization functions. Our approach is inspired by the work of Kleinberg and Tardos (2000) who connect the work in vision to a more general classi cation setting they call the \\metric labeling problem\". In fact, we will be converting the learning problems into a technically simpler version of their setting (a binary rather than multi-way classi cation) that can be solved exactly rather than just approximated. Thus, our focus will be on how to construct an appropriate graph rather than developing new algorithms for solving the graph problem as in Kleinberg and Tardos (2000). As with most other approaches to combining labeled and unlabeled data, the high level idea of this method is to assign values to the unlabeled examples in order to optimize an associated objective function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2594,
                                "start": 56
                            }
                        ],
                        "text": ", 1998; Blum & Mitchell, 1998; Bennett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles, 2000; Schuurmans, 1997). Recently, a method based on graph mincuts has been proposed in the vision literature for the problem of cleaning up 3-D pixel images (Greig et al., 1989; Roy & Cox, 1998; Boykov et al., 1998; Snow et al., 2000). Given an initial noisy image created from a stereo camera, the goal is to improve the image by minimizing an appropriate \\energy function.\" This energy function combines a term for each pair of neighboring pixels that are at di erent depths (encouraging the algorithm to \\smooth\" the image) and a term for the number of pixels changed from the original image (encouraging the algorithm not to change too many pixels). The insight of Greig et al. (1989) and Boykov et al. (1998) is that this energy function can be minimized by applying a graph mincut algorithm.1 In this paper, we show that this method can be applied to the machine learning problem of combining labeled and unlabeled data as well. Given a dataset of labeled and unlabeled examples, we construct a graph on the examples such that the minimum cut on this graph yields an \\optimal\" binary labeling of the unlabeled data according to certain optimization functions. Our approach is inspired by the work of Kleinberg and Tardos (2000) who connect the work in vision to a more general classi cation setting they call the \\metric labeling problem\". In fact, we will be converting the learning problems into a technically simpler version of their setting (a binary rather than multi-way classi cation) that can be solved exactly rather than just approximated. Thus, our focus will be on how to construct an appropriate graph rather than developing new algorithms for solving the graph problem as in Kleinberg and Tardos (2000). As with most other approaches to combining labeled and unlabeled data, the high level idea of this method is to assign values to the unlabeled examples in order to optimize an associated objective function. For the mincut approach, the kinds of functions that can be optimized are limited to depend only on pairwise relationships among examples. What makes this approach especially appealing, however, is that for the functions we can handle, graph mincuts give a polynomial-time algorithm to nd the true global optimum. Thus we trade o the generality of an approach such as EM or hill-climbing/gradient-descent (which can be applied almost anywhere) for con dence in nding the exact optimum. The natural question then is: how inter1A similar technique is used by Wu and Leahy (1993) for image partitioning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 775,
                                "start": 56
                            }
                        ],
                        "text": ", 1998; Blum & Mitchell, 1998; Bennett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles, 2000; Schuurmans, 1997). Recently, a method based on graph mincuts has been proposed in the vision literature for the problem of cleaning up 3-D pixel images (Greig et al., 1989; Roy & Cox, 1998; Boykov et al., 1998; Snow et al., 2000). Given an initial noisy image created from a stereo camera, the goal is to improve the image by minimizing an appropriate \\energy function.\" This energy function combines a term for each pair of neighboring pixels that are at di erent depths (encouraging the algorithm to \\smooth\" the image) and a term for the number of pixels changed from the original image (encouraging the algorithm not to change too many pixels). The insight of Greig et al. (1989) and Boykov et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text categorization with labeled and unlabeled"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144694337"
                        ],
                        "name": "D. H. Ferguson",
                        "slug": "D.-H.-Ferguson",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Ferguson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. H. Ferguson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70050678"
                        ],
                        "name": "Thomas I. Selling",
                        "slug": "Thomas-I.-Selling",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Selling",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas I. Selling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 262
                            }
                        ],
                        "text": "\u2026been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 220640206,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9359a138fe8684164f7e34ccee98714cca0444b2",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability-Analysis-Ferguson-Selling",
            "title": {
                "fragments": [],
                "text": "Probability Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49026551"
                        ],
                        "name": "D. Greig",
                        "slug": "D.-Greig",
                        "structuredName": {
                            "firstName": "Darryl",
                            "lastName": "Greig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Greig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146294528"
                        ],
                        "name": "B. Porteous",
                        "slug": "B.-Porteous",
                        "structuredName": {
                            "firstName": "Baroness",
                            "lastName": "Porteous",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Porteous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887111"
                        ],
                        "name": "A. Seheult",
                        "slug": "A.-Seheult",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Seheult",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Seheult"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "Theinsight of Greig et al. (1989) and Boykov et al. (1998)is that this energy function can be minimized by ap-plying a graph mincut algorithm.1In this paper, we show that this method can be ap-plied to the machine learning problem of combininglabeled and unlabeled data as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 16
                            }
                        ],
                        "text": "D pixel images (Greig et al., 1989; Roy& Cox, 1998; Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 115691220,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a717b20e99b76cb228b47694140ed3dce082b530",
            "isKey": false,
            "numCitedBy": 1257,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Exact-Maximum-A-Posteriori-Estimation-for-Binary-Greig-Porteous",
            "title": {
                "fragments": [],
                "text": "Exact Maximum A Posteriori Estimation for Binary Images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum- ow formulation of the"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "As a result, there has been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from a mixture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "The natural question then is: how inter-1A similar technique is used by Wu and Leahy (1993)for image partitioning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An optimal graph theoretic approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 198
                            }
                        ],
                        "text": "\u2026been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 247
                            }
                        ],
                        "text": "\u2026been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 139
                            }
                        ],
                        "text": "In a sense, the graph min-cut approach relates to nearest-neighbor style algo-rithms much like transductive SVM (Bennett & Demi-riz, 1998; Hofmann, 1999) relates to the standard SVMalgorithm: its goal is to assign labels to the unlabeleddata in such a way as to make the underlying learn-ing\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text categorization with labeled and unlabeleddata : A generative model approach"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 99 Workshop onUsing Unlabeled Data for Supervised Learning"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 281
                            }
                        ],
                        "text": "\u2026been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 292
                            }
                        ],
                        "text": "As a result, there has been a good deal ofwork in recent years on how unlabeled data can beusefully employed in order to produce better predic-tions (Ratsaby & Venkatesh, 1995; Castelli & Cover,1996; Nigam et al., 1998; Blum & Mitchell, 1998; Ben-nett & Demiriz, 1998; Hofmann, 1999; Zhang & Oles,2000; Schuurmans, 1997)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new metricbased approach to modelselection"
            },
            "venue": {
                "fragments": [],
                "text": "AAAI / IAAI"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 73
                            }
                        ],
                        "text": "D pixel images (Greig et al., 1989; Roy& Cox, 1998; Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 91
                            }
                        ],
                        "text": "This is not surprising given its use for reduction ofnoise in images (Boykov et al., 1998; Snow et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exact voxel occupancywith graph cuts"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision andPattern Recognition"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inducing featuresof random elds"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis andMachine Intelligence"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 24,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-from-Labeled-and-Unlabeled-Data-using-Blum-Chawla/0eedbab3ae55fd6a4e7bbc75fcc261293384f883?sort=total-citations"
}