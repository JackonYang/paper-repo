{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2697855"
                        ],
                        "name": "M. Ringuette",
                        "slug": "M.-Ringuette",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Ringuette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ringuette"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "This partition is similar to that employed in [1], but di ers from [13] who use the full collection including unla1A newly revised version, namely Reuters-21578, is available through http://www."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 229
                            }
                        ],
                        "text": "The information gain of term t is de ned to be: G(t) = Pmi=1Pr(ci) logPr(ci) +Pr(t)Pmi=1 Pr(cijt) logPr(cijt) +Pr( t)Pmi=1 Pr(cij t) logPr(cij t) This de nition is more general than the one employed in binary classi cation models[13, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 262
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 91
                            }
                        ],
                        "text": "The Reuters news story collection is commonly used corpora in text categorization research [13, 1, 21, 16, 3] 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Lewis &Ringuette[13] used an information gain measure to aggressively reduce the document vocabulary in a naive Bayes model and a decision-tree approach to binary classi cation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "The context sensitivity is in distinction to context-free methods based on explicit independence assumptions such as naive Bayes classi ers[13] and some other regression methods[8])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16894634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9fd1a7ae0322d417ab2d32017e373dd50efc063",
            "isKey": true,
            "numCitedBy": 745,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the use of inductive learning to categorize natural language documents into predeened content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it diicult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classiier and a decision tree learning algorithm on two text categorization data sets. We nd that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial preeltering of features, connrming the results found by Almuallim and Dietterich on artiicial data sets. We also demonstrate the impact of the time-varying nature of category deenitions."
            },
            "slug": "A-comparison-of-two-learning-algorithms-for-text-Lewis-Ringuette",
            "title": {
                "fragments": [],
                "text": "A comparison of two learning algorithms for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives, and the stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695611"
                        ],
                        "name": "W. Wilbur",
                        "slug": "W.-Wilbur",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Wilbur",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wilbur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "5 Term strength (TS) Term strength is originally proposed and evaluated by Wilbur and Sirotkin [22] for vocabulary reduction in text retrieval, and later applied by Yang and Wilbur to text categorization [24, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 22
                            }
                        ],
                        "text": "[28] Y. Yang and W.J. Wilbur."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "[22] J.W. Wilbur and K. Sirotkin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 97
                            }
                        ],
                        "text": "According to previous evaluations of retrieval and categorization on several document collections[22, 28], the AREL values between 10 to 20 yield satisfactory performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Yang & Wilbur [28] used document clustering techniques to estimate probabilistic \\term strength\", and used it to reduce the variables in linear regression and nearest neighbor classi cation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 7
                            }
                        ],
                        "text": "Yang & Wilbur[28] used document clustering techniques to estimateprobabilistic \\term strength\", and used it to reducethe variables in linear regression and nearest neighborclassi cation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 152
                            }
                        ],
                        "text": "Hence, the 2 statisticis known not to be reliable for low-frequency terms[6].2.5 Term strength (TS)Term strength is originally proposed and evaluated byWilbur and Sirotkin [22] for vocabulary reduction intext retrieval, and later applied by Yang and Wilburto text categorization [24, 28]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39962298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb7a6fb8b3f468bc5bdb041c32721c1873cf677b",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This article studies aggressive word removal in text categorization to reduce the noise in free texts and to enhance the computational efficiency of categorization. We use a novel stop word identification method to automatically generate domain specific stoplists which are much larger than a conventional domain-independent stoplist. In our tests with three categorization methods on text collections from different domains/applications, significant numbers of words were removed without sacrificing categorization effectiveness. In the test of the Expert Network method on CACM documents, for example, an 87% removal of unique words reduced the vocabulary of documents from 8,002 distinct words to 1,045 words, which resulted in a 63% time savings and a 74% memory savings in the computation of category ranking, with a 10% precision improvement on average over not using word removal. It is evident in this study that automated word removal based on corpus statistics has a practical and significant impact on the computational tractability of categorization methods in large databases."
            },
            "slug": "Using-Corpus-Statistics-to-Remove-Redundant-Words-Yang-Wilbur",
            "title": {
                "fragments": [],
                "text": "Using Corpus Statistics to Remove Redundant Words in Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is evident in this study that automated word removal based on corpus statistics has a practical and significant impact on the computational tractability of categorization methods in large databases."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69065228"
                        ],
                        "name": "CategorizationYiming YangSection",
                        "slug": "CategorizationYiming-YangSection",
                        "structuredName": {
                            "firstName": "CategorizationYiming",
                            "lastName": "YangSection",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "CategorizationYiming YangSection"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 111
                            }
                        ],
                        "text": "By \\large\" we mean that both the input and the output of a classi er can have thousands of dimensions or higher[25, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6492753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9edde8da1b557e3c3dcd6bc7acf3d602831a7fae",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies training set sampling strategies in the context of statistical learning for text cate-gorization. It is argued sampling strategies favoring common categories is superior to uniform coverage or mistake-driven approaches, if performance is measured by globally assessed precision and recall. The hypothesis is empirically validated by examining the performance of a nearest neighbor classiier on training samples drawn from a pool of 235,401 training texts with 29,741 distinct categories. The learning curves of the classiier are analyzed with respect to the choice of training resources, the sampling methods, the size, vocabulary and category coverage of a sample, and the category distribution over the texts in the sample. A nearly-optimal categorization performance of the classiier is achieved using a relatively small training sample, showing that statistical learning can be successfully applied to very large text categorization problems with aaordable computation."
            },
            "slug": "Sampling-Strategies-and-Learning-Eeciency-in-Text-YangSection",
            "title": {
                "fragments": [],
                "text": "Sampling Strategies and Learning Eeciency in Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A nearly-optimal categorization performance of the classiier is achieved using a relatively small training sample, showing that statistical learning can be successfully applied to very large text categorization problems with aaordable computation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211746"
                        ],
                        "name": "David A. Hull",
                        "slug": "David-A.-Hull",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hull",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Hull"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9131020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92062ccb796efbf56fe1ae2dcc8b3a943a2c989b",
            "isKey": false,
            "numCitedBy": 566,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we compare learning techniques based on statistical classification to traditional methods of relevance feedback for the document routing problem. We consider three classification techniques which have decision rules that are derived via explicit error minimization: linear discriminant analysis, logistic regression, and neural networks. We demonstrate that the classifiers perform 1015% better than relevance feedback via Rocchio expansion for the TREC-2 and TREC-3 routing tasks. Error minimization is difficult in high-dimensional feature spaces because the convergence process is slow and the models are prone to overfitting. We use two different strategies, latent semantic indexing and optimal term selection, to reduce the number of features. Our results indicate that features based on latent semantic indexing are more effective for techniques such as linear discriminant analysis and logistic regression, which have no way to protect against overfitting. Neural networks perform equally well with either set of features and can take advantage of the additional information available when both feature sets are used as input."
            },
            "slug": "A-comparison-of-classifiers-and-document-for-the-Sch\u00fctze-Hull",
            "title": {
                "fragments": [],
                "text": "A comparison of classifiers and document representations for the routing problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper compares learning techniques based on statistical classification to traditional methods of relevance feedback for the document routing problem and indicates that features based on latent semantic indexing are more effective for techniques such as linear discriminant analysis and logistic regression, which have no way to protect against overfitting."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792682"
                        ],
                        "name": "C. Chute",
                        "slug": "C.-Chute",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Chute",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chute"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 184
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 221
                            }
                        ],
                        "text": "1 Classi ers To assess the e ectiveness of feature selection methods we used two di erent m-ary classi ers, a knearest-neighbor classi er (kNN)[23] and a regression method named the Linear Least Squares Fit mapping (LLSF)[27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16063479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f926a0022e794485ec469124894aaaf29b087d70",
            "isKey": false,
            "numCitedBy": 489,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A unified model for text categorization and text retrieval is introduced. We use a training set of manually categorized documents to learn word-category associations, and use these associations to predict the categories of arbitrary documents. Similarly, we use a training set of queries and their related documents to obtain empirical associations between query words and indexing terms of documents, and use these associations to predict the related documents of arbitrary queries. A Linear Least Squares Fit (LLSF) technique is employed to estimate the likelihood of these associations. Document collections from the MEDLINE database and Mayo patient records are used for studies on the effectiveness of our approach, and on how much the effectiveness depends on the choices of training data, indexing language, word-weighting scheme, and morphological canonicalization. Alternative methods are also tested on these data collections for comparison. It is evident that the LLSF approach uses the relevance information effectively within human decisions of categorization and retrieval, and achieves a semantic mapping of free texts to their representations in an indexing language. Such a semantic mapping lead to a significant improvement in categorization and retrieval, compared to alternative approaches."
            },
            "slug": "An-example-based-mapping-method-for-text-and-Yang-Chute",
            "title": {
                "fragments": [],
                "text": "An example-based mapping method for text categorization and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is evident that the LLSF approach uses the relevance information effectively within human decisions of categorization and retrieval, and achieves a semantic mapping of free texts to their representations in an indexing language."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 111
                            }
                        ],
                        "text": "By \\large\" we mean that both the input and the output of a classi er can have thousands of dimensions or higher[25, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "5 Term strength (TS) Term strength is originally proposed and evaluated by Wilbur and Sirotkin [22] for vocabulary reduction in text retrieval, and later applied by Yang and Wilbur to text categorization [24, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "[3]) and approaches based on principal component analysis[5, 24, 21, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "That is, we computed only 200 largest singular values in solving LLSF, although the best results (which is similar to the performance of kNN) appeared with using 1000 singular values[24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2237198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d81d19d270106805389d22b8d54b1f755797d440",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies noise reduction for computational efficiency improvements in a statistical learning method for text categorization, the Linear Least Squares Fit (LLSF) mapping. Multiple noise reduction strategies are proposedand evaluated, including: an aggressive removal of \u201cnon-informative words\u201d from texts before training; the use of a truncated singular value decomposition to cut off noisy \u201clatent semantic structures\u201d during training; the elimination of non-influential components in the LLSF solution (a word-concept association matrix) after training. Text collections in different domains were used for evaluation. Significant improvements in computational efficiency without losing categorization accuracy were evident in the testing results."
            },
            "slug": "Noise-reduction-in-a-statistical-approach-to-text-Yang",
            "title": {
                "fragments": [],
                "text": "Noise reduction in a statistical approach to text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Noise reduction strategies are proposed and evaluated, including an aggressive removal of \u201cnon-informative words\u201d from texts before training; the use of a truncated singular value decomposition to cut off noisy \u201clatent semantic structures\u201d during training."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16041292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc8e59e4c7c2cbb6695ee5488aa569780449b212",
            "isKey": false,
            "numCitedBy": 485,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Expert Network (ExpNet) is our new approach to automatic categorization and retrieval of natural language texts. We use a training set of texts with expert-assigned categories to construct a network which approximately reflects the conditional probabilities of categories given a text. The input nodes of the network are words in the training texts, the nodes on the intermediate level are the training texts, and the output nodes are categories. The links between nodes are computed based on statistics of the word distribution and the category distribution over the training set. ExpNet is used for relevance ranking of candidate categories of an arbitrary text in the case of text categorization, and for relevance ranking of documents via categories in the case of text retrieval. We have evaluated ExpNet in categorization and retrieval on a document collection of the MEDLINE database, and observed a performance in recall and precision comparable to the Linear Least Squares Fit (LLSF) mapping method, and significantly better than other methods tested. Computationally, ExpNet has an O(N 1og N) time complexity which is much more efficient than the cubic complexity of the LLSF method. The simplicity of the model, the high recall-precision rates, and the efficient computation together make ExpNet preferable as a practical solution for real-world applications."
            },
            "slug": "Expert-network:-effective-and-efficient-learning-in-Yang",
            "title": {
                "fragments": [],
                "text": "Expert network: effective and efficient learning from human decisions in text categorization and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The simplicity of the model, the high recall-precision rates, and the efficient computation together make ExpNet preferable as a practical solution for real-world applications."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": "g.[3]) and approaches based on principal component analysis[5, 24, 21, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 335
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 91
                            }
                        ],
                        "text": "The Reuters news story collection is commonly used corpora in text categorization research [13, 1, 21, 16, 3] 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 281
                            }
                        ],
                        "text": "In a recent evaluation of classi cation methods[26] on the Reuters newswire collection (next section), the break-even point values were 85% for both kNN and LLSF, outperforming all the other systems evaluated on the same collection, including symbolic rule learning by RIPPER (80%)[3], SWAP1 (79%)[1] and CHARADE (78%)[16], a decision approach using C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "5 (79%)[15], inductive learning by Sleeping Experts (76%)[3], and a typical information retrieval approach named Rocchio (75%)[3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52835205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e1a665334b1ec35d77ab1cd4f21bd0da9745548",
            "isKey": true,
            "numCitedBy": 183,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Two recently implemented machine-learning algorithms, RIPPERand sleeping-experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the \u201ccontext\u201d of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences, both RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information."
            },
            "slug": "Context-sensitive-learning-methods-for-text-Cohen-Singer",
            "title": {
                "fragments": [],
                "text": "Context-sensitive learning methods for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods and are viewed as a confirmation of the usefulness of classifiers that represent contextual information."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764547"
                        ],
                        "name": "M. Sahami",
                        "slug": "M.-Sahami",
                        "structuredName": {
                            "firstName": "Mehran",
                            "lastName": "Sahami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sahami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "A recent theoretical comparison, for example, was based on the performance of decision tree algorithms in solving problems with 6 to 180 features in the native space[10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1455429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ed4e1dbe10c0ac9fa00b30d1882cae1249a5a6a",
            "isKey": false,
            "numCitedBy": 1746,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we examine a method for feature subset selection based on Information Theory. Initially, a framework for defining the theoretically optimal, but computationally intractable, method for feature subset selection is presented. We show that our goal should be to eliminate a feature if it gives us little or no additional information beyond that subsumed by the remaining features. In particular, this will be the case for both irrelevant and redundant features. We then give an efficient algorithm for feature selection which computes an approximation to the optimal feature selection criterion. The conditions under which the approximate algorithm is successful are examined. Empirical results are given on a number of data sets, showing that the algorithm effectively handles datasets with a very large number of features."
            },
            "slug": "Toward-Optimal-Feature-Selection-Koller-Sahami",
            "title": {
                "fragments": [],
                "text": "Toward Optimal Feature Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An efficient algorithm for feature selection which computes an approximation to the optimal feature selection criterion is given, showing that the algorithm effectively handles datasets with a very large number of features."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115377844"
                        ],
                        "name": "T. Dunning",
                        "slug": "T.-Dunning",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Dunning",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dunning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Hence, the 2 statistic is known not to be reliable for low-frequency terms[6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6465096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "025464b73f805e76689a7a20a48a9e9c0f4ff3ef",
            "isKey": false,
            "numCitedBy": 2829,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text."
            },
            "slug": "Accurate-Methods-for-the-Statistics-of-Surprise-and-Dunning",
            "title": {
                "fragments": [],
                "text": "Accurate Methods for the Statistics of Surprise and Coincidence"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basis of a measure based on likelihood ratios that can be applied to the analysis of text is described, and in cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145272844"
                        ],
                        "name": "C. Apt\u00e9",
                        "slug": "C.-Apt\u00e9",
                        "structuredName": {
                            "firstName": "Chidanand",
                            "lastName": "Apt\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Apt\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "68982679"
                        ],
                        "name": "Fred J. Damerau",
                        "slug": "Fred-J.-Damerau",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Damerau",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred J. Damerau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145700185"
                        ],
                        "name": "S. Weiss",
                        "slug": "S.-Weiss",
                        "structuredName": {
                            "firstName": "Sholom",
                            "lastName": "Weiss",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "This partition is similar to that employed in [1], but di ers from [13] who use the full collection including unla1A newly revised version, namely Reuters-21578, is available through http://www."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 297
                            }
                        ],
                        "text": "In a recent evaluation of classi cation methods[26] on the Reuters newswire collection (next section), the break-even point values were 85% for both kNN and LLSF, outperforming all the other systems evaluated on the same collection, including symbolic rule learning by RIPPER (80%)[3], SWAP1 (79%)[1] and CHARADE (78%)[16], a decision approach using C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 335
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 91
                            }
                        ],
                        "text": "The Reuters news story collection is commonly used corpora in text categorization research [13, 1, 21, 16, 3] 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 775418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "248380e4b3cc91a87bfb11d29fb95125496dd2c9",
            "isKey": true,
            "numCitedBy": 205,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the results of extensive machine learning experiments on large collections of Reuters\u2019 English and German newswires. The goal of these experiments was to automatically discover classification patterns that can be used for assignment of topics to the individual newswires. Our results with the English newswire collection show a very large gain in performance as compared to published benchmarks, while our initial results with the German newswires appear very promising. We present our methodology, which seems to be insensitive to the language of the document collections, and discuss issues related to the differences in results that we have obtained for the two collections."
            },
            "slug": "Towards-language-independent-automated-learning-of-Apt\u00e9-Damerau",
            "title": {
                "fragments": [],
                "text": "Towards language independent automated learning of text categorization models"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The results of extensive machine learning experiments on large collections of Reuters\u2019 English and German newswires are described, and the methodology, which seems to be insensitive to the language of the document collections, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "[3]) and approaches based on principal component analysis[5, 24, 21, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7019,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 154
                            }
                        ],
                        "text": "3 Mutual information (MI) Mutual information is a criterion commonly used in statistical language modelling of word associations and related applications [7, 2, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9558665,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
            "isKey": false,
            "numCitedBy": 4363,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "slug": "Word-Association-Norms,-Mutual-Information-and-Church-Hanks",
            "title": {
                "fragments": [],
                "text": "Word Association Norms, Mutual Information and Lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144987107"
                        ],
                        "name": "Jamie Callan",
                        "slug": "Jamie-Callan",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Callan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jamie Callan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47394834"
                        ],
                        "name": "R. Papka",
                        "slug": "R.-Papka",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Papka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Papka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 386,
                                "start": 379
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1650587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dc36b8d0c08613fb213ad419973d379a2264765",
            "isKey": false,
            "numCitedBy": 620,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Systems for text retrieval, routing, categorization and other IR tasks rely heavily on linear classifiers. We propose that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers. In contrast to most IR methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms. Experimental data is presented showing Widrow-Hoff and EG to be more effective than the widely used Rocchio algorithm on several categorization and routing tasks."
            },
            "slug": "Training-algorithms-for-linear-text-classifiers-Lewis-Schapire",
            "title": {
                "fragments": [],
                "text": "Training algorithms for linear text classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work proposes that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers for IR tasks, and theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889602"
                        ],
                        "name": "Erik D. Wiener",
                        "slug": "Erik-D.-Wiener",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Wiener",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik D. Wiener"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "[3]) and approaches based on principal component analysis[5, 24, 21, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 13
                            }
                        ],
                        "text": "Wiener et al.[21, 19] used mutual information and a 2 statistic to select features for input to neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Wiener et al.[21, 19] used mutual information and a 2statistic to select features for input to neural networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "[19, 21, 19] used principal component analysis to nd orthogonal dimensions in the vector space of documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 307
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 91
                            }
                        ],
                        "text": "The Reuters news story collection is commonly used corpora in text categorization research [13, 1, 21, 16, 3] 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "On another variation of the Reuters collection where the training set and the test set are partitioned di erently, kNN has a break-even point of 82% which is the same as the result of neural networks[21], and LLSF has a break-even point of 81%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 154
                            }
                        ],
                        "text": "3 Mutual information (MI) Mutual information is a criterion commonly used in statistical language modelling of word associations and related applications [7, 2, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17503448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abbe40b7503f51971c92f9f9b20ebea6c0b36d77",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an application of nonlinear neural networks to topic spotting. Neural networks allow us to model higher-order interaction between document terms and to simultaneously predict multiple topics using shared hidden features. In the context of this model, we compare two approaches to dimensionality reduction in representation: one based on term selection and another based on Latent Semantic Indexing (LSI). Two diierent methods are proposed for improving LSI representations for the topic spotting task. We nd that term selection and our modiied LSI representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus."
            },
            "slug": "A-neural-network-approach-to-topic-spotting-Wiener-Pedersen",
            "title": {
                "fragments": [],
                "text": "A neural network approach to topic spotting"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that term selection and the modiied LSI representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703148"
                        ],
                        "name": "N. Fuhr",
                        "slug": "N.-Fuhr",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Fuhr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Fuhr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103447"
                        ],
                        "name": "S. Hartmann",
                        "slug": "S.-Hartmann",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Hartmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hartmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084356644"
                        ],
                        "name": "G. Lustig",
                        "slug": "G.-Lustig",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Lustig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lustig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1959019"
                        ],
                        "name": "M. Schwantner",
                        "slug": "M.-Schwantner",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Schwantner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schwantner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334474"
                        ],
                        "name": "Kostas Tzeras",
                        "slug": "Kostas-Tzeras",
                        "structuredName": {
                            "firstName": "Kostas",
                            "lastName": "Tzeras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kostas Tzeras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084310078"
                        ],
                        "name": "Gerhard Knorz",
                        "slug": "Gerhard-Knorz",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Knorz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerhard Knorz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 184
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 177
                            }
                        ],
                        "text": "The context sensitivity is in distinction to context-free methods based on explicit independence assumptions such as naive Bayes classi ers[13] and some other regression methods[8])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15004699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f61812ea95500993fada9f12c23577d2ba670d33",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "AIR/X is a rule-based system for indexing with terms (descriptors) from a prescribed vocabulary. For this task, an indexing dictionary with rules for mapping terms from the text onto descriptors is required, which can be derived automatically from a set of manually indexed documents. Based on the Darmstadt Indexing Approach, the indexing task is divided into a description step and a decision step. First, terms (single words or phrases) are identiied in the document text. With term-descriptor rules from the dictionary, descriptor indications are formed. The set of all indications from a document leading to the same descriptor is called a relevance description. A probabilistic classiication procedure computes indexing weights for each relevance description. Since the whole system is rule-based, it can be adapted to diierent subject elds by appropriate modiications of the rule bases. A major application of AIR/X is the AIR/PHYS system developed for a large physics database. This application is described in more detail along with experimental results."
            },
            "slug": "AIR/X-A-rule-based-multistage-indexing-system-for-Fuhr-Hartmann",
            "title": {
                "fragments": [],
                "text": "AIR/X - A rule-based multistage indexing system for Iarge subject fields"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A rule-based system for indexing with terms (descriptors) from a prescribed vocabulary, AIR/X is the AIR/PHYS system developed for a large physics database and can be adapted to diierent subject elds by appropriate modiications of the rule bases."
            },
            "venue": {
                "fragments": [],
                "text": "RIAO"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688124"
                        ],
                        "name": "W. Hersh",
                        "slug": "W.-Hersh",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Hersh",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hersh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089238953"
                        ],
                        "name": "T. J. Leone",
                        "slug": "T.-J.-Leone",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Leone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J. Leone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760869"
                        ],
                        "name": "D. Hickam",
                        "slug": "D.-Hickam",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hickam",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hickam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "It is a subset of the MEDLINE database[9], consisting of 348,566 references from 270 medical journals from the years 1987 to 1991."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15094383,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "e91fc6cba8b23688d02b0dc3ead69ed05210bf33",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A series of information retrieval experiments was carried out with a computer installed in a medical practice setting for relatively inexperienced physician end-users. Using a commercial MEDLINE product based on the vector space model, these physicians searched just as effectively as more experienced searchers using Boolean searching. The results of this experiment were subsequently used to create a new large medical test collection, which was used in experiments with the SMART retrieval system to obtain baseline performance data as well as compare SMART with the other searchers."
            },
            "slug": "OHSUMED:-an-interactive-retrieval-evaluation-and-Hersh-Buckley",
            "title": {
                "fragments": [],
                "text": "OHSUMED: an interactive retrieval evaluation and new large test collection for research"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A series of information retrieval experiments was carried out with a computer installed in a medical practice setting for relatively inexperienced physician end-users using a commercial MEDLINE product based on the vector space model, finding that these physicians searched just as effectively as more experienced searchers using Boolean searching."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 122
                            }
                        ],
                        "text": "2 Information gain (IG) Information gain is frequently employed as a termgoodness criterion in the eld of machine learning[17, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15727,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334474"
                        ],
                        "name": "Kostas Tzeras",
                        "slug": "Kostas-Tzeras",
                        "structuredName": {
                            "firstName": "Kostas",
                            "lastName": "Tzeras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kostas Tzeras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103447"
                        ],
                        "name": "S. Hartmann",
                        "slug": "S.-Hartmann",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Hartmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hartmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 262
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2861704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a283fb395343cd26984425306ca24c85b09ccdb",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model."
            },
            "slug": "Automatic-indexing-based-on-Bayesian-inference-Tzeras-Hartmann",
            "title": {
                "fragments": [],
                "text": "Automatic indexing based on Bayesian inference networks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented, followed by an indexing example and some experimental results about the indexing performance of the network model."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40199624"
                        ],
                        "name": "R. Creecy",
                        "slug": "R.-Creecy",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Creecy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Creecy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127759"
                        ],
                        "name": "B. Masand",
                        "slug": "B.-Masand",
                        "structuredName": {
                            "firstName": "Brij",
                            "lastName": "Masand",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Masand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111051178"
                        ],
                        "name": "Stephen J. Smith",
                        "slug": "Stephen-J.-Smith",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smith",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 223
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18744432,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "7403fa3e56cee44e0f48185bb4a79d935eb9b01c",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "mission to profile and economy of ~e Census Bureau ~dustry and occupation data for individuals in the labor force. For the 1990 Decennial Census, each of an estimated 22 million natural language responses to questions on the census long form had to be classified into one of 232 industry categories and 504 occupation categories. If done fully by hand the cost of this task would be on the order of $15 million."
            },
            "slug": "Trading-MIPS-and-memory-for-knowledge-engineering-Creecy-Masand",
            "title": {
                "fragments": [],
                "text": "Trading MIPS and memory for knowledge engineering"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "mission to profile and economy of ~e Census Bureau ~dustry and occupation data for individuals in the labor force, for the 1990 Decennial Census, which had to be classified into one of 232 industry categories and 504 occupation categories."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016427292"
                        ],
                        "name": "Mill Johannes G.A. Van",
                        "slug": "Mill-Johannes-G.A.-Van",
                        "structuredName": {
                            "firstName": "Mill",
                            "lastName": "Van",
                            "middleNames": [
                                "Johannes",
                                "G.A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mill Johannes G.A. Van"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022277349"
                        ],
                        "name": "Roland G. Whiteing",
                        "slug": "Roland-G.-Whiteing",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Whiteing",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roland G. Whiteing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 154
                            }
                        ],
                        "text": "3 Mutual information (MI) Mutual information is a criterion commonly used in statistical language modelling of word associations and related applications [7, 2, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 434518,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "5d042e85682d53ddd12a199326acedd25b36e6af",
            "isKey": false,
            "numCitedBy": 972,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A warning device associated with a rotatable body comprising means on the rotatable body for creating a magnetic field, a receiving coil on a relatively stationary member arranged such that upon rotation of the rotatable body interaction with the magnetic field causes electrical pulses to be generated in the receiving coil and an electronic switching circuit operated by the pulses such that a warning is given upon occurrence of a predetermined condition, such as pressure of a vehicle wheel deviating outside a predetermined limit."
            },
            "slug": "Transmission-Of-Information-Van-Whiteing",
            "title": {
                "fragments": [],
                "text": "Transmission Of Information"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A warning device associated with arotatable body comprising means on the rotatable body for creating a magnetic field, a receiving coil on a relatively stationary member arranged such that a warning is given upon occurrence of a predetermined condition, such as pressure of a vehicle wheel deviating outside a predetermined limit."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 24
                            }
                        ],
                        "text": "[22] J.W. Wilbur and K. Sirotkin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 97
                            }
                        ],
                        "text": "According to previous evaluations of retrieval and categorization on several document collections[22, 28], the AREL values between 10 to 20 yield satisfactory performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "5 Term strength (TS) Term strength is originally proposed and evaluated by Wilbur and Sirotkin [22] for vocabulary reduction in text retrieval, and later applied by Yang and Wilbur to text categorization [24, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 152
                            }
                        ],
                        "text": "Hence, the 2 statisticis known not to be reliable for low-frequency terms[6].2.5 Term strength (TS)Term strength is originally proposed and evaluated byWilbur and Sirotkin [22] for vocabulary reduction intext retrieval, and later applied by Yang and Wilburto text categorization [24, 28]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The automatic  identi cation of stop words"
            },
            "venue": {
                "fragments": [],
                "text": "J. Inf. Sci.,"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "We also used the SMART system [18] for uni ed preprocessing followed feature selection, which includes word stemming and weighting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "100%, the system assigns in decreasing score order as many categories as needed until a given recall is achieved, and computes the precision value at that point[18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "2 Experimental settings Before applying feature selection to documents, we removed the words in a standard stop word list[18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34382228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f2f6772d96d972e3b2da5aaa8a0f2feefdf827f",
            "isKey": true,
            "numCitedBy": 3884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Text-Processing:-The-Transformation,-and-Salton",
            "title": {
                "fragments": [],
                "text": "Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16] used an inductive learning algorithm to obtain features in disjunc-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 229
                            }
                        ],
                        "text": "The information gain of term t is de ned to be: G(t) = Pmi=1Pr(ci) logPr(ci) +Pr(t)Pmi=1 Pr(cijt) logPr(cijt) +Pr( t)Pmi=1 Pr(cij t) logPr(cij t) This de nition is more general than the one employed in binary classi cation models[13, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 335
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 91
                            }
                        ],
                        "text": "The Reuters news story collection is commonly used corpora in text categorization research [13, 1, 21, 16, 3] 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 318
                            }
                        ],
                        "text": "In a recent evaluation of classi cation methods[26] on the Reuters newswire collection (next section), the break-even point values were 85% for both kNN and LLSF, outperforming all the other systems evaluated on the same collection, including symbolic rule learning by RIPPER (80%)[3], SWAP1 (79%)[1] and CHARADE (78%)[16], a decision approach using C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text  categorization: a symbolic approach"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceed-  ings of the Fifth Annual Symposium on Document  Analysis and Information Retrieval,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Wiener et al.[21, 19] used mutual information and a 2statistic to select features for input to neural networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "[3]) and approaches based on principal component analysis[5, 24, 21, 19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "[19, 21, 19] used principal component analysis to nd orthogonal dimensions in the vector space of documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 13
                            }
                        ],
                        "text": "Wiener et al.[21, 19] used mutual information and a 2 statistic to select features for input to neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A  comparison of classi ers and document represen-  tations for the routing problem"
            },
            "venue": {
                "fragments": [],
                "text": "Ann  Int ACM SIGIR Conference on Research and De-  velopment in Information Retrieval"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "In a recent evaluation of classi cation methods[26] on the Reuters newswire collection (next section), the break-even point values were 85% for both kNN and LLSF, outperforming all the other systems evaluated on the same collection, including symbolic rule learning by RIPPER (80%)[3], SWAP1 (79%)[1] and CHARADE (78%)[16], a decision approach using C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "This makes the evaluation results highly questionable or non-interpretable unless these unlabelled documents are discarded, as analyzed in [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An evaluation of statistical approach  to text categorization"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report  CMU-CS-97-127,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The automatic identiication of stop words"
            },
            "venue": {
                "fragments": [],
                "text": "J. Inf. Sci"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Lang [11] used a minimum description length principle to select terms for Netnews categorization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Newsweeder: Learning to lter netnews"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Twelfth International Con-  ference on Machine Learning,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contextsensitive learning metods for text categorization"
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine Learning. McCraw Hill"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning. McCraw Hill"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Is learning bias an issue on the text categorization problem?"
            },
            "venue": {
                "fragments": [],
                "text": "Is learning bias an issue on the text categorization problem?"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wordassociation norms , mutual information and lexicography Trading mips and memory for knowledgeengineering : classifying census returns on the connection machine"
            },
            "venue": {
                "fragments": [],
                "text": "Comm . ACM"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Induction of decision trees. Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 223
                            }
                        ],
                        "text": "A growing number of statistical classi cation methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models[8, 27], nearest neighbor classi cation[4, 23], Bayes probabilistic approaches[20, 13], decision trees[13], neural networks[21], symbolic rule learning[1, 16, 3] and inductive learning algorithms[3, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "1 Classi ers To assess the e ectiveness of feature selection methods we used two di erent m-ary classi ers, a knearest-neighbor classi er (kNN)[23] and a regression method named the Linear Least Squares Fit mapping (LLSF)[27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Expert network: E ective and e cient  learning from human decisions in text categoriza-  tion and retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Ann Int ACM SI-  GIR Conference on Research and Development in  Information Retrieval"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "5 (79%)[15], inductive learning by Sleeping Experts (76%)[3], and a typical information retrieval approach named Rocchio (75%)[3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Is learning bias an issue on the  text categorization problem? In Technical re-  port, LAFORIA-LIP6"
            },
            "venue": {
                "fragments": [],
                "text": "Universite Paris VI, page  (to appear),"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 20,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 35,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Comparative-Study-on-Feature-Selection-in-Text-Yang-Pedersen/c3ebcef26c22a373b6f26a67934213eb0582804e?sort=total-citations"
}