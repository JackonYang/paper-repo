{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332642"
                        ],
                        "name": "Jiqiang Song",
                        "slug": "Jiqiang-Song",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052052370"
                        ],
                        "name": "Min Cai",
                        "slug": "Min-Cai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18648576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e14cf92ecb1589d21324d934b2009451e602d1be",
            "isKey": false,
            "numCitedBy": 371,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Text in video is a very compact and accurate clue for video indexing and summarization. Most video text detection and extraction methods hold assumptions on text color, background contrast, and font style. Moreover, few methods can handle multilingual text well since different languages may have quite different appearances. This paper performs a detailed analysis of multilingual text characteristics, including English and Chinese. Based on the analysis, we propose a comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing. The proposed method is also robust to various background complexities and text appearances. The text detection is carried out by edge detection, local thresholding, and hysteresis edge recovery. The coarse-to-fine localization scheme is then performed to identify text regions accurately. The text extraction consists of adaptive thresholding, dam point labeling, and inward filling. Experimental results on a large number of video images and comparisons with other methods are reported in detail."
            },
            "slug": "A-comprehensive-method-for-multilingual-video-text-Lyu-Song",
            "title": {
                "fragments": [],
                "text": "A comprehensive method for multilingual video text detection, localization, and extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing, and is also robust to various background complexities and text appearances."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145900372"
                        ],
                        "name": "Wonjun Kim",
                        "slug": "Wonjun-Kim",
                        "structuredName": {
                            "firstName": "Wonjun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonjun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145568138"
                        ],
                        "name": "Changick Kim",
                        "slug": "Changick-Kim",
                        "structuredName": {
                            "firstName": "Changick",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changick Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10712944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61c41f1cea644ea2d65455f9c3277ffe3e35aff2",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Overlay text brings important semantic clues in video content analysis such as video information retrieval and summarization, since the content of the scene or the editor's intention can be well represented by using inserted text. Most of the previous approaches to extracting overlay text from videos are based on low-level features, such as edge, color, and texture information. However, existing methods experience difficulties in handling texts with various contrasts or inserted in a complex background. In this paper, we propose a novel framework to detect and extract the overlay text from the video scene. Based on our observation that there exist transient colors between inserted text and its adjacent background, a transition map is first generated. Then candidate regions are extracted by a reshaping method and the overlay text regions are determined based on the occurrence of overlay text in each candidate. The detected overlay text regions are localized accurately using the projection of overlay text pixels in the transition map and the text extraction is finally conducted. The proposed method is robust to different character size, position, contrast, and color. It is also language independent. Overlay text region update between frames is also employed to reduce the processing time. Experiments are performed on diverse videos to confirm the efficiency of the proposed method."
            },
            "slug": "A-New-Approach-for-Overlay-Text-Detection-and-From-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "A New Approach for Overlay Text Detection and Extraction From Complex Video Scene"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes a novel framework to detect and extract the overlay text from the video scene that is robust to different character size, position, contrast, and color, and is also language independent."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13999848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24591ec88e706697bffa18f36728f192e0d797b6",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new automatic text location approach for videos is proposed. First of all, the corner points of the selected video frames are detected. After deleting some isolate corners, we merge the remaining corners to form candidate text regions. The regions are then decomposed vertically and horizontally using edge maps of the video frames to get candidate text lines. Finally, a text box verification step based on the feature derived from edge maps is taken to significantly reduce false alarms. Experimental results show that the new text location scheme proposed in this paper is accurate."
            },
            "slug": "Automatic-location-of-text-in-video-frames-Hua-Chen",
            "title": {
                "fragments": [],
                "text": "Automatic location of text in video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the new text location scheme proposed in this paper is accurate and can be used to significantly reduce false alarms."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242803"
                        ],
                        "name": "L. Agnihotri",
                        "slug": "L.-Agnihotri",
                        "structuredName": {
                            "firstName": "Lalitha",
                            "lastName": "Agnihotri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Agnihotri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1667502629"
                        ],
                        "name": "N. Dimitrova",
                        "slug": "N.-Dimitrova",
                        "structuredName": {
                            "firstName": "Natasa",
                            "lastName": "Dimitrova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dimitrova"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60735577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3b2c6b7bfaf9ab0d44c7103585fa0c81f60f3b9",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual information brings important semantic clues in video content analysis. We describe a method for detection and representation of text in video segments. The method consists of seven steps: channel separation, image enhancement, edge detection, edge filtering, character detection, text box detection, and text line detection. Our results show that this method can be applied to English as well as non-English text (such as Korean) with precision and recall of 85%."
            },
            "slug": "Text-detection-for-video-analysis-Agnihotri-Dimitrova",
            "title": {
                "fragments": [],
                "text": "Text detection for video analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work describes a method for detection and representation of text in video segments that can be applied to English as well as non-English text (such as Korean) with precision and recall of 85%."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Workshop on Content-Based Access of Image and Video Libraries (CBAIVL'99)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107824420"
                        ],
                        "name": "C. Garcia",
                        "slug": "C.-Garcia",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346037"
                        ],
                        "name": "X. Apostolidis",
                        "slug": "X.-Apostolidis",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Apostolidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Apostolidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46620452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57930a675de539c59bc33f56d9894c999d264f72",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Text is a very powerful index in content-based image and video indexing. We propose a new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background. Our goal is to minimize the number of false alarms and to binarize efficiently the detected text areas so that they can be processed by standard OCR software. First, potential areas of text are detected by enhancement and clustering processes, considering most of constraints related to the texture of words. Then, classification and binarization of potential text areas are achieved in a single scheme performing color quantization and characters periodicity analysis. We report a high rate of good detection results with very few false alarms and reliable text binarization."
            },
            "slug": "Text-detection-and-segmentation-in-complex-color-Garcia-Apostolidis",
            "title": {
                "fragments": [],
                "text": "Text detection and segmentation in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background and to binarize efficiently the detected text areas so that they can be processed by standard OCR software."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109365947"
                        ],
                        "name": "Xiaoqing Liu",
                        "slug": "Xiaoqing-Liu",
                        "structuredName": {
                            "firstName": "Xiaoqing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804589"
                        ],
                        "name": "J. Samarabandu",
                        "slug": "J.-Samarabandu",
                        "structuredName": {
                            "firstName": "Jagath",
                            "lastName": "Samarabandu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Samarabandu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17603163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b49b3bfc48a6f9fa03889b219233f5fcc248e747",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in images contains important and useful information. Detection and extraction of text in images have been used in many applications. In this paper, we propose a multiscale edge-based text extraction algorithm, which can automatically detect and extract text in complex images. The proposed method is a general-purpose text detection and extraction algorithm, which can deal not only with printed document images but also with scene text. It is robust with respect to the font size, style, color, orientation, and alignment of text and can be used in a large variety of application fields, such as mobile robot navigation, vehicle license detection and recognition, object identification, document retrieving, page segmentation, etc"
            },
            "slug": "Multiscale-Edge-Based-Text-Extraction-from-Complex-Liu-Samarabandu",
            "title": {
                "fragments": [],
                "text": "Multiscale Edge-Based Text Extraction from Complex Images"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A multiscale edge-based text extraction algorithm, which can automatically detect and extract text in complex images, and is robust with respect to the font size, style, color, orientation, and alignment of text."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31459400"
                        ],
                        "name": "W. Mao",
                        "slug": "W.-Mao",
                        "structuredName": {
                            "firstName": "Weng",
                            "lastName": "Mao",
                            "middleNames": [
                                "Zu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145288211"
                        ],
                        "name": "K. Chung",
                        "slug": "K.-Chung",
                        "structuredName": {
                            "firstName": "Korris",
                            "lastName": "Chung",
                            "middleNames": [
                                "Fu-Lai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595159"
                        ],
                        "name": "Kenneth K. M. Lam",
                        "slug": "Kenneth-K.-M.-Lam",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Lam",
                            "middleNames": [
                                "K.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth K. M. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144250750"
                        ],
                        "name": "W. Siu",
                        "slug": "W.-Siu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Siu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Siu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5893059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c2098f5478256b50f8e1917ae3b60d5b69bb969",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a multiscale texture-based method using local energy analysis for hybrid Chinese/English text detection in images and video frames. Local energy analysis has been shown to work well in text detection, where remarkable local energy variations of pixels correspond to text region or boundary of other objects and lower local energy variations of pixels correspond to background or the interior of non-text objects. Local energy variation is calculated in a local region based on the wavelet transform coefficients of images. Hybrid Chinese/English text in images and video frames can be detected whether it is aligned horizontally or vertically. The font size of text to be detected may vary in a wide range of values. The proposed method has been tested on 321 frame images obtained from local TV programs and a tested dataset with low missed rate and false alarm rate."
            },
            "slug": "Hybrid-Chinese/English-text-detection-in-images-and-Mao-Chung",
            "title": {
                "fragments": [],
                "text": "Hybrid Chinese/English text detection in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A multiscale texture-based method using local energy analysis for hybrid Chinese/English text detection in images and video frames and a tested dataset with low missed rate and false alarm rate is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145211604"
                        ],
                        "name": "K. Karu",
                        "slug": "K.-Karu",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "Karu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29853292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a4af75831ed098d9fea02507f36cdbc38852fe6",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a substantial interest in retrieving images from a large database using the textual information contained in the images. An algorithm which will automatically locate the textual regions in the input image will facilitate this task; the optical character recognizer can then be applied to only those regions of the image which contain text. We present a method for automatically locating text in complex color images. The algorithm first finds the approximate locations of text lines using horizontal spatial variance, and then extracts text components in these boxes using color segmentation. The proposed method has been used to locate text in compact disc (CD) and book cover images, as well as in the images of traffic scenes captured by a video camera. Initial results are encouraging and suggest that these algorithms can be used in image retrieval applications."
            },
            "slug": "Locating-text-in-complex-color-images-Zhong-Karu",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed algorithm has been used to locate text in compact disc and book cover images, as well as in the images of traffic scenes captured by a video camera, and initial results suggest that these algorithms can be used in image retrieval applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152736800"
                        ],
                        "name": "M. Smith",
                        "slug": "M.-Smith",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15525071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94c4141cdd7615e8e6fccbfa864abd518a62efd8",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Digital video is rapidly becoming an important source for information, entertainment and a host of multimedia applications. With the size of these collections growing to thousands of hours, technology is needed to effectively browse segments in a short time without losing the content of the video. We propose a method to extract the significant audio and video information and create a \u201cskim\u201d video which represents a short synopsis of the original. The extraction of significant information, such as specific objects, audio keywords and relevant video structure, is made possible through the integration of techniques in image and language understanding. The resulting skim is much smaller, and retains the essential content of the original segment. This research is sponsored by the National Science Foundation under grant no. IRI9411299, the National Space and Aeronautics Administration, and the Advanced Research Projects Agency. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of the United States Government."
            },
            "slug": "Video-Skimming-for-Quick-Browsing-based-on-Audio-Smith",
            "title": {
                "fragments": [],
                "text": "Video Skimming for Quick Browsing based on Audio and Image Characterization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The extraction of significant information, such as specific objects, audio keywords and relevant video structure, is made possible through the integration of techniques in image and language understanding and a \u201cskim\u201d video is proposed which represents a short synopsis of the original."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10699750"
                        ],
                        "name": "Xinbo Gao",
                        "slug": "Xinbo-Gao",
                        "structuredName": {
                            "firstName": "Xinbo",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinbo Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7137861"
                        ],
                        "name": "Jianzhuang Liu",
                        "slug": "Jianzhuang-Liu",
                        "structuredName": {
                            "firstName": "Jianzhuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianzhuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15021030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1697bfbb2c701fba6032d63309a904d21b4f0d09",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a video caption detection and recognition system based on a fuzzy-clustering neural network (FCNN) classifier. Using a novel caption-transition detection scheme we locate both spatial and temporal positions of video captions with high precision and efficiency. Then employing several new character segmentation and binarization techniques, we improve the Chinese video-caption recognition accuracy from 13% to 86% on a set of news video captions. As the first attempt on Chinese video-caption recognition, our experiment results are very encouraging."
            },
            "slug": "A-spatial-temporal-approach-for-video-caption-and-Tang-Gao",
            "title": {
                "fragments": [],
                "text": "A spatial-temporal approach for video caption detection and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A video caption detection and recognition system based on a fuzzy-clustering neural network (FCNN) classifier that improves the Chinese video-caption recognition accuracy from 13% to 86% on a set of news video captions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6781817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7721138e41d82fedabca59c9a66e67d9b7053f3",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to automatically locate captions in MPEG video. Caption text regions are segmented from the background using their distinguishing texture characteristics. This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain. Therefore, only a small amount of decoding is required. The proposed algorithm achieves about 4.0% false reject rate and less than 5.7% false positive rate on a variety of MPEG compressed video containing more than 42,000 frames."
            },
            "slug": "Automatic-caption-localization-in-compressed-video-Zhong-Zhang",
            "title": {
                "fragments": [],
                "text": "Automatic caption localization in compressed video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain, so that only a small amount of decoding is required."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116561559"
                        ],
                        "name": "Hao Yan",
                        "slug": "Hao-Yan",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153912327"
                        ],
                        "name": "Yi Zhang",
                        "slug": "Yi-Zhang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108070193"
                        ],
                        "name": "Z. Hou",
                        "slug": "Z.-Hou",
                        "structuredName": {
                            "firstName": "Zeng-Guang",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49720667"
                        ],
                        "name": "M. Tan",
                        "slug": "M.-Tan",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3636292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01ebd9c01595610410e2dd94e2a9b55a97663824",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, one novel approach for text detection in video frames, which is based on bootstrap artificial neural network (BANN) and CED operator, is proposed. This method first uses a new color image edge operator (CED) to segment the image and achieve the elementary candidate text block. And then the neural network is introduced into the further classification of the text blocks and the non-text blocks in video frames. The idea of bootstrap is introduced into the training of the ANN, thus improving the effectiveness of the neural network greatly. Experiments results proved that this method is effective."
            },
            "slug": "Automatic-Text-Detection-In-Video-Frames-Based-on-Yan-Zhang",
            "title": {
                "fragments": [],
                "text": "Automatic Text Detection In Video Frames Based on Bootstrap Artificial Neural Network and CED"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "One novel approach for text detection in video frames, which is based on bootstrap artificial neural network (BANN) and CED operator, is proposed, which first uses a new color image edge operator (CED) to segment the image and achieve the elementary candidate text block."
            },
            "venue": {
                "fragments": [],
                "text": "WSCG"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38486814"
                        ],
                        "name": "Bongkee Sin",
                        "slug": "Bongkee-Sin",
                        "structuredName": {
                            "firstName": "Bongkee",
                            "lastName": "Sin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bongkee Sin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109631931"
                        ],
                        "name": "Seon-Kyu Kim",
                        "slug": "Seon-Kyu-Kim",
                        "structuredName": {
                            "firstName": "Seon-Kyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seon-Kyu Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2668078"
                        ],
                        "name": "Beom-Joon Cho",
                        "slug": "Beom-Joon-Cho",
                        "structuredName": {
                            "firstName": "Beom-Joon",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beom-Joon Cho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39283552,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "a5baf0480b140a17339547e24fc397aaf33937d3",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a (language-independent) method of locating rectangular text regions in natural scene images. The method consists of two steps that can be applied in succession or independently: the frequency of edge pixels across vertical and horizontal scan lines, and the fundamental frequency in the Fourier domain. The frequency feature of text images is highly intuitive, and this is the focus of the research. The detection of rectangles using a Hough transform is also addressed. Texts that are meaningful to many viewers usually appear in rectangles of colours of high contrast to the background. Hence it is natural to assume that the detection of rectangles may be helpful for locating desired texts correctly in natural outdoor scene images."
            },
            "slug": "Locating-characters-in-scene-images-using-frequency-Sin-Kim",
            "title": {
                "fragments": [],
                "text": "Locating characters in scene images using frequency features"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents a (language-independent) method of locating rectangular text regions in natural scene images using the frequency of edge pixels across vertical and horizontal scan lines and the fundamental frequency in the Fourier domain."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747647"
                        ],
                        "name": "S. Santini",
                        "slug": "S.-Santini",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Santini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Santini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722619"
                        ],
                        "name": "Amarnath Gupta",
                        "slug": "Amarnath-Gupta",
                        "structuredName": {
                            "firstName": "Amarnath",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amarnath Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938740"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2827898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b7c4096ed697696a5f4fc8f3a6a750dc0cdecfe",
            "isKey": false,
            "numCitedBy": 6727,
            "numCiting": 410,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap."
            },
            "slug": "Content-Based-Image-Retrieval-at-the-End-of-the-Smeulders-Worring",
            "title": {
                "fragments": [],
                "text": "Content-Based Image Retrieval at the End of the Early Years"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap are discussed, as well as aspects of system engineering: databases, system architecture, and evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17901853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bbcdc1744cc5982ffb64ea4755a72921d98d08",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The current paper presents a novel texture-based method for detecting texts in images. A support vector machine (SVM) is used to analyze the textural properties of texts. No external texture feature extraction module is used, but rather the intensities of the raw pixels that make up the textural pattern are fed directly to the SVM, which works well even in high-dimensional spaces. Next, text regions are identified by applying a continuously adaptive mean shift algorithm (CAMSHIFT) to the results of the texture analysis. The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "slug": "Texture-Based-Approach-for-Text-Detection-in-Images-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Texture-Based Approach for Text Detection in Images Using Support Vector Machines and Continuously Adaptive Mean Shift Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1830124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5c342ba0edbebadc7c95c7e59d1bef87d7e4add",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks. Text is first detected using multiscale texture segmentation and spatial cohesion constraints, then cleaned up and extracted using a histogram-based binarization algorithm. An automatic performance evaluation scheme is also proposed."
            },
            "slug": "TextFinder:-An-Automatic-System-to-Detect-and-Text-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "TextFinder: An Automatic System to Detect and Recognize Text In Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685742"
                        ],
                        "name": "Y. Aslandogan",
                        "slug": "Y.-Aslandogan",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Aslandogan",
                            "middleNames": [
                                "Alp"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aslandogan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642131"
                        ],
                        "name": "Clement T. Yu",
                        "slug": "Clement-T.-Yu",
                        "structuredName": {
                            "firstName": "Clement",
                            "lastName": "Yu",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clement T. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15845719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1d50337dabf3e71fba0d4283ad9837fd63cf634",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Storage and retrieval of multimedia has become a requirement for many contemporary information systems. These systems need to provide browsing, querying, navigation, and, sometimes, composition capabilities involving various forms of media. In this survey, we review techniques and systems for image and video retrieval. We first look at visual features for image retrieval such as color, texture, shape, and spatial relationships. The indexing techniques are discussed for these features. Nonvisual features include captions, annotations, relational attributes, and structural descriptions. Temporal aspects of video retrieval and video segmentation are discussed next. We review several systems for image and video retrieval including research, commercial, and World Wide Web-based systems. We conclude with an overview of current challenges and future trends for image and video retrieval."
            },
            "slug": "Techniques-and-Systems-for-Image-and-Video-Aslandogan-Yu",
            "title": {
                "fragments": [],
                "text": "Techniques and Systems for Image and Video Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This survey reviews techniques and systems for image and video retrieval including research, commercial, and World Wide Web-based systems and concludes with an overview of current challenges and future trends."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40309692"
                        ],
                        "name": "C. G. Harris",
                        "slug": "C.-G.-Harris",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Harris",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40365651"
                        ],
                        "name": "M. Stephens",
                        "slug": "M.-Stephens",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Stephens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stephens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "2) The distributions of corner points in text regions are usually more orderly in comparison to the nontext regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The flexibility of the criteria allows the adaption of our algorithm to different applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1694378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6818668fb895d95861a2eb9673ddc3a41e27b3b3",
            "isKey": false,
            "numCitedBy": 14111,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed."
            },
            "slug": "A-Combined-Corner-and-Edge-Detector-Harris-Stephens",
            "title": {
                "fragments": [],
                "text": "A Combined Corner and Edge Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem the authors are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work."
            },
            "venue": {
                "fragments": [],
                "text": "Alvey Vision Conference"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522582"
                        ],
                        "name": "F. Idris",
                        "slug": "F.-Idris",
                        "structuredName": {
                            "firstName": "Fayez",
                            "lastName": "Idris",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Idris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743991"
                        ],
                        "name": "S. Panchanathan",
                        "slug": "S.-Panchanathan",
                        "structuredName": {
                            "firstName": "Sethuraman",
                            "lastName": "Panchanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Panchanathan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32085074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce61e49a9a3c88152e1f1b1b6e2e803328cc48f1",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual database systems require efficient indexing to facilitate fast access to the images and video sequences in the database. Recently, several content-based indexing methods for image and video based on spatial relationships, color, texture, shape, sketch, object motion, and camera parameters have been reported in the literature. The goal of this paper is to provide a critical survey of existing literature on content-based indexing techniques and to point out the relative advantages and disadvantages of each approach."
            },
            "slug": "Review-of-Image-and-Video-Indexing-Techniques-Idris-Panchanathan",
            "title": {
                "fragments": [],
                "text": "Review of Image and Video Indexing Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A critical survey of existing literature on content-based indexing techniques is provided to point out the relative advantages and disadvantages of each approach."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145459057"
                        ],
                        "name": "Y. Rui",
                        "slug": "Y.-Rui",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Rui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Rui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2910032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fa15f525a9814247ecd7cd93636f278d6d9ab3b",
            "isKey": false,
            "numCitedBy": 2521,
            "numCiting": 220,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested."
            },
            "slug": "Image-Retrieval:-Current-Techniques,-Promising-and-Rui-Huang",
            "title": {
                "fragments": [],
                "text": "Image Retrieval: Current Techniques, Promising Directions, and Open Issues"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145299933"
                        ],
                        "name": "R. Mohr",
                        "slug": "R.-Mohr",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Mohr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mohr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692283"
                        ],
                        "name": "C. Bauckhage",
                        "slug": "C.-Bauckhage",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bauckhage",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bauckhage"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "3) The usage of corner points generates more flexible and efficient criteria, under which the margin between text and nontext regions in the feature space is discriminative."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14571093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2713e7a59105a832e20c01c3c202b9dcd2b5f889",
            "isKey": false,
            "numCitedBy": 1730,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Many different low-level feature detectors exist and it is widely agreed that the evaluation of detectors is important. In this paper we introduce two evaluation criteria for interest points' repeatability rate and information content. Repeatability rate evaluates the geometric stability under different transformations. Information content measures the distinctiveness of features. Different interest point detectors are compared using these two criteria. We determine which detector gives the best results and show that it satisfies the criteria well."
            },
            "slug": "Evaluation-of-Interest-Point-Detectors-Schmid-Mohr",
            "title": {
                "fragments": [],
                "text": "Evaluation of Interest Point Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two evaluation criteria for interest points' repeatability rate and information content are introduced and different interest point detectors are compared using these two criteria."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116003860"
                        ],
                        "name": "J. Bergen",
                        "slug": "J.-Bergen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145981906"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3315356"
                        ],
                        "name": "K. Hanna",
                        "slug": "K.-Hanna",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Hanna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hanna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557"
                        ],
                        "name": "R. Hingorani",
                        "slug": "R.-Hingorani",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Hingorani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hingorani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6267598,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "404f3544a7db67c38ba3b8f78f02759d2326684e",
            "isKey": false,
            "numCitedBy": 1510,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a hierarchical estimation framework for the computation of diverse representations of motion information. The key features of the resulting framework (or family of algorithms) are a global model that constrains the overall structure of the motion estimated, a local model that is used in the estimation process, and a coarse-fine refinement strategy. Four specific motion models: affine flow, planar surface flow, rigid body motion, and general optical flow, are described along with their application to specific examples."
            },
            "slug": "Hierarchical-Model-Based-Motion-Estimation-Bergen-Anandan",
            "title": {
                "fragments": [],
                "text": "Hierarchical Model-Based Motion Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper describes a hierarchical estimation framework for the computation of diverse representations of motion information that constrains the overall structure of the motion estimated, a local model that is used in the estimation process, and a coarse-fine refinement strategy."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40588702"
                        ],
                        "name": "B. D. Lucas",
                        "slug": "B.-D.-Lucas",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Lucas",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2121536,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "a06547951c97b2a32f23a6c2b5f79c8c75c9b9bd",
            "isKey": false,
            "numCitedBy": 13328,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is taster because it examines far fewer potential matches between the images than existing techniques Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show how our technique can be adapted tor use in a stereo vision system."
            },
            "slug": "An-Iterative-Image-Registration-Technique-with-an-Lucas-Kanade",
            "title": {
                "fragments": [],
                "text": "An Iterative Image Registration Technique with an Application to Stereo Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration, and can be generalized to handle rotation, scaling and shearing."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887059"
                        ],
                        "name": "W. Loh",
                        "slug": "W.-Loh",
                        "structuredName": {
                            "firstName": "Wei-Yin",
                            "lastName": "Loh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Loh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17654166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e708b0cd19818e0c7408765dd922835661f8a24",
            "isKey": false,
            "numCitedBy": 18464,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification and regression trees are machine\u2010learning methods for constructing prediction models from data. The models are obtained by recursively partitioning the data space and fitting a simple prediction model within each partition. As a result, the partitioning can be represented graphically as a decision tree. Classification trees are designed for dependent variables that take a finite number of unordered values, with prediction error measured in terms of misclassification cost. Regression trees are for dependent variables that take continuous or ordered discrete values, with prediction error typically measured by the squared difference between the observed and predicted values. This article gives an introduction to the subject by reviewing some widely available algorithms and comparing their capabilities, strengths, and weakness in two examples. \u00a9 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 14\u201023 DOI: 10.1002/widm.8"
            },
            "slug": "Classification-and-regression-trees-Loh",
            "title": {
                "fragments": [],
                "text": "Classification and regression trees"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This article gives an introduction to the subject of classification and regression trees by reviewing some widely available algorithms and comparing their capabilities, strengths, and weakness in two examples."
            },
            "venue": {
                "fragments": [],
                "text": "WIREs Data Mining Knowl. Discov."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29458883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8017699564136f93af21575810d557dba1ee6fc6",
            "isKey": false,
            "numCitedBy": 16308,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index."
            },
            "slug": "Classification-and-Regression-Trees-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Classification and Regression Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This chapter discusses tree classification in the context of medicine, where right Sized Trees and Honest Estimates are considered and Bayes Rules and Partitions are used as guides to optimal pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523363"
                        ],
                        "name": "W. Bright",
                        "slug": "W.-Bright",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bright",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 143516373,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "20125b001e18e80d9903c9f56e65233b33a86734",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Blackwell-encyclopedia-of-writing-systems-By-Bright",
            "title": {
                "fragments": [],
                "text": "The Blackwell encyclopedia of writing systems By Florian Coulmas (review)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-From-Corners:-A-Novel-Approach-to-Detect-Text-Zhao-Lin/dcbdd4ea38c2617f7a52e05b647f547d8a7dc60e?sort=total-citations"
}