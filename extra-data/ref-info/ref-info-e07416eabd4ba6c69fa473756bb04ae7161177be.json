{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818379"
                        ],
                        "name": "O. Guleryuz",
                        "slug": "O.-Guleryuz",
                        "structuredName": {
                            "firstName": "Onur",
                            "lastName": "Guleryuz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Guleryuz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "In fact, following Guleryuz\u2019s work, the DCT seems like such a plausible choice [38]\u2013[40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "results in order to prevent such blockiness artifacts, as, indeed, practiced in [38]\u2013[40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "Its numerical solution leads to a simple iterated patch-by-patch sparse coding and averaging algorithm that is closely related to the ideas explored in [38]\u2013[40] and generalizes them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1705097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "903695627931b447ada94b7df2a9c7c47cd24124",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the familiar scenario where independent and identically distributed (i.i.d) noise in an image is removed using a set of overcomplete linear transforms and thresholding. Rather than the standard approach where one obtains the denoised signal by ad hoc averaging of the denoised estimates (corresponding to each transform), we formulate the optimal combination as a linear estimation problem for each pixel and solve it for optimal estimates. Our approach is independent of the utilized transforms and the thresholding scheme, and extends established work by exploiting a separate degree of freedom that is in general not reachable using previous techniques. Surprisingly, our derivation of the optimal estimates does not require explicit image statistics but relies solely on the assumption that the utilized transforms provide sparse decompositions. Yet it can be seen that our adaptive estimates utilize implicit conditional statistics and they make the biggest impact around edges and singularities where standard sparsity assumptions fail."
            },
            "slug": "Weighted-overcomplete-denoising-Guleryuz",
            "title": {
                "fragments": [],
                "text": "Weighted overcomplete denoising"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Surprisingly, the derivation of the optimal estimates does not require explicit image statistics but relies solely on the assumption that the utilized transforms provide sparse decompositions, and it can be seen that the adaptive estimates utilize implicit conditional statistics and they make the biggest impact around edges and singularities where standard sparsity assumptions fail."
            },
            "venue": {
                "fragments": [],
                "text": "The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818379"
                        ],
                        "name": "O. Guleryuz",
                        "slug": "O.-Guleryuz",
                        "structuredName": {
                            "firstName": "Onur",
                            "lastName": "Guleryuz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Guleryuz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 278084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89549b0aaa6b7ae0b58155cdf6b0283028e8b99d",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the robust estimation of missing regions in images and video using adaptive, sparse reconstructions. Our primary application is on missing regions of pixels containing textures, edges, and other image features that are not readily handled by prevalent estimation and recovery algorithms. We assume that we are given a linear transform that is expected to provide sparse decompositions over missing regions such that a portion of the transform coefficients over missing regions are zero or close to zero. We adaptively determine these small magnitude coefficients through thresholding, establish sparsity constraints, and estimate missing regions in images using information surrounding these regions. Unlike prevalent algorithms, our approach does not necessitate any complex preconditioning, segmentation, or edge detection steps, and it can be written as a sequence of denoising operations. We show that the region types we can effectively estimate in a mean-squared error sense are those for which the given transform provides a close approximation using sparse nonlinear approximants. We show the nature of the constructed estimators and how these estimators relate to the utilized transform and its sparsity over regions of interest. The developed estimation framework is general, and can readily be applied to other nonstationary signals with a suitable choice of linear transforms. Part I discusses fundamental issues, and Part II is devoted to adaptive algorithms with extensive simulation examples that demonstrate the power of the proposed techniques."
            },
            "slug": "Nonlinear-approximation-based-image-recovery-using-Guleryuz",
            "title": {
                "fragments": [],
                "text": "Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising-part I: theory"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The robust estimation of missing regions in images and video using adaptive, sparse reconstructions using constructed estimators and how these estimators relate to the utilized transform and its sparsity over regions of interest is shown."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610924"
                        ],
                        "name": "A. Bruckstein",
                        "slug": "A.-Bruckstein",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Bruckstein",
                            "middleNames": [
                                "Marcel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruckstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121924283"
                        ],
                        "name": "Y. Katz",
                        "slug": "Y.-Katz",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Katz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1912619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6e0fb4c77906bc23fe59a8f848ce62ba9687181",
            "isKey": false,
            "numCitedBy": 6191,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a pre-specified set of linear transforms, or by adapting the dictionary to a set of training signals. Both these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method \u2013 the K-SVD algorithm \u2013 generalizing the K-Means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary, and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results on both synthetic tests and in applications on real image data."
            },
            "slug": "K-SVD-:-An-Algorithm-for-Designing-of-Overcomplete-Aharon-Elad",
            "title": {
                "fragments": [],
                "text": "K-SVD : An Algorithm for Designing of Overcomplete Dictionaries for Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel algorithm for adapting dictionaries in order to achieve sparse signal representations, K-SVD, an iterative method that alternates between sparse coding of the examples based on the current dictionary, and a process of updating the dictionary atoms to better fit the data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818379"
                        ],
                        "name": "O. Guleryuz",
                        "slug": "O.-Guleryuz",
                        "structuredName": {
                            "firstName": "Onur",
                            "lastName": "Guleryuz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Guleryuz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "In fact, following Guleryuz\u2019s work, the DCT seems like such a plausible choice [38]\u2013[40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "results in order to prevent such blockiness artifacts, as, indeed, practiced in [38]\u2013[40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Its numerical solution leads to a simple iterated patch-by-patch sparse coding and averaging algorithm that is closely related to the ideas explored in [38]\u2013[40] and generalizes them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7855497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c95ff30813f2fb6ff7e5d4da79c64cdea57c495a",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We combine the main ideas introduced in Part I with adaptive techniques to arrive at a powerful algorithm that estimates missing data in nonstationary signals. The proposed approach operates automatically based on a chosen linear transform that is expected to provide sparse decompositions over missing regions such that a portion of the transform coefficients over missing regions are zero or close to zero. Unlike prevalent algorithms, our method does not necessitate any complex preconditioning, segmentation, or edge detection steps, and it can be written as a progression of denoising operations. We show that constructing estimates based on nonlinear approximants is fundamentally a nonconvex problem and we propose a progressive algorithm that is designed to deal with this issue directly. The algorithm is applied to images through an extensive set of simulation examples, primarily on missing regions containing textures, edges, and other image features that are not readily handled by established estimation and recovery methods. We discuss the properties required of good transforms, and in conjunction, show the types of regions over which well-known transforms provide good predictors. We further discuss extensions of the algorithm where the utilized transforms are also chosen adaptively, where unpredictable signal components in the progressions are identified and not predicted, and where the prediction scenario is more general."
            },
            "slug": "Nonlinear-approximation-based-image-recovery-using-Guleryuz",
            "title": {
                "fragments": [],
                "text": "Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising-part II: adaptive algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work shows that constructing estimates based on nonlinear approximants is fundamentally a nonconvex problem and proposes a progressive algorithm that is designed to deal with this issue directly and is applied to images through an extensive set of simulation examples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610924"
                        ],
                        "name": "A. Bruckstein",
                        "slug": "A.-Bruckstein",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Bruckstein",
                            "middleNames": [
                                "Marcel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruckstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed, we might do better by using a redundant version of the DCT,1 as practiced in [ 36 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The K-SVD proposes an iterative algorithm designed to handle the above task effectively [ 36 ], [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Instead of supplying an artificial set of examples to train on, as proposed above, one could take the patches from the corrupted image, , where . Since the K-SVD dictionary learning process has in it a noise rejection capability (see experiments reported in [ 36 ]), this seems like a natural idea."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "K-SVD algorithm [ 36 ], [37] because of its simplicity and efficiency for this task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This way, the value of is guaranteed to drop per an update of each dictionary atom, and along with this update, the representation coefficients change as well (see [ 36 ] and [37] for more details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7477309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83b522f4bfa5db7f7d34f839475af7d078107634",
            "isKey": false,
            "numCitedBy": 7336,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data"
            },
            "slug": "$rm-K$-SVD:-An-Algorithm-for-Designing-Overcomplete-Aharon-Elad",
            "title": {
                "fragments": [],
                "text": "$rm K$-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A novel algorithm for adapting dictionaries in order to achieve sparse signal representations, the K-SVD algorithm, an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33523605"
                        ],
                        "name": "J. Portilla",
                        "slug": "J.-Portilla",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Portilla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Portilla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051519"
                        ],
                        "name": "V. Strela",
                        "slug": "V.-Strela",
                        "structuredName": {
                            "firstName": "Vasily",
                            "lastName": "Strela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Strela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Averaging the results that correspond to [ 23 ] in this table for noise levels lower than,2"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Fig. 4. Comparison between the three presented methods (overcomplete DCT, global trained dictionary, and adaptive dictionary trained on patches from the noisy image) and the results achieved recently in [ 23 ] for three test images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "All these lead to what is considered today as some of the best available image denoising methods (see [ 23 ]\u2010[26] for few representative works)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The tested images, as also the tested noise levels, are all the same ones as those used in the denoising experiments reported in [ 23 ], in order to enable a fair comparison."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Is there any chance of getting a comparable denoising performance with a local-sparsity based method? In that respect, the image denoising work reported in [ 23 ] is of great importance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Using the OMP, atoms were accumulated till the average error passed the threshold, chosen empirically to be . This means that our algorithm assumes the knowledge of \u2014very much like that assumed in [ 23 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In order to better visualize the results and their comparison to those in [ 23 ], Fig. 4 presents the difference of the denoising results of the two proposed methods and the overcomplete DCT compared with those of [23] (which appears as a zero straight reference line)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "SUMMARY OF THE DENOISING PSNR RESULTS IN DECIBELS .I N EACH CELL .F OUR DENOISING RESULTS ARE REPORTED .T OP LEFT :R ESULTS OF PORTILLA ET AL. [ 23 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In order to better visualize the results and their comparison to those in [23], Fig. 4 presents the difference of the denoising results of the two proposed methods and the overcomplete DCT compared with those of [ 23 ] (which appears as a zero straight reference line)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 52808855,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "85791491919e1f740f0e882366046acbe56fb14c",
            "isKey": true,
            "numCitedBy": 2403,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for removing noise from digital images, based on a statistical model of the coefficients of an overcomplete multiscale oriented basis. Neighborhoods of coefficients at adjacent positions and scales are modeled as the product of two independent random variables: a Gaussian vector and a hidden positive scalar multiplier. The latter modulates the local variance of the coefficients in the neighborhood, and is thus able to account for the empirically observed correlation between the coefficient amplitudes. Under this model, the Bayesian least squares estimate of each coefficient reduces to a weighted average of the local linear estimates over all possible values of the hidden multiplier variable. We demonstrate through simulations with images contaminated by additive white Gaussian noise that the performance of this method substantially surpasses that of previously published methods, both visually and in terms of mean squared error."
            },
            "slug": "Image-denoising-using-scale-mixtures-of-Gaussians-Portilla-Strela",
            "title": {
                "fragments": [],
                "text": "Image denoising using scale mixtures of Gaussians in the wavelet domain"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The performance of this method for removing noise from digital images substantially surpasses that of previously published methods, both visually and in terms of mean squared error."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Similar conversion has also been practiced by Roth and Black when handling an MRF prior [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "This aligns with a similar idea, appearing in [29], for turning a local MRF-based prior into a global one."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 260
                            }
                        ],
                        "text": "For example, assuming a spatial smoothness-based Markov random field prior of a specific structure, one can still question (and, thus, train) the derivative filters to apply on the image, and the robust function to use in weighting these filters\u2019 outcome [27]\u2013[29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2843211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2077d0f30507d51a0d3bbec4957d55e817d66a59",
            "isKey": false,
            "numCitedBy": 1051,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques."
            },
            "slug": "Fields-of-Experts:-a-framework-for-learning-image-Roth-Black",
            "title": {
                "fragments": [],
                "text": "Fields of Experts: a framework for learning image priors"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks, developed using a Products-of-Experts framework."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054912981"
                        ],
                        "name": "S. Lesage",
                        "slug": "S.-Lesage",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Lesage",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lesage"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731535"
                        ],
                        "name": "R. Gribonval",
                        "slug": "R.-Gribonval",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Gribonval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gribonval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9457270"
                        ],
                        "name": "F. Bimbot",
                        "slug": "F.-Bimbot",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Bimbot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bimbot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2434483"
                        ],
                        "name": "L. Benaroya",
                        "slug": "L.-Benaroya",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Benaroya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Benaroya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14749584,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d9ef403969035e022b1b61c7dc513ffe189f031",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new method to learn overcomplete dictionaries for sparse coding structured as unions of orthonormal bases. The interest of such a structure is manifold. Indeed, it seems that many signals or images can be modeled as the superimposition of several layers with sparse decompositions in as many bases. Moreover, in such dictionaries, the efficient block coordinate relaxation (BCR) algorithm can be used to compute sparse decompositions. We show that it is possible to design an iterative learning algorithm that produces a dictionary with the required structure. Each step is based on the coefficients estimation, using a variant of BCR, followed by the update of one chosen basis, using singular value decomposition. We assess experimentally how well the learning algorithm recovers dictionaries that may or may not have the required structure, and to what extent the noise level is a disturbing factor."
            },
            "slug": "Learning-unions-of-orthonormal-bases-with-singular-Lesage-Gribonval",
            "title": {
                "fragments": [],
                "text": "Learning unions of orthonormal bases with thresholded singular value decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that it is possible to design an iterative learning algorithm that produces a dictionary with the required structure, and how well the learning algorithm recovers dictionaries that may or may not have the necessary structure is assessed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31052092"
                        ],
                        "name": "J. Murray",
                        "slug": "J.-Murray",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Murray",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876925"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "Bhaskar",
                            "lastName": "Rao",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691592"
                        ],
                        "name": "K. Engan",
                        "slug": "K.-Engan",
                        "structuredName": {
                            "firstName": "Kjersti",
                            "lastName": "Engan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Engan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1218517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "306de9c553695822ae9e6de044b6856baf0cce7d",
            "isKey": false,
            "numCitedBy": 835,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial 25 words or less), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations. Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error)."
            },
            "slug": "Dictionary-Learning-Algorithms-for-Sparse-Kreutz-Delgado-Murray",
            "title": {
                "fragments": [],
                "text": "Dictionary Learning Algorithms for Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave negative log priors, showing improved performance over other independent component analysis methods."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742248"
                        ],
                        "name": "P. Moulin",
                        "slug": "P.-Moulin",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Moulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Moulin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108516799"
                        ],
                        "name": "Juan Liu",
                        "slug": "Juan-Liu",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8325093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c8c82ea1a458b821993865bc9d6a575a93576fb",
            "isKey": false,
            "numCitedBy": 566,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on universal and minimax wavelet shrinkage and thresholding methods has demonstrated near-ideal estimation performance in various asymptotic frameworks. However, image processing practice has shown that universal thresholding methods are outperformed by simple Bayesian estimators assuming independent wavelet coefficients and heavy-tailed priors such as generalized Gaussian distributions (GGDs). In this paper, we investigate various connections between shrinkage methods and maximum a posteriori (MAP) estimation using such priors. In particular, we state a simple condition under which MAP estimates are sparse. We also introduce a new family of complexity priors based upon Rissanen's universal prior on integers. One particular estimator in this class outperforms conventional estimators based on earlier applications of the minimum description length (MDL) principle. We develop analytical expressions for the shrinkage rules implied by GGD and complexity priors. This allows us to show the equivalence between universal hard thresholding, MAP estimation using a very heavy-tailed GGD, and MDL estimation using one of the new complexity priors. Theoretical analysis supported by numerous practical experiments shows the robustness of some of these estimates against mis-specifications of the prior-a basic concern in image processing applications."
            },
            "slug": "Analysis-of-Multiresolution-Image-Denoising-Schemes-Moulin-Liu",
            "title": {
                "fragments": [],
                "text": "Analysis of Multiresolution Image Denoising Schemes Using Generalized Gaussian and Complexity Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper investigates various connections between shrinkage methods and maximum a posteriori (MAP) estimation using such priors, and introduces a new family of complexity priors based upon Rissanen's universal prior on integers."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755101"
                        ],
                        "name": "V. Temlyakov",
                        "slug": "V.-Temlyakov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Temlyakov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Temlyakov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "The denoised image is, thus, given by [22], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "work established that those approximation techniques can be quite accurate if the solution is sparse enough to begin with [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14813938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7236864d8c2f62defea559465462c43a4b4b6b47",
            "isKey": false,
            "numCitedBy": 2166,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal."
            },
            "slug": "Stable-recovery-of-sparse-overcomplete-in-the-of-Donoho-Elad",
            "title": {
                "fragments": [],
                "text": "Stable recovery of sparse overcomplete representations in the presence of noise"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system and shows that similar stability is also available using the basis and the matching pursuit algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876925"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "Bhaskar",
                            "lastName": "Rao",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16705932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a09a81393b824bf7b2efe38e2049c3dc9941293",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for data-driven learning of domain-specific over complete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur- concave negative log-priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen dictionary. The elements of the dictionary can be interpreted as 'concepts,' features or 'words' capable of succinct expression of events encountered in the environment. This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries, but not necessarily as succinct as one entry. To learn an environmentally-adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS, an affine scaling transformation (ACT)-like sparse signal representation algorithm recently developed at UCSD, and an update of the dictionary using these sparse representations."
            },
            "slug": "FOCUSS-based-dictionary-learning-algorithms-Kreutz-Delgado-Rao",
            "title": {
                "fragments": [],
                "text": "FOCUSS-based dictionary learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "To learn an environmentally-adapted dictionary capable of concise expression of signals generated by the environment, this work develops algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS, an affine scaling transformation (ACT)-like sparse signal representation algorithm recently developed at UCSD, and an update of the dictionary using these sparse representations."
            },
            "venue": {
                "fragments": [],
                "text": "SPIE Optics + Photonics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145394553"
                        ],
                        "name": "Jean-Luc Starck",
                        "slug": "Jean-Luc-Starck",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Starck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Luc Starck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8284003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54b1e49e30ec32bfb7cc5d1c88cec7f96a320b8e",
            "isKey": false,
            "numCitedBy": 1682,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given, as follows. We present approximate digital implementations of two new mathematical transforms, namely, the ridgelet transform and the curvelet transform. Our implementations offer exact reconstruction, stability against perturbations, ease of implementation, and low computational complexity. We apply these digital transforms to the denoising of some standard images embedded in white noise. In the tests reported here, simple thresholding of the curvelet coefficients is very competitive with 'state of the art' techniques based on wavelets, including thresholding of decimated or undecimated wavelet transforms and also including tree-based Bayesian posterior mean methods. Moreover, the curvelet reconstructions exhibit higher perceptual quality than wavelet-based reconstructions, offering visually sharper images and, in particular, higher quality recovery of edges and of faint linear and curvilinear features."
            },
            "slug": "The-curvelet-transform-for-image-denoising-Starck-Cand\u00e8s",
            "title": {
                "fragments": [],
                "text": "The curvelet transform for image denoising"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In the tests reported here, simple thresholding of the curvelet coefficients is very competitive with 'state of the art' techniques based on wavelets, including thresholded of decimated or undecimated wavelet transforms and also including tree-based Bayesian posterior mean methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11995267,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8cc9918add61ee5ef3b848aba9646169cc5e364e",
            "isKey": false,
            "numCitedBy": 4682,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We attempt to recover a function of unknown smoothness from noisy sampled data. We introduce a procedure, SureShrink, that suppresses noise by thresholding the empirical wavelet coefficients. The thresholding is adaptive: A threshold level is assigned to each dyadic resolution level by the principle of minimizing the Stein unbiased estimate of risk (Sure) for threshold estimates. The computational effort of the overall procedure is order N \u00b7 log(N) as a function of the sample size N. SureShrink is smoothness adaptive: If the unknown function contains jumps, then the reconstruction (essentially) does also; if the unknown function has a smooth piece, then the reconstruction is (essentially) as smooth as the mother wavelet will allow. The procedure is in a sense optimally smoothness adaptive: It is near minimax simultaneously over a whole interval of the Besov scale; the size of this interval depends on the choice of mother wavelet. We know from a previous paper by the authors that traditional smoot..."
            },
            "slug": "Adapting-to-Unknown-Smoothness-via-Wavelet-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Adapting to Unknown Smoothness via Wavelet Shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610924"
                        ],
                        "name": "A. Bruckstein",
                        "slug": "A.-Bruckstein",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Bruckstein",
                            "middleNames": [
                                "Marcel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruckstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "The K-SVD proposes an iterative algorithm designed to handle the above task effectively [36], [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "K-SVD algorithm [36], [37] because of its simplicity and efficiency for this task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "This idea of learning a dictionary that yields sparse representations for a set of training image-patches has been studied in a sequence of works [30]\u2013[37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "This way, the value of is guaranteed to drop per an update of each dictionary atom, and along with this update, the representation coefficients change as well (see [36] and [37] for more details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1491283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb2d7003623a31ffabba86841cf05b8b57030796",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-uniqueness-of-overcomplete-dictionaries,-and-Aharon-Elad",
            "title": {
                "fragments": [],
                "text": "On the uniqueness of overcomplete dictionaries, and a practical way to retrieve them"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3082665"
                        ],
                        "name": "Boaz Matalon",
                        "slug": "Boaz-Matalon",
                        "structuredName": {
                            "firstName": "Boaz",
                            "lastName": "Matalon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boaz Matalon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767788"
                        ],
                        "name": "M. Zibulevsky",
                        "slug": "M.-Zibulevsky",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Zibulevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zibulevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15875448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "241614f1321b0cd16c482516984467fce5321a61",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the image denoising problem. One common approach found in the literature involves manipulating the coefficients in the transform domain, e.g. shrinkage, followed by the inverse transform. Several advanced methods that model the inter-coefficient dependencies were developed recently, and were shown to yield significant improvement. However, these methods operate on the transform domain error rather than on the image domain one. These errors are in general entirely different for redundant transforms. In this work we propose a novel denoising method, based on the Basis-Pursuit Denoising (BPDN). Our method combines the image domain error with the transform domain dependency structure, resulting in a general objective function, applicable for any wavelet-like transform. We focus here on the Contourlet Transform (CT) and on a redundant version of it, both relatively new transforms designed to sparsely represent images. The performance of our new method is compared favorably with the state-of-the-art method of Bayesian Least Squares Gaussian Scale Mixture (BLS-GSM), which we adapted to the CT as well, with further improvements still to come."
            },
            "slug": "Improved-denoising-of-images-using-modelling-of-a-Matalon-Elad",
            "title": {
                "fragments": [],
                "text": "Improved denoising of images using modelling of a redundant contourlet transform"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a novel denoising method, based on the Basis-Pursuit Denoising (BPDN), which combines the image domain error with the transform domain dependency structure, resulting in a general objective function, applicable for any wavelet-like transform."
            },
            "venue": {
                "fragments": [],
                "text": "SPIE Optics + Photonics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683740"
                        ],
                        "name": "A. Chambolle",
                        "slug": "A.-Chambolle",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Chambolle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chambolle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26373808"
                        ],
                        "name": "R. DeVore",
                        "slug": "R.-DeVore",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "DeVore",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. DeVore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3361418"
                        ],
                        "name": "N. Lee",
                        "slug": "N.-Lee",
                        "structuredName": {
                            "firstName": "Namgyun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32841039"
                        ],
                        "name": "B. Lucier",
                        "slug": "B.-Lucier",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Lucier",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lucier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 774736,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "48d9d162588aa024be03bdea1a98823c389de6d2",
            "isKey": false,
            "numCitedBy": 791,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the relationship between wavelet-based image processing algorithms and variational problems. Algorithms are derived as exact or approximate minimizers of variational problems; in particular, we show that wavelet shrinkage can be considered the exact minimizer of the following problem. Given an image F defined on a square I, minimize over all g in the Besov space B(1)(1)(L (1)(I)) the functional |F-g|(L2)(I)(2)+lambda|g|(B(1)(1 )(L(1(I)))). We use the theory of nonlinear wavelet image compression in L(2)(I) to derive accurate error bounds for noise removal through wavelet shrinkage applied to images corrupted with i.i.d., mean zero, Gaussian noise. A new signal-to-noise ratio (SNR), which we claim more accurately reflects the visual perception of noise in images, arises in this derivation. We present extensive computations that support the hypothesis that near-optimal shrinkage parameters can be derived if one knows (or can estimate) only two parameters about an image F: the largest alpha for which FinEpsilon(q)(alpha )(L(q)(I)),1/q=alpha/2+1/2, and the norm |F|B(q)(alpha)(L(q)(I)). Both theoretical and experimental results indicate that our choice of shrinkage parameters yields uniformly better results than Donoho and Johnstone's VisuShrink procedure; an example suggests, however, that Donoho and Johnstone's SureShrink method, which uses a different shrinkage parameter for each dyadic level, achieves a lower error than our procedure."
            },
            "slug": "Nonlinear-wavelet-image-processing:-variational-and-Chambolle-DeVore",
            "title": {
                "fragments": [],
                "text": "Nonlinear wavelet image processing: variational problems, compression, and noise removal through wavelet shrinkage"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Extensive computations are presented that support the hypothesis that near-optimal shrinkage parameters can be derived if one knows (or can estimate) only two parameters about an image F: the largest alpha for which FinEpsilon(q)(alpha )(L( q)(I)),1/q=alpha/2+1/2, and the norm |F|B(q) alpha)(L(Q)(I))."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1845031"
                        ],
                        "name": "E. L. Pennec",
                        "slug": "E.-L.-Pennec",
                        "structuredName": {
                            "firstName": "Erwan",
                            "lastName": "Pennec",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. L. Pennec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13017359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "974b50edc233d80f93012a2663e3cd2f82403868",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding efficient geometric representations of images is a central issue to improving image compression and noise removal algorithms. We introduce bandelet orthogonal bases and frames that are adapted to the geometric regularity of an image. Images are approximated by finding a best bandelet basis or frame that produces a sparse representation. For functions that are uniformly regular outside a set of edge curves that are geometrically regular, the main theorem proves that bandelet approximations satisfy an optimal asymptotic error decay rate. A bandelet image compression scheme is derived. For computational applications, a fast discrete bandelet transform algorithm is introduced, with a fast best basis search which preserves asymptotic approximation and coding error decay rates."
            },
            "slug": "Bandelet-Image-Approximation-and-Compression-Pennec-Mallat",
            "title": {
                "fragments": [],
                "text": "Bandelet Image Approximation and Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "For functions that are uniformly regular outside a set of edge curves that are geometrically regular, the main theorem proves that bandelet approximations satisfy an optimal asymptotic error decay rate."
            },
            "venue": {
                "fragments": [],
                "text": "Multiscale Model. Simul."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 235072,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "85a1725bfd3b4a2d3fe9a7272d66ebf03c016fed",
            "isKey": false,
            "numCitedBy": 765,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The classical solution to the noise removal problem is the Wiener filter, which utilizes the second-order statistics of the Fourier decomposition. Subband decompositions of natural images have significantly non-Gaussian higher-order point statistics; these statistics capture image properties that elude Fourier-based techniques. We develop a Bayesian estimator that is a natural extension of the Wiener solution, and that exploits these higher-order statistics. The resulting nonlinear estimator performs a \"coring\" operation. We provide a simple model for the subband statistics, and use it to develop a semi-blind noise removal algorithm based on a steerable wavelet pyramid."
            },
            "slug": "Noise-removal-via-Bayesian-wavelet-coring-Simoncelli-Adelson",
            "title": {
                "fragments": [],
                "text": "Noise removal via Bayesian wavelet coring"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A Bayesian estimator is developed that is a natural extension of the Wiener solution, and that exploits higher-order statistics of the Fourier decomposition to develop a semi-blind noise removal algorithm based on a steerable wavelet pyramid."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd IEEE International Conference on Image Processing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14009768,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "16eafce11acdd0034a3f7b76987aeab54ae9c09a",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "We attempt to recover an unknown function from noisy, sampled data. Using orthonormal bases of compactly supported wavelets, we develop a nonlinear method which works in the wavelet domain by simple nonlinear shrinkage of the empirical wavelet coefficients. The shrinkage can be tuned to be nearly minimax over any member of a wide range of Triebel- and Besov-type smoothness constraints and asymptotically minimax over Besov bodies with p < q. Linear estimates cannot achieve even the minimax rates over Triebel and Besov classes with p < 2, so the method can significantly outperform every linear method (e.g., kernel, smoothing spline, sieve) in a minimax sense. Variants of our method based on simple threshold nonlinear estimators are nearly minimax. Our method possesses the interpretation of spatial adaptivity; it reconstructs using a kernel which may vary in shape and bandwidth from point to point, depending on the data. Least favorable distributions for certain of the Triebel and Besov scales generate objects with sparse wavelet transforms. Many real objects have similarly sparse transforms, which suggests that these minimax results are relevant for practical problems. Sequels to this paper, which was first drafted in November 1990, discuss practical implementation, spatial adaptation properties, universal near minimaxity and applications to inverse problems."
            },
            "slug": "Minimax-estimation-via-wavelet-shrinkage-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Minimax estimation via wavelet shrinkage"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A nonlinear method which works in the wavelet domain by simple nonlinear shrinkage of the empirical wavelet coefficients is developed, andVariants of this method based on simple threshold nonlinear estimators are nearly minimax."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 263
                            }
                        ],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15209351,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "bf9c49c3ab09924337cf302494a1299cd7cdc200",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a simple \\Horizon Model\" for the problem of recovering an image from noisy data; in this model the image has an edge with fi-Holder regularity. Adopting the viewpoint of computational harmonic analysis, we develop an overcomplete collection of atoms called wedgelets, dyadically organized indicator functions with a variety of locations, scales, and orientations. The wedgelet representation provides nearly-optimal representations of objects in the Horizon model, as measured by minimax description length. We show how to rapidly compute a wedgelet approximation to noisy data by flnding a special edgelet-decorated recursive partition which minimizes a complexity-penalized sum of squares. This estimate, using su-cient sub-pixel resolution, achieves nearly the minimax mean-squared error in the Horizon Model. In fact, the method is adaptive in the sense that it achieves nearly the minimax risk for any value of the unknown degree of regularity of the Horizon, 1\u2022 fi\u2022 2. Wedgelet analysis and de-noising may be used successfully outside the Horizon model. We study images modelled as indicators of star-shaped sets with smooth bound- aries and show that complexity-penalized wedgelet partitioning achieves nearly the minimax risk in that setting also."
            },
            "slug": "Wedgelets:-nearly-minimax-estimation-of-edges-Donoho",
            "title": {
                "fragments": [],
                "text": "Wedgelets: nearly minimax estimation of edges"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An overcomplete collection of atoms called wedgelets, dyadically organized indicator functions with a variety of locations, scales, and orientations are developed, which provides nearly-optimal representations of objects in the Horizon model, as measured by minimax description length."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1845031"
                        ],
                        "name": "E. L. Pennec",
                        "slug": "E.-L.-Pennec",
                        "structuredName": {
                            "firstName": "Erwan",
                            "lastName": "Pennec",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. L. Pennec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [ 16 ], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2112160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67b1cc159aacf2f30239577e6cf7a741b64f31e0",
            "isKey": false,
            "numCitedBy": 929,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new class of bases, called bandelet bases, which decompose the image along multiscale vectors that are elongated in the direction of a geometric flow. This geometric flow indicates directions in which the image gray levels have regular variations. The image decomposition in a bandelet basis is implemented with a fast subband-filtering algorithm. Bandelet bases lead to optimal approximation rates for geometrically regular images. For image compression and noise removal applications, the geometric flow is optimized with fast algorithms so that the resulting bandelet basis produces minimum distortion. Comparisons are made with wavelet image compression and noise-removal algorithms."
            },
            "slug": "Sparse-geometric-image-representations-with-Pennec-Mallat",
            "title": {
                "fragments": [],
                "text": "Sparse geometric image representations with bandelets"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new class of bases are introduced, called bandelet bases, which decompose the image along multiscale vectors that are elongated in the direction of a geometric flow, which leads to optimal approximation rates for geometrically regular images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818391"
                        ],
                        "name": "G. Kerkyacharian",
                        "slug": "G.-Kerkyacharian",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Kerkyacharian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kerkyacharian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145897897"
                        ],
                        "name": "D. Picard",
                        "slug": "D.-Picard",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Picard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12737710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b20b4095236f461ab067d0c23bc9653d3d1f9c0",
            "isKey": false,
            "numCitedBy": 1647,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "Much recent effort has sought asymptotically minimax methods for recovering infinite dimensional objects-curves, densities, spectral densities, images-from noisy data. A now rich and complex body of work develops nearly or exactly minimax estimators for an array of interesting problems. Unfortunately, the results have rarely moved into practice, for a variety of reasons-among them being similarity to known methods, computational intractability and lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount \u221a(2 log n) /\u221an. The proposal differs from those in current use, is computationally practical and is spatially adaptive; it thus avoids several of the previous objections. Further, the method is nearly minimax both for a wide variety of loss functions-pointwise error, global error measured in L p -norms, pointwise and global error in estimation of derivatives-and for a wide range of smoothness classes, including standard Holder and Sobolev classes, and bounded variation. This is a much broader near optimality than anything previously proposed: we draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity"
            },
            "slug": "Wavelet-Shrinkage:-Asymptopia-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Wavelet Shrinkage: Asymptopia?"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount \u221a(2 log n) /\u221an and draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "While this problem is, in general, very hard to solve, the matching and the basis pursuit algorithms can be used quite effectively [20]\u2013[22] to get an approximated solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "The denoised image is, thus, given by [22], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "In parallel, the introduction of the matching pursuit [20], [21] and the basis pursuit denoising [22] gave rise to the ability to address the image denoising problem as a direct sparse decomposition technique over redundant dictionaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9733,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1834451"
                        ],
                        "name": "M. Do",
                        "slug": "M.-Do",
                        "structuredName": {
                            "firstName": "Minh",
                            "lastName": "Do",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Do"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145876744"
                        ],
                        "name": "M. Vetterli",
                        "slug": "M.-Vetterli",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Vetterli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vetterli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 248
                            }
                        ],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10539752,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e3207926ad8213a452c5ab0e99cde0d0066509c6",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Burt and Adelson (1983) introduced the Laplacian pyramid (LP) as a multiresolution representation for images. We study the LP using the frame theory, and this reveals that the usual reconstruction is suboptimal. We show that the LP with orthogonal filters is a tight frame, and thus, the optimal linear reconstruction using the dual frame operator has a simple structure that is symmetric with the forward transform. In more general cases, we propose an efficient filterbank (FB) for the reconstruction of the LP using projection that leads to a proved improvement over the usual method in the presence of noise. Setting up the LP as an oversampled FB, we offer a complete parameterization of all synthesis FBs that provide perfect reconstruction for the LP. Finally, we consider the situation where the LP scheme is iterated and derive the continuous-domain frames associated with the LP."
            },
            "slug": "Framing-pyramids-Do-Vetterli",
            "title": {
                "fragments": [],
                "text": "Framing pyramids"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the LP with orthogonal filters is a tight frame, and thus, the optimal linear reconstruction using the dual frame operator has a simple structure that is symmetric with the forward transform."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6254191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42d906c733f273109c0ed716a5ef6e2a379beb26",
            "isKey": false,
            "numCitedBy": 1255,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be sparser, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual cortex. Previous work has focused on finding the best representation of a signal using a fixed overcomplete basis (or dictionary). We present an algorithm for learning an overcomplete basis by viewing it as probabilistic model of the observed data. We show that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency. This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "slug": "Learning-Overcomplete-Representations-Lewicki-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency and provide a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9238746,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "601ba02449e03c7b36e882b80de24c42b887aa4d",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a model problem of recovering a function $f(x_1,x_2)$ from noisy Radon data. The function $f$ to be recovered is assumed smooth apart from a discontinuity along a $C^2$ curve, that is, an edge. We use the continuum white-noise model, with noise level $\\varepsilon$. \n \nTraditional linear methods for solving such inverse problems behave poorly in the presence of edges. Qualitatively, the reconstructions are blurred near the edges; quantitatively, they give in our model mean squared errors (MSEs) that tend to zero with noise level $\\varepsilon$ only as $O(\\varepsilon^{1/2})$ as $\\varepsilon\\to 0$. A recent innovation--nonlinear shrinkage in the wavelet domain--visually improves edge sharpness and improves MSE convergence to $O(\\varepsilon^{2/3})$. However, as we show here, this rate is not optimal. \n \nIn fact, essentially optimal performance is obtained by deploying the recently-introduced tight frames of curvelets in this setting. Curvelets are smooth, highly anisotropic elements ideally suited for detecting and synthesizing curved edges. To deploy them in the Radon setting, we construct a curvelet-based biorthogonal decomposition of the Radon operator and build \"curvelet shrinkage\" estimators based on thresholding of the noisy curvelet coefficients. In effect, the estimator detects edges at certain locations and orientations in the Radon domain and automatically synthesizes edges at corresponding locations and directions in the original domain. \n \nWe prove that the curvelet shrinkage can be tuned so that the estimator will attain, within logarithmic factors, the MSE $O(\\varepsilon^{4/5})$ as noise level $\\varepsilon\\to 0$. This rate of convergence holds uniformly over a class of functions which are $C^2$ except for discontinuities along $C^2$ curves, and (except for log terms) is the minimax rate for that class. Our approach is an instance of a general strategy which should apply in other inverse problems; we sketch a deconvolution example."
            },
            "slug": "Recovering-edges-in-ill-posed-inverse-problems:-of-Cand\u00e8s-Donoho",
            "title": {
                "fragments": [],
                "text": "Recovering edges in ill-posed inverse problems: optimality of curvelet frames"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "It is proved that the curvelet shrinkage can be tuned so that the estimator will attain, within logarithmic factors, the MSE $O(\\varepsilon^{4/5})$ as noise level $\\varePSilon\\to 0$."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145761835"
                        ],
                        "name": "E. Haber",
                        "slug": "E.-Haber",
                        "structuredName": {
                            "firstName": "Eldad",
                            "lastName": "Haber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Haber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145615086"
                        ],
                        "name": "Luis Tenorio",
                        "slug": "Luis-Tenorio",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Tenorio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Tenorio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121534787,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "477786b0121bfeb50b017b58f06fffde4a85aa4e",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the solution of a distributed parameter estimation problem where the data are contaminated by noise. A common approach to solve such a problem is to use Tikhonov style regularization; however, it is not always clear what type of regularization penalty should be used for a given problem as different regularization operators may yield very different solutions. Here we use supervised learning techniques to determine a regularization functional given a training set of feasible solutions. Our approach leads to a constraint optimization problem that we solve using inexact sequential quadratic programming type methods. We illustrate the methodology with two examples."
            },
            "slug": "Learning-regularization-functionals\u2014a-supervised-Haber-Tenorio",
            "title": {
                "fragments": [],
                "text": "Learning regularization functionals\u2014a supervised training approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "This idea of learning a dictionary that yields sparse representations for a set of training image-patches has been studied in a sequence of works [30]\u2013[37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": false,
            "numCitedBy": 3573,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 239520,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "12e5939a80ec59f81214d7e729c577c350af9501",
            "isKey": false,
            "numCitedBy": 7815,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, RiskShrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink mimics the performance of an oracle for selective wavelet reconstruction as well as it is possible to do so. A new inequality in multivariate normal decision theory which we call the oracle inequality shows that attained performance differs from ideal performance by at most a factor of approximately 2 log n, where n is the sample size. Moreover no estimator can give a better guarantee than this. Within the class of spatially adaptive procedures, RiskShrink is essentially optimal. Relying only on the data, it comes within a factor log 2 n of the performance of piecewise polynomial and variableknot spline methods equipped with an oracle. In contrast, it is unknown how or if piecewise polynomial methods could be made to function this well when denied access to an oracle and forced to rely on data alone."
            },
            "slug": "Ideal-spatial-adaptation-by-wavelet-shrinkage-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Ideal spatial adaptation by wavelet shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34817070"
                        ],
                        "name": "Y. C. Pati",
                        "slug": "Y.-C.-Pati",
                        "structuredName": {
                            "firstName": "Yagyensh",
                            "lastName": "Pati",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. C. Pati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368882"
                        ],
                        "name": "R. Rezaiifar",
                        "slug": "R.-Rezaiifar",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Rezaiifar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rezaiifar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426392"
                        ],
                        "name": "P. Krishnaprasad",
                        "slug": "P.-Krishnaprasad",
                        "structuredName": {
                            "firstName": "Perinkulam",
                            "lastName": "Krishnaprasad",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Krishnaprasad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "In this work, we will make use mainly of the orthonormal matching pursuit (OMP) because of its simplicity [21] and efficiency."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "Solving this using the orthonormal matching pursuit [21] is easy, gathering one atom at a time, and"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "In parallel, the introduction of the matching pursuit [20], [21] and the basis pursuit denoising [22] gave rise to the ability to address the image denoising problem as a direct sparse decomposition technique over redundant dictionaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16513805,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "2071f3ee9ec4d17250b00626d55e47bf75ae2726",
            "isKey": false,
            "numCitedBy": 4151,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively.<<ETX>>"
            },
            "slug": "Orthogonal-matching-pursuit:-recursive-function-to-Pati-Rezaiifar",
            "title": {
                "fragments": [],
                "text": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual at every step and thereby leads to improved convergence is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 27th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043078"
                        ],
                        "name": "R. Eslami",
                        "slug": "R.-Eslami",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Eslami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687782"
                        ],
                        "name": "H. Radha",
                        "slug": "H.-Radha",
                        "structuredName": {
                            "firstName": "Hayder",
                            "lastName": "Radha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Radha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12730054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ff1a03acb3a69bfd049db8490a724fca0b7d25e",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Most subsampled filter banks lack the feature of translation invariance, which is an important characteristic in denoising applications. In this paper, we study and develop new methods to convert a general multichannel, multidimensional filter bank to a corresponding translation-invariant (TI) framework. In particular, we propose a generalized algorithme agrave trous, which is an extension of the algorithme agrave trous introduced for 1-D wavelet transforms. Using the proposed algorithm, as well as incorporating modified versions of directional filter banks, we construct the TI contourlet transform (TICT). To reduce the high redundancy and complexity of the TICT, we also introduce semi-translation-invariant contourlet transform (STICT). Then, we employ an adapted bivariate shrinkage scheme to the STICT to achieve an efficient image denoising approach. Our experimental results demonstrate the benefits and potential of the proposed denoising approach. Complexity analysis and efficient realization of the proposed TI schemes are also presented"
            },
            "slug": "Translation-Invariant-Contourlet-Transform-and-Its-Eslami-Radha",
            "title": {
                "fragments": [],
                "text": "Translation-Invariant Contourlet Transform and Its Application to Image Denoising"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper studies and develops new methods to convert a general multichannel, multidimensional filter bank to a corresponding translation-invariant (TI) framework, and proposes a generalized algorithme agrave trous, which is an extension of the algorithms introduced for 1-D wavelet transforms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": false,
            "numCitedBy": 8850,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788107"
                        ],
                        "name": "J. Tropp",
                        "slug": "J.-Tropp",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tropp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tropp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788107"
                        ],
                        "name": "J. Tropp",
                        "slug": "J.-Tropp",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tropp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tropp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Once this is set, we will discuss how local treatment on image patches turns into a global prior in a Bayesian reconstruction framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1454355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff71f4a25b0a4c97b20ff49ced0e94cdcf6784e9",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "Subset selection and sparse approximation problems request a good approximation of an input signal using a linear combination of elementary signals, yet they stipulate that the approximation may only involve a few of the elementary signals. This class of problems arises throughout electrical engineering, applied mathematics and statistics, but small theoretical progress has been made over the last fifty years. Subset selection and sparse approximation both admit natural convex relaxations, but the literature contains few results on the behavior of these relaxations for general input signals. This report demonstrates that the solution of the convex program frequently coincides with the solution of the original approximation problem. The proofs depend essentially on geometric properties of the ensemble of elementary signals. The results are powerful because sparse approximation problems are combinatorial, while convex programs can be solved in polynomial time with standard software. Comparable new results for a greedy algorithm, Orthogonal Matching Pursuit, are also stated. This report should have a major practical impact because the theory applies immediately to many real-world signal processing problems."
            },
            "slug": "JUST-RELAX:-CONVEX-PROGRAMMING-METHODS-FOR-SUBSET-Tropp-Tropp",
            "title": {
                "fragments": [],
                "text": "JUST RELAX: CONVEX PROGRAMMING METHODS FOR SUBSET SELECTION AND SPARSE APPROXIMATION"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that the solution of the convex program frequently coincides with the solutionof the original approximation problem, and comparable new results for a greedy algorithm, Orthogonal Matching Pursuit, are stated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 149055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cc257b0c7db92f90c3224c35df7b8e85f57a090",
            "isKey": false,
            "numCitedBy": 8941,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "Donoho and Johnstone (1994) proposed a method for reconstructing an unknown function f on [0,1] from noisy data d/sub i/=f(t/sub i/)+/spl sigma/z/sub i/, i=0, ..., n-1,t/sub i/=i/n, where the z/sub i/ are independent and identically distributed standard Gaussian random variables. The reconstruction f/spl circ/*/sub n/ is defined in the wavelet domain by translating all the empirical wavelet coefficients of d toward 0 by an amount /spl sigma//spl middot//spl radic/(2log (n)/n). The authors prove two results about this type of estimator. [Smooth]: with high probability f/spl circ/*/sub n/ is at least as smooth as f, in any of a wide variety of smoothness measures. [Adapt]: the estimator comes nearly as close in mean square to f as any measurable estimator can come, uniformly over balls in each of two broad scales of smoothness classes. These two properties are unprecedented in several ways. The present proof of these results develops new facts about abstract statistical inference and its connection with an optimal recovery model. >"
            },
            "slug": "De-noising-by-soft-thresholding-Donoho",
            "title": {
                "fragments": [],
                "text": "De-noising by soft-thresholding"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "The authors prove two results about this type of estimator that are unprecedented in several ways: with high probability f/spl circ/*/sub n/ is at least as smooth as f, in any of a wide variety of smoothness measures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "One reason to turn to redundant representations was the desire to have the shift invariance property [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16806403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c7aaa61b15ea65b68055125e8bbd0189c464eb8",
            "isKey": false,
            "numCitedBy": 1880,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "De-Noising with the traditional (orthogonal, maximally-decimated) wavelet transform sometimes exhibits visual artifacts; we attribute some of these\u2014for example, Gibbs phenomena in the neighborhood of discontinuities\u2014to the lack of translation invariance of the wavelet basis. One method to suppress such artifacts, termed \u201ccycle spinning\u201d by Coifman, is to \u201caverage out\u201d the translation dependence. For a range of shifts, one shifts the data (right or left as the case may be), De-Noises the shifted data, and then unshifts the de-noised data. Doing this for each of a range of shifts, and averaging the several results so obtained, produces a reconstruction subject to far weaker Gibbs phenomena than thresholding based De-Noising using the traditional orthogonal wavelet transform."
            },
            "slug": "Translation-Invariant-De-Noising-Coifman-Donoho",
            "title": {
                "fragments": [],
                "text": "Translation-Invariant De-Noising"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A reconstruction subject to far weaker Gibbs phenomena than thresholding based De-Noising using the traditional orthogonal wavelet transform is produced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115919167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "832f6a52920db02135d751c46c026a78ad16128c",
            "isKey": false,
            "numCitedBy": 1598,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces new tight frames of curvelets to address the problem of finding optimally sparse representations of objects with discontinuities along piecewise C2 edges. Conceptually, the curvelet transform is a multiscale pyramid with many directions and positions at each length scale, and needle\u2010shaped elements at fine scales. These elements have many useful geometric multiscale features that set them apart from classical multiscale representations such as wavelets. For instance, curvelets obey a parabolic scaling relation which says that at scale 2\u2212j, each element has an envelope that is aligned along a \u201cridge\u201d of length 2\u2212j/2 and width 2\u2212j."
            },
            "slug": "New-tight-frames-of-curvelets-and-optimal-of-with-Cand\u00e8s-Donoho",
            "title": {
                "fragments": [],
                "text": "New tight frames of curvelets and optimal representations of objects with piecewise C2 singularities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper introduces new tight frames of curvelets to address the problem of finding optimally sparse representations of objects with discontinuities along piecewise C2 edges."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43701174,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8515604037444b3f079a9d328b0c560f33da0a19",
            "isKey": false,
            "numCitedBy": 1427,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >"
            },
            "slug": "Shiftable-multiscale-transforms-Simoncelli-Freeman",
            "title": {
                "fragments": [],
                "text": "Shiftable multiscale transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two examples of jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored and the usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 255
                            }
                        ],
                        "text": "For example, assuming a spatial smoothness-based Markov random field prior of a specific structure, one can still question (and, thus, train) the derivative filters to apply on the image, and the robust function to use in weighting these filters\u2019 outcome [27]\u2013[29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7423086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adc69aebcee4af29e115f54e3a5f210c5cc7dadc",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This article addresses two important themes in early visual computation: it presents a novel theory for learning the universal statistics of natural images, and, it proposes a general framework of designing reaction-diffusion equations for image processing. We studied the statistics of natural images including the scale invariant properties, then generic prior models were learned to duplicate the observed statistics, based on minimax entropy theory. The resulting Gibbs distributions have potentials of the form U(I; /spl Lambda/, S)=/spl Sigma//sub /spl alpha/=1//sup k//spl Sigma//sub x,y//spl lambda//sup (/spl alpha/)/((F/sup (/spl alpha/)/*I)(x,y)) with S={F/sup (1)/, F/sup (2)/,...,F/sup (K)/} being a set of filters and /spl Lambda/={/spl lambda//sup (1)/(),/spl lambda//sup (2)/(),...,/spl lambda//sup (K)/()} the potential functions. The learned Gibbs distributions confirm and improve the form of existing prior models such as line-process, but, in contrast to all previous models, inverted potentials were found to be necessary. We find that the partial differential equations given by gradient descent on U(I; /spl Lambda/, S) are essentially reaction-diffusion equations, where the usual energy terms produce anisotropic diffusion, while the inverted energy terms produce reaction associated with pattern formation, enhancing preferred image features. We illustrate how these models can be used for texture pattern rendering, denoising, image enhancement, and clutter removal by careful choice of both prior and data models of this type, incorporating the appropriate features."
            },
            "slug": "Prior-Learning-and-Gibbs-Reaction-Diffusion-Zhu-Mumford",
            "title": {
                "fragments": [],
                "text": "Prior Learning and Gibbs Reaction-Diffusion"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that the partial differential equations given by gradient descent on U(I; /spl Lambda/, S) are essentially reaction-diffusion equations, where the usualEnergy terms produce anisotropic diffusion, while the inverted energy terms produce reaction associated with pattern formation, enhancing preferred image features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143851999"
                        ],
                        "name": "M. Jansen",
                        "slug": "M.-Jansen",
                        "structuredName": {
                            "firstName": "Maarten",
                            "lastName": "Jansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jansen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "At first, sparsity of the unitary wavelet coefficients was considered, leading to the celebrated shrinkage algorithm [1]\u2013[9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116900477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "539fc4735f52e6acc7549142b12d29a2759ad8b8",
            "isKey": false,
            "numCitedBy": 417,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Wavelets and wavelet thresholding.- The minimum mean squared error threshold.- Estimating the minimum MSE threshold.- Thresholding and GCV applicability in more realistic situations.- Bayesian correction with geometrical priors for image noise reduction.- Smoothing non-equidistantly spaced data using second generation wavelets and thresholding."
            },
            "slug": "Noise-Reduction-by-Wavelet-Thresholding-Jansen",
            "title": {
                "fragments": [],
                "text": "Noise Reduction by Wavelet Thresholding"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Wavelets and wavelet thresholding Smoothing non-equidistantly spaced data using second generation wavelets and thresholding and Bayesian correction with geometrical priors for image noise reduction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 315
                            }
                        ],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29187618,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "993b1083455b5c4d631eaf44f230b061994e75c3",
            "isKey": false,
            "numCitedBy": 3378,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation. Steerable filters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation. The authors show how to design and steer the filters and present examples of their use in the analysis of orientation and phase, angularly adaptive filtering, edge detection, and shape from shading. One can also build a self-similar steerable pyramid representation. The same concepts can be generalized to the design of 3-D steerable filters. >"
            },
            "slug": "The-Design-and-Use-of-Steerable-Filters-Freeman-Adelson",
            "title": {
                "fragments": [],
                "text": "The Design and Use of Steerable Filters"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691592"
                        ],
                        "name": "K. Engan",
                        "slug": "K.-Engan",
                        "structuredName": {
                            "firstName": "Kjersti",
                            "lastName": "Engan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Engan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3046011"
                        ],
                        "name": "S. O. Aase",
                        "slug": "S.-O.-Aase",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Aase",
                            "middleNames": [
                                "Ole"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. O. Aase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31557075"
                        ],
                        "name": "J. H. Hus\u00f8y",
                        "slug": "J.-H.-Hus\u00f8y",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hus\u00f8y",
                            "middleNames": [
                                "H\u00e5kon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hus\u00f8y"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33097614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "684732677d91a93b115f57e8d671ef7f5f13ee14",
            "isKey": false,
            "numCitedBy": 1228,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A frame design technique for use with vector selection algorithms, for example matching pursuits (MP), is presented. The design algorithm is iterative and requires a training set of signal vectors. The algorithm, called method of optimal directions (MOD), is an improvement of the algorithm presented by Engan, Aase and Husoy see (Proc. ICASSP '98, Seattle, USA, p.1817-20, 1998). The MOD is applied to speech and electrocardiogram (ECG) signals, and the designed frames are tested on signals outside the training sets. Experiments demonstrate that the approximation capabilities, in terms of mean squared error (MSE), of the optimized frames are significantly better than those obtained using frames designed by the algorithm of Engan et. al. Experiments show typical reduction in MSE by 20-50%."
            },
            "slug": "Method-of-optimal-directions-for-frame-design-Engan-Aase",
            "title": {
                "fragments": [],
                "text": "Method of optimal directions for frame design"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experiments demonstrate that the approximation capabilities, in terms of mean squared error (MSE), of the optimized frames are significantly better than those obtained using frames designed by the algorithm of Engan et."
            },
            "venue": {
                "fragments": [],
                "text": "1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Averaging the results that correspond to [23] in this table for noise levels lower than,2"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Is there any chance of getting a comparable denoising performance with a local-sparsity based method? In that respect, the image denoising work reported in [23] is of great importance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "In order to better visualize the results and their comparison to those in [23], Fig."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "All these lead to what is considered today as some of the best available image denoising methods (see [23]\u2013[26] for few representative works)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "DCT compared with those of [23] (which appears as a zero straight reference line)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 195
                            }
                        ],
                        "text": "Comparison between the three presented methods (overcomplete DCT, global trained dictionary, and adaptive dictionary trained on patches from the noisy image) and the results achieved recently in [23] for three test images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "The tested images, as also the tested noise levels, are all the same ones as those used in the denoising experiments reported in [23], in order to enable a fair comparison."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image denoising using scale mixtures of gaussians in the wavelet doma"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process., vol. 12, no. 11, pp. 1338\u20131351, Nov. 2003."
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Once this is set, we will discuss how local treatment on image patches turns into a global prior in a Bayesian reconstruction framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "In this work, we will make use mainly of the orthonormal matching pursuit (OMP) because of its simplicity [21] and efficiency."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "We define a maximum a posteriori probability (MAP) estimator as the minimizer of a well-defined global penalty term."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "While this problem is, in general, very hard to solve, the matching and the basis pursuit algorithms can be used quite effectively [20]\u2013[22] to get an approximated solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weighted overcomplete denoising, \" presented at the Asilomar Conf. Signals and Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Weighted overcomplete denoising, \" presented at the Asilomar Conf. Signals and Systems"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weighted overcomplete denoising , \u201d presented at the Asilomar Conf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 225
                            }
                        ],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New tight frames of curvelets and the problem of approximating piecewise C images with piecewise C edges"
            },
            "venue": {
                "fragments": [],
                "text": "Commun. Pure Appl. Math., vol. 57, pp. 219\u2013266, Feb. 2004."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved denoising of images using modeling of the redundant contourlet transform , \u201d presented at the SPIE Conf . Wavelets , Jul ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Once this is set, we will discuss how local treatment on image patches turns into a global prior in a Bayesian reconstruction framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "In parallel, the introduction of the matching pursuit [20], [21] and the basis pursuit denoising [22] gave rise to the ability to address the image denoising problem as a direct sparse decomposition technique over redundant dictionaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved denoising of images using modeling of the redundant contourlet transform, \" presented at the SPIE Conf. Wavelets"
            },
            "venue": {
                "fragments": [],
                "text": "Improved denoising of images using modeling of the redundant contourlet transform, \" presented at the SPIE Conf. Wavelets"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved denoising of images using modeling of the redundant contourlet transform , \u201d presented at the SPIE Conf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 325,
                                "start": 321
                            }
                        ],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shiftable multi-scale transforms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans Inf. Theory, vol. 38, no. 2, pp. 587\u2013607, Mar. 1992."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "While this problem is, in general, very hard to solve, the matching and the basis pursuit algorithms can be used quite effectively [20]\u2013[22] to get an approximated solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "In parallel, the introduction of the matching pursuit [20], [21] and the basis pursuit denoising [22] gave rise to the ability to address the image denoising problem as a direct sparse decomposition technique over redundant dictionaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matching pursuit in a time-frequency dictionary"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process., vol. 41, no. 12, pp. 3397\u20133415, Dec. 1993."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 225
                            }
                        ],
                        "text": "Also, with the growing realization that regular separable 1-D wavelets are inappropriate for handling images, several new tailored multiscale and directional redundant transforms were introduced, including the curvelet [11], [12], contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the steerable wavelet [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New tight frames of curvelets and the problem of approximating piecewise images with piecewise edges"
            },
            "venue": {
                "fragments": [],
                "text": "Commun. Pure Appl. Math., vol. 57, pp. 219\u2013266, Feb. 2004."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning regularization functionals"
            },
            "venue": {
                "fragments": [],
                "text": "Inv. Probl., vol. 19, pp. 611\u2013626, 2003."
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 7,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 53,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Image-Denoising-Via-Sparse-and-Redundant-Over-Elad-Aharon/e07416eabd4ba6c69fa473756bb04ae7161177be?sort=total-citations"
}