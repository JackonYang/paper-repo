{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34528526"
                        ],
                        "name": "H. K\u00fchnel",
                        "slug": "H.-K\u00fchnel",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "K\u00fchnel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K\u00fchnel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "A solution of the deterministic annealing procedure for vector quantization with different rate constraints was suggested in [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28539161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b829f51389c328f5afa13ceaa85f385aec0a0876",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Data clustering is a complex optimization problem with applications ranging from vision and speech processing to data transmission and data storage in technical as well as in biological systems. We discuss a clustering strategy that explicitly reflects the tradeoff between simplicity and precision of a data representation. The resulting clustering algorithm jointly optimizes distortion errors and complexity costs. A maximum entropy estimation of the clustering cost function yields an optimal number of clusters, their positions, and their cluster probabilities. Our approach establishes a unifying framework for different clustering methods like K-means clustering, fuzzy clustering, entropy constrained vector quantization, or topological feature maps and competitive neural networks."
            },
            "slug": "Complexity-Optimized-Data-Clustering-by-Competitive-Buhmann-K\u00fchnel",
            "title": {
                "fragments": [],
                "text": "Complexity Optimized Data Clustering by Competitive Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work discusses a clustering strategy that explicitly reflects the tradeoff between simplicity and precision of a data representation, and establishes a unifying framework for different clustering methods like K-means clustering, fuzzy clusters, entropy constrained vector quantization, or topological feature maps and competitive neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121009"
                        ],
                        "name": "J. Puzicha",
                        "slug": "J.-Puzicha",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Puzicha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Puzicha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33113587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3e27cb0ea3b35ddf2e67927533de90c064320fc",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach to unsupervised texture segmentation is presented which is formulated as a combinatorial optimization problem known as pairwise data clustering with a sparse neighborhood structure. Pairwise dissimilarities between texture blocks are measured in terms of distribution differences of multi-resolution features. The feature vectors are based on a Gabor wavelet image representation. To efficiently solve the data clustering problem a deterministic annealing algorithm on the basis of a mean field approximation is derived. An application to collages of Brodatz-like microtexture is demonstrated. The adequacy of the proposed segmentation cost function is statistically validated. The deterministic annealing algorithm outperforms its stochastic variants in terms of quality and efficiency."
            },
            "slug": "Unsupervised-segmentation-of-textured-images-by-Hofmann-Puzicha",
            "title": {
                "fragments": [],
                "text": "Unsupervised segmentation of textured images by pairwise data clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A novel approach to unsupervised texture segmentation is presented which is formulated as a combinatorial optimization problem known as pairwise data clustering with a sparse neighborhood structure and the adequacy of the proposed segmentation cost function is statistically validated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd IEEE International Conference on Image Processing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10339287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a40f8a315772e15966b48df1ede981f848fd870",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The unsupervised detection of hierarchical structures is a major topic in unsupervised learning and one of the key questions in data analysis and representation. We propose a novel algorithm for the problem of learning decision trees for data clustering and related problems. In contrast to many other methods based on successive tree growing and pruning, we propose an objective function for tree evaluation and we derive a non-greedy technique for tree growing. Applying the principles of maximum entropy and minimum cross entropy, a deterministic annealing algorithm is derived in a meanfield approximation. This technique allows us to canonically superimpose tree structures and to fit parameters to averaged or 'fuzzified' trees."
            },
            "slug": "Inferring-Hierarchical-Clustering-Structures-by-Hofmann-Buhmann",
            "title": {
                "fragments": [],
                "text": "Inferring Hierarchical Clustering Structures by Deterministic Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An objective function for tree evaluation and a deterministic annealing algorithm is derived in a meanfield approximation that allows us to canonically superimpose tree structures and to fit parameters to averaged or 'fuzzified' trees."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629177"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Rose",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043610"
                        ],
                        "name": "E. Gurewitz",
                        "slug": "E.-Gurewitz",
                        "structuredName": {
                            "firstName": "Eitan",
                            "lastName": "Gurewitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gurewitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153840193"
                        ],
                        "name": "G. Fox",
                        "slug": "G.-Fox",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Fox",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "in a series of papers [16], [17], [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15793486,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf584e89bac9daa74e77a35ee65e63deaf8aadf6",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A deterministic annealing approach to clustering is derived on the basis of the principle of maximum entropy. This approach is independent of the initial state and produces natural hierarchical clustering solutions by going through a sequence of phase transitions. It is modified for a larger class of optimization problems by adding constraints to the free energy. The concept of constrained clustering is explained, and three examples are are given in which it is used to introduce deterministic annealing. The previous clustering method is improved by adding cluster mass variables and a total mass constraint. The traveling salesman problem is reformulated as constrained clustering, yielding the elastic net (EN) approach to the problem. More insight is gained by identifying a second Lagrange multiplier that is related to the tour length and can also be used to control the annealing process. The open path constraint formulation is shown to relate to dimensionality reduction by self-organization in unsupervised learning. A similar annealing procedure is applicable in this case as well. >"
            },
            "slug": "Constrained-Clustering-as-an-Optimization-Method-Rose-Gurewitz",
            "title": {
                "fragments": [],
                "text": "Constrained Clustering as an Optimization Method"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The traveling salesman problem is reformulated as constrained clustering, yielding the elastic net (EN) approach to the problem, and the open path constraint formulation is shown to relate to dimensionality reduction by self-organization in unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629177"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Rose",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043610"
                        ],
                        "name": "E. Gurewitz",
                        "slug": "E.-Gurewitz",
                        "structuredName": {
                            "firstName": "Eitan",
                            "lastName": "Gurewitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gurewitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153840193"
                        ],
                        "name": "G. Fox",
                        "slug": "G.-Fox",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Fox",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "in a series of papers [16], [17], [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36027870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7984546a154eae54375fcda9bc9bcde08c096d0",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-deterministic-annealing-approach-to-clustering-Rose-Gurewitz",
            "title": {
                "fragments": [],
                "text": "A deterministic annealing approach to clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34528526"
                        ],
                        "name": "H. K\u00fchnel",
                        "slug": "H.-K\u00fchnel",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "K\u00fchnel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K\u00fchnel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28379134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c829b01d6aac1825acae7ea90a00fee4fbc2689e",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector quantization is a data compression method by which a set of data points is encoded by a reduced set of reference vectors: the codebook. A vector quantization strategy is discussed that jointly optimizes distortion errors and the codebook complexity, thereby determining the size of the codebook. A maximum entropy estimation of the cost function yields an optimal number of reference vectors, their positions, and their assignment probabilities. The dependence of the codebook density on the data density for different complexity functions is investigated in the limit of asymptotic quantization levels. How different complexity measures influence the efficiency of vector quantizers is studied for the task of image compression. The wavelet coefficients of gray-level images are quantized, and the reconstruction error is measured. The approach establishes a unifying framework for different quantization methods like K-means clustering and its fuzzy version, entropy constrained vector quantization or topological feature maps, and competitive neural networks. >"
            },
            "slug": "Vector-quantization-with-complexity-costs-Buhmann-K\u00fchnel",
            "title": {
                "fragments": [],
                "text": "Vector quantization with complexity costs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The approach establishes a unifying framework for different quantization methods like K-means clustering and its fuzzy version, entropy constrained vector quantization or topological feature maps, and competitive neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3262189"
                        ],
                        "name": "C. Graffigne",
                        "slug": "C.-Graffigne",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Graffigne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Graffigne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052290053"
                        ],
                        "name": "P. Dong",
                        "slug": "P.-Dong",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[41] we formulate texture segmentation as a grouping problem with constraints about valid region shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "The segmentation was postprocessed with additional penalties for thin regions as suggested in [41] to enforce local texture consistency and to prevent a too large fragmentation of the texture regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "3) The presented deterministic annealing algorithm replaces the Monte Carlo method proposed in [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Three major modifications compared to [41] have been introduced: 1) Dissimilarities are calculated based on a Gabor wavelet scale\u2013space representation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9794339,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8c3e33fb1d7ffae33d4c97d4cc82e002d772e0a5",
            "isKey": true,
            "numCitedBy": 569,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "A statistical framework is used for finding boundaries and for partitioning scenes into homogeneous regions. The model is a joint probability distribution for the array of pixel gray levels and an array of labels. In boundary finding, the labels are binary, zero, or one, representing the absence or presence of boundary elements. In partitioning, the label values are generic: two labels are the same when the corresponding scene locations are considered to belong to the same region. The distribution incorporates a measure of disparity between certain spatial features of block pairs of pixel gray levels, using the Kolmogorov-Smirnov nonparametric measures of difference between the distributions of these features. The number of model parameters is minimized by forbidding label configurations, which are assigned probability zero. The maximum a posteriori estimator of boundary placements and partitionings is examined. The forbidden states introduce constraints into the calculation of these configurations. Stochastic relaxation methods are extended to accommodate constrained optimization. >"
            },
            "slug": "Boundary-Detection-by-Constrained-Optimization-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Boundary Detection by Constrained Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A statistical framework is used for finding boundaries and for partitioning scenes into homogeneous regions and incorporates a measure of disparity between certain spatial features of block pairs of pixel gray levels, using the Kolmogorov-Smirnov nonparametric measures of difference between the distributions of these features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629177"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Rose",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043610"
                        ],
                        "name": "E. Gurewitz",
                        "slug": "E.-Gurewitz",
                        "structuredName": {
                            "firstName": "Eitan",
                            "lastName": "Gurewitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gurewitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153840193"
                        ],
                        "name": "G. Fox",
                        "slug": "G.-Fox",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Fox",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "in a series of papers [16], [17], [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36450624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ee14dd35886c44c87d66f8490528fa58c19fc25",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A deterministic annealing approach is suggested to search for the optimal vector quantizer given a set of training data. The problem is reformulated within a probabilistic framework. No prior knowledge is assumed on the source density, and the principle of maximum entropy is used to obtain the association probabilities at a given average distortion. The corresponding Lagrange multiplier is inversely related to the 'temperature' and is used to control the annealing process. In this process, as the temperature is lowered, the system undergoes a sequence of phase transitions when existing clusters split naturally, without use of heuristics. The resulting codebook is independent of the codebook used to initialize the iterations. >"
            },
            "slug": "Vector-quantization-by-deterministic-annealing-Rose-Gurewitz",
            "title": {
                "fragments": [],
                "text": "Vector quantization by deterministic annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A deterministic annealing approach is suggested to search for the optimal vector quantizer given a set of training data and the resulting codebook is independent of the codebook used to initialize the iterations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12874899"
                        ],
                        "name": "J. Sammon",
                        "slug": "J.-Sammon",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sammon",
                            "middleNames": [
                                "W."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sammon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 159
                            }
                        ],
                        "text": "The task of embedding given dissimilarity data D in a ddimensional Euclidian space, a prerequisite for visual inspection, is known as multidimensional scaling [8, 34]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 43151050,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "154f8a9906bcc99fca9b17aa521649b1c3734093",
            "isKey": false,
            "numCitedBy": 3461,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for the analysis of multivariate data is presented along with some experimental results. The algorithm is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved."
            },
            "slug": "A-Nonlinear-Mapping-for-Data-Structure-Analysis-Sammon",
            "title": {
                "fragments": [],
                "text": "A Nonlinear Mapping for Data Structure Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "An algorithm for the analysis of multivariate data is presented along with some experimental results that is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46356930"
                        ],
                        "name": "Petar D. Simic",
                        "slug": "Petar-D.-Simic",
                        "structuredName": {
                            "firstName": "Petar",
                            "lastName": "Simic",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petar D. Simic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "overview. The method of deterministic annealing is described in various papers mostly in the literature on neural networks [9,  10 , 11, 12] and on computer vision [13, 14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18106757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dfc18acbdbdc1767968fcde31bae9fbcd63e2a7",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Some time ago Durbin and Willshaw proposed an interesting parallel algorithm (the \"elastic net\") for approximately solving some geometric optimization problems, such as the Traveling Salesman Problem. Recently it has been shown that their algorithm is related to neural networks of Hopfield and Tank, and that they both can be understood as the semiclassical approximation to statistical mechanics of related physical models. The main point of the elastic net algorithm is seen to be in the way one deals with the constraints when evaluating the effective cost function (free energy in the thermodynamic analogy), and not in its geometric foundation emphasized originally by Durbin and Willshaw. As a consequence, the elastic net algorithm is a special case of the more general physically based computations and can be generalized to a large class of nongeometric problems. In this paper we further elaborate on this observation, and generalize the elastic net to the quadratic assignment problem. We work out in detail its special case, the graph matching problem, because it is an important problem with many applications in computational vision and neural modeling. Simulation results on random graphs, and on structured (hand-designed) graphs of moderate size (20-100 nodes) are discussed."
            },
            "slug": "Constrained-Nets-for-Graph-Matching-and-Other-Simic",
            "title": {
                "fragments": [],
                "text": "Constrained Nets for Graph Matching and Other Quadratic Assignment Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main point of the elastic net algorithm is seen to be in the way one deals with the constraints when evaluating the effective cost function (free energy in the thermodynamic analogy), and not in its geometric foundation emphasized originally by Durbin and Willshaw."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Comput."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "Data compression is achieved by transmission and storage of the indices of reference vectors rather than the original data vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Sets of relational data are abundant in many applications, e.g., in molecular biology, psychology, linguistics, economics and image processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16928,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92279580"
                        ],
                        "name": "Y. Tikochinsky",
                        "slug": "Y.-Tikochinsky",
                        "structuredName": {
                            "firstName": "Yoel",
                            "lastName": "Tikochinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tikochinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92475219"
                        ],
                        "name": "R. Levine",
                        "slug": "R.-Levine",
                        "structuredName": {
                            "firstName": "Raphael",
                            "lastName": "Levine",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Levine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[26] have proven that the maximum entropy probability distribution is maximally stable in terms of theL2 norm if the expected cost hHi is lowered or raised by changes of the temperature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120472330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "115462633cadff2e37e6eec8e953543d38e1db63",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A consistent approach to the inference of a probability distribution given a limited number of expectation values of relevant variables is discussed. There are two key assumptions: that the experiment can be independently repeated a finite number (not necessarily large) of times and that the theoretical expectation values of the relevant observables are to be estimated from their measured sample averages. Three independent but complementary routes for deriving the form of the distribution from these two assumptions are reviewed. All three lead to a unique distribution which is identical with the one obtained by the maximum-entropy formalism. The present derivation thus provides an alternative approach to the inference problem which does not invoke Shannon's notion of missing information or entropy. The approach is more limited in scope than the one proposed by Jaynes, but has the advantage that it is objective and that the operational origin of the \"given\" expectation values is specified."
            },
            "slug": "Alternative-approach-to-maximum-entropy-inference-Tikochinsky-Tishby",
            "title": {
                "fragments": [],
                "text": "Alternative approach to maximum-entropy inference"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The present derivation provides an alternative approach to the inference problem which does not invoke Shannon's notion of missing information or entropy and is more limited in scope than the one proposed by Jaynes, but has the advantage that it is objective."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756798"
                        ],
                        "name": "I. Csisz\u00e1r",
                        "slug": "I.-Csisz\u00e1r",
                        "structuredName": {
                            "firstName": "Imre",
                            "lastName": "Csisz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Csisz\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "Using concepts from differential geometry, the family of Gibbs distributions parameterized by the temperature forms a trajectory in the space of probability distributions which has minimal length [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18053591,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "72b2aeeb76dbff312321ccbcc58e85009e0b57ae",
            "isKey": false,
            "numCitedBy": 1453,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and extend access to The Annals of Probability. Some geometric properties of PD's are established, Kullback's I-divergence playing the role of squared Euclidean distance. The minimum discrimination information problem is viewed as that of projecting a PD onto a convex set of PD's and useful existence theorems for and characterizations of the minimizing PD are arrived at. A natural generalization of known iterative algorithms converging to the minimizing PD in special situations is given; even for those special cases, our convergence proof is more generally valid than those previously published. As corollaries of independent interest, generalizations of known results on the existence of PD's or nonnegative matrices of a certain form are obtained. The Lagrange multiplier technique is not used."
            },
            "slug": "$I$-Divergence-Geometry-of-Probability-and-Problems-Csisz\u00e1r",
            "title": {
                "fragments": [],
                "text": "$I$-Divergence Geometry of Probability Distributions and Minimization Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14489533"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 152
                            }
                        ],
                        "text": "The method of deterministic annealing is described in various papers mostly in the literature on neural networks [9, 10, 11, 12] and on computer vision [13, 14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46023735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d5a193fdbf9d34118f136935cfd07a81b3e0d77",
            "isKey": false,
            "numCitedBy": 502,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Deterministic approximations to Markov random field (MRF) models are derived. One of the models is shown to give in a natural way the graduated nonconvexity (GNC) algorithm proposed by A. Blake and A. Zisserman (1987). This model can be applied to smooth a field preserving its discontinuities. A class of more complex models is then proposed in order to deal with a variety of vision problems. All the theoretical results are obtained in the framework of statistical mechanics and mean field techniques. A parallel, iterative algorithm to solve the deterministic equations of the two models is presented, together with some experiments on synthetic and real images. >"
            },
            "slug": "Parallel-and-Deterministic-Algorithms-from-MRFs:-Geiger-Girosi",
            "title": {
                "fragments": [],
                "text": "Parallel and Deterministic Algorithms from MRFs: Surface Reconstruction"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Deterministic approximations to Markov random field (MRF) models are derived and one of the models is shown to give in a natural way the graduated nonconvexity (GNC) algorithm proposed by A. Blake and A. Zisserman (1987)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35380459"
                        ],
                        "name": "S. Gold",
                        "slug": "S.-Gold",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Gold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "overview. The method of deterministic annealing is described in various papers mostly in the literature on neural networks [9, 10, 11,  12 ] and on computer vision [13, 14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10369341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9899003369af99d02f699cbcbf48b79019666158",
            "isKey": false,
            "numCitedBy": 1159,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "A graduated assignment algorithm for graph matching is presented which is fast and accurate even in the presence of high noise. By combining graduated nonconvexity, two-way (assignment) constraints, and sparsity, large improvements in accuracy and speed are achieved. Its low order computational complexity [O(lm), where l and m are the number of links in the two graphs] and robustness in the presence of noise offer advantages over traditional combinatorial approaches. The algorithm, not restricted to any special class of graph, is applied to subgraph isomorphism, weighted graph matching, and attributed relational graph matching. To illustrate the performance of the algorithm, attributed relational graphs derived from objects are matched. Then, results from twenty-five thousand experiments conducted on 100 mode random graphs of varying types (graphs with only zero-one links, weighted graphs, and graphs with node attributes and multiple link types) are reported. No comparable results have been reported by any other graph matching algorithm before in the research literature. Twenty-five hundred control experiments are conducted using a relaxation labeling algorithm and large improvements in accuracy are demonstrated."
            },
            "slug": "A-Graduated-Assignment-Algorithm-for-Graph-Matching-Gold-Rangarajan",
            "title": {
                "fragments": [],
                "text": "A Graduated Assignment Algorithm for Graph Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A graduated assignment algorithm for graph matching is presented which is fast and accurate even in the presence of high noise, and not restricted to any special class of graph, is applied to subgraph isomorphism, weighted graph matching, and attributed relational graph matching."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The method of deterministic annealing is described in various papers mostly in the literature on neural networks [9], [10], [11], [12] and on computer vision [13], [14], [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45670932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59fbb0b127de1ea9be27d5f8c1e30d0cf394a503",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe how to formulate matching and combinatorial problems of vision and neural network theory by generalizing elastic and deformable templates models to include binary matching elements. Techniques from statistical physics, which can be interpreted as computing marginal probability distributions, are then used to analyze these models and are shown to (1) relate them to existing theories and (2) give insight into the relations between, and relative effectivenesses of, existing theories. In particular we exploit the power of statistical techniques to put global constraints on the set of allowable states of the binary matching elements. The binary elements can then be removed analytically before minimization. This is demonstrated to be preferable to existing methods of imposing such constraints by adding bias terms in the energy functions. We give applications to winner-take-all networks, correspondence for stereo and long-range motion, the traveling salesman problem, deformable template matching, learning, content addressable memories, and models of brain development. The biological plausibility of these networks is briefly discussed."
            },
            "slug": "Generalized-Deformable-Models,-Statistical-Physics,-Yuille",
            "title": {
                "fragments": [],
                "text": "Generalized Deformable Models, Statistical Physics, and Matching Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Techniques from statistical physics are used to exploit the power of statistical techniques to put global constraints on the set of allowable states of the binary matching elements and be preferable to existing methods of imposing such constraints by adding bias terms in the energy functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Why should we consider a stochastic or deterministic search strategy based on principles from statistical physics? The fundamental relationship between statistical physics and robust statistics has been established by Jaynes [1], [2], [3] who postulated the principle of maximum entropy inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We will describe a stochastic optimization approach to data clustering which relies on the well-known robustness of maximum entropy inference [1], [2], [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42335268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c32c4eda83a9e464ab01ee74f243127c38d8662",
            "isKey": false,
            "numCitedBy": 1521,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the relations between maximum-entropy (MAXENT) and other methods of spectral analysis such as the Schuster, Blackman-Tukey, maximum-likelihood, Bayesian, and Autoregressive (AR, ARMA, or ARIMA) models, emphasizing that they are not in conflict, but rather are appropriate in different problems. We conclude that: 1) \"Orthodox\" sampling theory methods are useful in problems where we have a known model (sampling distribution) for the properties of the noise, but no appreciable prior information about the quantities being estimated. 2) MAXENT is optimal in problems where we have prior information about multiplicities, but no noise. 3) The full Bayesian solution includes both of these as special cases and is needed in problems where we have both prior information and noise. 4) AR models are in one sense a special case of MAXENT, but in another sense they are ubiquitous in all spectral analysis problems with discrete time series. 5) Empirical methods such as Blackman-Tukey, which do not invoke even a likelihood function, are useful in the preliminary, exploratory phase of a problem where our knowledge is sufficient to permit intuitive judgments about how to organize a calculation (smoothing, decimation, windows, prewhitening, padding with zeroes, etc.) but insufficient to set up a quantitative model which would do the proper things for us automatically and optimally."
            },
            "slug": "On-the-rationale-of-maximum-entropy-methods-Jaynes",
            "title": {
                "fragments": [],
                "text": "On the rationale of maximum-entropy methods"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The relations between maximum-entropy (MAXENT) and other methods of spectral analysis such as the Schuster, Blackman-Tukey, maximum-likelihood, Bayesian, and Autoregressive models are discussed, emphasizing that they are not in conflict, but rather are appropriate in different problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Gabor transformation possesses a bandpass characteristic and is known to display good texture discrimination properties [42], [43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9271650,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "02f89cd1fd6f013a1a301a292936ff8fb06aff25",
            "isKey": false,
            "numCitedBy": 3420,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Two-dimensional spatial linear filters are constrained by general uncertainty relations that limit their attainable information resolution for orientation, spatial frequency, and two-dimensional (2D) spatial position. The theoretical lower limit for the joint entropy, or uncertainty, of these variables is achieved by an optimal 2D filter family whose spatial weighting functions are generated by exponentiated bivariate second-order polynomials with complex coefficients, the elliptic generalization of the one-dimensional elementary functions proposed in Gabor's famous theory of communication [J. Inst. Electr. Eng. 93, 429 (1946)]. The set includes filters with various orientation bandwidths, spatial-frequency bandwidths, and spatial dimensions, favoring the extraction of various kinds of information from an image. Each such filter occupies an irreducible quantal volume (corresponding to an independent datum) in a four-dimensional information hyperspace whose axes are interpretable as 2D visual space, orientation, and spatial frequency, and thus such a filter set could subserve an optimally efficient sampling of these variables. Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution. The variety of their receptive-field dimensions and orientation and spatial-frequency bandwidths, and the correlations among these, reveal several underlying constraints, particularly in width/length aspect ratio and principal axis organization, suggesting a polar division of labor in occupying the quantal volumes of information hyperspace.(ABSTRACT TRUNCATED AT 250 WORDS)"
            },
            "slug": "Uncertainty-relation-for-resolution-in-space,-and-Daugman",
            "title": {
                "fragments": [],
                "text": "Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2915042"
                        ],
                        "name": "P. Stolorz",
                        "slug": "P.-Stolorz",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Stolorz",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Stolorz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082896"
                        ],
                        "name": "J. Utans",
                        "slug": "J.-Utans",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Utans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Utans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "overview. The method of deterministic annealing is described in various papers mostly in the literature on neural networks [9, 10,  11 , 12] and on computer vision [13, 14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8147965,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "56ab8eebda89b251418ed39b794706c9652b0067",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that there are strong relationships between approaches to optmization and learning based on statistical physics or mixtures of experts. In particular, the EM algorithm can be interpreted as converging either to a local maximum of the mixtures model or to a saddle point solution to the statistical physics system. An advantage of the statistical physics approach is that it naturally gives rise to a heuristic continuation method, deterministic annealing, for finding good solutions."
            },
            "slug": "Statistical-Physics,-Mixtures-of-Distributions,-and-Yuille-Stolorz",
            "title": {
                "fragments": [],
                "text": "Statistical Physics, Mixtures of Distributions, and the EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "There are strong relationships between approaches to optmization and learning based on statistical physics or mixtures of experts, and the EM algorithm can be interpreted as converging either to a local maximum of the mixtures model or to a saddle point solution to the statistical physics system."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400423"
                        ],
                        "name": "Petar D. Simi\u0107",
                        "slug": "Petar-D.-Simi\u0107",
                        "structuredName": {
                            "firstName": "Petar",
                            "lastName": "Simi\u0107",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petar D. Simi\u0107"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 113
                            }
                        ],
                        "text": "The method of deterministic annealing is described in various papers mostly in the literature on neural networks [9, 10, 11, 12] and on computer vision [13, 14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121778028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4c5184e949a7702e2c89ba04a867cfa14711609",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "There is an interesting connection between two, recently popular, methods for finding good approximate solutions to hard optimisation problems, the \u2018neural\u2019 approach of Hopfield and Tank and the elastic-net method of Durbin and Willshaw. They both have an underlying statistical mechanics foundation and can be derived as the leading approximation to the thermodynamic free energy of related physical models. The apparent difference in the form of the two algorithms comes from different handling of constraints when evaluating the thermodynamic partition function. If all the constraints are enforced \u2018softly\u2019, the \u2018mean-field\u2019 approximation to the thermodynamic free energy is just the neural network Lyapunov function. If, on the other hand, half of the constraints are enforced \u2018strongly\u2019, the leading approximation to the thermodynamic free energy is the elastic-net Lyapunov function. Our results have interesting implications for the general problem of mapping optimisation problems to \u2018neural\u2019 and \u2018elastic\u2019 netw..."
            },
            "slug": "Statistical-mechanics-as-the-underlying-theory-of-Simi\u0107",
            "title": {
                "fragments": [],
                "text": "Statistical mechanics as the underlying theory of \u2018elastic\u2019 and \u2018neural\u2019 optimisations"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "There is an interesting connection between two, recently popular, methods for finding good approximate solutions to hard optimisation problems, the \u2018neural\u2019 approach of Hopfield and Tank and the elastic-net method of Durbin and Willshaw."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145198149"
                        ],
                        "name": "V. Cern\u00fd",
                        "slug": "V.-Cern\u00fd",
                        "structuredName": {
                            "firstName": "Vladim\u00edr",
                            "lastName": "Cern\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Cern\u00fd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 143
                            }
                        ],
                        "text": "2 STOCHASTIC OPTIMIZATION BY MAXIMUM ENTROPY INFERENCE\n2.1 Simulated Annealing In seminal papers Kirkpatrick et al. [23] and, independently, ( Cerny [24] have proposed the stochastic optimization\nstrategy Simulated Annealing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "[23] and, independently, \u010cerny [24] have proposed the stochastic optimization strategy Simulated Annealing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "[24] V. ( Cerny , \u201cThermodynamical Approach to the Traveling Sales-\nman Problem: an Efficient Simulation Algorithm,\u201d J. Optimization\nTheory and Applications, vol. 45, pp. 41-51, 1985."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122729427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc643e958e762ab72da9277ca64ef3bf74f90e4f",
            "isKey": true,
            "numCitedBy": 2194,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a Monte Carlo algorithm to find approximate solutions of the traveling salesman problem. The algorithm generates randomly the permutations of the stations of the traveling salesman trip, with probability depending on the length of the corresponding route. Reasoning by analogy with statistical thermodynamics, we use the probability given by the Boltzmann-Gibbs distribution. Surprisingly enough, using this simple algorithm, one can get very close to the optimal solution of the problem or even find the true optimum. We demonstrate this on several examples.We conjecture that the analogy with thermodynamics can offer a new insight into optimization problems and can suggest efficient algorithms for solving them."
            },
            "slug": "Thermodynamical-approach-to-the-traveling-salesman-Cern\u00fd",
            "title": {
                "fragments": [],
                "text": "Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is conjecture that the analogy with thermodynamics can offer a new insight into optimization problems and can suggest efficient algorithms for solving them."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145847131"
                        ],
                        "name": "S. Kirkpatrick",
                        "slug": "S.-Kirkpatrick",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Kirkpatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5882723"
                        ],
                        "name": "C. D. Gelatt",
                        "slug": "C.-D.-Gelatt",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gelatt",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Gelatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88645967"
                        ],
                        "name": "M. Vecchi",
                        "slug": "M.-Vecchi",
                        "structuredName": {
                            "firstName": "Michelle",
                            "lastName": "Vecchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vecchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[23] and, independently, ( Cerny [24] have proposed the stochastic optimization strategy Simulated Annealing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205939,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "isKey": false,
            "numCitedBy": 39640,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods."
            },
            "slug": "Optimization-by-Simulated-Annealing-Kirkpatrick-Gelatt",
            "title": {
                "fragments": [],
                "text": "Optimization by Simulated Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Equations (14) and (15) are efficiently solved in an iterative fashion using the expectation maximization (EM) algorithm [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The reader should realize that entropy maximization implies y to be a centroid which is also optimal in the sense of rate distortion theory [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42795,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326400"
                        ],
                        "name": "B. Julesz",
                        "slug": "B.-Julesz",
                        "structuredName": {
                            "firstName": "B\u00e9la",
                            "lastName": "Julesz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Julesz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Following the three stage procedure, a set of 1 independently calculated dissimilarity matrices has been generated, which are combined with a simple maximum rule max0 . This is reminiscent of Julesz\u2019 theory of texture perception [ 41 ], conjecturing that a dissimilarity in a single feature channel is sufficient to discriminate textures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29648250,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4754bfb45726057d98ad47499481e8c172233e20",
            "isKey": false,
            "numCitedBy": 895,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual discrimination experiments were conducted using unfamiliar displays generated by a digital computer. The displays contained two side-by-side fields with different statistical, topological or heuristic properties. Discrimination was defined as that spontaneous visual process which gives the immediate impression of two distinct fields. The condition for such discrimination was found to be based primarily on clusters or lines formed by proximate points of uniform brightness. A similar rule of connectivity with hue replacing brightness was obtained by using varicolored dots of equal subjective brightness. The limitations in discriminating complex line structures were also investigated."
            },
            "slug": "Visual-Pattern-Discrimination-Julesz",
            "title": {
                "fragments": [],
                "text": "Visual Pattern Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The condition for discrimination was found to be based primarily on clusters or lines formed by proximate points of uniform brightness, and a similar rule of connectivity with hue replacing brightness was obtained by using varicolored dots of equal subjective brightness."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16702335"
                        ],
                        "name": "R. Peierls",
                        "slug": "R.-Peierls",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Peierls",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Peierls"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "[34] R.E. Peierls, \u201cOn a Minimum Property of the Free Energy,\u201d Physical Review, vol. 54, p. 918, 1938."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 136
                            }
                        ],
                        "text": "Since the KL\u2013divergence is always positive and vanishes only for P Gb\n(+0) PGb(+pc) we obtain the wellknown upper bound first derived by Peierls [34]:\n)(+pc) )(+0) + \u00c3+pc \u2212 +0\u00d3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Since the KL\u2013divergence is always positive and vanishes only for P Gb (+) \u009c P(+) we obtain the wellknown upper bound first derived by Peierls [34]: )(+) \u0085 )(+) + \u00c3+ \u2212 +\u00d3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120081029,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7cea474cd1a7952f403a338cb6c0a26e502d98b9",
            "isKey": true,
            "numCitedBy": 122,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "a minimum ; the value of the integral is the corresponding eigenvalue El of the Hamiltonian H. It is also well known that this property leads to a powerful method of approximating, at least qualitatively, 4,1 and Er, by minimizing (1) among a restricted class of functions. No similarly simple minimum property exists for the higher eigenvalues . One can obtain the nth eigenvalue by minimizing (1) with the subsidiary condition that >G be orthogonal to \u00a2l, 42, y n-r\u2022 However, this procedure becomes progressively more cumbersome as n increases and besides it is of no use unless 4,1, 02, \u2022 \u2022 \u2022 , 4'n-1 are very exactly known. For problems of thermal equilibrium one is often concerned with the free energy, rather than with the individual eigenvalues . It is the purpose of this note to draw attention to a simple minimum property of the free energy which may be considered as a generalization of the variation principle for the lowest eigenvalue. The free energy has the following property: If v1, W2 , \u2022 \u2022 \u2022 , pn, \u2022 \u2022 \u2022 are an arbitrary set of orthogonal and normalized functions, and"
            },
            "slug": "On-a-Minimum-Property-of-the-Free-Energy-Peierls",
            "title": {
                "fragments": [],
                "text": "On a Minimum Property of the Free Energy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1938
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "overview. The method of deterministic annealing is described in various papers mostly in the literature on neural networks [9, 10, 11, 12] and on computer vision [13, 14,  15 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8783809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56efe7bf4bd52a6369d9ebbe55033e81e716f7d0",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Most connectionist research has focused on learning mappings from one space to another (eg. classification and regression). This paper introduces the more general task of learning constraint surfaces. It describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data. We demonstrate the technique on low dimensional synthetic surfaces and compare it to nearest neighbor approaches. We then show its utility in learning the space of lip images in a system for improving speech recognition by lip reading. This learned surface is used to improve the visual tracking performance during recognition."
            },
            "slug": "Surface-Learning-with-Applications-to-Lipreading-Bregler-Omohundro",
            "title": {
                "fragments": [],
                "text": "Surface Learning with Applications to Lipreading"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data and shows its utility in learning the space of lip images in a system for improving speech recognition by lip reading."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "A partitioning approach known as central clustering derives a set of reference or prototype vectors which quantize a set of vectorial data with minimal quantization error [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14754287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81a5952532cdd48eec5e3dc326907c36a70e0a24",
            "isKey": false,
            "numCitedBy": 2922,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications."
            },
            "slug": "Vector-quantization-Gray",
            "title": {
                "fragments": [],
                "text": "Vector quantization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17870175,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "08b67692bc037eada8d3d7ce76cc70994e7c8116",
            "isKey": false,
            "numCitedBy": 10876,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of \"statistical complementarity\" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation."
            },
            "slug": "Information-Theory-and-Statistical-Mechanics-Jaynes",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistical Mechanics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Distortions with a lowdimensional, topological arrangement defining a chain or a two-dimensional grid are very popular such as selforganizing topological maps in the area of neural computing [28], [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 222292199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10055eb6f2f711a36d9aa8f759d3b3f01ebddb5d",
            "isKey": false,
            "numCitedBy": 6561,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Various Aspects of Memory.- 1.1 On the Purpose and Nature of Biological Memory.- 1.1.1 Some Fundamental Concepts.- 1.1.2 The Classical Laws of Association.- 1.1.3 On Different Levels of Modelling.- 1.2 Questions Concerning the Fundamental Mechanisms of Memory.- 1.2.1 Where Do the Signals Relating to Memory Act Upon?.- 1.2.2 What Kind of Encoding is Used for Neural Signals?.- 1.2.3 What are the Variable Memory Elements?.- 1.2.4 How are Neural Signals Addressed in Memory?.- 1.3 Elementary Operations Implemented by Associative Memory.- 1.3.1 Associative Recall.- 1.3.2 Production of Sequences from the Associative Memory.- 1.3.3 On the Meaning of Background and Context.- 1.4 More Abstract Aspects of Memory.- 1.4.1 The Problem of Infinite-State Memory.- 1.4.2 Invariant Representations.- 1.4.3 Symbolic Representations.- 1.4.4 Virtual Images.- 1.4.5 The Logic of Stored Knowledge.- 2. Pattern Mathematics.- 2.1 Mathematical Notations and Methods.- 2.1.1 Vector Space Concepts.- 2.1.2 Matrix Notations.- 2.1.3 Further Properties of Matrices.- 2.1.4 Matrix Equations.- 2.1.5 Projection Operators.- 2.1.6 On Matrix Differential Calculus.- 2.2 Distance Measures for Patterns.- 2.2.1 Measures of Similarity and Distance in Vector Spaces.- 2.2.2 Measures of Similarity and Distance Between Symbol Strings.- 2.2.3 More Accurate Distance Measures for Text.- 3. Classical Learning Systems.- 3.1 The Adaptive Linear Element (Adaline).- 3.1.1 Description of Adaptation by the Stochastic Approximation.- 3.2 The Perceptron.- 3.3 The Learning Matrix.- 3.4 Physical Realization of Adaptive Weights.- 3.4.1 Perceptron and Adaline.- 3.4.2 Classical Conditioning.- 3.4.3 Conjunction Learning Switches.- 3.4.4 Digital Representation of Adaptive Circuits.- 3.4.5 Biological Components.- 4. A New Approach to Adaptive Filters.- 4.1 Survey of Some Necessary Functions.- 4.2 On the \"Transfer Function\" of the Neuron.- 4.3 Models for Basic Adaptive Units.- 4.3.1 On the Linearization of the Basic Unit.- 4.3.2 Various Cases of Adaptation Laws.- 4.3.3 Two Limit Theorems.- 4.3.4 The Novelty Detector.- 4.4 Adaptive Feedback Networks.- 4.4.1 The Autocorrelation Matrix Memory.- 4.4.2 The Novelty Filter.- 5. Self-Organizing Feature Maps.- 5.1 On the Feature Maps of the Brain.- 5.2 Formation of Localized Responses by Lateral Feedback.- 5.3 Computational Simplification of the Process.- 5.3.1 Definition of the Topology-Preserving Mapping.- 5.3.2 A Simple Two-Dimensional Self-Organizing System.- 5.4 Demonstrations of Simple Topology-Preserving Mappings.- 5.4.1 Images of Various Distributions of Input Vectors.- 5.4.2 \"The Magic TV\".- 5.4.3 Mapping by a Feeler Mechanism.- 5.5 Tonotopic Map.- 5.6 Formation of Hierarchical Representations.- 5.6.1 Taxonomy Example.- 5.6.2 Phoneme Map.- 5.7 Mathematical Treatment of Self-Organization.- 5.7.1 Ordering of Weights.- 5.7.2 Convergence Phase.- 5.8 Automatic Selection of Feature Dimensions.- 6. Optimal Associative Mappings.- 6.1 Transfer Function of an Associative Network.- 6.2 Autoassociative Recall as an Orthogonal Projection.- 6.2.1 Orthogonal Projections.- 6.2.2 Error-Correcting Properties of Projections.- 6.3 The Novelty Filter.- 6.3.1 Two Examples of Novelty Filter.- 6.3.2 Novelty Filter as an Autoassociative Memory.- 6.4 Autoassociative Encoding.- 6.4.1 An Example of Autoassociative Encoding.- 6.5 Optimal Associative Mappings.- 6.5.1 The Optimal Linear Associative Mapping.- 6.5.2 Optimal Nonlinear Associative Mappings.- 6.6 Relationship Between Associative Mapping, Linear Regression, and Linear Estimation.- 6.6.1 Relationship of the Associative Mapping to Linear Regression.- 6.6.2 Relationship of the Regression Solution to the Linear Estimator.- 6.7 Recursive Computation of the Optimal Associative Mapping.- 6.7.1 Linear Corrective Algorithms.- 6.7.2 Best Exact Solution (Gradient Projection).- 6.7.3 Best Approximate Solution (Regression).- 6.7.4 Recursive Solution in the General Case.- 6.8 Special Cases.- 6.8.1 The Correlation Matrix Memory.- 6.8.2 Relationship Between Conditional Averages and Optimal Estimator.- 7. Pattern Recognition.- 7.1 Discriminant Functions.- 7.2 Statistical Formulation of Pattern Classification.- 7.3 Comparison Methods.- 7.4 The Subspace Methods of Classification.- 7.4.1 The Basic Subspace Method.- 7.4.2 The Learning Subspace Method (LSM).- 7.5 Learning Vector Quantization.- 7.6 Feature Extraction.- 7.7 Clustering.- 7.7.1 Simple Clustering (Optimization Approach).- 7.7.2 Hierarchical Clustering (Taxonomy Approach).- 7.8 Structural Pattern Recognition Methods.- 8. More About Biological Memory.- 8.1 Physiological Foundations of Memory.- 8.1.1 On the Mechanisms of Memory in Biological Systems.- 8.1.2 Structural Features of Some Neural Networks.- 8.1.3 Functional Features of Neurons.- 8.1.4 Modelling of the Synaptic Plasticity.- 8.1.5 Can the Memory Capacity Ensue from Synaptic Changes?.- 8.2 The Unified Cortical Memory Model.- 8.2.1 The Laminar Network Organization.- 8.2.2 On the Roles of Interneurons.- 8.2.3 Representation of Knowledge Over Memory Fields.- 8.2.4 Self-Controlled Operation of Memory.- 8.3 Collateral Reading.- 8.3.1 Physiological Results Relevant to Modelling.- 8.3.2 Related Modelling.- 9. Notes on Neural Computing.- 9.1 First Theoretical Views of Neural Networks.- 9.2 Motives for the Neural Computing Research.- 9.3 What Could the Purpose of the Neural Networks be?.- 9.4 Definitions of Artificial \"Neural Computing\" and General Notes on Neural Modelling.- 9.5 Are the Biological Neural Functions Localized or Distributed?.- 9.6 Is Nonlinearity Essential to Neural Computing?.- 9.7 Characteristic Differences Between Neural and Digital Computers.- 9.7.1 The Degree of Parallelism of the Neural Networks is Still Higher than that of any \"Massively Parallel\" Digital Computer.- 9.7.2 Why the Neural Signals Cannot be Approximated by Boolean Variables.- 9.7.3 The Neural Circuits do not Implement Finite Automata.- 9.7.4 Undue Views of the Logic Equivalence of the Brain and Computers on a High Level.- 9.8 \"Connectionist Models\".- 9.9 How can the Neural Computers be Programmed?.- 10. Optical Associative Memories.- 10.1 Nonholographic Methods.- 10.2 General Aspects of Holographic Memories.- 10.3 A Simple Principle of Holographic Associative Memory.- 10.4 Addressing in Holographic Memories.- 10.5 Recent Advances of Optical Associative Memories.- Bibliography on Pattern Recognition.- References."
            },
            "slug": "Self-Organization-and-Associative-Memory-Kohonen",
            "title": {
                "fragments": [],
                "text": "Self-Organization and Associative Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The purpose and nature of Biological Memory, as well as some of the aspects of Memory Aspects, are explained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88803699"
                        ],
                        "name": "D. Thouless",
                        "slug": "D.-Thouless",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Thouless",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Thouless"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35192847"
                        ],
                        "name": "P. Anderson",
                        "slug": "P.-Anderson",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120672380,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a2ea14dc3f89e5f28612f33ccac471c4cdd719a2",
            "isKey": false,
            "numCitedBy": 938,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The Sherrmgton-Kirkpatrick model of a spin glass is solved by a mean field technique which is probably exact in the limit of infinite range interactions. At and above T c the solution is identical to that obtained by Sherrington and Kirkpatrick (1975) using the n \u2192 O replica method, but below T c the new result exhibits several differences and remains physical down to T = 0."
            },
            "slug": "Solution-of-'Solvable-model-of-a-spin-glass'-Thouless-Anderson",
            "title": {
                "fragments": [],
                "text": "Solution of 'Solvable model of a spin glass'"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10398168"
                        ],
                        "name": "J. Kruskal",
                        "slug": "J.-Kruskal",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Kruskal",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kruskal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The stress MDS was introduced by Kruskal in [ 35 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11709679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5abd46dc29463d0e2da7905194e79123be7eaa82",
            "isKey": false,
            "numCitedBy": 4366,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the numerical methods required in our approach to multi-dimensional scaling. The rationale of this approach has appeared previously."
            },
            "slug": "Nonmetric-multidimensional-scaling:-A-numerical-Kruskal",
            "title": {
                "fragments": [],
                "text": "Nonmetric multidimensional scaling: A numerical method"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The numerical methods required in the approach to multi-dimensional scaling are described and the rationale of this approach has appeared previously."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The EM algorithm alternates an estimation step to determine the expected assignments with a maximization step to estimate maximum likelihood values for the cluster centers . Dempster et al. [ 6 ] have proven that the likelihood increas es monotonically under this alternation scheme which demonstrates convergence of the algorithm toward a local maximum of the likelihood function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The most frequently used approach are mixture density models, e.g., Gaussian mixture models with the EM algorithm to infer mixture parameters which locally maximize the likelihood function [ 6 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The equations (14,15) are efficiently solved in an iterative fashion using the expectation maximization (EM) algorithm [ 6 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": true,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30258243"
                        ],
                        "name": "H. Ritter",
                        "slug": "H.-Ritter",
                        "structuredName": {
                            "firstName": "Helge",
                            "lastName": "Ritter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ritter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764560"
                        ],
                        "name": "T. Martinetz",
                        "slug": "T.-Martinetz",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Martinetz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Martinetz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144340430"
                        ],
                        "name": "K. Schulten",
                        "slug": "K.-Schulten",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schulten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schulten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27216232,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "f087259663c7f770cb201bbb07ce86cd6de2067f",
            "isKey": false,
            "numCitedBy": 610,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A process for removing sulfur from crude oil by contacting with calcium carbonate-containing material at atmospheric pressures and temperatures less than about 100 DEG F."
            },
            "slug": "Neural-computation-and-self-organizing-maps-an-Ritter-Martinetz",
            "title": {
                "fragments": [],
                "text": "Neural computation and self-organizing maps - an introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A process for removing sulfur from crude oil by contacting with calcium carbonate-containing material at atmospheric pressures and temperatures less than about 100 DEG F."
            },
            "venue": {
                "fragments": [],
                "text": "Computation and neural systems series"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2338183"
                        ],
                        "name": "M. M\u00e9zard",
                        "slug": "M.-M\u00e9zard",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "M\u00e9zard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00e9zard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145052965"
                        ],
                        "name": "G. Parisi",
                        "slug": "G.-Parisi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Parisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2275144"
                        ],
                        "name": "M. Virasoro",
                        "slug": "M.-Virasoro",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Virasoro",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Virasoro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88803699"
                        ],
                        "name": "D. Thouless",
                        "slug": "D.-Thouless",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Thouless",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Thouless"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ". An in depth discussion when the assumption (50) is valid can be found in [ 31 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The question about the range of validity of Eq. (28) is subtle and it has been studied extensively in statistical physics of disordered systems [ 31 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[4.1] does not capture these fluctuations adequately, as is k nown from statistical physics [ 31 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "... heuristics from the instance size . In the case of completely consistent dissimilarities, i.e ., large for data in different clusters and small for data in the same clusters, constant costs per data point require a scaling 1 . In the opposite case of random dissimilarity values averaging effects necessitate a scaling . A thorough discussion of this point can be found in the statistical physics literature of o ptimization problems [ 31 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120860082,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a96d481911bd25cbfcd58a947db7e69780fab55f",
            "isKey": true,
            "numCitedBy": 3494,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book contains a detailed and self-contained presentation of the replica theory of infinite range spin glasses. The authors also explain recent theoretical developments, paying particular attention to new applications in the study of optimization theory and neural networks. About two-thirds of the book are a collection of the most interesting and pedagogical articles on the subject."
            },
            "slug": "Spin-Glass-Theory-and-Beyond-M\u00e9zard-Parisi",
            "title": {
                "fragments": [],
                "text": "Spin Glass Theory and Beyond"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 131
                            }
                        ],
                        "text": "We introduce an effective internal field ~ hin which simulates the indirect influence of the disorder on the data assignments (see Thouless, Anderson and Palmer [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "[35] D.J. Thouless, P.W. Anderson, and R.G. Palmer, \u201cA Solution to a \u201cSolvable\u201d Model of a Spin Glass,\u201d Philosophical Magazine, vol. 35, p. 593, 1977."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "Naively interchanging the averaging brackets with the nonlinear function in (27) yields the equations (25,26); a refined mean\u2013field approach which is known as the TAP approach [35] models the feedback effects in strongly disordered clustering instances more faithfully than the naive approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "We introduce an effective internal field ~hi which simulates the indirect influence of the disorder on the data assignments (see Thouless, Anderson and Palmer [35])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A solution to a \u201csolvable\u201d model of a spin glass,\u201dPhilosophical"
            },
            "venue": {
                "fragments": [],
                "text": "Magazine  ,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "The standard Mean Average Dissimilarity Linkage algorithms (MADL), also known as Ward\u2019s method (see [4], Sec."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 175
                            }
                        ],
                        "text": "tering on the basis of this data description is achievable by grouping the data to clusters such that the sum of dissimilarities between data of the same cluster is minimized [4], [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "Data clustering is viewed as a partitioning problem throughout this paper and not as a density estimation problem of a mixture model [4], [5] in the sense of parametric statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 23
                            }
                        ],
                        "text": "[4] A.K. Jain and R.C. Dubes, Algorithms for Clustering Data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 148
                            }
                        ],
                        "text": "There is a large body of literature available on this topic and the reader is referred to the text books of Duda and Hart [8] and of Jain and Dubes [4] for an overview."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dubes, Algorithms for Clustering Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 84
                            }
                        ],
                        "text": "The inequality F\u0303(P) F(PGibbs) holds since the Gibbs distribution maximizes entropy [1, 2] for hHiP being kept fixed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 9
                            }
                        ],
                        "text": "[2] E.T. Jaynes, \u201cInformation Theory and Statistical Mechanics II,\u201d\nPhysical Review, vol. 108, pp. 171-190, 1957."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 142
                            }
                        ],
                        "text": "We will describe a stochastic optimization approach to data clustering which relies on the well-known robustness of maximum entropy inference [1, 2, 3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "Equation (61) simplifies to a vector equation for every xi:\nK x y y y i i i\nK\ni i\nK\nM M\u00aa - - = = \u00c2 \u00c2 F HG\nI KJ 1 2 1 2 1 n n n n n m m m ( * .e j (62)\nREFERENCES [1] E.T. Jaynes, \u201cInformation Theory and Statistical Mechanics,\u201d\nPhysical Review, vol. 106, pp. 620-630, 1957."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 9
                            }
                        ],
                        "text": "[3] E.T. Jaynes, \u201cOn the Rationale of Maximum-Entropy Methods,\u201d\nProc IEEE, vol. 70, pp. 939-952, 1982."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 225
                            }
                        ],
                        "text": "Why should we consider a stochastic or deterministic search strategy based on principles from statistical physics? The fundamental relationship between statistical physics and robust statistics has been established by Jaynes [1, 2, 3] who postulated the principle of maximum"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 103
                            }
                        ],
                        "text": "The fundamental relationship between statistical physics and robust statistics has been established by Jaynes [1], [2], [3] who postulated the principle of maximum entropy inference."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information theory and statistical mechanics II"
            },
            "venue": {
                "fragments": [],
                "text": "Physical Review, vol. 108, pp. 171\u2013190, 1957."
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "The presented deterministic annealing algorithm replaces the Monte Carlo method proposed in [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[41] we formulate texture segmentation as a grouping problem with constraints about valid region shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "The segmentation was postprocessed with additional penalties for thin regions as suggested in [41] to enforce local texture consistency and to prevent a too large fragmentation of the texture regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Three major modifications compared to [41] have been introduced: 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boundary detection by constrained optimization,\u201dIEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions on Pattern Analysis and Machine Intelligence ,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "[23] and, independently, \u010cerny [24] have proposed the stochastic optimization strategy Simulated Annealing ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 143
                            }
                        ],
                        "text": "2 STOCHASTIC OPTIMIZATION BY MAXIMUM ENTROPY INFERENCE\n2.1 Simulated Annealing In seminal papers Kirkpatrick et al. [23] and, independently, ( Cerny [24] have proposed the stochastic optimization\nstrategy Simulated Annealing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "[24] V. ( Cerny , \u201cThermodynamical Approach to the Traveling Sales-\nman Problem: an Efficient Simulation Algorithm,\u201d J. Optimization\nTheory and Applications, vol. 45, pp. 41-51, 1985."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Thermodynamical approach to the travelling salesman problem: an efficient simulation algorithm,\u201dJournal"
            },
            "venue": {
                "fragments": [],
                "text": "Optimization Theory and Applications ,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 176227351,
            "fieldsOfStudy": [],
            "id": "79ec67bbbcea16a55afeb7ca782cbd7672f3bd9f",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constrained Nets for Graph Matching and Other Quadratic Assignment Problems"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143929773"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33734211"
                        ],
                        "name": "R. Sibson",
                        "slug": "R.-Sibson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sibson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Other linear projection methods like projection pursuit [38] yield comparable results since no direction is distinguished in the data generation procedure."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 125481163,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ebb53a7e5cff86b2b42d1108a0fa81f571d8894",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-projection-pursuit-Jones-Sibson",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52130176"
                        ],
                        "name": "C. Gardiner",
                        "slug": "C.-Gardiner",
                        "structuredName": {
                            "firstName": "Crispin",
                            "lastName": "Gardiner",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gardiner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A Markov process with a transition matrix (1) converges to an equilibrium probability distribution [ 25 ]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 60479498,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "42914d64da7a84b9d763d8e57b25b4a6ca628c54",
            "isKey": false,
            "numCitedBy": 6650,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Handbook-of-Stochastic-Methods-Gardiner",
            "title": {
                "fragments": [],
                "text": "Handbook of Stochastic Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121401690"
                        ],
                        "name": "Rose",
                        "slug": "Rose",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Rose",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30179787"
                        ],
                        "name": "Gurewitz",
                        "slug": "Gurewitz",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Gurewitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gurewitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123661964"
                        ],
                        "name": "Fox",
                        "slug": "Fox",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[16] for central clustering, states that the assignments are distributed according to the Gibbs distribution"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "in a series of papers [16], [17], [18], [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34476883,
            "fieldsOfStudy": [
                "Physics",
                "Medicine"
            ],
            "id": "3da017fac82eb805c751c3653e323a47f8ee5eb4",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-mechanics-and-phase-transitions-in-Rose-Gurewitz",
            "title": {
                "fragments": [],
                "text": "Statistical mechanics and phase transitions in clustering."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087848"
                        ],
                        "name": "R. Dubes",
                        "slug": "R.-Dubes",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dubes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dubes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "Sets of relational data are abundant in many applications, e.g., in molecular biology, psychology, linguistics, economics and image processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29535089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa4bddbd10eafd8e1b54338517eedfee408f03ae",
            "isKey": false,
            "numCitedBy": 10560,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithms-for-Clustering-Data-Jain-Dubes",
            "title": {
                "fragments": [],
                "text": "Algorithms for Clustering Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Buhmann received a PhD degree in theoretical physics from the Technical University of Munich in 1988. He held postdoctoral positions at the University of Southern California and at the Lawrence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 113
                            }
                        ],
                        "text": "The method of deterministic annealing is described in various papers mostly in the literature on neural networks [9, 10, 11, 12] and on computer vision [13, 14, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Utans, \u201cStatistical physics, mixtures of distributions and the EM algorithm,\u201dNeural"
            },
            "venue": {
                "fragments": [],
                "text": "Computation  ,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Non-Linear Mapping for Data Structure in computer science from the University of"
            },
            "venue": {
                "fragments": [],
                "text": "A Non-Linear Mapping for Data Structure in computer science from the University of"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principal Curves J. American Statistitory data analysis. His research interests in- C U ~ Ass'n"
            },
            "venue": {
                "fragments": [],
                "text": "Principal Curves J. American Statistitory data analysis. His research interests in- C U ~ Ass'n"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spin Glnss Theory and Beyond"
            },
            "venue": {
                "fragments": [],
                "text": "Singapore: World Scientific"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical mechanics and phase transitions in cluster"
            },
            "venue": {
                "fragments": [],
                "text": "Physical Review Letters"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "4, in which 825 word fragments have been compared by a dynamic programming algorithm [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representing words in connectionist models."
            },
            "venue": {
                "fragments": [],
                "text": "Abstract of the 34th Annual Meeting of the Psychonomics Society,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "A partitioning approach known as central clustering derives a set of reference or prototype vectors which quantize a set of vectorial data with minimal quantization error [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vector Quantization and Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Vector Quantization and Signal Processing"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elements oflnformation Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Elements oflnformation Theory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 125
                            }
                        ],
                        "text": "The Gabor transformation possesses a bandpass charactericstics and is known to display good texture disrimination properties [39, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gabor filters as texture discriminators"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics, vol. 61, pp. 103\u2013113, 1989."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Equations (14,15) are efficiently solved in an iterative fashion using the expectation maximization (EM) algorithm [31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[31] have proven that the likelihood increases monotonically under this alternation scheme which demonstrates convergence of the algorithm toward a local maximum of the likelihood function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM algorithm,\u201dJournal"
            },
            "venue": {
                "fragments": [],
                "text": "Royal Statistical Society Ser. B (methodological)  ,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Buhmann received a PhD degree in theoretical physics from the Technical University of Munich in"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical Field Theory"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representing Words in Connectionist graphical models, machine learning, and Models"
            },
            "venue": {
                "fragments": [],
                "text": "Abstract 34th Ann. Meeting Psychonomics Society"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kiihnel, \"Vector Quantization with Complexity Costs,"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 49
                            }
                        ],
                        "text": "with different rate constraints was suggested in [20, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "uhnel, \u201cComplexity optimized data clustering by competitive neural networks,\u201dNeural"
            },
            "venue": {
                "fragments": [],
                "text": "Computation  ,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization by Simulated AnnealingThermodynamical Approach to the Traveling Salesman Problem: an Efficient Simulation Algorithm Handbook of Stochastic Methods"
            },
            "venue": {
                "fragments": [],
                "text": "Science J. Optimization Theory and Applications"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Solution to a \u201dSolvable"
            },
            "venue": {
                "fragments": [],
                "text": "Model of a Spin Glass,\u201d Philosophical Magazine,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonmetric Multidimensional Scaling: a Numerical joined the Computer Vision and Pattern Rec- Method"
            },
            "venue": {
                "fragments": [],
                "text": "Analysis IEEE Trans. Computers he 1371 J.B. Kruskal Psychometrika"
            },
            "year": 1964
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 26,
            "methodology": 22,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 62,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Pairwise-Data-Clustering-by-Deterministic-Annealing-Hofmann-Buhmann/3a58c3eafcc642ffa2e571e069e53f20bb1d1150?sort=total-citations"
}