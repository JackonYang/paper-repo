{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103188660"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "A.R.",
                            "lastName": "Barron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 51276807,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "72d761afbe35634213849419ff63fad5bc9fabeb",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Convergence properties of empirically estimated neural networks are examined. In this theory, an appropriate size feedforward network is automatically determined from the data. The networks studied include two- and three-layer networks with an increasing number of simple sigmoidal nodes, multiple-layer polynomial networks, and networks with certain fixed structures but an increasing complexity in each unit. Each of these classes of networks is dense in the space of continuous functions on compact subsets of d-dimensional Euclidean space, with respect to the topology of uniform convergence. It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases. Bounds on the rate of convergence are given in terms of an index of the approximation capability of the class of networks.<<ETX>>"
            },
            "slug": "Statistical-properties-of-artificial-neural-Barron",
            "title": {
                "fragments": [],
                "text": "Statistical properties of artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 28th IEEE Conference on Decision and Control,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14470590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee7f0bc85b339d781c2e0c7e6db8e339b6b9fec2",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "K.M. Hornik, M. Stinchcombe, and H. White (Univ. of California at San Diego, Dept. of Economics Discussion Paper, June 1988; to appear in Neural Networks) showed that multilayer feedforward networks with as few as one hidden layer, no squashing at the output layer, and arbitrary sigmoid activation function at the hidden layer are universal approximators: they are capable of arbitrarily accurate approximation to arbitrary mappings, provided sufficiently many hidden units are available. The present authors obtain identical conclusions but do not require the hidden-unit activation to be sigmoid. Instead, it can be a rather general nonlinear function. Thus, multilayer feedforward networks possess universal approximation capabilities by virtue of the presence of intermediate layers with sufficiently many parallel processors; the properties of the intermediate-layer activation function are not so crucial. In particular, sigmoid activation functions are not necessary for universal approximation.<<ETX>>"
            },
            "slug": "Universal-approximation-using-feedforward-networks-Stinchcombe-White",
            "title": {
                "fragments": [],
                "text": "Universal approximation using feedforward networks with non-sigmoid hidden layer activation functions"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Multilayer feedforward networks possess universal approximation capabilities by virtue of the presence of intermediate layers with sufficiently many parallel processors; the properties of the intermediate-layer activation function are not so crucial."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10203109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386cbc45ceb59a7abb844b5078e5c944f17723b4",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximate-realization-of-continuous-by-Funahashi",
            "title": {
                "fragments": [],
                "text": "On the approximate realization of continuous mappings by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17355,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43711678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656a33c1db546da8490d6eba259e2a849d73a001",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."
            },
            "slug": "Learning-in-Artificial-Neural-Networks:-A-White",
            "title": {
                "fragments": [],
                "text": "Learning in Artificial Neural Networks: A Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398229863"
                        ],
                        "name": "R. Hecht-Nielsen",
                        "slug": "R.-Hecht-Nielsen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hecht-Nielsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hecht-Nielsen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5691634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4457792a247c0eb6c6fb11a1d92f6f45b82acc1",
            "isKey": false,
            "numCitedBy": 1796,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-the-backpropagation-neural-network-Hecht-Nielsen",
            "title": {
                "fragments": [],
                "text": "Theory of the backpropagation neural network"
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10158697,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "21e82ed12c620fba1f5ee42162962aae74a23510",
            "isKey": false,
            "numCitedBy": 4061,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "In the paper \"Approximation by Superpositions of a SigmoidaI Function\" [C], the proof given for Lemma i is incorrect since it relies on the erroneous statement that simple functions are dense in L=(R). The author has pointed out that the proof in I'C] can be corrected by changing, at the bottom of page 307 and the top of page 308, the occurrences of L~(R) to L=(J) for a compact interval, J, containing {yrx lx ~ I,}, where y is fLxed. It should also be noted that the reduction of multidimensional density to one-dimensional density as in the proof of Lemma 1 had previously been obtained by Dahmen and Micchelli, using the same techniques, in work on ridge regression (see Lemma 3.2 of [DM]). We thank Raymond T, Melton, who pointed out the error in the proof of Lemma 1 in [C] and supplied a proof, showing that the Fourier transform of the measure /~ must be zero because the/~-measure of every half-plane is zero [M]."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The reduction of multidimensional density to one-dimensional density as in the proof of Lemma 1 had previously been obtained by Dahmen and Micchelli, using the same techniques, in work on ridge regression."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955900"
                        ],
                        "name": "C. Hwang",
                        "slug": "C.-Hwang",
                        "structuredName": {
                            "firstName": "Chii-Ruey",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hwang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14650349,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2d00d92e9ff17ce7fe991e700a1ce8ced639c2c8",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum likelihood estimation often fails when the parameter takes values in an infinite dimensional space. For example, the maximum likelihood method cannot be applied to the completely nonparametric estimation of a density function from an iid sample; the maximum of the likelihood is not attained by any density. In this example, as in many other examples, the parameter space (positive functions with area one) is too big. But the likelihood method can often be salvaged if we first maximize over a constrained subspace of the parameter space and then relax the constraint as the sample size grows. This is Grenander's \"method of sieves.\" Application of the method sometimes leads to new estimators for familiar problems, or to a new motivation for an already well-studied technique. We will establish some general consistency results for the method, and then we will focus on three applications."
            },
            "slug": "Nonparametric-Maximum-Likelihood-Estimation-by-the-Geman-Hwang",
            "title": {
                "fragments": [],
                "text": "Nonparametric Maximum Likelihood Estimation by the Method of Sieves"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144371774"
                        ],
                        "name": "S. M. Carroll",
                        "slug": "S.-M.-Carroll",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Carroll",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. M. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2203239"
                        ],
                        "name": "B. Dickinson",
                        "slug": "B.-Dickinson",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dickinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18058503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e4bd5422c82009290a5cd71457388f0780530d6",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/. The net uses n input nodes, a single hidden layer whose width is determined by the function to be implemented and the allowable mean square error, and a linear output neuron. Error bounds and an example are given for the method.<<ETX>>"
            },
            "slug": "Construction-of-neural-nets-using-the-radon-Carroll-Dickinson",
            "title": {
                "fragments": [],
                "text": "Construction of neural nets using the radon transform"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748381"
                        ],
                        "name": "N. Baba",
                        "slug": "N.-Baba",
                        "structuredName": {
                            "firstName": "Norio",
                            "lastName": "Baba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Baba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6030126,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec77bf664e4ddef9299c2f5da2235f0b18e38522",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-approach-for-finding-the-global-minimum-of-of-Baba",
            "title": {
                "fragments": [],
                "text": "A new approach for finding the global minimum of error function of neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 189781595,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "02cb8ef325adcc6f29e0b1759920527836ea8b2b",
            "isKey": false,
            "numCitedBy": 1334,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how to choose the smoothing parameter when a smoothing periodic spline of degree 2m\u22121 is used to reconstruct a smooth periodic curve from noisy ordinate data. The noise is assumed \u201cwhite\u201d, and the true curve is assumed to be in the Sobolev spaceW2(2m) of periodic functions with absolutely continuousv-th derivative,v=0, 1, ..., 2m\u22121 and square integrable 2m-th derivative. The criteria is minimum expected square error, averaged over the data points. The dependency of the optimum smoothing parameter on the sample size, the noise variance, and the smoothness of the true curve is found explicitly."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50012345"
                        ],
                        "name": "W. Rudin",
                        "slug": "W.-Rudin",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Rudin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rudin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119027153,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f20b8e819512d17f59435043a18573127fea7d82",
            "isKey": false,
            "numCitedBy": 6995,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Prologue: The Exponential Function Chapter 1: Abstract Integration Set-theoretic notations and terminology The concept of measurability Simple functions Elementary properties of measures Arithmetic in [0, ] Integration of positive functions Integration of complex functions The role played by sets of measure zero Exercises Chapter 2: Positive Borel Measures Vector spaces Topological preliminaries The Riesz representation theorem Regularity properties of Borel measures Lebesgue measure Continuity properties of measurable functions Exercises Chapter 3: Lp-Spaces Convex functions and inequalities The Lp-spaces Approximation by continuous functions Exercises Chapter 4: Elementary Hilbert Space Theory Inner products and linear functionals Orthonormal sets Trigonometric series Exercises Chapter 5: Examples of Banach Space Techniques Banach spaces Consequences of Baire's theorem Fourier series of continuous functions Fourier coefficients of L1-functions The Hahn-Banach theorem An abstract approach to the Poisson integral Exercises Chapter 6: Complex Measures Total variation Absolute continuity Consequences of the Radon-Nikodym theorem Bounded linear functionals on Lp The Riesz representation theorem Exercises Chapter 7: Differentiation Derivatives of measures The fundamental theorem of Calculus Differentiable transformations Exercises Chapter 8: Integration on Product Spaces Measurability on cartesian products Product measures The Fubini theorem Completion of product measures Convolutions Distribution functions Exercises Chapter 9: Fourier Transforms Formal properties The inversion theorem The Plancherel theorem The Banach algebra L1 Exercises Chapter 10: Elementary Properties of Holomorphic Functions Complex differentiation Integration over paths The local Cauchy theorem The power series representation The open mapping theorem The global Cauchy theorem The calculus of residues Exercises Chapter 11: Harmonic Functions The Cauchy-Riemann equations The Poisson integral The mean value property Boundary behavior of Poisson integrals Representation theorems Exercises Chapter 12: The Maximum Modulus Principle Introduction The Schwarz lemma The Phragmen-Lindelof method An interpolation theorem A converse of the maximum modulus theorem Exercises Chapter 13: Approximation by Rational Functions Preparation Runge's theorem The Mittag-Leffler theorem Simply connected regions Exercises Chapter 14: Conformal Mapping Preservation of angles Linear fractional transformations Normal families The Riemann mapping theorem The class L Continuity at the boundary Conformal mapping of an annulus Exercises Chapter 15: Zeros of Holomorphic Functions Infinite Products The Weierstrass factorization theorem An interpolation problem Jensen's formula Blaschke products The Muntz-Szas theorem Exercises Chapter 16: Analytic Continuation Regular points and singular points Continuation along curves The monodromy theorem Construction of a modular function The Picard theorem Exercises Chapter 17: Hp-Spaces Subharmonic functions The spaces Hp and N The theorem of F. and M. Riesz Factorization theorems The shift operator Conjugate functions Exercises Chapter 18: Elementary Theory of Banach Algebras Introduction The invertible elements Ideals and homomorphisms Applications Exercises Chapter 19: Holomorphic Fourier Transforms Introduction Two theorems of Paley and Wiener Quasi-analytic classes The Denjoy-Carleman theorem Exercises Chapter 20: Uniform Approximation by Polynomials Introduction Some lemmas Mergelyan's theorem Exercises Appendix: Hausdorff's Maximality Theorem Notes and Comments Bibliography List of Special Symbols Index"
            },
            "slug": "Real-and-complex-analysis-Rudin",
            "title": {
                "fragments": [],
                "text": "Real and complex analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117841"
                        ],
                        "name": "A. Gallant",
                        "slug": "A.-Gallant",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gallant",
                            "middleNames": [
                                "Ronald"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gallant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751657"
                        ],
                        "name": "D. Nychka",
                        "slug": "D.-Nychka",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Nychka",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nychka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121519004,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "da588a04e39cc38762828bb42d288b938b5e0bee",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Often maximum likelihood is the method of choice for fitting an econometric model to data but cannot be used because the correct specific ation of (multivariate) density that defines the likelihood is unknown. In this situation, simply put the density equal to a Hermite series and apply standard finite dimensional maximum likelihood methods. Model parameters and nearly all aspects of the unknown density itself will be estimated consistently provided that the length of the series increases with sample size. The rule for increasing series length can be data dependent. The method is applied to nonlinear regression with sample selection. Copyright 1987 by The Econometric Society."
            },
            "slug": "Semi-nonparametric-Maximum-Likelihood-Estimation-Gallant-Nychka",
            "title": {
                "fragments": [],
                "text": "Semi-nonparametric Maximum Likelihood Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34923849"
                        ],
                        "name": "G. Lorentz",
                        "slug": "G.-Lorentz",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Lorentz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lorentz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121058713,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9744f03355bccb69a0563d1f47d59206e5f9e9ec",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Possibility of Approximation: 1. Basic notions 2. Linear operators 3. Approximation theorems 4. The theorem of Stone 5. Notes Polynomials of Best Approximation: 1. Existence of polynomials of best approximation 2. Characterization of polynomials of best approximation 3. Applications of convexity 4. Chebyshev systems 5. Uniqueness of polynomials of best approximation 6. Chebyshev's theorem 7. Chebyshev polynomials 8. Approximation of some complex functions 9. Notes Properties of Polynomials and Moduli of Continuity: 1. Interpolation 2. Inequalities of Bernstein 3. The inequality of Markov 4. Growth of polynomials in the complex plane 5. Moduli of continuity 6. Moduli of smoothness 7. Classes of functions 8. Notes The Degree of Approximation by Trigonometric Polynomials: 1. Generalities 2. The theorem of Jackson 3. The degree of approximation of differentiable functions 4. Inverse theorems 5. Differentiable functions 6. Notes The Degree of Approximation by Algebraic Polynomials: 1. Preliminaries 2. The approximation theorems 3. Inequalities for the derivatives of polynomials 4. Inverse theorems 5. Approximation of analytic functions 6. Notes Approximation by Rational Functions. Functions of Several Variables: 1. Degree of rational approximation 2. Inverse theorems 3. Periodic functions of several variables 4. Approximation by algebraic polynomials 5. Notes Approximation by Linear Polynomial Operators: 1. Sums of de la Vallee-Poussin. Positive operators 2. The principle of uniform boundedness 3. Operators that preserve trigonometric polynomials 4. Trigonometric saturation classes 5. The saturation class of the Bernstein polynomials 6. Notes Approximation of Classes of Functions: 1. Introduction 2. Approximation in the space 3. The degree of approximation of the classes 4. Distance matrices 5. Approximation of the classes 6. Arbitrary moduli of continuity Approximation by operators 7. Analytic functions 8. Notes Widths: 1. Definitions and basic properties 2. Sets of continuous and differentiable functions 3. Widths of balls 4. Applications of theorem 2 5. Differential operators 6. Widths of the sets 7. Notes Entropy: 1. Entropy and capacity 2. Sets of continuous and differentiable functions 3. Entropy of classes of analytic functions 4. More general sets of analytic functions 5. Relations between entropy and widths 6. Notes Representation of Functions of Several Variables by Functions of One Variable: 1. The Theorem of Kolmogorov 2. The fundamental lemma 3. The completion of the proof 4. Functions not representable by superpositions 5. Notes Bibliography Index."
            },
            "slug": "Approximation-of-Functions-Lorentz",
            "title": {
                "fragments": [],
                "text": "Approximation of Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13533363,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "37807e97c624fb846df7e559553b32539ba2ea5d",
            "isKey": false,
            "numCitedBy": 1805,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Universal-approximation-of-an-unknown-mapping-and-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2637505"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119965968,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b14391b13324eb91b7ed1cf4c4a29e0bc47373ae",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Given data $z_i = g(t_i ) + \\varepsilon _i , 1 \\leqq i \\leqq n$, where g is the unknown function, the $t_i $ are known d-dimensional variables in a domain $\\Omega $, and the $\\varepsilon _i $ are i..."
            },
            "slug": "MULTIVARIATE-SMOOTHING-SPLINE-FUNCTIONS-Cox",
            "title": {
                "fragments": [],
                "text": "MULTIVARIATE SMOOTHING SPLINE FUNCTIONS"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Given data, g is the unknown function, the t_i and the varepsilon variables are known d-dimensional variables in a domain $\\Omega $, and the $\\varpsilon _i $ are i..."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145343252"
                        ],
                        "name": "S. Wold",
                        "slug": "S.-Wold",
                        "structuredName": {
                            "firstName": "Svante",
                            "lastName": "Wold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wold"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120116272,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "99a0f1bc2b4535406d780957cc5cc937d987d176",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The cross validation mean square error technique is used to determine the correct degree of smoothing, in fitting smoothing solines to discrete, noisy observations from some unknown smooth function. Monte Cario results snow amazing success in estimating the true smooth function as well as its derivative."
            },
            "slug": "A-completely-automatic-french-curve:-fitting-spline-Wahba-Wold",
            "title": {
                "fragments": [],
                "text": "A completely automatic french curve: fitting spline functions by cross validation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121982591,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d38e40481ccebc43c9c2e50b1739f2c951a7e368",
            "isKey": false,
            "numCitedBy": 1667,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Linear Model and Instrumental Variables Estimators. Consistency. Laws of Large Numbers. Asymptotic Normality. Central Limit Theory. Estimating Asymptotic Covariance Matrices. Functional Central Limit Theory and Applications. Directions for Further Study. Solution Set. References. Index."
            },
            "slug": "Asymptotic-theory-for-econometricians-White",
            "title": {
                "fragments": [],
                "text": "Asymptotic theory for econometricians"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14942773"
                        ],
                        "name": "M. Stone",
                        "slug": "M.-Stone",
                        "structuredName": {
                            "firstName": "Mervyn",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62698647,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "7b28610d2d681a11398eb614de0d70d7de41c20c",
            "isKey": false,
            "numCitedBy": 7502,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance."
            },
            "slug": "Cross\u2010Validatory-Choice-and-Assessment-of-Stone",
            "title": {
                "fragments": [],
                "text": "Cross\u2010Validatory Choice and Assessment of Statistical Predictions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115790757"
                        ],
                        "name": "A. Kolmogorov",
                        "slug": "A.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Andrey",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145580592"
                        ],
                        "name": "S. V. Fomin",
                        "slug": "S.-V.-Fomin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Fomin",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Fomin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62679408,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7b7ae57c9380fe4248743678c5a9e8360e2849bf",
            "isKey": false,
            "numCitedBy": 1261,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introductory real analysis , Introductory real analysis , \u06a9\u062a\u0627\u0628\u062e\u0627\u0646\u0647 \u062f\u06cc\u062c\u06cc\u062a\u0627\u0644 \u062c\u0646\u062f\u06cc \u0634\u0627\u067e\u0648\u0631 \u0627\u0647\u0648\u0627\u0632"
            },
            "slug": "Introductory-Real-Analysis-Kolmogorov-Fomin",
            "title": {
                "fragments": [],
                "text": "Introductory Real Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49243178"
                        ],
                        "name": "Ker-Chau Li",
                        "slug": "Ker-Chau-Li",
                        "structuredName": {
                            "firstName": "Ker-Chau",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ker-Chau Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121197559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb78be46f2b0e0a6a8117ed0949e62a1b09b962b",
            "isKey": false,
            "numCitedBy": 517,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Asymptotic-Optimality-for-$C_p,-C_L$,-and-Discrete-Li",
            "title": {
                "fragments": [],
                "text": "Asymptotic Optimality for $C_p, C_L$, Cross-Validation and Generalized Cross-Validation: Discrete Index Set"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12753750"
                        ],
                        "name": "H. Bierens",
                        "slug": "H.-Bierens",
                        "structuredName": {
                            "firstName": "Herman",
                            "lastName": "Bierens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bierens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120368951,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "af8b3c62eb35efc23a50e65547828e38d8ff9eab",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Advances-in-Econometrics:-Kernel-estimators-of-Bierens",
            "title": {
                "fragments": [],
                "text": "Advances in Econometrics: Kernel estimators of regression functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145856628"
                        ],
                        "name": "L. Scales",
                        "slug": "L.-Scales",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Scales",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Scales"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117177932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b58ce0f74c05daeee3c66158d5c1e6ec8c2397b6",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Non-Linear-Optimization-Scales",
            "title": {
                "fragments": [],
                "text": "Introduction to Non-Linear Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "~ - cntropy and ~capacity of sets in functional spaces"
            },
            "venue": {
                "fragments": [],
                "text": "American Mathematical Society Translations"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multilayer feed forward networks arc universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cross - wfliditorv choice and assessment of statistical predictions , Journal O [ ' the Royal Stati , stical Society"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new approach tor finding the global mJ m m u m of crror fu action of neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks . 2 . Proceedings of the 28 th IEEE ( onference on Dectsion and Control"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate smooth ing splines"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Journal of Numerical Analysis"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Seminonparametric maximum likelihood estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Econometrica"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmold function"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematics qf Control , Signals and Systems"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 32,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Connectionist-nonparametric-regression:-Multilayer-White/77fdd39ab366b65a617015a72fe8dc9d0b394d64?sort=total-citations"
}