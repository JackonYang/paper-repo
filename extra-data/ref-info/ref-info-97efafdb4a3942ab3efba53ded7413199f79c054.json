{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 76
                            }
                        ],
                        "text": "Eligibility traces came into reinforcement learning via the fecund ideas of Klopf (1972). Our use of eligibility traces is based on Klopf\u2019s work (Sutton, 1978a, 1978b, 1978c; Barto and Sutton, 1981a, 1981b; Sutton and Barto, 1981a; Barto, Sutton, and Anderson, 1983; Sutton, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1708582,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "12d1d070a53d4084d88a77b8b143bad51c40c38f",
            "isKey": false,
            "numCitedBy": 7376,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."
            },
            "slug": "Reinforcement-Learning:-A-Survey-Kaelbling-Littman",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Central issues of reinforcement learning are discussed, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3248358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
            "isKey": false,
            "numCitedBy": 990,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks:adaptive heuristic critic (AHC) learning due to Sutton,Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks."
            },
            "slug": "Self-improving-reactive-agents-based-on-learning,-Lin",
            "title": {
                "fragments": [],
                "text": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper compares eight reinforcement learning frameworks: Adaptive heuristic critic (AHC) learning due to Sutton, Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning and two extensions are experience replay, learning action models for planning, and teaching."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3466704"
                        ],
                        "name": "Gavin Adrian Rummery",
                        "slug": "Gavin-Adrian-Rummery",
                        "structuredName": {
                            "firstName": "Gavin",
                            "lastName": "Rummery",
                            "middleNames": [
                                "Adrian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gavin Adrian Rummery"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 99
                            }
                        ],
                        "text": "7 Sarsa(\u03bb) with accumulating traces was first explored as a control method by Rummery and Niranjan (1994; Rummery, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14043639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae35e9d47cc2db815ccd4346d4df462fce953960",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis is concerned with practical issues surrounding the application of reinforcement learning techniques to tasks that take place in high dimensional continuous state-space environments. In particular, the extension of on-line updating methods is considered, where the term implies systems that learn as each experience arrives, rather than storing the experiences for use in a separate oo-line learning phase. Firstly, the use of alternative update rules in place of standard Q-learning (Watkins 1989) is examined to provide faster convergence rates. Secondly, the use of multi-layer perceptron (MLP) neural networks (Rumelhart, Hinton and Williams 1986) is investigated to provide suitable generalising function approximators. Finally, consideration is given to the combination of Adaptive Heuristic Critic (AHC) methods and Q-learning to produce systems combining the beneets of real-valued actions and discrete switching. The diierent update rules examined are based on Q-learning combined with the TD() algorithm (Sutton 1988). Several new algorithms, including Modiied Q-Learning and Summation Q-Learning, are examined, as well as alternatives such as Q() (Peng and Williams 1994). In addition, algorithms are presented for applying these Q-learning updates to train MLPs on-line during trials, as opposed to the backward-replay method used by Lin (1993b) that requires waiting until the end of each trial before updating can occur. The performance of the update rules is compared on the Race Track problem of Barto, Bradtke and Singh (1993) using a lookup table representation for the Q-function. Some of the methods are found to perform almost as well as Real-Time Dynamic Programming, despite the fact that the latter has the advantage of a full world model. The performance of the connectionist algorithms is compared on a larger and more complex robot navigation problem. Here a simulated mobile robot is trained to guide itself to a goal position in the presence of obstacles. The robot must rely on limited sensory feedback from its surroundings and make decisions that can be generalised to arbitrary layouts of obstacles. These simulations show that the performance of on-line learning algorithms is less sensitive to the choice of training parameters than backward-replay, and that the alternative Q-learning rules of Modiied Q-Learning and Q() are more robust than standard Q-learning updates. Finally, a combination of real-valued AHC and Q-learning, called Q-AHC learning, is presented, and various architectures are compared in performance on the robot problem. The resulting reinforcement learning system has the properties of providing on-line training, parallel computation, generalising function \u2026"
            },
            "slug": "Problem-solving-with-reinforcement-learning-Rummery",
            "title": {
                "fragments": [],
                "text": "Problem solving with reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This thesis is concerned with practical issues surrounding the application of reinforcement learning techniques to tasks that take place in high dimensional continuous state-space environments, and the extension of on-line updating methods is considered, where the term implies systems that learn as each experience arrives, rather than storing the experiences in a separate oo-line learning phase."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055155244"
                        ],
                        "name": "David Chapman",
                        "slug": "David-Chapman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chapman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chapman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7213327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45d2bdf5e7072c1d5a91a38efa365715def2f45d",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Delayed reinforcement learning is an attractive framework for the unsupervised learning of action policies for autonomous agents. Some existing delayed reinforcement learning techniques have shown promise in simple domains. However, a number of hurdles must be passed before they are applicable to realistic problems. This paper describes one such difficulty, the input generalization problem (whereby the system must generalize to produce similar actions in similar situations) and an implemented solution, the G algorithm. This algorithm is based on recursive splitting of the state space based on statistical measures of differences in reinforcements received. Connectionist backpropagation has previously been used for input generalization in reinforcement learning. We compare the two techniques analytically and empirically. The G algorithm's sound statistical basis makes it easy to predict when it should and should not work, whereas the behavior of back-propagation is unpredictable. We found that a previous successful use of backpropagation can be explained by the linearity of the application domain. We found that in another domain, G reliably found the optimal policy, whereas none of a set of runs of backpropagation with many combinations of parameters did."
            },
            "slug": "Input-Generalization-in-Delayed-Reinforcement-An-Chapman-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes the input generalization problem (whereby the system must generalize to produce similar actions in similar situations) and an implemented solution, the G algorithm, which is based on recursive splitting of the state space based on statistical measures of differences in reinforcements received."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18335610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d8f86d4e674fe735af032a5518b0cf1c4a34ace",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Computational-Economics-of-Reinforcement-Barto-Singh",
            "title": {
                "fragments": [],
                "text": "On the Computational Economics of Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3564227"
                        ],
                        "name": "C. Shelton",
                        "slug": "C.-Shelton",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Shelton",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shelton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 912182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee2c379e8124d8dd58dd1f0272fc45d32c7d1d36",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis considers three complications that arise from applying reinforcement learning to a real-world application. In the process of using reinforcement learning to build an adaptive electronic market-maker, we find the sparsity of data, the partial observability of the domain, and the multiple objectives of the agent to cause serious problems for existing reinforcement learning algorithms. We employ importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data. Our importance sampling estimator requires no knowledge about the environment and places few restrictions on the method of collecting data. It can be used efficiently with reactive controllers, finite-state controllers, or policies with function approximation. We present theoretical analyses of the estimator and incorporate it into a reinforcement learning algorithm. Additionally, this method provides a complete return surface which can be used to balance multiple objectives dynamically. We demonstrate the need for multiple goals in a variety of applications and natural solutions based on our sampling method. The thesis concludes with example results from employing our algorithm to the domain of automated electronic market-making. Thesis Supervisor: Tomaso Poggio Title: Professor of Brain and Cognitive Science"
            },
            "slug": "Importance-sampling-for-reinforcement-learning-with-Shelton",
            "title": {
                "fragments": [],
                "text": "Importance sampling for reinforcement learning with multiple objectives"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This thesis considers three complications that arise from applying reinforcement learning to a real-world application, and employs importance sampling (likelihood ratios) to achieve good performance in partially observable Markov decision processes with few data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150165387"
                        ],
                        "name": "J. Urgen Schmidhuber",
                        "slug": "J.-Urgen-Schmidhuber",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Urgen Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urgen Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15755817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "290d1e46b71673b35231cb3bcd848912782af6f1",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Much of the recent research on adaptive neuro-control and reinforcement learning focusses on systems with adaptiv\u00e8world models'. Previous approaches, however, do not address the problem of modelling the reliability of the world model's predictions in uncertain environments. Furthermore, with previous approaches usually some ad-hoc method (like random search) is used to train the world model to predict future environmental inputs from previous inputs and control outputs of the system. This paper introduces ways for modelling the reliability of the outputs of adaptive predictors, and it describes more sophisticated and sometimes more ecient methods for their adaptive construction by on-line state space exploration: For instance, a 4-network reinforcement learning system is described which tries to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. The system is`curious' in the sense that it actively tries to provoke situations for which it learned to expect to learn something about the environment. An experiment with an articial non-deterministic environment demonstrates that the method can be faster than the conventional model-building strategy. Much of the recent research on adaptive neuro-control and reinforcement learning focusses on systems with sub-modules that learn to predict inputs from the environment. These sub-modules often are called\u00e0daptive world models'; they are useful for a whole variety of control tasks. For instance, Werbos' and Jordan's architectures for neuro-control [16][3] contain an adaptive world model in form of a back-propagation module (the model network) which is trained to predict the next input, given the current input and the current output of an adaptive control network. The model network allows to compute error gradients for the controller outputs. This is essential, since with typical adaptive neuro-control tasks there is no teacher who provides desired controller outputs. There is only a desired environmental input. Extensions of this approach (e.g. [11]) rely on the same basic principles. Sutton's `DYNA-systems' [13] use adaptive world models for limiting the number of`real-world experiences' necessary to solve certain reinforcement learning tasks. There are at least two important problems with all of these approaches that have not been addressed so far: 1. Previous model-building control systems are not well-suited for uncertain non-deterministic environments. In particular, they do not model the reliability of the predictions of the adaptive world models. Therefore, if credit assignment for the controller is based on the assumption of a correct world model, unexpected results may be obtained. 2. Previous model-building control \u2026"
            },
            "slug": "Adaptive-Confidence-and-Adaptive-Curiosity-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Adaptive Confidence and Adaptive Curiosity"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper introduces ways for modelling the reliability of the outputs of adaptive predictors, and it describes more sophisticated and sometimes more ecient methods for their adaptive construction by on-line state space exploration."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144492550"
                        ],
                        "name": "C. Tham",
                        "slug": "C.-Tham",
                        "structuredName": {
                            "firstName": "Chen-Khong",
                            "lastName": "Tham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tham"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 44
                            }
                        ],
                        "text": "used in many reinforcement learning systems (e.g., Shewchuk and Dean, 1990; Lin and Kim, 1991; Miller, Scalera, and Kim, 1994; Sofge and White, 1992; Tham, 1994; Sutton, 1996; Watkins, 1989) as well as in other types of learning control systems (e."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 77
                            }
                        ],
                        "text": "2 Action-value methods for our k-armed bandit problem were first proposed by Thathachar and Sastry (1985). These are often called estimator algorithms in the learning automata literature."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15813899,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb4a564d2c116a614b9d35aa1f320bb396ced97",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement l e a r n i n g i s a p o werful learning paradigm for autonomous agents which i n teract with unknown environments with the objective of maximizing cumulative p a yoo. Recent research has addressed issues concerning the scaling up of reinforcement learning methods in order to solve problems with large state spaces, composite tasks and tasks involving non-Markovian situations. In this dissertation, I extend existing ways of scaling up reinforcement learning methods and propose several new approaches. An array of Cerebellar Model Articulation Controller (CMAC) networks is used as fast function approximators so that the evaluation function and policy can be learnt on-line as the agent i n teracts with the environment. Learning systems which combine reinforcement learning techniques with CMAC networks are developed to solve problems with large state and action spaces. Actions can be either discrete or real-valued. The problem of generating a sequence of torque or position change commands in order to drive a simulated multi-linked manipulator towards desired arm conngurations is examined. A hierarchical and modular function approximation architecture using CMAC n e t works is then developed, following the Hierarchical Mixtures of Experts framework. The non-linear function approximation ability o f C M A C n e t works enables non-linear functions to be modelled in expert and gating networks, while permitting fast linear learning rules to be used. An on-line gradient ascent learning procedure derived from the Expectation Maximization algorithm is proposed, enabling faster learning to be achieved. The new architecture can be used to enable reinforcement learning agents to acquire context-dependent evaluation functions and policies. This is demonstrated in an implementation of the Compositional Q-Learning framework in which composite tasks consisting of several elemental tasks are decomposed using reinforcement learning. The framework is extended to the case where rewards can be received in non-terminal states of elemental tasks, and t\u00f2vector of actions' situations where the agent produces several coordinated actions in order to achieve a goal. The resulting system is employed to enable the simulated multi-linked manipulator to position its end-eeector at several positions in the workspace sequentially. Finally, the beneets of using prior knowledge in order to extend the capabilities of reinforcement learning agents are examined. A classiier system-based Q-learning scheme is developed to enable agents to reason using condition-action rules. The utility o f t h i s s c heme is illustrated in a \u2026"
            },
            "slug": "Modular-on-line-function-approximation-for-scaling-Tham",
            "title": {
                "fragments": [],
                "text": "Modular on-line function approximation for scaling up reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This dissertation extends existing ways of scaling up reinforcement learning methods and proposes several new approaches that can be used to enable reinforcement learning agents to acquire context-dependent evaluation functions and policies."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 211
                            }
                        ],
                        "text": "Indeed, proponents of this method have proposed increasing its speed by combining it with faster semi-gradient methods initially, then gradually switching over to residual gradient for the convergence guarantee (Baird and Moore, 1999)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4512463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab2ada3c97ad6f0ae79ca8b7b459f18d77119f9c",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple learning rule is derived, the VAPS algorithm, which can be instantiated to generate a wide range of new reinforcement-learning algorithms. These algorithms solve a number of open problems, define several new approaches to reinforcement learning, and unify different approaches to reinforcement learning under a single theory. These algorithms all have guaranteed convergence, and include modifications of several existing algorithms that were known to fail to converge on simple MOPs. These include Q-learning, SARSA, and advantage learning. In addition to these value-based algorithms it also generates pure policy-search reinforcement-learning algorithms, which learn optimal policies without learning a value function. In addition, it allows policy-search and value-based algorithms to be combined, thus unifying two very different approaches to reinforcement learning into a single Value and Policy Search (VAPS) algorithm. And these algorithms converge for POMDPs without requiring a proper belief state. Simulations results are given, and several areas for future research are discussed."
            },
            "slug": "Gradient-Descent-for-General-Reinforcement-Learning-Baird-Moore",
            "title": {
                "fragments": [],
                "text": "Gradient Descent for General Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A simple learning rule is derived, the VAPS algorithm, which can be instantiated to generate a wide range of new reinforcement-learning algorithms, and allows policy-search and value-based algorithms to be combined, thus unifying two very different approaches to reinforcement learning into a single Value and Policy Search algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 353,
                                "start": 209
                            }
                        ],
                        "text": "Most responsible for this is an uncanny similarity between the behavior of temporal-difference algorithms and the activity of dopamine producing neurons in the brain, as pointed out by a number of researchers (Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague, Dayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7630553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e989437ab0ce2988bc5f1c2df4cfab58b01d7ac",
            "isKey": false,
            "numCitedBy": 494,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most active areas of research in artificial intelligence is the study of learning methods by which \u201cembedded agents\u201d can improve performance while acting in complex dynamic environments. An agent, or decision maker, is embedded in an environment when it receives information from, and acts on, that environment in an ongoing closed-loop interaction. An embedded agent has to make decisions under time pressure and uncertainty and has to learn without the help of an ever-present knowledgeable teacher. Although the novelty of this emphasis may be inconspicuous to a biologist, animals being the prototypical embedded agents, this emphasis is a significant departure from the more traditional focus in artificial intelligence on reasoning within circumscribed domains removed from the flow of real-world events. One consequence of the embedded agent view is the increasing interest in the learning paradigm called reinforcement learning (RL). Unlike the more widely studied supervised learning systems, which learn from a set of examples of correct input/output behavior, RL systems adjust their behavior with the goal of maximizing the frequency and/or magnitude of the reinforcing events they encounter over time."
            },
            "slug": "Adaptive-Critics-and-the-Basal-Ganglia-Barto",
            "title": {
                "fragments": [],
                "text": "Adaptive Critics and the Basal Ganglia"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "One consequence of the embedded agent view is the increasing interest in the learning paradigm called reinforcement learning (RL)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 86
                            }
                        ],
                        "text": "Our presentation is based on the slightly modified treatment by Jaakkola, Jordan, and Singh (1994). The results in the random-walk examples were made for this text based on the prior work by Sutton (1988) and Singh and Sutton (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "These results were then extended and strengthened by Jaakkola, Jordan and Singh (1994) and Tsitsiklis (1994) by using extensions of the powerful existing theory of stochastic approximation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 57
                            }
                        ],
                        "text": "Some of these ideas were explored formally by Sutton and Singh (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 86
                            }
                        ],
                        "text": "Our presentation is based on the slightly modified treatment by Jaakkola, Jordan, and Singh (1994). The results in the random-walk examples were made for this text based on the prior work by Sutton (1988) and Singh and Sutton (1996). The use of backup diagrams to describe these and other algorithms in this chapter is new, as are the terms ``forward view\" and ``backward view."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 89
                            }
                        ],
                        "text": "But what of the traces for the other actions for that state? The approach recommended by Singh and Sutton (1996) is to set the traces of all the other actions from the re-visited state to 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Singh and Sutton (1996) distinguished between every-visit and first-visit MC methods and proved results relating these methods to reinforcement learning algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 70
                            }
                        ],
                        "text": "More general convergence results were proved by Jaakkola, Jordan, and Singh (1994) and Tsitsiklis (1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5972929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5cc2b2829a1fcf0260a1405f5f931efb21ffd5a",
            "isKey": true,
            "numCitedBy": 136,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning (RL) algorithms have traditionally been thought of as trial and error learning methods that use actual control experience to incrementally improve a control policy. Sutton's DYNA architecture demonstrated that RL algorithms can work as well using simulated experience from an environment model, and that the resulting computation was similar to doing one-step lookahead planning. Inspired by the literature on hierarchical planning, I propose learning a hierarchy of models of the environment that abstract temporal detail as a means of improving the scalability of RL algorithms. I present H-DYNA (Hierarchical DYNA), an extension to Sutton's DYNA architecture that is able to learn such a hierarchy of abstract models. H-DYNA differs from hierarchical planners in two ways: first, the abstract models are learned using experience gained while learning to solve other tasks in the same environment, and second, the abstract models can be used to solve stochastic control tasks. Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms. The abstract models also serve as mechanisms for achieving transfer of learning across multiple tasks."
            },
            "slug": "Reinforcement-Learning-with-a-Hierarchy-of-Abstract-Singh",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with a Hierarchy of Abstract Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms, and the abstract models can be used to solve stochastic control tasks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 85
                            }
                        ],
                        "text": "The pole-balancing example is from Michie and Chambers (1968) and Barto, Sutton, and Anderson (1983)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 742,
                                "start": 17
                            }
                        ],
                        "text": ", Kohonen, 1977; Anderson, Silverstein, Ritz, and Jones, 1977) to reinforcement learning. Barto, Anderson, and Sutton (1982) used a two-layer ANN to learn a nonlinear control policy, and emphasized the first layer\u2019s role of learning a suitable representation. Hampson (1983, 1989) was an early proponent of multilayer ANNs for learning value functions. Barto, Sutton, and Anderson (1983) presented an actor-critic algorithm in the form of an ANN learning to balance a simulated pole (see Sections 15.7 and 15.8). Barto and Anandan (1985) introduced a stochastic version of Widrow, Gupta, and Maitra\u2019s (1973) selective bootstrap algorithm called the associative reward-penalty (AR\u2212P ) algorithm. Barto (1985, 1986) and Barto and Jordan (1987) described multi-layer ANNs consisting of AR\u2212P units trained with a globally-broadcast reinforcement signal to learn classification rules that are not linearly separable."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 400,
                                "start": 236
                            }
                        ],
                        "text": "At this time we developed a method for using temporal-difference learning in trial-and-error learning, known as the actor\u2013 critic architecture, and applied this method to Michie and Chambers\u2019s pole-balancing problem (Barto, Sutton, and Anderson, 1983). This method was extensively studied in Sutton\u2019s (1984) Ph.D. dissertation and extended to use backpropagation neural networks in Anderson\u2019s (1986) Ph."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 453,
                                "start": 236
                            }
                        ],
                        "text": "At this time we developed a method for using temporal-difference learning in trial-and-error learning, known as the actor\u2013 critic architecture, and applied this method to Michie and Chambers\u2019s pole-balancing problem (Barto, Sutton, and Anderson, 1983). This method was extensively studied in Sutton\u2019s (1984) Ph.D. dissertation and extended to use backpropagation neural networks in Anderson\u2019s (1986) Ph.D. dissertation. Around this time, Holland (1986) incorporated temporal-difference ideas explicitly into his classifier systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 388,
                                "start": 17
                            }
                        ],
                        "text": ", Kohonen, 1977; Anderson, Silverstein, Ritz, and Jones, 1977) to reinforcement learning. Barto, Anderson, and Sutton (1982) used a two-layer ANN to learn a nonlinear control policy, and emphasized the first layer\u2019s role of learning a suitable representation. Hampson (1983, 1989) was an early proponent of multilayer ANNs for learning value functions. Barto, Sutton, and Anderson (1983) presented an actor-critic algorithm in the form of an ANN learning to balance a simulated pole (see Sections 15."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 236
                            }
                        ],
                        "text": "At this time we developed a method for using temporal-difference learning in trial-and-error learning, known as the actor\u2013 critic architecture, and applied this method to Michie and Chambers\u2019s pole-balancing problem (Barto, Sutton, and Anderson, 1983). This method was extensively studied in Sutton\u2019s (1984) Ph."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 538,
                                "start": 17
                            }
                        ],
                        "text": ", Kohonen, 1977; Anderson, Silverstein, Ritz, and Jones, 1977) to reinforcement learning. Barto, Anderson, and Sutton (1982) used a two-layer ANN to learn a nonlinear control policy, and emphasized the first layer\u2019s role of learning a suitable representation. Hampson (1983, 1989) was an early proponent of multilayer ANNs for learning value functions. Barto, Sutton, and Anderson (1983) presented an actor-critic algorithm in the form of an ANN learning to balance a simulated pole (see Sections 15.7 and 15.8). Barto and Anandan (1985) introduced a stochastic version of Widrow, Gupta, and Maitra\u2019s (1973) selective bootstrap algorithm called the associative reward-penalty (AR\u2212P ) algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 17
                            }
                        ],
                        "text": ", Kohonen, 1977; Anderson, Silverstein, Ritz, and Jones, 1977) to reinforcement learning. Barto, Anderson, and Sutton (1982) used a two-layer ANN to learn a nonlinear control policy, and emphasized the first layer\u2019s role of learning a suitable representation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 325,
                                "start": 2
                            }
                        ],
                        "text": ") Anderson (1986, 1987, 1989) evaluated numerous methods for training multilayer ANNs and showed that an actor-critic algorithm in which both the actor and critic were implemented by two-layer ANNs trained by error backpropagation outperformed singlelayer ANNs in the pole-balancing and tower of Hanoi tasks. Williams (1988) described several ways that backpropagation and reinforcement learning can be combined for training ANNs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18649966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11463e2a6ed218e87e22cba2c2f24fb5992d0293",
            "isKey": true,
            "numCitedBy": 138,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "THE DIFFICULTIES OF LEARNING IN MULTILAYERED NETWORKS OF COMPUTATIONAL UNITS HAS LIMITED THE USE OF CONNECTIONIST SYSTEMS IN COMPLEX DOMAINS. THIS DISSERTATION ELUCIDATES THE ISSUES OF LEARNING IN A NETWORK''S HIDDEN UNITS, AND REVIEWS METHODS FOR ADDRESSING THESE ISSUES THAT HAVE BEEN DEVELOPED THROUGH THE YEARS. ISSUES OF LEARNING IN HIDDEN UNITS ARE SHOWN TO BE ANALOGOUS TO LEARNING ISSUES FOR MULTILAYER SYSTEMS EMPLOYING SYMBOLIC REPRSENTATIONS. COMPARISONS OF A NUMBER OF ALGORITHMS FOR LEARNING IN HIDDEN UNITS ARE MADE BY APPLYING THEM IN A CONSISTENT MANNER TO SEVERAL TASKS. RECENTLY DEVELOPED ALGORITHMS, INCLUDING RUMELHART, ET AL''S, ERROR BACK-PROPOGATIONS ALGORITHM AND BARTO, ET AL''S, REINFORCEMENT-LEARNING ALGORITHMS, LEARN THE SOLUTIONS TO THE TASKS MUCH MORE SUCCESSFULLY THAN METHODS OF THE PAST. A NOVEL ALGORITHM IS EXAMINED THAT COMBINES ASPECTS OF REINFORCEMENT LEARNING AND A DATA-DIRECTED SEARCH FOR USEFUL WEIGHTS, AND IS SHOWN TO OUT PERFORM REINFORMCEMENT-LEARNING ALGORITHMS. A CONNECTIONIST FRAMEWORK FOR THE LEARNING OF STRATEGIES IS DESCRIBED WHICH COMBINES THE ERROR BACK-PROPOGATION ALGORITHM FOR LEARNING IN HIDDEN UNITS WITH SUTTON''S AHC ALGORITHM TO LEARN EVALUATION FUNCTIONS AND WITH A REINFORCEMENT-LEARNING ALGORITHM TO LEARN SEARCH HEURISTICS. THE GENERAL- ITY OF THIS HYBRID SYSTEM IS DEMONSTRATED THROUGH SUCCESSFUL APPLICATIONS"
            },
            "slug": "Learning-and-problem-solving-with-multilayer-neural-Anderson",
            "title": {
                "fragments": [],
                "text": "Learning and problem-solving with multilayer connectionist systems (adaptive, strategy learning, neural networks, reinforcement learning)"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel algorithm is examined that combines ASPECTS of REINFORCEMENT LEARNING and a DATA-DIRECTED SEARCH for USEFUL WEIGHTS, and is shown to out perform reinFORMCEMENT-LEARNING ALGORITHMS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748374"
                        ],
                        "name": "R. Crites",
                        "slug": "R.-Crites",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Crites",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Crites"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60764470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cf7c5d6c731a6d42db9da9ef4c62f7f3e32166e",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 135,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent algorithmic and theoretical advances in reinforcement learning (RL) are attracting widespread interest. RL algorithms have appeared that approximate dynamic programming (DP) on an incremental basis. Unlike traditional DP algorithms, these algorithms do not require knowledge of the state transition probabilities or reward structure of a system. This allows them to be trained using real or simulated experiences, focusing their computations on the areas of state space that are actually visited during control, making them computationally tractable on very large problems. RL algorithms can be used as components of multi-agent algorithms. If each member of a team of agents employs one of these algorithms, a new collective learning algorithm emerges for the team as a whole. In this dissertation we demonstrate that such collective RL algorithms can be powerful heuristic methods for addressing large-scale control problems. \nElevator group control serves as our primary testbed. The elevator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are non-stationary due to changing passenger arrival rates. As a way of streamlining the search through policy space, we use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility."
            },
            "slug": "Large-scale-dynamic-optimization-using-teams-of-Crites-Barto",
            "title": {
                "fragments": [],
                "text": "Large-scale dynamic optimization using teams of reinforcement learning agents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This dissertation uses a team of RL agents, each of which is responsible for controlling one elevator car, to demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850503"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 386824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a8149fb5aa8a5684e7d530c264451a5cb9250f5",
            "isKey": false,
            "numCitedBy": 908,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting."
            },
            "slug": "Recent-Advances-in-Hierarchical-Reinforcement-Barto-Mahadevan",
            "title": {
                "fragments": [],
                "text": "Recent Advances in Hierarchical Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work reviews several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed and discusses extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability."
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Event Dyn. Syst."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14921614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66e3294feae7f0dba35c94a7214fc5cba41e9def",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scaling-Reinforcement-Learning-Algorithms-by-Models-Singh",
            "title": {
                "fragments": [],
                "text": "Scaling Reinforcement Learning Algorithms by Learning Variable Temporal Resolution Models"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46333892"
                        ],
                        "name": "J. Si",
                        "slug": "J.-Si",
                        "structuredName": {
                            "firstName": "Jennie",
                            "lastName": "Si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Si"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852241"
                        ],
                        "name": "Warrren B Powell",
                        "slug": "Warrren-B-Powell",
                        "structuredName": {
                            "firstName": "Warrren",
                            "lastName": "Powell",
                            "middleNames": [
                                "B"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Warrren B Powell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145033828"
                        ],
                        "name": "D. Wunsch",
                        "slug": "D.-Wunsch",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Wunsch",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wunsch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14804899,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "269688bee08e2a61b7350806304a6654ba90d730",
            "isKey": false,
            "numCitedBy": 758,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword. 1. ADP: goals, opportunities and principles. Part I: Overview. 2. Reinforcement learning and its relationship to supervised learning. 3. Model-based adaptive critic designs. 4. Guidance in the use of adaptive critics for control. 5. Direct neural dynamic programming. 6. The linear programming approach to approximate dynamic programming. 7. Reinforcement learning in large, high-dimensional state spaces. 8. Hierarchical decision making. Part II: Technical advances. 9. Improved temporal difference methods with linear function approximation. 10. Approximate dynamic programming for high-dimensional resource allocation problems. 11. Hierarchical approaches to concurrency, multiagency, and partial observability. 12. Learning and optimization - from a system theoretic perspective. 13. Robust reinforcement learning using integral-quadratic constraints. 14. Supervised actor-critic reinforcement learning. 15. BPTT and DAC - a common framework for comparison. Part III: Applications. 16. Near-optimal control via reinforcement learning. 17. Multiobjective control problems by reinforcement learning. 18. Adaptive critic based neural network for control-constrained agile missile. 19. Applications of approximate dynamic programming in power systems control. 20. Robust reinforcement learning for heating, ventilation, and air conditioning control of buildings. 21. Helicopter flight control using direct neural dynamic programming. 22. Toward dynamic stochastic optimal power flow. 23. Control, optimization, security, and self-healing of benchmark power systems."
            },
            "slug": "Handbook-of-Learning-and-Approximate-Dynamic-Si-Barto",
            "title": {
                "fragments": [],
                "text": "Handbook of Learning and Approximate Dynamic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This chapter discusses reinforcement learning in large, high-dimensional state spaces, model-based adaptive critic designs, and applications of approximate dynamic programming in power systems control."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 106
                            }
                        ],
                        "text": "The soap bubble example is a classical Dirichlet problem whose Monte Carlo solution was first proposed by Kakutani (1945). (see Hersh and Griego, 1969; Doyle and Snell, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 41302553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "168bd65ae047afecf3eea54786d8e6becba932f7",
            "isKey": false,
            "numCitedBy": 794,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation addresses the problem of designing algorithms for learning in embedded systems. This problem differs from the traditional supervised learning problem. An agent, finding itself in a particular input situation must generate an action. It then receives a reinforcement value from the environment, indicating how valuable the current state of the environment is for the agent. The agent cannot, however, deduce the reinforcement value that would have resulted from executing any of its other actions. A number of algorithms for learning action strategies from reinforcement values are presented and compared empirically with existing reinforcement-learning algorithms. \nThe interval-estimation algorithm uses the statistical notion of confidence intervals to guide its generation of actions in the world, trading off acting to gain information against acting to gain reinforcement. It performs well in simple domains but does not exhibit any generalization and is computationally complex. \nThe cascade algorithm is a structural credit-assignment method that allows an action strategy with many output bits to be learned by a collection of reinforcement-learning modules that learn Boolean functions. This method represents an improvement in computational complexity and often in learning rate. \nTwo algorithms for learning Boolean functions in k-DNF are described. Both are based on Valiant's algorithm for learning such functions from input-output instances. The first uses Sutton's techniques for linear association and reinforcement comparison, while the second uses techniques from the interval estimation algorithm. They both perform well and have tractable complexity. \nA generate-and-test reinforcement-learning algorithm is presented. It allows symbolic representations of Boolean functions to be constructed incrementally and tested in the environment. It is highly parametrized and can be tuned to learn a broad range of function classes. Low-complexity functions can be learned very efficiently even in the presence of large numbers of irrelevant input bits. This algorithm is extended to construct simple sequential networks using a set-reset operator, which allows the agent to learn action strategies with state. \nThese algorithms, in addition to being studied in simulation, were implemented and tested on a physical mobile robot."
            },
            "slug": "Learning-in-embedded-systems-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Learning in embedded systems"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This dissertation addresses the problem of designing algorithms for learning in embedded systems using Sutton's techniques for linear association and reinforcement comparison, while the interval estimation algorithm uses the statistical notion of confidence intervals to guide its generation of actions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 482,
                                "start": 54
                            }
                        ],
                        "text": "Interval estimation methods are due to Lai (1987) and Kaelbling (1993). Bellman (1956) was the first to show how dynamic programming could be used to compute the optimal balance between exploration and exploitation within a Bayesian formulation of the problem. The survey by Kumar (1985) provides a good discussion of Bayesian and nonBayesian approaches to these problems. The term information state comes from the literature on partially observable MDPs, see, e.g., Lovejoy (1991). The Gittins index approach is due to Gittins and Jones (1974)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 54
                            }
                        ],
                        "text": "Interval estimation methods are due to Lai (1987) and Kaelbling (1993). Bellman (1956) was the first to show how dynamic programming could be used to compute the optimal balance between exploration and exploitation within a Bayesian formulation of the problem. The survey by Kumar (1985) provides a good discussion of Bayesian and nonBayesian approaches to these problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 54
                            }
                        ],
                        "text": "Interval estimation methods are due to Lai (1987) and Kaelbling (1993). Bellman (1956) was the first to show how dynamic programming could be used to compute the optimal balance between exploration and exploitation within a Bayesian formulation of the problem."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6578281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a208167b153e6dfc3327415068ae4a7a6dcd006",
            "isKey": true,
            "numCitedBy": 412,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Increasing attention has been paid to reinforcement learning algorithms in recent years, partly due to successes in the theoretical analysis of their behavior in Markov environments. If the Markov assumption is removed, however, neither generally the algorithms nor the analyses continue to be usable. We propose and analyze a new learning algorithm to solve a certain class of non-Markov decision problems. Our algorithm applies to problems in which the environment is Markov, but the learner has restricted access to state information. The algorithm involves a Monte-Carlo policy evaluation combined with a policy improvement method that is similar to that of Markov decision problems and is guaranteed to converge to a local maximum. The algorithm operates in the space of stochastic policies, a space which can yield a policy that performs considerably better than any deterministic policy. Although the space of stochastic policies is continuous--even for a discrete action space--our algorithm is computationally tractable."
            },
            "slug": "Reinforcement-Learning-Algorithm-for-Partially-Jaakkola-Singh",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes and analyze a new learning algorithm to solve a certain class of non-Markov decision problems and operates in the space of stochastic policies, a space which can yield a policy that performs considerably better than any deterministic policy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 180
                            }
                        ],
                        "text": "A definition of the sample complexity of exploration for a reinforcement learning algorithm is the number of time steps in which the algorithm does not select near-optimal actions (Kakade, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 35
                            }
                        ],
                        "text": "mals are surprised is derived from Kamin (1969). Models of classical conditioning other than Rescorla and Wagner\u2019s include the models of Klopf (1988), Grossberg (1975), Mackintosh (1975), Moore and Stickney (1980), Pearce and Hall (1980), and Courville, Daw, and Touretzky (2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59669372,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b14af216f6a667c4e03c6964babe828c680a05a",
            "isKey": false,
            "numCitedBy": 547,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis is a detailed investigation into the following question: how much data must an agent collect in order to perform \u201creinforcement learning\u201d successfully? This question is analogous to the classical issue of the sample complexity in supervised learning, but is harder because of the increased realism of the reinforcement learning setting. This thesis summarizes recent sample complexity results in the reinforcement learning literature and builds on these results to provide novel algorithms with strong performance guarantees. We focus on a variety of reasonable performance criteria and sampling models by which agents may access the environment. For instance, in a policy search setting, we consider the problem of how much simulated experience is required to reliably choose a \u201cgood\u201d policy among a restricted class of policies (as in Kearns, Mansour, and Ng [2000]). In a more online setting, we consider the case in which an agent is placed in an environment and must follow one unbroken chain of experience with no access to \u201coffline\u201d simulation (as in Kearns and Singh [1998]). We build on the sample based algorithms suggested by Kearns, Mansour, and Ng [2000]. Their sample complexity bounds have no dependence on the size of the state space, an exponential dependence on the planning horizon time, and linear dependence on the complexity of . We suggest novel algorithms with more restricted guarantees whose sample complexities are again independent of the size of the state space and depend linearly on the complexity of the policy class , but have only a polynomial dependence on the horizon time. We pay particular attention to the tradeoffs made by such algorithms."
            },
            "slug": "On-the-sample-complexity-of-reinforcement-learning.-Kakade",
            "title": {
                "fragments": [],
                "text": "On the sample complexity of reinforcement learning."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Novel algorithms with more restricted guarantees are suggested whose sample complexities are again independent of the size of the state space and depend linearly on the complexity of the policy class, but have only a polynomial dependence on the horizon time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1522994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a7acaf6469c06ae5876d92f013184db5897bb13",
            "isKey": false,
            "numCitedBy": 3237,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "slug": "Neuronlike-adaptive-elements-that-can-solve-control-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Neuronlike adaptive elements that can solve difficult learning control problems"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem and the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11023955"
                        ],
                        "name": "Mark B. Ring",
                        "slug": "Mark-B.-Ring",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Ring",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark B. Ring"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27150180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "082b1f5c791cadef18c4920ecc1396615a3fe7cb",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "Continual learning is the constant development of complex behaviors with no nal end in mind. It is the process of learning ever more complicated skills by building on those skills already developed. In order for learning at one stage of development to serve as the foundation for later learning, a continual-learning agent should learn hierarchically. CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development is proposed, described, tested, and evaluated in this dissertation. CHILD accumulates useful behaviors in reinforcement environments by using the Temporal Transition Hierarchies learning algorithm, also derived in the dissertation. This constructive algorithm generates a hierarchical, higher-order neural network that can be used for predicting context-dependent temporal sequences and can learn sequential-task benchmarks more than two orders of magnitude faster than competing neural-network systems. Consequently, CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still. This continual-learning approach is made possible by the unique properties of Temporal Transition Hierarchies, which allow existing skills to be amended and augmented in precisely the same way that they were constructed in the rst place. Table of"
            },
            "slug": "Continual-learning-in-reinforcement-environments-Ring",
            "title": {
                "fragments": [],
                "text": "Continual learning in reinforcement environments"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development is proposed, described, tested, and evaluated in this dissertation and generates a hierarchical, higher-order neural network that can be used for predicting context-dependent temporal sequences and can learn sequential-task benchmarks more than two orders of magnitude faster than competing neural-network systems."
            },
            "venue": {
                "fragments": [],
                "text": "GMD-Bericht"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709927"
                        ],
                        "name": "R. Maclin",
                        "slug": "R.-Maclin",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Maclin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Maclin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734317"
                        ],
                        "name": "J. Shavlik",
                        "slug": "J.-Shavlik",
                        "structuredName": {
                            "firstName": "Jude",
                            "lastName": "Shavlik",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shavlik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14203505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fa730736ede1574abb2f9245a5e3ec5575446f",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning from reinforcements is a promising approach for creating intelligent agents. However, reinforcement learning usually requires a large number of training episodes. We present an approach that addresses this shortcoming by allowing a connectionist Q-learner to accept advice given, at any time and in a natural manner, by an external observer. In our approach, the advice-giver watches the learner and occasionally makes suggestions, expressed as instructions in a simple programming language. Based on techniques from knowledge-based neural networks, these programs axe inserted directly into the agent's utility function. Subsequent reinforcement learning further integrates and refines the advice. We present empirical evidence that shows our approach leads to statistically-significant gains in expected reward. Importantly, the advice improves the expected reward regardless of the stage of training at which it is given."
            },
            "slug": "Incorporating-Advice-into-Agents-that-Learn-from-Maclin-Shavlik",
            "title": {
                "fragments": [],
                "text": "Incorporating Advice into Agents that Learn from Reinforcements"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents an approach that allows a connectionist Q-learner to accept advice given, at any time and in a natural manner, by an external observer, and shows that the advice improves the expected reward regardless of the stage of training at which it is given."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23072417,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c393ec4cb7bc630d2bdd324547606daf343b76f",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Consistency-of-HDP-applied-to-a-simple-learning-Werbos",
            "title": {
                "fragments": [],
                "text": "Consistency of HDP applied to a simple reinforcement learning problem"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59715909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c376317f738fa4795cb47c3b5c8918c9348ca965",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation is about building learning control architectures for agents embedded in finite, stationary, and Markovian environments. Such architectures give embedded agents the ability to improve autonomously the efficiency with which they can achieve goals. Machine learning researchers have developed reinforcement learning (RL) algorithms based on dynamic programming (DP) that use the agent''s experience in its environment to improve its decision policy incrementally. This is achieved by adapting an evaluation function in such a way that the decision policy that is ``greedy'''' with respect to it improves with experience. This dissertation focuses on finite, stationary and Markovian environments for two reasons: it allows the development and use of a strong theory of RL, and there are many challenging real-world RL tasks that fall into this category. This dissertation establishes a novel connection between stochastic approximation theory and RL that provides a uniform framework for understanding all the different RL algorithms that have been proposed to date. It also highlights a dimension that clearly separates all RL research from prior work on DP. Two other theoretical results showing how approximations affect performance in RL provide partial justification for the use of compact function approximators in RL. In addition, a new family of ``soft'''' DP algorithms is presented. These algorithms converge to solutions that are more robust than the solutions found by classical DP algorithms. Despite all of the theoretical progress, conventional RL architectures scale poorly enough to make them impractical for many real-world problems. This dissertation studies two aspects of the scaling issue: the need to accelerate RL, and the need to build RL architectures that can learn to solve multiple tasks. It presents three RL architectures, CQ-L, H-DYNA, and BB-RL, that accelerate learning by facilitating transfer of training from simple to complex tasks. Each architecture uses a different method to achieve transfer of training: CQ-L uses the evaluation functions for simple tasks as building blocks to construct the evaluation function for complex tasks. H-DYNA uses the evaluation functions for simple tasks to build an abstract environment model, and BB-RL uses the decision policies found for the simple tasks as the primitive actions for the complex tasks. A mixture of theoretical and empirical results are presented to support the new RL architectures developed in this dissertation."
            },
            "slug": "Learning-to-Solve-Markovian-Decision-Processes-Singh",
            "title": {
                "fragments": [],
                "text": "Learning to Solve Markovian Decision Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This dissertation establishes a novel connection between stochastic approximation theory and RL that provides a uniform framework for understanding all the different RL algorithms that have been proposed to date and highlights a dimension that clearly separates all RL research from prior work on DP."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2332513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "isKey": false,
            "numCitedBy": 5181,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms."
            },
            "slug": "Simple-statistical-gradient-following-algorithms-Williams",
            "title": {
                "fragments": [],
                "text": "Simple statistical gradient-following algorithms for connectionist reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units that are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reInforcement tasks, and they do this without explicitly computing gradient estimates."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728505"
                        ],
                        "name": "J. Mendel",
                        "slug": "J.-Mendel",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Mendel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mendel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50167028"
                        ],
                        "name": "R. Mclaren",
                        "slug": "R.-Mclaren",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mclaren",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mclaren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118480548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7ec6db9093a8b0b4007a1fbc1d4d86cc84f8fc2",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reinforcement-learning-control-and-pattern-systems-Mendel-Mclaren",
            "title": {
                "fragments": [],
                "text": "Reinforcement-learning control and pattern recognition systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2302845"
                        ],
                        "name": "N. Flann",
                        "slug": "N.-Flann",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Flann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Flann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 121
                            }
                        ],
                        "text": "Explanation-based learning methods have also been adapted for learning value functions, yielding compact representations (Yee, Saxena, Utgoff, and Barto, 1990; Dietterich and Flann, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1906622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f13891132dff5d564f30c58c6c4a4bfdc632784",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "In speedup-learning problems, where full descriptions of operators are known, both explanation-based learning (EBL) and reinforcement learning (RL) methods can be applied. This paper shows that both methods involve fundamentally the same process of propagating information backward from the goal toward the starting state. Most RL methods perform this propagation on a state-by-state basis, while EBL methods compute the weakest preconditions of operators, and hence, perform this propagation on a region-by-region basis. Barto, Bradtke, and Singh (1995) have observed that many algorithms for reinforcement learning can be viewed as asynchronous dynamic programming. Based on this observation, this paper shows how to develop dynamic programming versions of EBL, which we call region-based dynamic programming or Explanation-Based Reinforcement Learning (EBRL). The paper compares batch and online versions of EBRL to batch and online versions of point-based dynamic programming and to standard EBL. The results show that region-based dynamic programming combines the strengths of EBL (fast learning and the ability to scale to large state spaces) with the strengths of reinforcement learning algorithms (learning of optimal policies). Results are shown in chess endgames and in synthetic maze tasks."
            },
            "slug": "Explanation-Based-Learning-and-Reinforcement-A-View-Dietterich-Flann",
            "title": {
                "fragments": [],
                "text": "Explanation-Based Learning and Reinforcement Learning: A Unified View"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper shows how to develop dynamic programming versions of EBL, which it is called region-based dynamic programming or Explanation-Based Reinforcement Learning (EBRL), and compares batch and online versions of EBRL to batch andOnline versions of point-basedynamic programming and to standard EBL."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819261"
                        ],
                        "name": "S. Bradtke",
                        "slug": "S.-Bradtke",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bradtke",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bradtke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8716495,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a4ac3bf8dd1b7aaf8ddc7c7f29d70b1ec62aedee",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent research on reinforcement learning has focused on algorithms based on the principles of Dynamic Programming (DP). One of the most promising areas of application for these algorithms is the control of dynamical systems, and some impressive results have been achieved. However, there are significant gaps between practice and theory. In particular, there are no convergence proofs for problems with continuous state and action spaces, or for systems involving non-linear function approximators (such as multilayer perceptrons). This paper presents research applying DP-based reinforcement learning theory to Linear Quadratic Regulation (LQR), an important class of control problems involving continuous state and action spaces and requiring a simple type of non-linear function approximator. We describe an algorithm based on Q-learning that is proven to converge to the optimal controller for a large class of LQR problems. We also describe a slightly different algorithm that is only locally convergent to the optimal Q-function, demonstrating one of the possible pitfalls of using a non-linear function approximator with DP-based learning."
            },
            "slug": "Reinforcement-Learning-Applied-to-Linear-Quadratic-Bradtke",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning Applied to Linear Quadratic Regulation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An algorithm based on Q-learning is described that is proven to converge to the optimal controller for a large class of LQR problems, an important class of control problems involving continuous state and action spaces and requiring a simple type of non-linear function approximator."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18693648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e08da64c0175139d7094a9bfbb3ec38648f8457f",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A description is given of several ways that backpropagation can be useful in training networks to perform associative reinforcement learning tasks. One way is to train a second network to model the environmental reinforcement signal and to backpropagate through this network into the first network. This technique has been proposed and explored previously in various forms. Another way is based on the use of the reinforce algorithm and amounts to backpropagating through deterministic parts of the network while performing a correlation-style computation where the behavior is stochastic. A third way, which is an extension of the second, allows backpropagation through the stochastic parts of the network as well. The mathematical validity of this third technique rests on the use of continuous-valued stochastic units. Some implications of this result for using supervised learning to train networks of stochastic units are noted, and it is also observed that such an approach even permits a seamless blend of associative reinforcement learning and supervised learning within the same network.<<ETX>>"
            },
            "slug": "On-the-use-of-backpropagation-in-associative-Williams",
            "title": {
                "fragments": [],
                "text": "On the use of backpropagation in associative reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A description is given of several ways that backpropagation can be useful in training networks to perform associative reinforcement learning tasks and it is observed that such an approach even permits a seamless blend of associatives reinforcement learning and supervised learning within the same network."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7962049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f8a0858fb82ce0e50b55446577a70e40137aaf",
            "isKey": false,
            "numCitedBy": 1551,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Integrated-Architectures-for-Learning,-Planning,-on-Sutton",
            "title": {
                "fragments": [],
                "text": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3466704"
                        ],
                        "name": "Gavin Adrian Rummery",
                        "slug": "Gavin-Adrian-Rummery",
                        "structuredName": {
                            "firstName": "Gavin",
                            "lastName": "Rummery",
                            "middleNames": [
                                "Adrian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gavin Adrian Rummery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145387873"
                        ],
                        "name": "M. Niranjan",
                        "slug": "M.-Niranjan",
                        "structuredName": {
                            "firstName": "Mahesan",
                            "lastName": "Niranjan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Niranjan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 59
                            }
                        ],
                        "text": "Sarsa( ) with function approximation was first explored by Rummery and Niranjan (1994). The mountain-car example is based on a similar task studied by Moore (1990). The results on it presented here are from Sutton (1995) and Singh and Sutton (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 51
                            }
                        ],
                        "text": "Sarsa( ) was first explored as a control method by Rummery and Niranjan (1994) and Rummery (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 42
                            }
                        ],
                        "text": "The Sarsa algorithm was first explored by Rummery and Niranjan (1994), who called it modified Q-learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 42
                            }
                        ],
                        "text": "The Sarsa algorithm was first explored by Rummery and Niranjan (1994), who called it modified Q-learning. The name ``Sarsa\" was introduced by Sutton (1996). The convergence of 1-step tabular Sarsa (the form treated in this chapter) has been proven by Satinder Singh (personal communication)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 59
                            }
                        ],
                        "text": "Sarsa( ) with function approximation was first explored by Rummery and Niranjan (1994). The mountain-car example is based on a similar task studied by Moore (1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59872172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a09464f26e18a25a948baaa736270bfb84b5e12",
            "isKey": true,
            "numCitedBy": 1398,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete nite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to generalise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of di erent algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Di erence algorithm (Sutton 1988), including a new algorithm (Modi ed Connectionist Q-Learning), and Q( ) (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each trial before updating can occur. On-line updating is found to be more robust to the choice of training parameters than backward replay, and also enables the algorithms to be used in continuously operating systems where no end of trial conditions occur. We compare the performance of these algorithms on a realistic robot navigation problem, where a simulated mobile robot is trained to guide itself to a goal position in the presence of obstacles. The robot must rely on limited sensory feedback from its surroundings, and make decisions that can be generalised to arbitrary layouts of obstacles. These simulations show that on-line learning algorithms are less sensitive to the choice of training parameters than backward replay, and that the alternative update rules of MCQ-L and Q( ) are more robust than standard Q-learning updates. 1"
            },
            "slug": "On-line-Q-learning-using-connectionist-systems-Rummery-Niranjan",
            "title": {
                "fragments": [],
                "text": "On-line Q-learning using connectionist systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Simulations show that on-line learning algorithms are less sensitive to the choice of training parameters than backward replay, and that the alternative update rules of MCQ-L and Q( ) are more robust than standard Q-learning updates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850503"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 71
                            }
                        ],
                        "text": ", Puterman, 1994) and from the point of view of reinforcement learning (Mahadevan, 1996; Tadepalli and Ok, 1994; Bertsekas and Tsitiklis, 1996; Tsitsiklis and Van Roy, 1999)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3291174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a62d15d6b73c3323e69270ab995aafa6a692a59c",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a detailed study of average reward reinforcement learning, an undiscounted optimality framework that is more appropriate for cyclical tasks than the much better studied discounted framework. A wide spectrum of average reward algorithms are described, ranging from synchronous dynamic programming methods to several (provably convergent) asynchronous algorithms from optimal control and learning automata. A general sensitive discount optimality metric calledn-discount-optimality is introduced, and used to compare the various algorithms. The overview identifies a key similarity across several asynchronous algorithms that is crucial to their convergence, namely independent estimation of the average reward and the relative values. The overview also uncovers a surprising limitation shared by the different algorithms while several algorithms can provably generategain-optimal policies that maximize average reward, none of them can reliably filter these to producebias-optimal (orT-optimal) policies that also maximize the finite reward to absorbing goal states. This paper also presents a detailed empirical study of R-learning, an average reward reinforcement learning method, using two empirical testbeds: a stochastic grid world domain and a simulated robot environment. A detailed sensitivity analysis of R-learning is carried out to test its dependence on learning rates and exploration levels. The results suggest that R-learning is quite sensitive to exploration strategies and can fall into sub-optimal limit cycles. The performance of R-learning is also compared with that of Q-learning, the best studied discounted RL method. Here, the results suggest that R-learning can be fine-tuned to give better performance than Q-learning in both domains."
            },
            "slug": "Average-reward-reinforcement-learning:-Foundations,-Mahadevan",
            "title": {
                "fragments": [],
                "text": "Average reward reinforcement learning: Foundations, algorithms, and empirical results"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents a detailed study of average reward reinforcement learning, an undiscounted optimality framework that is more appropriate for cyclical tasks than the much better studied discounted framework, and a detailed sensitivity analysis of R-learning is carried out to test its dependence on learning rates and exploration levels."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368601"
                        ],
                        "name": "Doina Precup",
                        "slug": "Doina-Precup",
                        "structuredName": {
                            "firstName": "Doina",
                            "lastName": "Precup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doina Precup"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28998047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "985f2c1baba284e9b7b604b7169a2e2778540fe6",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Decision making usually involves choosing among different courses of action over a broad range of time scales. For instance, a person planning a trip to a distant location makes high-level decisions regarding what means of transportation to use, but also chooses low-level actions, such as the movements for getting into a car. The problem of picking an appropriate time scale for reasoning and learning has been explored in artificial intelligence, control theory and robotics. In this dissertation we develop a framework that allows novel solutions to this problem, in the context of Markov Decision Processes (MDPs) and reinforcement learning. \nIn this dissertation, we present a general framework for prediction, control and learning at multiple temporal scales. In this framework, temporally extended actions are represented by a way of behaving (a policy) together with a termination condition. An action represented in this way is called an option. Options can be easily incorporated in MDPs, allowing an agent to use existing controllers, heuristics for picking actions, or learned courses of action. \nThe effects of behaving according to an option can be predicted using multi-time models, learned by interacting with the environment. In this dissertation we develop multi-time models, and we illustrate the way in which they can be used to produce plans of behavior very quickly, using classical dynamic programming or reinforcement learning techniques. \nThe most interesting feature of our framework is that it allows an agent to work simultaneously with high-level and low-level temporal representations. The interplay of these levels can be exploited in order to learn and plan more efficiently and more accurately. We develop new algorithms that take advantage of this structure to improve the quality of plans, and to learn in parallel about the effects of many different options."
            },
            "slug": "Temporal-abstraction-in-reinforcement-learning-Precup-Sutton",
            "title": {
                "fragments": [],
                "text": "Temporal abstraction in reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A general framework for prediction, control and learning at multiple temporal scales, and the way in which multi-time models can be used to produce plans of behavior very quickly, using classical dynamic programming or reinforcement learning techniques is developed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123333909"
                        ],
                        "name": "M.I. Jordan",
                        "slug": "M.I.-Jordan",
                        "structuredName": {
                            "firstName": "M.I.",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M.I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 19
                            }
                        ],
                        "text": "As we were finalizing our work on the actor\u2013critic architecture in 1981, we discovered a paper by Ian Witten (1977) that contains the earliest known publication of a temporal-difference learning rule."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 60
                            }
                        ],
                        "text": "5 Optimistic initialization was used in reinforcement learning by Sutton (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 718,
                                "start": 139
                            }
                        ],
                        "text": "Widrow, Gupta, and Maitra (1973) modified the Least-Mean-Square (LMS) algorithm of Widrow and Hoff (1960) to produce a reinforcement learning rule that could learn from success and failure signals instead of from training examples. They called this form of learning \u201cselective bootstrap adaptation\u201d and described it as \u201clearning with a critic\u201d instead of \u201clearning with a teacher.\u201d They analyzed this rule and showed how it could learn to play blackjack. This was an isolated foray into reinforcement learning by Widrow, whose contributions to supervised learning were much more influential. Our use of the term \u201ccritic\u201d is derived from Widrow, Gupta, and Maitra\u2019s paper. Buchanan, Mitchell, Smith, and Johnson (1978) inde-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 510,
                                "start": 133
                            }
                        ],
                        "text": "(1977) suggested approximate solution methods for stochastic optimal control problems that are related to modern reinforcement learning methods (see also Werbos, 1982, 1987, 1988, 1989, 1992). Although Werbos\u2019s ideas were not widely recognized at the time, they were prescient in emphasizing the importance of approximately solving optimal control problems in a variety of domains, including artificial intelligence. The most influential integration of reinforcement learning and MDPs is due to Watkins (1989). His treatment of reinforcement learning using the MDP formalism has been widely adopted."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 44
                            }
                        ],
                        "text": "Our presentation of the reinforcement learning problem was influenced by Watkins (1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 84
                            }
                        ],
                        "text": "1\u20132 Most of the specific material from these sections is from Sutton (1988), including the TD(0) algorithm, the random walk example, and the term \u201ctemporaldifference learning.\u201d The characterization of the relationship to dynamic programming and Monte Carlo methods was influenced by Watkins (1989), Werbos (1987), and others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 434,
                                "start": 73
                            }
                        ],
                        "text": "The theory of MDPs evolved from efforts to understand the problem of making sequences of decisions under uncertainty, where each decision can depend on the previous decisions and their outcomes. It is sometimes called the theory of multistage decision processes, or sequential decision processes, and has roots in the statistical literature on sequential sampling beginning with the papers by Thompson (1933, 1934) and Robbins (1952) that we cited in Chapter 2 in connection with bandit problems (which are prototypical MDPs if formulated as multiple-situation problems)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 57
                            }
                        ],
                        "text": "These are often called estimator algorithms in the learning automata literature. The term action value is due to Watkins (1989). The first to use \u03b5-greedy methods may also have been Watkins (1989, p."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 99
                            }
                        ],
                        "text": "They explored it in conjunction with neural networks and called it \u201cModified Connectionist Q-learning\u201d. The name \u201cSarsa\u201d was introduced by Sutton (1996). The convergence of one-step tabular Sarsa (the form treated in this chapter) has been proved by Satinder Singh (personal communication)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 107939718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06c65fb31e5d1febf73770c02b4665d12cb0411b",
            "isKey": true,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "To make reinforcement learning algorithms run in a reasonable amount of time, it is frequently necessary to use a well-chosen reward function that gives appropriate \u201chints\u201d to the learning algorithm. But, the selection of these hints\u2014called shaping rewards\u2014often entails significant trial and error, and poorly chosen shaping rewards often change the problem in unanticipated ways that cause poor solutions to be learned. In this dissertation, we give a theory of reward shaping that shows how these problems can be eliminated. This theory further gives guidelines for selecting good shaping rewards that in practice give significant speedups of the learning process. We also show that shaping can allow us to use \u201cmyopic\u201d learning algorithms and still do well. \nThe \u201ccurse of dimensionality\u201d refers to the observation that many simple reinforcement learning algorithms, ones based on discretization, scale exponentially with the size of the problem and are thus impractical for many applications. In this dissertation, we consider the policy search approach to reinforcement learning. Here, we wish to select a controller from among some restricted set of controllers for a task. We see that a key issue in policy search is obtaining uniformly good estimates of the quality of the controllers being considered. We show that simple Monte Carlo methods will not in general give uniformly good estimates. We then present the P EGASUS policy search method, which is derived using the surprising observation that all reinforcement learning problems can be transformed into ones in which all state transitions (given the current state and action) are deterministic. We show that PEGASUS has sample complexity that scales at most polynomially with the size of the problem, and give strong guarantees on the quality of the solutions it finds. In deriving these results, we also take the ideas of VC dimension and sample complexity that are familiar from supervised learning and apply them to the reinforcement learning setting, thus putting the two problems on a more equal footing. \nFinally, we apply these ideas to designing a controller for an autonomous helicopter. Autonomous helicopter flight is widely viewed as a difficult control problem. Using shaping and the PEGASUS policy search method, we are able to automatically design a stable hovering controller for a helicopter, as well as make it fly a number of challenging maneuvers taken from an RC helicopter competition. (Abstract shortened by UMI.)"
            },
            "slug": "Shaping-and-policy-search-in-reinforcement-learning-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "Shaping and policy search in reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A theory of reward shaping is given that shows how poorly chosen shaping rewards can be eliminated and guidelines for selecting good shaping rewards that in practice give significant speedups of the learning process are given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650569"
                        ],
                        "name": "A. Klopf",
                        "slug": "A.-Klopf",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Klopf",
                            "middleNames": [
                                "Harry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Klopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56506644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc1a6753de779a5871ee5481e455fa5e00404f1d",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Many reinforcement learning systems, such as Q-learning (Watkins, 1989), or advantage updating (Baird, 1993), require that a function f(x,u) be learned, and that the value of argmax f(x,u) be calculated quickly for any given x. The function f could be learned by a function approximation system such as a multilayer preceptron, but the maximum of f for a given x cannot found analytically and is difficult to approximate numerically for high-dimensional u vectors. A new method is proposed, wire fitting, in which a function approximation system is used to learn a set of functions called control wires, and the function f is found by fitting a surface to the control wires. Wire fitting has the following four properties: (1) any continuous f function can represented to any desired accuracy given sufficient parameters; (2) the function f(x,u) can be evaluated quickly; (3) argmax f(x,u) can found exactly in constant time after evaluating f(x,U); (4) wire fitting can incorporate any general function approximation system. These four properties are discussed and it is shown how wire fitting can be combined with a memory-based learning system and Q-learning to control an inverted-pendulum system"
            },
            "slug": "Reinforcement-Learning-With-High-Dimensional,-Baird-Klopf",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning With High-Dimensional, Continuous Actions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115648163"
                        ],
                        "name": "Leonid Kuvayev",
                        "slug": "Leonid-Kuvayev",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Kuvayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leonid Kuvayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7088173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2870f60f54d08dd13bf825230859b62e779d0527",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Model-based reinforcement learning, in which a model of the environment's dynamics is learned and used to supplement direct learning from experience, has been proposed as a general approach to learning and planning. We present the rst experiments with this idea in which the model of the environment's dynamics is both approximate and learned online. These experiments involve the Mountain Car task, which requires approximation of both value function and model because it has continuous state variables. We used models of the simplest possible form, state-aggregation or \\grid\" models, and CMACs to represent the value function. We nd that model-based methods do indeed perform better than model-free reinforcement learning."
            },
            "slug": "Model-Based-Reinforcement-Learning-with-an-Learned-Kuvayev-Sutton",
            "title": {
                "fragments": [],
                "text": "Model-Based Reinforcement Learning with an Approximate, Learned Model"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that model-based methods do indeed perform better than model-free reinforcement learning, and these experiments involve the Mountain Car task, which requires approximation of both value function and model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2568346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3e10675b2ef79d8431b8011f909ee0d05e92d92",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel incremental algorithm that combines Q-learning, a well-known dynamic-programming based reinforcement learning method, with the TD(\u03bb) return estimation process, which is typically used in actor-critic learning, another well-known dynamic-programming based reinforcement learning method. The parameter \u03bb is used to distribute credit throughout sequences of actions, leading to faster learning and also helping to alleviate the non-Markovian effect of coarse state-space quatization. The resulting algorithm.Q(\u03bb)-learning, thus combines some of the best features of the Q-learning and actor-critic learning paradigms. The behavior of this algorithm has been demonstrated through computer simulations."
            },
            "slug": "Incremental-multi-step-Q-learning-Peng-Williams",
            "title": {
                "fragments": [],
                "text": "Incremental multi-step Q-learning"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel incremental algorithm that combines Q-learning with the TD(\u03bb) return estimation process, which is typically used in actor-critic learning, leading to faster learning and also helping to alleviate the non-Markovian effect of coarse state-space quatization."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810016"
                        ],
                        "name": "M. Colombetti",
                        "slug": "M.-Colombetti",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Colombetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Colombetti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153570946"
                        ],
                        "name": "M. Dorigo",
                        "slug": "M.-Dorigo",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Dorigo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dorigo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18150092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3f87d045341e713fa62d465531f29f18d4a0559",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This article is concerned with training an agent to perform sequential behavior. In previous work, we have been applying reinforcement learning techniques to control a reactive agent. Obviously, a purely reactive system is limited in the kind of interactions it can learn. In particular, it can learn what we call pseudosequences\u2014that is, sequences of actions in which each action is selected on the basis of current sensory stimuli. It cannot learn proper sequences, in which actions must be selected also on the basis of some internal state. Moreover, it is a result of our research that effective learning of proper sequences is improved by letting the agent and the trainer communicate. First, we consider trainer-to-agent communication, introducing the concept of reinforcement sensor, which lets the learning robot explicitly know whether the last reinforcement was a reward or a punishment. We also show how the use of this sensor makes error recovery rules emerge. Then we introduce agent-to-trainer communication, which is used to disambiguate ambiguous training situations\u2014that is, situations in which the observation of the agent's behavior does not provide the trainer with enough information to decide whether the agent's move is right or wrong. We also show an alternative solution to the problem of ambiguous situations, which involves learning to coordinate behavior in a simpler, unambiguous setting and then transferring what has been learned to a more complex situation. All the design choices we make are discussed and compared by means of experiments in a simulated world."
            },
            "slug": "Training-Agents-to-Perform-Sequential-Behavior-Colombetti-Dorigo",
            "title": {
                "fragments": [],
                "text": "Training Agents to Perform Sequential Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article introduces agent-to-trainer communication, which is used to disambiguate ambiguous training situations, and shows an alternative solution to the problem of ambiguous situations, which involves learning to coordinate behavior in a simpler, unambiguous setting and then transferring what has been learned to a more complex situation."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680506"
                        ],
                        "name": "R. Brafman",
                        "slug": "R.-Brafman",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Brafman",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brafman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708847"
                        ],
                        "name": "Moshe Tennenholtz",
                        "slug": "Moshe-Tennenholtz",
                        "structuredName": {
                            "firstName": "Moshe",
                            "lastName": "Tennenholtz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moshe Tennenholtz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 71
                            }
                        ],
                        "text": "The E3 algorithm of Kearns and Singh (2002) and the R-max algorithm of Brafman and Tennenholtz (2003) are guaranteed to find a near-optimal solution in time polynomial in the number of states and actions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 175713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5fa00d361e9e4d4344235ad4e354459f3f24e1e",
            "isKey": false,
            "numCitedBy": 1221,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "R-MAX is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-MAX, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-MAX improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo."
            },
            "slug": "R-MAX-A-General-Polynomial-Time-Algorithm-for-Brafman-Tennenholtz",
            "title": {
                "fragments": [],
                "text": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "R-MAX is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time and formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47191491"
                        ],
                        "name": "M. Duff",
                        "slug": "M.-Duff",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Duff",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Duff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1957250,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5e5b25e046f120b296b7c0bad24692ceea492427",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the relationship between certain reinforcement learning (RL) methods based on dynamic programming (DP) and a class of unorthodox Monte Carlo methods for solving systems of linear equations proposed in the 1950's. These methods recast the solution of the linear system as the expected value of a statistic suitably defined over sample paths of a Markov chain. The significance of our observations lies in arguments (Curtiss, 1954) that these Monte Carlo methods scale better with respect to state-space size than do standard, iterative techniques for solving systems of linear equations. This analysis also establishes convergence rate estimates. Because methods used in RL systems for approximating the evaluation function of a fixed control policy also approximate solutions to systems of linear equations, the connection to these Monte Carlo methods establishes that algorithms very similar to TD algorithms (Sutton, 1988) are asymptotically more efficient in a precise sense than other methods for evaluating policies. Further, all DP-based RL methods have some of the properties of these Monte Carlo algorithms, which suggests that although RL is often perceived to be slow, for sufficiently large problems, it may in fact be more efficient than other known classes of methods capable of producing the same results."
            },
            "slug": "Monte-Carlo-Matrix-Inversion-and-Reinforcement-Barto-Duff",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Matrix Inversion and Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "All DP-based RL methods have some of the properties of these Monte Carlo algorithms, which suggests that although RL is often perceived to be slow, for sufficiently large problems, it may in fact be more efficient than other known classes of methods capable of producing the same results."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66846174"
                        ],
                        "name": "Pawe\u0142 Cichosz",
                        "slug": "Pawe\u0142-Cichosz",
                        "structuredName": {
                            "firstName": "Pawe\u0142",
                            "lastName": "Cichosz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pawe\u0142 Cichosz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Cichosz (1995) has demonstrated a further implementation technique which reduces complexity to a constant independent of and ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16649081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69e492c7debf575ad1fba8fa0dfa2acc2170a3bf",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor \u03bb. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(\u03bb) for arbitrary \u03bb, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(\u03bb), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using \u03bb > 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning."
            },
            "slug": "Truncating-Temporal-Differences:-On-the-Efficient-Cichosz",
            "title": {
                "fragments": [],
                "text": "Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Examination of the issues of the efficient and general implementation of TD(\u03bb) for arbitrary \u03bb, for use with reinforcement learning algorithms optimizing the discounted sum of rewards suggests that using \u03bb > 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153570946"
                        ],
                        "name": "M. Dorigo",
                        "slug": "M.-Dorigo",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Dorigo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dorigo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810016"
                        ],
                        "name": "M. Colombetti",
                        "slug": "M.-Colombetti",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Colombetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Colombetti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2787851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73fb548322b36310483809c5c9dff9f3bee1872a",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robot-Shaping:-Developing-Autonomous-Agents-Through-Dorigo-Colombetti",
            "title": {
                "fragments": [],
                "text": "Robot Shaping: Developing Autonomous Agents Through Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69563214"
                        ],
                        "name": "J. Clouse",
                        "slug": "J.-Clouse",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Clouse",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Clouse"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 652,
                                "start": 124
                            }
                        ],
                        "text": "In 1961 and 1963 he described a simple trial-and-error learning system for learning how to play tic-tac-toe (or naughts and crosses) called MENACE (for Matchbox Educable Naughts and Crosses Engine). It consisted of a matchbox for each possible game position, each matchbox containing a number of colored beads, a different color for each possible move from that position. By drawing a bead at random from the matchbox corresponding to the current game position, one could determine MENACE\u2019s move. When a game was over, beads were added to or removed from the boxes used during play to reinforce or punish MENACE\u2019s decisions. Michie and Chambers (1968) described another tic-tac-toe reinforcement learner called GLEE (Game Learning Expectimaxing Engine) and a reinforcement learning controller called BOXES."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1071,
                                "start": 124
                            }
                        ],
                        "text": "In 1961 and 1963 he described a simple trial-and-error learning system for learning how to play tic-tac-toe (or naughts and crosses) called MENACE (for Matchbox Educable Naughts and Crosses Engine). It consisted of a matchbox for each possible game position, each matchbox containing a number of colored beads, a different color for each possible move from that position. By drawing a bead at random from the matchbox corresponding to the current game position, one could determine MENACE\u2019s move. When a game was over, beads were added to or removed from the boxes used during play to reinforce or punish MENACE\u2019s decisions. Michie and Chambers (1968) described another tic-tac-toe reinforcement learner called GLEE (Game Learning Expectimaxing Engine) and a reinforcement learning controller called BOXES. They applied BOXES to the task of learning to balance a pole hinged to a movable cart on the basis of a failure signal occurring only when the pole fell or the cart reached the end of a track. This task was adapted from the earlier work of Widrow and Smith (1964), who used supervised learning methods, assuming instruction from a teacher already able to balance the pole."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 47
                            }
                        ],
                        "text": "See Lin (1992), Maclin and Shavlik (1994), and Clouse (1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 114750270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee4bee01b6a0618b7220b4c64ce5cea128c15e5b",
            "isKey": true,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Apprentice learning and reinforcement learning have each been developed in order to endow computerized agents with the capacity to learn to perform multiple-step tasks. In apprentice learning, the autonomous agent learns based on examples of a training agent''s problem-solving performance; and in reinforcement learning, the agent changes its performance based on scalar feedback about the consequences of its own actions. A thorough comparison of these disparate methods reveals that each possesses strengths as well as weaknesses and that neither is clearly better than the other. This dissertation demonstrates that a careful integration of the two learning methods can produce a more powerful method than either one alone. An argument based on the characteristics of the two methods maintains that a hybrid will be an improvement because of the complimentary strengths of its constituents. In designing a new integrated approach, we consider the following: how the learner and trainer interact during training, how the learner will assimilate the trainer''s expertise, how the proficiency of the trainer affects the learner''s ability to perform the task, and when during training the learner should acquire information from the trainer. The study demonstrates that even a straightforward technique of asking the trainer for aid randomly throughout training is clearly better than reinforcement learning alone, and sometimes better than apprentice learning alone. Moreover, the results indicate that the trainer''s expertise in performing the task must be considered thoroughly, because it has a direct bearing on how well the learner will benefit from the trainer''s aid, and may even preclude the learner from acquiring an appropriate policy. Thus, one must be careful when designing an integrated system to examine the trainer''s ability to perform the task. Finally, a more sophisticated strategy of acquiring the trainer''s help is shown to be an improvement over asking for aid uniformly. This suggests that sophisticated strategies for obtaining information from the trainer are worth studying further."
            },
            "slug": "On-integrating-apprentice-learning-and-learning-Clouse",
            "title": {
                "fragments": [],
                "text": "On integrating apprentice learning and reinforcement learning TITLE2"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is demonstrated that a careful integration of the two learning methods can produce a more powerful method than either one alone, and sometimes better than apprentice learning alone."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729906"
                        ],
                        "name": "Prasad Tadepalli",
                        "slug": "Prasad-Tadepalli",
                        "structuredName": {
                            "firstName": "Prasad",
                            "lastName": "Tadepalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasad Tadepalli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1811287"
                        ],
                        "name": "DoKyeong Ok",
                        "slug": "DoKyeong-Ok",
                        "structuredName": {
                            "firstName": "DoKyeong",
                            "lastName": "Ok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "DoKyeong Ok"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18010224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "983265d0fe0ba349ec74d6d8e23e7381a9e32e2c",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce a model-bases reinforcement learning method called H-learning, which optimizes undiscounted average reward. We compare it with three other reinforcement learning methods in the domain of scheduling Automatic Guided Vehicles, and transportation robots used in modern manufacturing plants and facilities. The four methods differ along two dimensions. They are either model-based or model-free, and optimize discounted total reward or undiscounted average reward. Our experimental results indicate that H-learning is more robust with respect to changes in the domain parameters, and in many cases, converges in fewer steps to better average reward per time step than all the other methods. An added advantage is that unlike the other methods it does not have any parameters to tune."
            },
            "slug": "H-Learning:-A-Reinforcement-Learning-Method-for-Tadepalli-Ok",
            "title": {
                "fragments": [],
                "text": "H-Learning: A Reinforcement Learning Method for Optimizing Undiscounted Average Reward"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper introduces a model-bases reinforcement learning method called H-learning, which optimizes undiscounted average reward, and compares it with three other reinforcement learning methods in the domain of scheduling Automatic Guided Vehicles, and transportation robots used in modern manufacturing plants and facilities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819261"
                        ],
                        "name": "S. Bradtke",
                        "slug": "S.-Bradtke",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bradtke",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bradtke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13624034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eaec01700f5ea63af311cfd7a70a3869460ce080",
            "isKey": false,
            "numCitedBy": 1304,
            "numCiting": 170,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-to-Act-Using-Real-Time-Dynamic-Programming-Barto-Bradtke",
            "title": {
                "fragments": [],
                "text": "Learning to Act Using Real-Time Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784519"
                        ],
                        "name": "Peter Norvig",
                        "slug": "Peter-Norvig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Norvig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Norvig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 112
                            }
                        ],
                        "text": "For further reading on heuristic search, the reader is encouraged to consult texts and surveys such as those by Russell and Norvig (1995) and Korf (1988). Peng and Williams (1993) explored a forward focusing of backups much as is suggested in this section."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 112
                            }
                        ],
                        "text": "For further reading on heuristic search, the reader is encouraged to consult texts and surveys such as those by Russell and Norvig (1995) and Korf (1988)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 53142908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3524cdf7cf8344e7eb74886f71fcbb5c6732c337",
            "isKey": false,
            "numCitedBy": 26734,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence."
            },
            "slug": "Artificial-Intelligence:-A-Modern-Approach-Russell-Norvig",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence: A Modern Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065656651"
                        ],
                        "name": "Pawea Cichosz",
                        "slug": "Pawea-Cichosz",
                        "structuredName": {
                            "firstName": "Pawea",
                            "lastName": "Cichosz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pawea Cichosz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7989057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5a00e16d536caf3c3ebe50283da202797fc71ed",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Temporal di erence (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor . Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the e cient and general implementation of TD( ) for arbitrary , for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to su er from both ine ciency and lack of generality. The TTD (Truncated Temporal Di erences) procedure is proposed as an alternative, that indeed only approximates TD( ), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using > 0 with the TTD procedure allows one to obtain a signi cant learning speedup at essentially the same cost as usual TD(0) learning."
            },
            "slug": "Truncating-Temporal-Diierences:-on-the-Eecient-of-Cichosz",
            "title": {
                "fragments": [],
                "text": "Truncating Temporal Diierences: on the Eecient Implementation of Td() for Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The TTD (Truncated Temporal Di erences) procedure is proposed as an alternative, that indeed only approximates TD( ), but requires very little computation per action and can be used with arbitrary function representation methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144745483"
                        ],
                        "name": "M. Tan",
                        "slug": "M.-Tan",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 79
                            }
                        ],
                        "text": "The idea that stimuli produce aftereffects in the nervous system that are important for learning is very old. Animal learning psychologists at least as far back as Pavlov (1927) and Hull (1943, 1952) included such ideas in their theories."
                    },
                    "intents": []
                }
            ],
            "corpusId": 33830667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "044d1758742d5250a871f4d78f3d9eb1128a6f51",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-a-Cost-Sensitive-Internal-Representation-Tan",
            "title": {
                "fragments": [],
                "text": "Learning a Cost-Sensitive Internal Representation for Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1211821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a20f0ce0616def7cc9a87446c228906cd5da093b",
            "isKey": false,
            "numCitedBy": 4404,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy."
            },
            "slug": "Policy-Gradient-Methods-for-Reinforcement-Learning-Sutton-McAllester",
            "title": {
                "fragments": [],
                "text": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proves for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 649822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "085fb3acabcbf80ef1bf47daec50d246475b072b",
            "isKey": false,
            "numCitedBy": 690,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes (POMDPs) controlled by parameterized stochastic policies. A similar algorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free parameter \u03b2 \u2208 [0, 1] (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of GPOMDP, and show how the correct choice of the parameter \u03b2 is related to the mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter, Bartlett, & Weaver, 2001) we show how the gradient estimates generated by GPOMDP can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward."
            },
            "slug": "Infinite-Horizon-Policy-Gradient-Estimation-Baxter-Bartlett",
            "title": {
                "fragments": [],
                "text": "Infinite-Horizon Policy-Gradient Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes (POMDPs) controlled by parameterized stochastic policies, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 169
                            }
                        ],
                        "text": "The two left panels are applications to simple continuous-state control tasks using the Sarsa(\u03bb) algorithm and tile coding, with either replacing or accumulating traces (Sutton, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 65
                            }
                        ],
                        "text": "Tile coding has been used in many reinforcement learning systems (e.g., Shewchuk and Dean, 1990; Lin and Kim, 1991; Miller, Scalera, and Kim, 1994; Sofge and White, 1992; Tham, 1994; Sutton, 1996; Watkins, 1989) as well as in other types of learning control systems (e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10253791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbeb58496711887bf563ad7b0a2860fd46bcd725",
            "isKey": false,
            "numCitedBy": 1278,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "On large problems, reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and computational results have been mixed. In particular, Boyan and Moore reported at last year's meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces. In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger. The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned offline. Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcomes (\"rollouts\"), as in classical Monte Carlo methods, and as in the TD(\u03bb) algorithm when \u03bb = 1. However, in our experiments this always resulted in substantially poorer performance. We conclude that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general \u03bb."
            },
            "slug": "Generalization-in-Reinforcement-Learning:-Examples-Sutton",
            "title": {
                "fragments": [],
                "text": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is concluded that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general \u03bb."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742183"
                        ],
                        "name": "M. Matari\u0107",
                        "slug": "M.-Matari\u0107",
                        "structuredName": {
                            "firstName": "Maja",
                            "lastName": "Matari\u0107",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Matari\u0107"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7304077,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "563cf7359970015db2f8a05c41e9efef7025dda2",
            "isKey": false,
            "numCitedBy": 447,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reward-Functions-for-Accelerated-Learning-Matari\u0107",
            "title": {
                "fragments": [],
                "text": "Reward Functions for Accelerated Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740513"
                        ],
                        "name": "B. Buchanan",
                        "slug": "B.-Buchanan",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Buchanan",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Buchanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109403047"
                        ],
                        "name": "Reid G. Smith",
                        "slug": "Reid-G.-Smith",
                        "structuredName": {
                            "firstName": "Reid",
                            "lastName": "Smith",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reid G. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107485976"
                        ],
                        "name": "C. Johnson",
                        "slug": "C.-Johnson",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Johnson",
                            "middleNames": [
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13540072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "827411d2ebf389a5a8289abd6f17174ed25a278e",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 170,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The terms adaptation, learning, concept-formation, induction, self-organization, and self-repair have all been used in the context of learning system (LS) research. In this article, three distinct approaches to machine learning and adaptation are considered: (i) the adaptive control approach, (ii) the pattern recognition approach, and (iii) the artificial intelligence approach. Progress in each of these areas is summarized in the first part of the article. In the next part a general model for learning systems is presented that allows characterization and comparison of individual algorithms and programs in all of these areas. The model details the functional components felt to be essential for any learning system, independent of the techniques used for its construction, and the specific environment in which it operates. Specific examples of learning systems are described in terms of the model. (Author)"
            },
            "slug": "Models-of-Learning-Systems.-Buchanan-Mitchell",
            "title": {
                "fragments": [],
                "text": "Models of Learning Systems."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A general model for learning systems is presented that allows characterization and comparison of individual algorithms and programs in all of these areas and details the functional components felt to be essential for any learning system, independent of the techniques used for its construction, and the specific environment in which it operates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 391,
                                "start": 239
                            }
                        ],
                        "text": "Although much simpler than modern video games, Atari 2600 games are still entertaining and challenging for human players, and they have been attractive as testbeds for developing and evaluating reinforcement learning methods (Diuk, Cohen, Littman, 2008; Naddaf, 2010; Cobo, Zang, Isbell, and Thomaz, 2011; Bellemare, Veness, and Bowling, 2012). Bellemare, Naddaf, Veness, and Bowling (2012) developed the publicly available Arcade Learning Environment (ALE) to encourage and simplify using Atari 2600 games to study learning and planning algorithms."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8108362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Markov-Games-as-a-Framework-for-Multi-Agent-Littman",
            "title": {
                "fragments": [],
                "text": "Markov Games as a Framework for Multi-Agent Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766844"
                        ],
                        "name": "Eric Wiewiora",
                        "slug": "Eric-Wiewiora",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Wiewiora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Wiewiora"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15648605,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbbcf78c6fc189f1f8d1b76dbf314b1132114cfc",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior. \n \nIn this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping."
            },
            "slug": "Potential-Based-Shaping-and-Q-Value-Initialization-Wiewiora",
            "title": {
                "fragments": [],
                "text": "Potential-Based Shaping and Q-Value Initialization are Equivalent"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is proved that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059884"
                        ],
                        "name": "V. Gullapalli",
                        "slug": "V.-Gullapalli",
                        "structuredName": {
                            "firstName": "Vijaykumar",
                            "lastName": "Gullapalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Gullapalli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2377633,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d5b1239cfb88f6d5f9af62a16969cf9802896027",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning complex control behavior by building some initial control knowledge into the learning controller through shaping is addressed. The principle underlying shaping is that learning to solve complex problems can be facilitated by first learning to solve related simpler problems. The authors present experimental results illustrating the utility of shaping in training controllers by means of reinforcement learning methods. Shaping a reinforcement learning controller's behavior over time by gradually increasing the complexity of the control task as the controller learns makes it possible to scale reinforcement learning methods to more complex tasks. This is illustrated by an example.<<ETX>>"
            },
            "slug": "Shaping-as-a-method-for-accelerating-reinforcement-Gullapalli-Barto",
            "title": {
                "fragments": [],
                "text": "Shaping as a method for accelerating reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Shaping a reinforcement learning controller's behavior over time by gradually increasing the complexity of the control task as the controller learns makes it possible to scale reinforcement learning methods to more complex tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1992 IEEE International Symposium on Intelligent Control"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145290695"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Watkins",
                            "middleNames": [
                                "J.",
                                "C.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 208910339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
            "isKey": false,
            "numCitedBy": 7189,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one."
            },
            "slug": "Q-learning-Watkins-Dayan",
            "title": {
                "fragments": [],
                "text": "Q-learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989), showing that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action- values are represented discretely."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 73
                            }
                        ],
                        "text": "4 Prioritized sweeping was developed simultaneously and independently by Moore and Atkeson (1993) and Peng and Williams (1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 73
                            }
                        ],
                        "text": "4 Prioritized sweeping was developed simultaneously and independently by Moore and Atkeson (1993) and Peng and Williams (1993). The results in Figure 8."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 73
                            }
                        ],
                        "text": "4 Prioritized sweeping was developed simultaneously and independently by Moore and Atkeson (1993) and Peng and Williams (1993). The results in Figure 8.7 are due to Peng and Williams (1993). The results in Figure 8."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 427,
                                "start": 143
                            }
                        ],
                        "text": "Reynolds and Wickens (2002) proposed a three-factor rule for synaptic plasticity in the corticostriatal pathway in which dopamine modulates changes in corticostriatal synaptic efficacy. They discussed the experimental support for this kind of learning rule and its possible molecular basis. The definitive demonstration of spike-timing-dependent plasticity (STDP) is attributed to Markram, L\u00fcbke, Frotscher, and Sakmann (1997), with evidence from earlier experiments by Levy and Steward (1983) and others that the relative timing of pre- and postsynaptic spikes is critical for inducing changes in synaptic efficacy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 46
                            }
                        ],
                        "text": "example is based on a similar task studied by Moore (1990). The results on it presented in Figure 10."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 46
                            }
                        ],
                        "text": "example is based on a similar task studied by Moore (1990). The results on it presented in Figure 10.1 are from Sutton (1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60851166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "874b3a63422eeaf24c14435ee6091ed48247bff3",
            "isKey": true,
            "numCitedBy": 427,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation is about the application of machine learning to robot control. A system which has no initial model of the robot/world dynamics should be able to construct such a model using data received through its sensors|an approach which is formalized here as the SAB (State-ActionBehaviour) control cycle. A method of learning is presented in which all the experiences in the lifetime of the robot are explicitly remembered. The experiences are stored in a manner which permits fast recall of the closest previous experience to any new situation, thus permitting very quick predictions of the e ects of proposed actions and, given a goal behaviour, permitting fast generation of a candidate action. The learning can take place in high-dimensional non-linear control spaces with real-valued ranges of variables. Furthermore, the method avoids a number of shortcomings of earlier learning methods in which the controller can become trapped in inadequate performance which does not improve. Also considered is how the system is made resistant to noisy inputs and how it adapts to environmental changes. A well founded mechanism for choosing actions is introduced which solves the experiment/perform dilemma for this domain with adequate computational e ciency, and with fast convergence to the goal behaviour. The dissertation explains in detail how the SAB control cycle can be integrated into both low and high complexity tasks. The methods and algorithms are evaluated with numerous experiments using both real and simulated robot domains. The nal experiment also illustrates how a compound learning task can be structured into a hierarchy of simple learning tasks."
            },
            "slug": "Efficient-memory-based-learning-for-robot-control-Moore",
            "title": {
                "fragments": [],
                "text": "Efficient memory-based learning for robot control"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A method of learning is presented in which all the experiences in the lifetime of the robot are explicitly remembered, thus permitting very quick predictions of the e ects of proposed actions and, given a goal behaviour, permitting fast generation of a candidate action."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5600863,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a579d06ac278e14948f67748cd651e4eb617ae4e",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Without-State-Estimation-in-Partially-Singh-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Learning Without State-Estimation in Partially Observable Markovian Decision Processes"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 81
                            }
                        ],
                        "text": "The upper-right panel is for policy evaluation on a random walk task using TD(\u03bb) (Singh and Sutton, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 95
                            }
                        ],
                        "text": "Every-visit MC is less straightforward, but its estimates also converge quadratically to v\u03c0(s) (Singh and Sutton, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5860317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c21accc28f1ab02404948ca4c315ef4a8596ed1",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The eligibility trace is one of the basic mechanisms used in reinforcement learning to handle delayed reward. In this paper we introduce a new kind of eligibility trace, the replacing trace, analyze it theoretically, and show that it results in faster, more reliable learning than the conventional trace. Both kinds of trace assign credit to prior events according to how recently they occurred, but only the conventional trace gives greater credit to repeated events. Our analysis is for conventional and replace-trace versions of the offline TD(1) algorithm applied to undiscounted absorbing Markov chains. First, we show that these methods converge under repeated presentations of the training set to the same predictions as two well known Monte Carlo methods. We then analyze the relative efficiency of the two Monte Carlo methods. We show that the method corresponding to conventional TD is biased, whereas the method corresponding to replace-trace TD is unbiased. In addition, we show that the method corresponding to replacing traces is closely related to the maximum likelihood solution for these tasks, and that its mean squared error is always lower in the long run. Computational results confirm these analyses and show that they are applicable more generally. In particular, we show that replacing traces significantly improve performance and reduce parameter sensitivity on the \"Mountain-Car\" task, a full reinforcement-learning problem with a continuous state space, when using a feature-based function approximator."
            },
            "slug": "Reinforcement-Learning-with-Replacing-Eligibility-Singh-Sutton",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with Replacing Eligibility Traces"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This paper introduces a new kind of eligibility trace, the replacing trace, analyze it theoretically, and shows that it results in faster, more reliable learning than the conventional trace, and significantly improves performance and reduces parameter sensitivity on the \"Mountain-Car\" task."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5843415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00b225d925dfa9ae8fe9ba52cafd4eef2092845a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Combining elements of the theory of dynamic programming with features appropriate for on-line learning has led to an approach Watkins has called incre-mental dynamic programming. Here we adopt this incremental dynamic programming point of view and obtain some preliminary mathematical results relevant to understanding the capabilities and limitations of actor-critic learning systems. Examples of such systems are Samuel's learning checker player, Hol-land's bucket brigade algorithm, Witten's adaptive controller, and the adaptive heuristic critic algorithm of Barto, Sutton, and Anderson. Particular emphasis here is on the eeect of complete asynchrony in the updating of the actor and the critic across individual states or state-action pairs. The main results are that, while convergence to optimal performance is not guaranteed in general, there are a number of situations in which such convergence is assured."
            },
            "slug": "A-MATHEMATICAL-ANALYSIS-OF-ACTOR-CRITIC-FOR-OPTIMAL-Williams-Baird",
            "title": {
                "fragments": [],
                "text": "A MATHEMATICAL ANALYSIS OF ACTOR-CRITIC ARCHITECTURES FOR LEARNING OPTIMAL CONTROLS THROUGH INCREMENTAL DYNAMIC PROGRAMMING"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Some preliminary mathematical results are obtained relevant to understanding the capabilities and limitations of actor-critic learning systems and it is found that, while convergence to optimal performance is not guaranteed, there are a number of situations in which such convergence is assured."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15773832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87bfc4daa478433d8b18a47edf9112a25098cada",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Planning-by-Incremental-Dynamic-Programming-Sutton",
            "title": {
                "fragments": [],
                "text": "Planning by Incremental Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149607686"
                        ],
                        "name": "Anton Schwartz",
                        "slug": "Anton-Schwartz",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anton Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 21
                            }
                        ],
                        "text": "R-learning is due to Schwartz (1993). Mahadevan (1996), Tadepalli and Ok (1994), and Bertsekas and Tsitsiklis (1996) have also studied reinforcement learning for undiscounted continual tasks."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10564390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99b2fd28dcab3657c5f1271a05223f4740e4b65c",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Reinforcement-Learning-Method-for-Maximizing-Schwartz",
            "title": {
                "fragments": [],
                "text": "A Reinforcement Learning Method for Maximizing Undiscounted Rewards"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143637056"
                        ],
                        "name": "P. Sastry",
                        "slug": "P.-Sastry",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Sastry",
                            "middleNames": [
                                "Shanti"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sastry"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3042022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab3940e97626da3b696eaa1bb7b8af7e321e63a7",
            "isKey": false,
            "numCitedBy": 404,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Automata models of learning systems introduced in the 1960s were popularized as learning automata (LA) in a survey paper by Narendra and Thathachar (1974). Since then, there have been many fundamental advances in the theory as well as applications of these learning models. In the past few years, the structure of LA, has been modified in several directions to suit different applications. Concepts such as parameterized learning automata (PLA), generalized learning,automata (GLA), and continuous action-set learning automata (CALA) have been proposed, analyzed, and applied to solve many significant learning problems. Furthermore, groups of LA forming teams and feedforward networks have been shown to converge to desired solutions under appropriate learning algorithms. Modules of LA have been used for parallel operation with consequent increase in speed of convergence. All of these concepts and results are relatively new and are scattered in technical literature. An attempt has been made in this paper to bring together the main ideas involved in a unified framework and provide pointers to relevant references."
            },
            "slug": "Varieties-of-learning-automata:-an-overview-Thathachar-Sastry",
            "title": {
                "fragments": [],
                "text": "Varieties of learning automata: an overview"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An attempt has been made to bring together the main ideas involved in a unified framework of learning automata and provide pointers to relevant references."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 36
                            }
                        ],
                        "text": "These difficulties notwithstanding, Abbeel and Ng (2004) argue that the inverse reinforcement learning approach can sometimes be more effective than supervised learning for benefiting from the behavior of an expert."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207155342,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
            "isKey": false,
            "numCitedBy": 2545,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using \"inverse reinforcement learning\" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function."
            },
            "slug": "Apprenticeship-learning-via-inverse-reinforcement-Abbeel-Ng",
            "title": {
                "fragments": [],
                "text": "Apprenticeship learning via inverse reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work thinks of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and gives an algorithm for learning the task demonstrated by the expert, based on using \"inverse reinforcement learning\" to try to recover the unknown reward function."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10082747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea899c8a3806a02a225061a35f802b00d90a0a20",
            "isKey": false,
            "numCitedBy": 345,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "It is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning (RL) algorithms to real-world problems. Unfortunately almost all of the theory of reinforcement learning assumes lookup table representations. In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approximator based on a simple extension to state aggregation (a commonly used form of compact representation), namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non-discrete nature of soft state aggregation. Preliminary empirical results are also presented."
            },
            "slug": "Reinforcement-Learning-with-Soft-State-Aggregation-Singh-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with Soft State Aggregation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a function approximator based on a simple extension to state aggregation (a commonly used form of compact representation), namely soft state aggregation, a theory of convergence for RL with arbitrary, but fixed, softstate aggregation, and a novel intuitive understanding of the effect of state aggregation on online RL."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153662959"
                        ],
                        "name": "J. W. Moore",
                        "slug": "J.-W.-Moore",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Moore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32904380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "884bf8cb17fb159c56a9c1cac8fda478dcc44ffa",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "|In this report we show how the class of adaptive prediction methods that Sutton called \\temporal di erence,\" or TD, methods are related to the theory of squential decision making. TD methods have been used as \\adaptive critics\" in connectionist learning systems, and have been proposed as models of animal learning in classical conditioning experiments. Here we relate TD methods to decision tasks formulated in terms of a stochastic dynamical system whose behavior unfolds over time under the in uence of a decision maker's actions. Strategies are sought for selecting actions so as to maximize a measure of long-term payo gain. Mathematically, tasks such as this can be formulated as Markovian decision problems, and numerous methods have been proposed for learning how to solve such problems. We show how a TD method can be understood as a novel synthesis of concepts from the theory of stochastic dynamic programming, which comprises the standard method for solving such tasks when a model of the dynamical system is available, and the theory of parameter estimation, which provides the appropriate context for studying learning rules in the form of equations for updating associative strengths in behavioral models, or connection weights in connectionist networks. Because this report is oriented primarily toward the non-engineer interested in animal learning, it presents tutorials on stochastic sequential decision tasks, stochastic dynamic programming, and parameter estimation. y The authors acknowledge their indebtedness to C. W. Anderson, who has contributed greatly to the development of the ideas presented here. We also thank S. Bradtke, J. E. Desmond, J. Franklin, J. C. Houk, A. I. Houston, and E. J. Kehoe for their helpful comments on earlier drafts of this report, and we especially thank J. W. Moore for his extremely detailed and helpful criticism. A. G. Barto acknowledges the support of the Air Force O ce of Scienti c Research, Bolling AFB, through grant AFOSR-87-0030, and the King's College Research Centre, King's College Cambridge, England, where much of this report was written. A version of this report will appear as a chapter in the forthcoming book Learning and Computational Neuroscience, M. Gabriel and J. W. Moore, editors, The MIT Press, Cambridge, MA."
            },
            "slug": "Learning-and-Sequential-Decision-Making-Moore",
            "title": {
                "fragments": [],
                "text": "Learning and Sequential Decision Making"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how a TD method can be understood as a novel synthesis of concepts from the theory of stochastic dynamic programming, which comprises the standard method for solving such tasks when a model of the dynamical system is available."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 88
                            }
                        ],
                        "text": "Klopf\u2019s ideas were especially influential on the authors because our assessment of them (Barto and Sutton, 1981a) led to our appreciation of the distinction between supervised and reinforcement learning, and to our eventual focus on reinforcement learning."
                    },
                    "intents": []
                }
            ],
            "corpusId": 58960670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a725e66a9975300512852cffa67938b8fecf2702",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This report assesses the promise of a network approach to adaptive problem solving in which the network components themselves possess considerable adaptive power. We show that components designed with attention to the temporal aspects of reinforcement learning can acquire knowledge about feedback pathways in which they are embedded and can use this knowledge to seek their preferred inputs, thus combining pattern recognition, search, and control functions. A review of adaptive network research shows that networks of components having these capabilities have not been studied previously. We demonstrate that simple networks of these elements can solve types of problems that are beyond the capabilities of networks studied in the past. An associative memory is presented that retains the generalization capabilities and noise resistance of associative memories previously studied but does not require a 'teacher' to provide the desired associations. It conducts active, closed-loop searches for the most rewarding associations. We provide an example in whcih these searches are conducted through the system's external environment and an example in which they are conducted through an internal predictive model of that environment. The latter system is capable of a simple form of latent learning."
            },
            "slug": "Goal-Seeking-Components-for-Adaptive-Intelligence:-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Goal Seeking Components for Adaptive Intelligence: An Initial Assessment."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that components designed with attention to the temporal aspects of reinforcement learning can acquire knowledge about feedback pathways in which they are embedded and can use this knowledge to seek their preferred inputs, thus combining pattern recognition, search, and control functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2796200"
                        ],
                        "name": "Gregory R. Galperin",
                        "slug": "Gregory-R.-Galperin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Galperin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory R. Galperin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10886094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3552fba431aa866bf9de293bebf7eff168e9e19c",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP1 and SP2 parallel-RISC supercomputers. \n \nWe have obtained promising initial results in applying this algorithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. In each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment."
            },
            "slug": "On-line-Policy-Improvement-using-Monte-Carlo-Search-Tesauro-Galperin",
            "title": {
                "fragments": [],
                "text": "On-line Policy Improvement using Monte-Carlo Search"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller and results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720087"
                        ],
                        "name": "S. Whitehead",
                        "slug": "S.-Whitehead",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Whitehead",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Whitehead"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14606394,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89a4ddffb62dd31d174d6964a72453b2541c7bc3",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Online-Learning-with-Random-Representations-Sutton-Whitehead",
            "title": {
                "fragments": [],
                "text": "Online Learning with Random Representations"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70560270"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143637056"
                        ],
                        "name": "P. Sastry",
                        "slug": "P.-Sastry",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Sastry",
                            "middleNames": [
                                "Shanti"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sastry"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 2
                            }
                        ],
                        "text": "5 Thistlethwaite (1951) provides an extensive review of latent learning experiments up to the time of its publication."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12960941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c6ebb3ad0ec3839bae24912945e31804edd8bc7",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of reinforcement schemes for learning automata that makes use of estimates of the random characteristics of the environment is introduced. Both a single automaton and a hierarchy of learning automata are considered. It is shown that under small values for the parameters, these algorithms converge in probability to the optimal choice of actions. By simulation it is observed that, for both cases, these algorithms converge quite rapidly. Finally, the generality of this method of designing learning schemes is pointed out, and it is shown that a very minor modification will enable the algorithm to learn in a multiteacher environment as well."
            },
            "slug": "A-new-approach-to-the-design-of-reinforcement-for-Thathachar-Sastry",
            "title": {
                "fragments": [],
                "text": "A new approach to the design of reinforcement schemes for learning automata"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The generality of this method of designing learning schemes is pointed out, and it is shown that a very minor modification will enable the algorithm to learn in a multiteacher environment as well."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648880"
                        ],
                        "name": "Jooyoung Park",
                        "slug": "Jooyoung-Park",
                        "structuredName": {
                            "firstName": "Jooyoung",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jooyoung Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117146000"
                        ],
                        "name": "Jongho Kim",
                        "slug": "Jongho-Kim",
                        "structuredName": {
                            "firstName": "Jongho",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jongho Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3011308"
                        ],
                        "name": "Daesung Kang",
                        "slug": "Daesung-Kang",
                        "structuredName": {
                            "firstName": "Daesung",
                            "lastName": "Kang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daesung Kang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15300609,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "c3b3fd8624621dd10ff9ed63ea35fff8da6d1015",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, actor-critic methods have drawn much interests in the area of reinforcement learning, and several algorithms have been studied along the line of the actor-critic strategy. This paper studies an actor-critic type algorithm utilizing the RLS(recursive least-squares) method, which is one of the most efficient techniques for adaptive signal processing, together with natural policy gradient. In the actor part of the studied algorithm, we follow the strategy of performing parameter update via the natural gradient method, while in its update for the critic part, the recursive least-squares method is employed in order to make the parameter estimation for the value functions more efficient. The studied algorithm was applied to locomotion of a two-linked robot arm, and showed better performance compared to the conventional stochastic gradient ascent algorithm."
            },
            "slug": "An-RLS-Based-Natural-Actor-Critic-Algorithm-for-of-Park-Kim",
            "title": {
                "fragments": [],
                "text": "An RLS-Based Natural Actor-Critic Algorithm for Locomotion of a Two-Linked Robot Arm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An actor-critic type algorithm utilizing the RLS(recursive least-squares) method, which is one of the most efficient techniques for adaptive signal processing, together with natural policy gradient, showed better performance than the conventional stochastic gradient ascent algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "CIS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453007"
                        ],
                        "name": "A. Cassandra",
                        "slug": "A.-Cassandra",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Cassandra",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cassandra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11466814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a666a97335c9ec87cce86b359bd28e85fecae580",
            "isKey": false,
            "numCitedBy": 731,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Policies-for-Partially-Observable-Scaling-Littman-Cassandra",
            "title": {
                "fragments": [],
                "text": "Learning Policies for Partially Observable Environments: Scaling Up"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720087"
                        ],
                        "name": "S. Whitehead",
                        "slug": "S.-Whitehead",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Whitehead",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Whitehead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9134331,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6104d0e68c7f67da58b2f84a663df45d82d86b18",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This article considers adaptive control architectures that integrate active sensory-motor systems with decision systems based on reinforcement learning. One unavoidable consequence of active perception is that the agent's internal representation often confounds external world states. We call this phoenomenon Perceptual aliasing and show that it destabilizes existing reinforcement learning algorithms with respect to the optimal decision policy. We then describe a new decision system that overcomes these difficulties for a restricted class of decision problems. The system incorporates a perceptual subcycle within the overall decision cycle and uses a modified learning algorithm to suppress the effects of perceptual aliasing. The result is a control architecture that learns not only how to solve a task but also where to focus its visual attention in order to collect necessary sensory information."
            },
            "slug": "Learning-to-perceive-and-act-by-trial-and-error-Whitehead-Ballard",
            "title": {
                "fragments": [],
                "text": "Learning to perceive and act by trial and error"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This article considers adaptive control architectures that integrate active sensory-motor systems with decision systems based on reinforcement learning and shows that perceptual aliasing destabilizes existing reinforcement learning algorithms with respect to the optimal decision policy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368601"
                        ],
                        "name": "Doina Precup",
                        "slug": "Doina-Precup",
                        "structuredName": {
                            "firstName": "Doina",
                            "lastName": "Precup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doina Precup"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 76564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
            "isKey": false,
            "numCitedBy": 2806,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Between-MDPs-and-Semi-MDPs:-A-Framework-for-in-Sutton-Precup",
            "title": {
                "fragments": [],
                "text": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 133
                            }
                        ],
                        "text": "He and Barto refined these ideas and developed a psychological model of classical conditioning based on temporal-difference learning (Sutton and Barto, 1981a; Barto and Sutton, 1982)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 55
                            }
                        ],
                        "text": "Our use of eligibility traces is based on Klopf\u2019s work (Sutton, 1978a, 1978b, 1978c; Barto and Sutton, 1981a, 1981b; Sutton and Barto, 1981a; Barto, Sutton, and Anderson, 1983; Sutton, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 63
                            }
                        ],
                        "text": "We may have been the first to use the term \u201celigibility trace\u201d (Sutton and Barto, 1981a)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2831441,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "60944c5243db70a687a320a2622d3bd1610802a8",
            "isKey": false,
            "numCitedBy": 1448,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Many adaptive neural network theories are based on neuronlike adaptive elements that can behave as single unit analogs of associative conditioning. In this article we develop a similar adaptive element, but one which is more closely in accord with the facts of animal learning theory than elements commonly studied in adaptive network research. We suggest that an essential feature of classical conditioning that has been largely overlooked by adaptive network theorists is its predictive nature. The adaptive element we present learns to increase its response rate in anticipation of increased stimulation, producing a conditioned response before the occurrence of the unconditioned stimulus. The element also is in strong agreement with the behavioral data regarding the effects of stimulus context, since it is a temporally refined extension of the Rescorla-Wagner model. We show by computer simulation that the element becomes sensitive to the most reliable, nonredundant, and earliest predictors of reinforcement . We also point out that the model solves many of the stability and saturation problems encountered in network simulations. Finally, we discuss our model in light of recent advances in the physiology and biochemistry of synaptic mechanisms."
            },
            "slug": "Toward-a-modern-theory-of-adaptive-networks:-and-Sutton-Barto",
            "title": {
                "fragments": [],
                "text": "Toward a modern theory of adaptive networks: expectation and prediction."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The adaptive element presented learns to increase its response rate in anticipation of increased stimulation, producing a conditioned response before the occurrence of the unconditioned stimulus, and is in strong agreement with the behavioral data regarding the effects of stimulus context."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60631127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccd8a9a39a2d01c28f0ae5730bbbfd2fe1693871",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "THIS REPORT IS AN INTRODUCTORY OVERVIEW OF LEARNING BY CONNECTIONIST NETWORKS, ALSO CALLED ARTIFICIAL NEURAL NETWORKS, WITH A FOCUS ON THE IDEAS AND METHODS MOST RELEVANT TO THE CONTROL OF DYNAMICAL SYSTEMS. IT IS INTENDED BOTH TO PROVIDE AN OVERVIEW OF CONNECTIONIST IDEAS FOR CONTROL THEORISTS AND TO PROVIDE CONNECTIONIST RESEARCHERS WITH AN INTRODUCTION TO CERTAIN ISSUES IN CONTROL. THE PERSPECTIVE TAKEN EMPHASIZES THE CONTINUITY OF THE CURRENT CONNECTIONIST RESEARCH WITH MORE TRADITIONAL RESEARCH IN CONTROL, SIGNAL PROCESSING, AND PATTERN CLASSIFICATION. CONTROL THEORY IS A WELL-DEVELOPED FIELD WITH A LARGE LITERATURE, AND MANY OF THE LEARNING METHODS BEING DESCRIBED BY CONNECTIONISTS ARE CLOSELY RELATED TO METHODS THAT ALREADY HAVE BEEN INTENSIVELY STUDIED BY ADAPTIVE CONTROL THEORISTS. ON THE OTHER HAND, THE DIRECTIONS THAT CONNECTIONISTS ARE TAKING THESE METHODS HAVE CHARACTERISTICS THAT ARE ABSENT IN THE TRADITIONAL ENGINEERING APPROACHES. THIS REPORT DESCRIBES THESE CHARACTERISTICS AND DISCUSSES THEIR POSITIVE AND NEGATIVE ASPECTS. IT IS ARGUED THAT CONNECTIONISTS APPROACHES TO CONTROL ARE SPECIAL CASES OF MEMORY--INTENTIVE APPROACHES, PROVIDED A SUFFICIENTLY GENERALIZED VIEW OF MEMORY IS ADOPTED. BECAUSE ADAPTIVE CONNECTIONIST NETWORKS CAN COVER THE RANGE BETWEEN STRUCTURELESS LOOKUP TABLES AND HIGHLY CONSTRAINED MODEL-BASED PARAMETER ESTIMATION, THEY SEEM WELL-SUITED FOR THE ACQUISITION AND STORAGE OF CONTROL INFORMATION. AD"
            },
            "slug": "Connectionist-learning-for-control:-an-overview-Barto",
            "title": {
                "fragments": [],
                "text": "Connectionist learning for control: an overview"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that CONNECTIONISTS APPROACHes to control are special cases of MEMORY--intentive approach, and a SUFFICIENTLY GENERALIZed view of memory is adopted."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69563214"
                        ],
                        "name": "J. Clouse",
                        "slug": "J.-Clouse",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Clouse",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Clouse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195972"
                        ],
                        "name": "P. Utgoff",
                        "slug": "P.-Utgoff",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Utgoff",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Utgoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43766241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3da3e6a6c76868e0178adf149ef96d4e99a620d",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Teaching-Method-for-Reinforcement-Learning-Clouse-Utgoff",
            "title": {
                "fragments": [],
                "text": "A Teaching Method for Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026553"
                        ],
                        "name": "O. Selfridge",
                        "slug": "O.-Selfridge",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Selfridge",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Selfridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5776535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f926f229755a617630ff241789bb4ef09f9209c",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the use of learning schemes in training and adapting performance on simple coordination tasks. The tasks are 1-D pole balancing. Several programs incorporating learning have already achieved this (1, S, 8): the problem is to move a cart along a short piece of track to at to keep a pole balanced on its end; the pole is hinged to the cart at its bottom, and the cart is moved either to the left or to the right by a force of constant magnitude. The form of the task considered here, after (3), involves a genuinely difficult credit-assignment problem. We use a learning scheme previously developed and analysed (1, 7) to achieve performance through reinforcement, and extend it to include changing and new requirements. For example, the length or mast of the pole can change, the bias of the force, its strength, and so on; and the system can be tasked to avoid certain regions altogether. In this way we explore the learning system's ability to adapt to changes and to profit from a selected training sequence, both of which are of obvious utility in practical robotics applications. \n \nThe results described here were obtained using a computer simulation of the pole-balancing problem. A movie will be shown of the performance of the system under the various requirements and tasks."
            },
            "slug": "Training-and-Tracking-in-Robotics-Selfridge-Sutton",
            "title": {
                "fragments": [],
                "text": "Training and Tracking in Robotics"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The learning system's ability to adapt to changes and to profit from a selected training sequence are explored, both of which are of obvious utility in practical robotics applications."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166760"
                        ],
                        "name": "D. Baras",
                        "slug": "D.-Baras",
                        "structuredName": {
                            "firstName": "Dorit",
                            "lastName": "Baras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Baras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766683"
                        ],
                        "name": "R. Meir",
                        "slug": "R.-Meir",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Meir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Meir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 425,
                                "start": 129
                            }
                        ],
                        "text": "Publications include the following (chronologically and alphabetically): Bartlett and Baxter (1999, 2000), Xie and Seung (2004), Baras and Meir (2007), Farries and Fairhall (2007), Florian (2007), Izhikevich (2007), Pecevski, Maass, and Legenstein (2007), Legenstein, Pecevski, and Maass (2008), Kolodziejski, Porr, and W\u00f6rg\u00f6tter (2009), Urbanczik and Senn (2009), and Vasilaki, Fr\u00e9maux, Urbanczik, Senn, and Gerstner (2009). Now\u00e9, Vrancx, and De Hauwere (2012) review more recent developments in the wider field of multi-agent reinforcement learning"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 129
                            }
                        ],
                        "text": "Publications include the following (chronologically and alphabetically): Bartlett and Baxter (1999, 2000), Xie and Seung (2004), Baras and Meir (2007), Farries and Fairhall (2007), Florian (2007), Izhikevich (2007), Pecevski, Maass, and Legenstein (2007), Legenstein, Pecevski, and Maass (2008), Kolodziejski, Porr, and W\u00f6rg\u00f6tter (2009), Urbanczik and Senn (2009), and Vasilaki, Fr\u00e9maux, Urbanczik, Senn, and Gerstner (2009)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 40872097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee1ed2df6f6ab79446e1827ec57e84461cd13f86",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning agents, whether natural or artificial, must update their internal parameters in order to improve their behavior over time. In reinforcement learning, this plasticity is influenced by an environmental signal, termed a reward, that directs the changes in appropriate directions. We apply a recently introduced policy learning algorithm from machine learning to networks of spiking neurons and derive a spike-time-dependent plasticity rule that ensures convergence to a local optimum of the expected average reward. The approach is applicable to a broad class of neuronal models, including the Hodgkin-Huxley model. We demonstrate the effectiveness of the derived rule in several toy problems. Finally, through statistical analysis, we show that the synaptic plasticity rule established is closely related to the widely used BCM rule, for which good biological evidence exists."
            },
            "slug": "Reinforcement-Learning,-Spike-Time-Dependent-and-Baras-Meir",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning, Spike-Time-Dependent Plasticity, and the BCM Rule"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A recently introduced policy learning algorithm from machine learning is applied to networks of spiking neurons and derived a spike-time-dependent plasticity rule that ensures convergence to a local optimum of the expected average reward."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729906"
                        ],
                        "name": "Prasad Tadepalli",
                        "slug": "Prasad-Tadepalli",
                        "structuredName": {
                            "firstName": "Prasad",
                            "lastName": "Tadepalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasad Tadepalli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1811287"
                        ],
                        "name": "DoKyeong Ok",
                        "slug": "DoKyeong-Ok",
                        "structuredName": {
                            "firstName": "DoKyeong",
                            "lastName": "Ok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "DoKyeong Ok"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 827249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3f149ba4518679bbf2a99acd7947e929f00222d",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Almost all the work in Average-reward Reinforcement Learning (ARL) so far has fo-cused on table-based methods which do not scale to domains with large state spaces. In this paper, we propose two extensions to a model-based ARL method called H-learning to address the scale-up problem. We extend H-learning to learn action models and reward functions in the form of Bayesian networks, and approximate its value function using local linear regression. We test our algorithms on several scheduling tasks for a simulated Automatic Guided Vehicle (AGV) and show that they are eeective in signiicantly reducing the space requirement of H-learning and making it converge faster. To the best of our knowledge, our results are the rst in applying function approximation to ARL."
            },
            "slug": "Scaling-Up-Average-Reward-Reinforcement-Learning-by-Tadepalli-Ok",
            "title": {
                "fragments": [],
                "text": "Scaling Up Average Reward Reinforcement Learning by Approximating the Domain Models and the Value Function"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes two extensions to a model-based ARL method called H-learning, which is extended to learn action models and reward functions in the form of Bayesian networks, and approximate its value function using local linear regression."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559845"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 86
                            }
                        ],
                        "text": "The term associative reinforcement learning has also been used for associative search (Barto and Anandan, 1985), but we prefer to reserve that term as a synonym for the full reinforcement learning problem (as in Sutton, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 138
                            }
                        ],
                        "text": "that we and colleagues accomplished was directed toward showing that reinforcement learning and supervised learning were indeed different (Barto, Sutton, and Brouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5915714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0120eefaf05bfad5293e87f56d2e787c05f78cf7",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A class of learning tasks is described that combines aspects of learning automation tasks and supervised learning pattern-classification tasks. These tasks are called associative reinforcement learning tasks. An algorithm is presented, called the associative reward-penalty, or AR-P algorithm for which a form of optimal performance is proved. This algorithm simultaneously generalizes a class of stochastic learning automata and a class of supervised learning pattern-classification methods related to the Robbins-Monro stochastic approximation procedure. The relevance of this hybrid algorithm is discussed with respect to the collective behaviour of learning automata and the behaviour of networks of pattern-classifying adaptive elements. Simulation results are presented that illustrate the associative reinforcement learning task and the performance of the AR-P algorithm as compared with that of several existing algorithms."
            },
            "slug": "Pattern-recognizing-stochastic-learning-automata-Barto-Anandan",
            "title": {
                "fragments": [],
                "text": "Pattern-recognizing stochastic learning automata"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A class of learning tasks is described that combines aspects of learning automation tasks and supervised learning pattern-classification tasks, called associative reinforcement learning tasks, and an algorithm is presented, called the associative reward-penalty, or AR-P algorithm, for which a form of optimal performance is proved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37228807"
                        ],
                        "name": "Lex Weaver",
                        "slug": "Lex-Weaver",
                        "structuredName": {
                            "firstName": "Lex",
                            "lastName": "Weaver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lex Weaver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14284564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a754de170e099eafb72d74e70e7a300846c31902",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP). These algorithms are based on GPOMDP, an algorithm introduced in a companion paper (Baxter & Bartlett, 2001), which computes biased estimates of the performance gradient in POMDPs. The algorithm's chief advantages are that it uses only one free parameter \u03b2 \u2208 [0, 1], which has a natural interpretation in terms of bias-variance trade-off, it requires no knowledge of the underlying state, and it can be applied to infinite state, control and observation spaces. We show how the gradient estimates produced by GPOMDP can be used to perform gradient ascent, both with a traditional stochastic-gradient algorithm, and with an algorithm based on conjugate-gradients that utilizes gradient information to bracket maxima in line searches. Experimental results are presented illustrating both the theoretical results of Baxter and Bartlett (2001) on a toy problem, and practical aspects of the algorithms on a number of more realistic problems."
            },
            "slug": "Experiments-with-Infinite-Horizon,-Policy-Gradient-Baxter-Bartlett",
            "title": {
                "fragments": [],
                "text": "Experiments with Infinite-Horizon, Policy-Gradient Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP) based on GPOMDP, an algorithm introduced in a companion paper (Baxter & Bartlett, 2001), which computes biased estimates of the performance gradient in POMDPs."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784072"
                        ],
                        "name": "M. Lagoudakis",
                        "slug": "M.-Lagoudakis",
                        "structuredName": {
                            "firstName": "Michail",
                            "lastName": "Lagoudakis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lagoudakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145726861"
                        ],
                        "name": "Ronald E. Parr",
                        "slug": "Ronald-E.-Parr",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Parr",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald E. Parr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 3
                            }
                        ],
                        "text": "Williams (1988) described several ways that backpropagation and reinforcement learning can be combined for training artificial neural networks."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3226593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b750a17921d32936425e05f8b00b96569e2fc5a6",
            "isKey": false,
            "numCitedBy": 1316,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new approach to reinforcement learning for control problems which combines value-function approximation with linear architectures and approximate policy iteration. This new approach is motivated by the least-squares temporal-difference learning algorithm (LSTD) for prediction problems, which is known for its efficient use of sample experiences compared to pure temporal-difference algorithms. Heretofore, LSTD has not had a straightforward application to control problems mainly because LSTD learns the state value function of a fixed policy which cannot be used for action selection and control without a model of the underlying process. Our new algorithm, least-squares policy iteration (LSPI), learns the state-action value function which allows for action selection without a model and for incremental policy improvement within a policy-iteration framework. LSPI is a model-free, off-policy method which can use efficiently (and reuse in each iteration) sample experiences collected in any manner. By separating the sample collection method, the choice of the linear approximation architecture, and the solution method, LSPI allows for focused attention on the distinct elements that contribute to practical reinforcement learning. LSPI is tested on the simple task of balancing an inverted pendulum and the harder task of balancing and riding a bicycle to a target location. In both cases, LSPI learns to control the pendulum or the bicycle by merely observing a relatively small number of trials where actions are selected randomly. LSPI is also compared against Q-learning (both with and without experience replay) using the same value function architecture. While LSPI achieves good performance fairly consistently on the difficult bicycle task, Q-learning variants were rarely able to balance for more than a small fraction of the time needed to reach the target location."
            },
            "slug": "Least-Squares-Policy-Iteration-Lagoudakis-Parr",
            "title": {
                "fragments": [],
                "text": "Least-Squares Policy Iteration"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The new algorithm, least-squares policy iteration (LSPI), learns the state-action value function which allows for action selection without a model and for incremental policy improvement within a policy-iteration framework."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40423197"
                        ],
                        "name": "R. Suri",
                        "slug": "R.-Suri",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Suri",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Suri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 62
                            }
                        ],
                        "text": "1\u20132 Most of the specific material from these sections is from Sutton (1988), including the TD(0) algorithm, the random walk example, and the term \u201ctemporal-difference learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 43
                            }
                        ],
                        "text": "3 The deadly triad was first identified by Sutton (1995b) and thoroughly analyzed by Tsitsiklis and Van Roy (1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206924776,
            "fieldsOfStudy": [
                "Psychology",
                "Biology",
                "Computer Science"
            ],
            "id": "2fc73ec66c8e2428b185e60da5c49b842be5283c",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\u2002Dopamine neurons appear to code an error in the prediction of reward. They are activated by unpredicted rewards, are not influenced by predicted rewards, and are depressed when a predicted reward is omitted. After conditioning, they respond to reward-predicting stimuli in a similar manner. With these characteristics, the dopamine response strongly resembles the predictive reinforcement teaching signal of neural network models implementing the temporal difference learning algorithm. This study explored a neural network model that used a reward-prediction error signal strongly resembling dopamine responses for learning movement sequences. A different stimulus was presented in each step of the sequence and required a different movement reaction, and reward occurred at the end of the correctly performed sequence. The dopamine-like predictive reinforcement signal efficiently allowed the model to learn long sequences. By contrast, learning with an unconditional reinforcement signal required synaptic eligibility traces of longer and biologically less-plausible durations for obtaining satisfactory performance. Thus, dopamine-like neuronal signals constitute excellent teaching signals for learning sequential behavior."
            },
            "slug": "Learning-of-sequential-movements-by-neural-network-Suri-Schultz",
            "title": {
                "fragments": [],
                "text": "Learning of sequential movements by neural network model with dopamine-like reinforcement signal"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This study explored a neural network model that used a reward-prediction error signal strongly resembling dopamine responses for learning movement sequences and efficiently allowed the model to learn long sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Experimental Brain Research"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40423197"
                        ],
                        "name": "R. Suri",
                        "slug": "R.-Suri",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Suri",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Suri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 62
                            }
                        ],
                        "text": "1\u20132 Most of the specific material from these sections is from Sutton (1988), including the TD(0) algorithm, the random walk example, and the term \u201ctemporaldifference learning."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15874307,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "22dcdc9853f6405ff65ed79fd28f24a09bc6f8cb",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-neural-network-model-with-dopamine-like-signal-a-Suri-Schultz",
            "title": {
                "fragments": [],
                "text": "A neural network model with dopamine-like reinforcement signal that learns a spatial delayed response task"
            },
            "venue": {
                "fragments": [],
                "text": "Neuroscience"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103477787"
                        ],
                        "name": "M. Waltz",
                        "slug": "M.-Waltz",
                        "structuredName": {
                            "firstName": "Marion",
                            "lastName": "Waltz",
                            "middleNames": [
                                "Dwain"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Waltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97306284"
                        ],
                        "name": "K. Fu",
                        "slug": "K.-Fu",
                        "structuredName": {
                            "firstName": "King-Sun",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62489576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28c567f9633871a8aeece63209263ceb1c7a5738",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a learning control system using a reinforcement technique. The controller is capable of controlling a plant that may be nonlinear and nonstationary. The only a priori information required by the controller is the order of the plant. The approach is to design a controller which partitions the control measurement space into sets called control situations and then learns the best control choice for each control situation. The control measurements are those indicating the state of the plant and environment. The learning is accomplished by reinforcement of the probability of choosing a particular control choice for a given control situation. The system was stimulated on an IBM 1710-GEDA hybrid computer facility. Experimental results obtained from the simulation are presented."
            },
            "slug": "A-heuristic-approach-to-reinforcement-learning-Waltz-Fu",
            "title": {
                "fragments": [],
                "text": "A heuristic approach to reinforcement learning control systems"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A learning control system using a reinforcement technique that is capable of controlling a plant that may be nonlinear and nonstationary and which learns the best control choice for each control situation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "Williams and Baird (1990) presented DP algorithms that are asynchronous at a finer grain than the ones we have discussed: the backup operations themselves are broken into steps that can be performed asynchronously."
                    },
                    "intents": []
                }
            ],
            "corpusId": 621595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f518bffb712a298bff18248c67f6fc0181018ae6",
            "isKey": false,
            "numCitedBy": 1063,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Residual-Algorithms:-Reinforcement-Learning-with-Baird",
            "title": {
                "fragments": [],
                "text": "Residual Algorithms: Reinforcement Learning with Function Approximation"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11257350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "beef9a0f27e4ca54eb5c5311f6ac90d90fa88f12",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a series of formal and empirical results comparing the e\u2010ciency of various policy-gradient methods|methods for reinforcement learning that directly update a parameterized policy according to an approximation of the gradient of performance with respect to the policy parameter. Such methods have recently become of interest as an alternative to value-function-based methods because of superior convergence guarantees, ability to flnd stochastic policies, and ability to handle large and continuous action spaces. Our results include: 1) formal and empirical demonstrations that a policy-gradient method suggested by Sutton et al. (2000) and Konda and Tsitsiklis (2000) is no better than REINFORCE, 2) derivation of the optimal baseline for policy-gradient methods, which difiers from the widely used V \u2026 (s) previously thought to be optimal, 3) introduction of a new all-action policy-gradient algorithm that is unbiased and requires no baseline, and demonstrating empirically and semi-formally that it is more e\u2010cient than the methods mentioned above, and 4) an overall comparison of methods on the mountain-car problem including value-function-based methods and bootstrapping actor-critic methods. One general conclusion we draw is that the bias of conventional value functions is a feature, not a bug; it seems required is order for the value function to signiflcantly accelerate learning."
            },
            "slug": "Comparing-Policy-Gradient-Algorithms-Sutton-Singh",
            "title": {
                "fragments": [],
                "text": "Comparing Policy-Gradient Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A series of formal and empirical results comparing the e\u2010ciency of various policy-gradient methods for reinforcement learning that directly update a parameterized policy according to an approximation of the gradient of performance with respect to the policy parameter draw a general conclusion that the bias of conventional value functions is a feature not a bug."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4498650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18f4e6b5b5b226a47e4aed700ea8b9ae6f121308",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Reinforcement learning is often done using parameterized function approximators to store value functions. Algorithms are typically developed for lookup tables, and then applied to function approximators by using backpropagation. This can lead to algorithms diverging on very small, simple MDPs and Markov chains, even with linear function approximators and epoch wise training. These algorithms are also very difficult to analyze, and difficult to combine with other algorithms. A series of new families of algorithms are derived based on stochastic gradient descent. Since they are derived from first principles with function approximators in mind, they have guaranteed convergence to local minima, even on general nonlinear function approximators. For both residual algorithms and VAPS algorithms, it is possible to take any of the standard algorithms in the field, such as Q learning or SARSA or value iteration, and rederive a new form of it with provable convergence. In addition to better convergence properties, it is shown how gradient descent allows an inelegant, inconvenient algorithm like Advantage updating to be converted into a much simpler and more easily analyzed algorithm like Advantage learning. hi this case that is very useful, since Advantages can be learned thousands of times faster than Q values for continuous time problems. In this case, there are significant practical benefits of using gradient descent based techniques. In addition to improving both the theory and practice of existing types of algorithms, the gradient descent approach makes it possible to create entirely new classes of reinforcement learning algorithms. VAPS algorithms can be derived that ignore values altogether, and simply learn good policies directly. One hallmark of gradient descent is the ease with which different algorithms can be combined, and this is a prime example."
            },
            "slug": "Reinforcement-Learning-Through-Gradient-Descent-Baird",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning Through Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In addition to better convergence properties, it is shown how gradient descent allows an inelegant, inconvenient algorithm like Advantage updating to be converted into a much simpler and more easily analyzed algorithmlike Advantage learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40868287"
                        ],
                        "name": "Csaba Szepesvari",
                        "slug": "Csaba-Szepesvari",
                        "structuredName": {
                            "firstName": "Csaba",
                            "lastName": "Szepesvari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Csaba Szepesvari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1212267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "712ec1bd9287ac210a7630ce03ca2b0930ebd351",
            "isKey": false,
            "numCitedBy": 602,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "An important application of reinforcement learning (RL) is to finite-state control problems and one of the most difficult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot separate exploration from learning and therefore must confront the exploration problem directly. We prove convergence results for several related on-policy algorithms with both decaying exploration and persistent exploration. We also provide examples of exploration strategies that can be followed during learning that result in convergence to both optimal values and optimal policies."
            },
            "slug": "Convergence-Results-for-Single-Step-On-Policy-Singh-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper examines the convergence of single-step on-policy RL algorithms for control with both decaying exploration and persistent exploration and provides examples of exploration strategies that result in convergence to both optimal values and optimal policies."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847175"
                        ],
                        "name": "M. Minsky",
                        "slug": "M.-Minsky",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Minsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Minsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 83
                            }
                        ],
                        "text": "Particularly influential was Minsky\u2019s paper \u201cSteps Toward Artificial Intelligence\u201d (Minsky, 1961), which discussed several issues relevant to trialand-error learning, including prediction, expectation, and what he called the basic credit-assignment problem for complex reinforcement learning systems: How do you"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14250548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b",
            "isKey": false,
            "numCitedBy": 1320,
            "numCiting": 142,
            "paperAbstract": {
                "fragments": [],
                "text": "The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date."
            },
            "slug": "Steps-toward-Artificial-Intelligence-Minsky",
            "title": {
                "fragments": [],
                "text": "Steps toward Artificial Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IRE"
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913418"
                        ],
                        "name": "B. Widrow",
                        "slug": "B.-Widrow",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Widrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Widrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115344719"
                        ],
                        "name": "Narendra K. Gupta",
                        "slug": "Narendra-K.-Gupta",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Gupta",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Narendra K. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34601642"
                        ],
                        "name": "S. Maitra",
                        "slug": "S.-Maitra",
                        "structuredName": {
                            "firstName": "Sidhartha",
                            "lastName": "Maitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maitra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1487792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6587a531da06b9cef73e93b6b7627e466ad51d1b",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive threshold element is able to \"learn\" a strategy of play for the game blackjack (twenty-one) with a performance close to that of the Thorp optimal strategy although the adaptive system has no prior knowledge of the game and of the objective of play. After each winning game the decisions of the adaptive system are \"rewarded.\" After each losing game the decisions are \"punished.\" Reward is accomplished by adapting while accepting the actual decision as the desired response. Punishment is accomplished by adapting while taking the desired response to be the opposite of that of the actual decision. This learning scheme is unlike \"learning with a teacher\" and unlike \"unsupervised learning.\" It involves \"bootstrap adaptation\" or \"learning with a critic.\" The critic rewards decisions which are members of successful chains of decisions and punishes other decisions. A general analytical model for learning with a critic is formulated and analyzed. The model represents bootstrap learning per se. Although the hypotheses on which the model is based do not perfectly fit blackjack learning, it is applied heuristically to predict adaptation rates with good experimental success. New applications are being explored for bootstrap learning in adaptive controls and multilayered adaptive systems."
            },
            "slug": "Punish/Reward:-Learning-with-a-Critic-in-Adaptive-Widrow-Gupta",
            "title": {
                "fragments": [],
                "text": "Punish/Reward: Learning with a Critic in Adaptive Threshold Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An adaptive threshold element is able to \"learn\" a strategy of play for the game blackjack (twenty-one) with a performance close to that of the Thorp optimal strategy although the adaptive system has no prior knowledge of the game and of the objective of play."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624386"
                        ],
                        "name": "L. Chrisman",
                        "slug": "L.-Chrisman",
                        "structuredName": {
                            "firstName": "Lonnie",
                            "lastName": "Chrisman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chrisman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1963904,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a1b055577a86141df13f13a3203c76a32bffdc3a",
            "isKey": false,
            "numCitedBy": 395,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "It is known that Perceptual Aliasing may significantly diminish the effectiveness of reinforcement learning algorithms [Whitehead and Ballard, 1991]. Perceptual aliasing occurs when multiple situations that are indistinguishable from immediate perceptual input require different responses from the system. For example, if a robot can only see forward, yet the presence of a battery charger behind it determines whether or not it should backup, immediate perception alone is insufficient for determining the most appropriate action. It is problematic since reinforcement algorithms typically learn a control policy from immediate perceptual input to the optimal choice of action. \n \nThis paper introduces the predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world. An additional component, a predictive model, is utilized to track aspects of the world that may not be visible at all times. In addition to the control policy, the model must also be learned, and to allow for stochastic actions and noisy perception, a probabilistic model is learned from experience. In the process, the system must discover, on its own, the important distinctions in the world. Experimental results are given for a simple simulated domain, and additional issues are discussed."
            },
            "slug": "Reinforcement-Learning-with-Perceptual-Aliasing:-Chrisman",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world is introduced and Experimental results are given for a simple simulated domain, and additional issues are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3054729"
                        ],
                        "name": "Evan Greensmith",
                        "slug": "Evan-Greensmith",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Greensmith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Greensmith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5259564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1187a77f857ad029168863ba0005ddf6d2b957c8",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function."
            },
            "slug": "Variance-Reduction-Techniques-for-Gradient-in-Greensmith-Bartlett",
            "title": {
                "fragments": [],
                "text": "Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper considers variance reduction methods that were developed for Monte Carlo estimates of integrals, and gives bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2801572,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "1678bd32846b1aded5b1e80a617170812e80f562",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. \n \nWe illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map."
            },
            "slug": "Feudal-Reinforcement-Learning-Dayan-Hinton",
            "title": {
                "fragments": [],
                "text": "Feudal Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows how to create a Q-learning managerial hierarchy in which high level managers learning how to set tasks to their submanagers who, in turn, learn how to satisfy them."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924970"
                        ],
                        "name": "H. Seung",
                        "slug": "H.-Seung",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Seung",
                            "middleNames": [
                                "Sebastian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11534967,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9ce3519e91e28a47602be0ddbadcab6d0acb83dd",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-in-Spiking-Neural-Networks-by-of-Synaptic-Seung",
            "title": {
                "fragments": [],
                "text": "Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12561523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84cdfa79e6eb9bf9e625e3af38d9f968df18a880",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since the usual approaches to cooperative computation in networks of neuron-like computating elements do not assume that network components have any \"preferences\", they do not make substantive contact with game theoretic concepts, despite their use of some of the same terminology. In the approach presented here, however, each network component, or adaptive element, is a self-interested agent that prefers some inputs over others and \"works\" toward obtaining the most highly preferred inputs. Here we describe an adaptive element that is robust enough to learn to cooperate with other elements like itself in order to further its self-interests. It is argued that some of the longstanding problems concerning adaptation and learning by networks might be solvable by this form of cooperativity, and computer simulation experiments are described that show how networks of self-interested components that are sufficiently robust can solve rather difficult learning problems. We then place the approach in its proper historical and theoretical perspective through comparison with a number of related algorithms. A secondary aim of this article is to suggest that beyond what is explicitly illustrated here, there is a wealth of ideas from game theory and allied disciplines such as mathematical economics that can be of use in thinking about cooperative computation in both nervous systems and man-made systems."
            },
            "slug": "Learning-by-statistical-cooperation-of-neuron-like-Barto",
            "title": {
                "fragments": [],
                "text": "Learning by statistical cooperation of self-interested neuron-like computing elements."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that some of the longstanding problems concerning adaptation and learning by networks might be solvable by this form of cooperativity, and computer simulation experiments are described that show how networks of self-interested components that are sufficiently robust can solve rather difficult learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "Human neurobiology"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368601"
                        ],
                        "name": "Doina Precup",
                        "slug": "Doina-Precup",
                        "structuredName": {
                            "firstName": "Doina",
                            "lastName": "Precup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doina Precup"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1153355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78020db7e3d968f6e6cc26d18e31e5b668ca7fee",
            "isKey": false,
            "numCitedBy": 573,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Eligibility traces have been shown to speed reinforcement learning, to make it more robust to hidden states, and to provide a link between Monte Carlo and temporal-difference methods. Here we generalize eligibility traces to off-policy learning, in which one learns about a policy different from the policy that generates the data. Off-policy methods can greatly multiply learning, as many policies can be learned about from the same data stream, and have been identified as particularly useful for learning about subgoals and temporally extended macro-actions. In this paper we consider the off-policy version of the policy evaluation problem, for which only one eligibility trace algorithm is known, a Monte Carlo method. We analyze and compare this and four new eligibility trace algorithms, emphasizing their relationships to the classical statistical technique known as importance sampling. Our main results are 1) to establish the consistency and bias properties of the new methods and 2) to empirically rank the new methods, showing improvement over one-step and Monte Carlo methods. Our results are restricted to model-free, table-lookup methods and to offline updating (at the end of each episode) although several of the algorithms could be applied more generally."
            },
            "slug": "Eligibility-Traces-for-Off-Policy-Policy-Evaluation-Precup-Sutton",
            "title": {
                "fragments": [],
                "text": "Eligibility Traces for Off-Policy Policy Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper considers the off-policy version of the policy evaluation problem, for which only one eligibility trace algorithm is known, a Monte Carlo method, and analyzes and compares this and four new eligibility trace algorithms, emphasizing their relationships to the classical statistical technique known as importance sampling."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 207162288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "831edc3d67457db83da40d260e93bfd7559347ae",
            "isKey": false,
            "numCitedBy": 676,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures."
            },
            "slug": "Dyna,-an-integrated-architecture-for-learning,-and-Sutton",
            "title": {
                "fragments": [],
                "text": "Dyna, an integrated architecture for learning, planning, and reacting"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Dyna is an AI architecture that integrates learning, planning, and reactive execution that relies on machine learning methods for learning from examples, yet is not tied to any particular method."
            },
            "venue": {
                "fragments": [],
                "text": "SGAR"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819261"
                        ],
                        "name": "S. Bradtke",
                        "slug": "S.-Bradtke",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bradtke",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bradtke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47191491"
                        ],
                        "name": "M. Duff",
                        "slug": "M.-Duff",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Duff",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Duff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17149277,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "96a25df486c7dfa475a93a0ca31d0418f79a8771",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-Markov Decision Problems are continuous time generalizations of discrete time Markov Decision Problems. A number of reinforcement learning algorithms have been developed recently for the solution of Markov Decision Problems, based on the ideas of asynchronous dynamic programming and stochastic approximation. Among these are TD(\u03bb), Q-learning, and Real-time Dynamic Programming. After reviewing semi-Markov Decision Problems and Bellman's optimality equation in that context, we propose algorithms similar to those named above, adapted to the solution of semi-Markov Decision Problems. We demonstrate these algorithms by applying them to the problem of determining the optimal control for a simple queueing system. We conclude with a discussion of circumstances under which these algorithms may be usefully applied."
            },
            "slug": "Reinforcement-Learning-Methods-for-Continuous-Time-Bradtke-Duff",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work proposes algorithms similar to those named above, adapted to the solution of semi-Markov Decision Problems, and demonstrates these algorithms by applying them to the problem of determining the optimal control for a simple queueing system."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18060048,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
            "isKey": false,
            "numCitedBy": 370,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a framework for`curious neural controllers' which employ an adaptive world model for goal directed on-line learning. First an on-line reinforcement learning algorithm for autonomous\u00e0nimats' is described. The algorithm is based on two fully recurrent`self-supervised' continually running networks which learn in parallel. One of the networks learns to represent a complete model of the environmental dynamics and is called th\u00e8model network'. It provides complet\u00e8credit assignment paths' into the past for the second network which controls the animats physical actions in a possibly reactive environment. The an-imats goal is to maximize cumulative reinforcement and minimize cumulativ\u00e8pain'. The algorithm has properties which allow to implement something like the desire to improve the model network's knowledge about the world. This is related to curiosity. It is described how the particular algorithm (as well as similar model-building algorithms) may be augmented by dynamic curiosity and boredom in a natural manner. This may be done by introducing (delayed) reinforcement for actions that increase the model network's knowledge about the world. This in turn requires the model network to model its own ignorance, thus showing a rudimentary form of self-introspective behavior."
            },
            "slug": "A-possibility-for-implementing-curiosity-and-in-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "A possibility for implementing curiosity and boredom in model-building neural controllers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is described how the particular algorithm (as well as similar model-building algorithms) may be augmented by dynamic curiosity and boredom in a natural manner by introducing (delayed) reinforcement for actions that increase the model network's knowledge about the world."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368601"
                        ],
                        "name": "Doina Precup",
                        "slug": "Doina-Precup",
                        "structuredName": {
                            "firstName": "Doina",
                            "lastName": "Precup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doina Precup"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880237"
                        ],
                        "name": "S. Dasgupta",
                        "slug": "S.-Dasgupta",
                        "structuredName": {
                            "firstName": "Sanjoy",
                            "lastName": "Dasgupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dasgupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7784749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e54152403da98a0403afef8477d42383d606e1f9",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the first algorithm for off-policy temporal-difference learning that is stable with linear function approximation. Off-policy learning is of interest because it forms the basis for popular reinforcement learning methods such as Q-learning, which has been known to diverge with linear function approximation, and because it is critical to the practical utility of multi-scale, multi-goal, learning frameworks such as options, HAMs, and MAXQ. Our new algorithm combines TD(\u03bb) over state\u2013action pairs with importance sampling ideas from our previous work. We prove that, given training under any -soft policy, the algorithm converges w.p.1 to a close approximation (as in Tsitsiklis and Van Roy, 1997; Tadic, 2001) to the action-value function for an arbitrary target policy. Variations of the algorithm designed to reduce variance introduce additional bias but are also guaranteed convergent. We also illustrate our method empirically on a small policy evaluation problem. Our current results are limited to episodic tasks with episodes of bounded length. 1Although Q-learning remains the most popular of all reinforcement learning algorithms, it has been known since about 1996 that it is unsound with linear function approximation (see Gordon, 1995; Bertsekas and Tsitsiklis, 1996). The most telling counterexample, due to Baird (1995) is a seven-state Markov decision process with linearly independent feature vectors, for which an exact solution exists, yet This is a re-typeset version of an article published in the Proceedings of the 18th International Conference on Machine Learning (2001). It differs from the original in line and page breaks, is crisper for electronic viewing, and has this funny footnote, but otherwise it is identical to the published article. for which the approximate values found by Q-learning diverge to infinity. This problem prompted the development of residual gradient methods (Baird, 1995), which are stable but much slower than Q-learning, and fitted value iteration (Gordon, 1995, 1999), which is also stable but limited to restricted, weaker-than-linear function approximators. Of course, Q-learning has been used with linear function approximation since its invention (Watkins, 1989), often with good results, but the soundness of this approach is no longer an open question. There exist non-pathological Markov decision processes for which it diverges; it is absolutely unsound in this sense. A sensible response is to turn to some of the other reinforcement learning methods, such as Sarsa, that are also efficient and for which soundness remains a possibility. An important distinction here is between methods that must follow the policy they are learning about, called on-policy methods, and those that can learn from behavior generated by a different policy, called off-policy methods. Q-learning is an off-policy method in that it learns the optimal policy even when actions are selected according to a more exploratory or even random policy. Q-learning requires only that all actions be tried in all states, whereas on-policy methods like Sarsa require that they be selected with specific probabilities. Although the off-policy capability of Q-learning is appealing, it is also the source of at least part of its instability problems. For example, in one version of Baird\u2019s counterexample, the TD(\u03bb) algorithm, which underlies both Qlearning and Sarsa, is applied with linear function approximation to learn the action-value function Q for a given policy \u03c0. Operating in an on-policy mode, updating state\u2013 action pairs according to the same distribution they would be experienced under \u03c0, this method is stable and convergent near the best possible solution (Tsitsiklis and Van Roy, 1997; Tadic, 2001). However, if state-action pairs are updated according to a different distribution, say that generated by following the greedy policy, then the estimated values again diverge to infinity. This and related counterexamples suggest that at least some of the reason for the instability of Q-learning is that it is an off-policy method; they also make it clear that this part of the problem can be studied in a purely policy-evaluation context. Despite these problems, there remains substantial reason for interest in off-policy learning methods. Several researchers have argued for an ambitious extension of reinforcement learning ideas into modular, multi-scale, and hierarchical architectures (Sutton, Precup & Singh, 1999; Parr, 1998; Parr & Russell, 1998; Dietterich, 2000). These architectures rely on off-policy learning to learn about multiple subgoals and multiple ways of behaving from the singular stream of experience. For these approaches to be feasible, some efficient way of combining off-policy learning and function approximation must be found. Because the problems with current off-policy methods become apparent in a policy evaluation setting, it is there that we focus in this paper. In previous work we considered multi-step off-policy policy evaluation in the tabular case. In this paper we introduce the first off-policy policy evaluation method consistent with linear function approximation. Our mathematical development focuses on the episodic case, and in fact on a single episode. Given a starting state and action, we show that the expected offpolicy update under our algorithm is the same as the expected on-policy update under conventional TD(\u03bb). This, together with some variance conditions, allows us to prove convergence and bounds on the error in the asymptotic approximation identical to those obtained by Tsitsiklis and Van Roy (1997; Bertsekas and Tsitsiklis, 1996). 1. Notation and Main Result We consider the standard episodic reinforcement learning framework (see, e.g., Sutton & Barto, 1998) in which a learning agent interacts with a Markov decision process (MDP). Our notation focuses on a single episode of T time steps, s0, a0, r1, s1, a1, r2, . . . , rT , sT , with states st \u2208 S, actions at \u2208 A, and rewards rt \u2208 <. We take the initial state and action, s0 and a0, to be given arbitrarily. Given a state and action, st and at, the next reward, rt+1, is a random variable with mean rt st and the next state, st+1, is chosen with probabilities pt stst+1 . The final state is a special terminal state that may not occur on any preceding time step. Given a state, st, 0 < t < T , the action at is selected according to probability \u03c0(st, at) or b(st, at) depending on whether policy \u03c0 or policy b is in force. We always use \u03c0 to denote the target policy, the policy that we are learning about. In the on-policy case, \u03c0 is also used to generate the actions of the episode. In the off-policy case, the actions are instead generated by b, which we call the behavior policy. In either case, we seek an approximation to the action-value function Q : S \u00d7A 7\u2192 < for the target policy \u03c0: Q(s, a) = E\u03c0 { rt+1 + \u00b7 \u00b7 \u00b7+ \u03b3rT | st = s, at = a } , where 0 \u2264 \u03b3 \u2264 1 is a discount-rate parameter. We consider approximations that are linear in a set of feature vectors {\u03c6sa}, s \u2208 S, a \u2208 A: Q(s, a) \u2248 \u03b8\u03c6sa = n \u2211"
            },
            "slug": "Off-Policy-Temporal-Difference-Learning-with-Precup-Sutton",
            "title": {
                "fragments": [],
                "text": "Off-Policy Temporal Difference Learning with Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The first algorithm for off-policy temporal-difference learning that is stable with linear function approximation is introduced and it is proved that, given training under any -soft policy, the algorithm converges w.p.1 to a close approximation to the action-value function for an arbitrary target policy."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398582662"
                        ],
                        "name": "J. Contreras-Vidal",
                        "slug": "J.-Contreras-Vidal",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Contreras-Vidal",
                            "middleNames": [
                                "Luis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Contreras-Vidal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 48
                            }
                        ],
                        "text": "11 The central ideas of MCTS were introduced by Coulom (2006) and by Kocsis and Szepesv\u00e1ri (2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12918307,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "5d231d089bc0127ec364c11313aea1bdbc6040c4",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 102,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network model of how dopamine and prefrontal cortex activity guides short- and long-term information processing within the cortico-striatal circuits during reward-related learning of approach behavior is proposed. The model predicts two types of reward-related neuronal responses generated during learning: (1) cell activity signaling errors in the prediction of the expected time of reward delivery and (2) neural activations coding for errors in the prediction of the amount and type of reward or stimulus expectancies. The former type of signal is consistent with the responses of dopaminergic neurons, while the latter signal is consistent with reward expectancy responses reported in the prefrontal cortex. It is shown that a neural network architecture that satisfies the design principles of the adaptive resonance theory of Carpenter and Grossberg (1987) can account for the dopamine responses to novelty, generalization, and discrimination of appetitive and aversive stimuli. These hypotheses are scrutinized via simulations of the model in relation to the delivery of free food outside a task, the timed contingent delivery of appetitive and aversive stimuli, and an asymmetric, instructed delay response task."
            },
            "slug": "A-Predictive-Reinforcement-Model-of-Dopamine-for-Contreras-Vidal-Schultz",
            "title": {
                "fragments": [],
                "text": "A Predictive Reinforcement Model of Dopamine Neurons for Learning Approach Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a neural network architecture that satisfies the design principles of the adaptive resonance theory of Carpenter and Grossberg (1987) can account for the dopamine responses to novelty, generalization, and discrimination of appetitive and aversive stimuli."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computational Neuroscience"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819261"
                        ],
                        "name": "S. Bradtke",
                        "slug": "S.-Bradtke",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bradtke",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bradtke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59680556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adb1612bdcbe9e1c52c126f6058728ebbee8d497",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning algorithms based on the principles of Dynamic Programming (DP) have enjoyed a great deal of recent attention both empirically and theoretically. These algorithms have been referred to generically as Incremental Dynamic Programming (IDP) algorithms. IDP algorithms are intended for use in situations where the information or computational resources needed by traditional dynamic programming algorithms are not available. IDP algorithms attempt to find a global solution to a DP problem by incrementally improving local constraint satisfaction properties as experience is gained through interaction with the environment. This class of algorithms is not new, going back at least as far as Samuel's adaptive checkers-playing programs, but the links to DP have only been noted and understood very recently. \nThis dissertation expands the theoretical and empirical understanding of IDP algorithms and increases their domain of practical application. We address a number of issues concerning the use of IDP algorithms for on-line adaptive optimal control. We present a new algorithm, Real-Time Dynamic Programming, that generalizes Korf's Learning Real-Time A* to a stochastic domain, and show that it has computational advantages over conventional DP approaches to such problems. We then describe several new IDP algorithms based on the theory of Least Squares function approximation. Finally, we begin the extension of IDP theory to continuous domains by considering the problem of Linear Quadratic Regulation. We present an algorithm based on Policy Iteration and Watkins' Q-functions and prove convergence of the algorithm (under the appropriate conditions) to the optimal policy. This is the first result proving convergence of a DP-based reinforcement learning algorithm to the optimal policy for any continuous domain. We also demonstrate that IDP algorithms cannot be applied blindly to problems from continuous domains, even such simple domains as Linear Quadratic Regulation."
            },
            "slug": "Incremental-dynamic-programming-for-on-line-optimal-Bradtke",
            "title": {
                "fragments": [],
                "text": "Incremental dynamic programming for on-line adaptive optimal control"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This dissertation expands the theoretical and empirical understanding of IDP algorithms and increases their domain of practical application, and proves convergence of a DP-based reinforcement learning algorithm to the optimal policy for any continuous domain."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6042780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cca494cd58f483547a4bd059b319a915e5751bc",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD() algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD() and Q-learning belong."
            },
            "slug": "On-the-Convergence-of-Stochastic-Iterative-Dynamic-Jaakkola-Jordan",
            "title": {
                "fragments": [],
                "text": "On the Convergence of Stochastic Iterative Dynamic Programming Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A rigorous proof of convergence of DP-based learning algorithms is provided by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem, which establishes a general class of convergent algorithms to which both TD() and Q-learning belong."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57234353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "003d912198ad1a718a84e430d47f6c513bb4df92",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The behavior of theoretical neural networks is often described in terms of competition and cooperation. I present an approach to network learning that is related to game and team problems in which competition and cooperation have more technical meanings. I briefly describe the application of stochastic learning automata to game and team problems and then present an adaptive element that is a synthesis of aspects of stochastic learning automata and typical neuron\u2010like adaptive elements. These elements act as self\u2010interested agents that work toward improving their performance with respect to their individual preference orderings. Networks of these elements can solve a variety of team decision problems, some of which take the form of layered networks in which the \u2018\u2018hidden units\u2019\u2019 become appropriate functional components as they attempt to improve their own payoffs."
            },
            "slug": "Game-theoretic-cooperativity-in-networks-of-units-Barto",
            "title": {
                "fragments": [],
                "text": "Game-theoretic cooperativity in networks of self-interested units"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An adaptive element is presented that is a synthesis of aspects of stochastic learning automata and typical neuron\u2010like adaptive elements that act as self\u2010interested agents that work toward improving their performance with respect to their individual preference orderings."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21889436"
                        ],
                        "name": "Geoffrey J. Gordon",
                        "slug": "Geoffrey-J.-Gordon",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey J. Gordon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 151
                            }
                        ],
                        "text": "Linear semi-gradient Sarsa with \u03b5-greedy action selection does not converge in the usual sense, but does enter a bounded region near the best solution (Gordon, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11137395,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0c4edc609f977cefb305f76a991514a83f8088e3",
            "isKey": false,
            "numCitedBy": 589,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stable-Function-Approximation-in-Dynamic-Gordon",
            "title": {
                "fragments": [],
                "text": "Stable Function Approximation in Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14948407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2d6788ef0cf32334b121f52625f4f61fc9dac0d",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Parti-game is a new algorithm for learning feasible trajectories to goal regions in high dimensional continuous state-spaces. In high dimensions it is essential that neither planning nor exploration occurs uniformly over a state-space. Parti-game maintains a decision-tree partitioning of state-space and applies techniques from game-theory and computational geometry to efficiently and adaptively concentrate high resolution only on critical areas. The current version of the algorithm is designed to find feasible paths or trajectories to goal regions in high dimensional spaces. Future versions will be designed to find a solution that optimizes a real-valued criterion. Many simulated problems have been rested, ranging from two-dimensional to nine-dimensional state-spaces, including mazes, path planning, non-linear dynamics, and planar snake robots in restricted spaces. In all cases, a good solution is found in less than ten trials and a few minutes."
            },
            "slug": "The-parti-game-algorithm-for-variable-resolution-in-Moore-Atkeson",
            "title": {
                "fragments": [],
                "text": "The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Parti-game is a new algorithm for learning feasible trajectories to goal regions in high dimensional continuous state-spaces and applies techniques from game-theory and computational geometry to efficiently and adaptively concentrate high resolution only on critical areas."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2419696"
                        ],
                        "name": "V. V. Phansalkar",
                        "slug": "V.-V.-Phansalkar",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Phansalkar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. V. Phansalkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 242
                            }
                        ],
                        "text": "Methods that we now see as related to policy gradients were actually some of the earliest to be studied in reinforcement learning (Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984; Williams, 1987, 1992) and in predecessor fields (Phansalkar and Thathachar, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 49741580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "855ce3491013292a31bc0445bc5d57d4819740d3",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the long-term behavior of the REINFORCE and related algorithms (Williams 1986, 1988, 1992) for generalized learning automata (Narendra and Thathachar 1989) for the associative reinforcement learning problem (Barto and Anandan 1985). The learning system considered here is a feedforward connectionist network of generalized learning automata units. We show that REINFORCE is a gradient ascent algorithm but can exhibit unbounded behavior. A modified version of this algorithm, based on constrained optimization techniques, is suggested to overcome this disadvantage. The modified algorithm is shown to exhibit local optimization properties. A global version of the algorithm, based on constant temperature heat bath techniques, is also described and shown to converge to the global maximum. All algorithms are analyzed using weak convergence techniques."
            },
            "slug": "Local-and-Global-Optimization-Algorithms-for-Phansalkar-Thathachar",
            "title": {
                "fragments": [],
                "text": "Local and Global Optimization Algorithms for Generalized Learning Automata"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that REINFORCE is a gradient ascent algorithm but can exhibit unbounded behavior, and a modified version of this algorithm, based on constrained optimization techniques, is suggested to overcome this disadvantage."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81338045"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2695116,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc649486b881e672eea6546da48c46e1f98daf32",
            "isKey": false,
            "numCitedBy": 1003,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off."
            },
            "slug": "Near-Optimal-Reinforcement-Learning-in-Polynomial-Kearns-Singh",
            "title": {
                "fragments": [],
                "text": "Near-Optimal Reinforcement Learning in Polynomial Time"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "New algorithms for reinforcement learning are presented and it is proved that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 26
                            }
                        ],
                        "text": "Discover how to implement Q-learning on \u2018grid world\u2019 environments, teach your agent to buy and trade stocks, and find out how natural language models are driving the boom in chatbots."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 138
                            }
                        ],
                        "text": "What you will learn Master the key skills of deep learning,\nPage 10/11\nreinforcement learning, and deep reinforcement learning Understand Q-learning and deep Q-learning Learn from friendly, plain English explanations and practical activities Build fun projects, including a virtual-self-driving car Use AI to solve real-world business problems and win classic video games Build an intelligent, virtual robot warehouse worker Who this book is for If you want to add AI to your skillset, this book is for you."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 10
                            }
                        ],
                        "text": "5 Q-learning was introduced by Watkins (1989), whose outline of a convergence proof was made rigorous by Watkins and Dayan (1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17464562,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5884b40f776c85d7689ea2c440f7982cb1bec46",
            "isKey": true,
            "numCitedBy": 653,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic automata operating in an unknown random environment have been proposed earlier as models of learning. These automata update their action probabilities in accordance with the inputs received from the environment and can improve their own performance during operation. In this context they are referred to as learning automata. A survey of the available results in the area of learning automata has been attempted in this paper. Attention has been focused on the norms of behavior of learning automata, issues in the design of updating schemes, convergence of the action probabilities, and interaction of several automata. Utilization of learning automata in parameter optimization and hypothesis testing is discussed, and potential areas of application are suggested."
            },
            "slug": "Learning-Automata-A-Survey-Narendra-Thathachar",
            "title": {
                "fragments": [],
                "text": "Learning Automata - A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Attention has been focused on the norms of behavior of learning automata, issues in the design of updating schemes, convergence of the action probabilities, and interaction of several automata."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51984593"
                        ],
                        "name": "Olle G\u00e4llmo",
                        "slug": "Olle-G\u00e4llmo",
                        "structuredName": {
                            "firstName": "Olle",
                            "lastName": "G\u00e4llmo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olle G\u00e4llmo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91795259"
                        ],
                        "name": "L. Asplund",
                        "slug": "L.-Asplund",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Asplund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Asplund"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15159517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91ebbe7905d3b62af24744b7fab0c2a161cae526",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A general approach to delayed reinforcement learning by the use of supervised training algorithms is proposed. The approach is monolithic, direct and involves minor modifications to any supervised learning algorithm. Each connection has two weight change registers, one for eventual success and one for eventual failure, and the network is trained on self-generated hypothetical target vectors. The method is a close, but more general, relative to selective bootstrap adaption and is tested on an abstract model of the Link Allocation problem in Asynchronous Transfer Mode (ATM) telecommunication networks."
            },
            "slug": "Reinforcement-Learning-by-Construction-of-Targets-G\u00e4llmo-Asplund",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning by Construction of Hypothetical Targets"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A general approach to delayed reinforcement learning by the use of supervised training algorithms is proposed and tested on an abstract model of the Link Allocation problem in Asynchronous Transfer Mode (ATM) telecommunication networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1868677"
                        ],
                        "name": "D. Harada",
                        "slug": "D.-Harada",
                        "structuredName": {
                            "firstName": "Daishi",
                            "lastName": "Harada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5730166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94066dc12fe31e96af7557838159bde598cb4f10",
            "isKey": false,
            "numCitedBy": 1603,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates conditions under which modi cations to the reward function of a Markov decision process preserve the op timal policy It is shown that besides the positive linear transformation familiar from utility theory one can add a reward for tran sitions between states that is expressible as the di erence in value of an arbitrary poten tial function applied to those states Further more this is shown to be a necessary con dition for invariance in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP These results shed light on the practice of reward shap ing a method used in reinforcement learn ing whereby additional training rewards are used to guide the learning agent In par ticular some well known bugs in reward shaping procedures are shown to arise from non potential based rewards and methods are given for constructing shaping potentials corresponding to distance based and subgoal based heuristics We show that such po tentials can lead to substantial reductions in learning time"
            },
            "slug": "Policy-Invariance-Under-Reward-Transformations:-and-Ng-Harada",
            "title": {
                "fragments": [],
                "text": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Conditions under which modi cations to the reward function of a Markov decision process preserve the op timal policy are investigated to shed light on the practice of reward shap ing a method used in reinforcement learn ing whereby additional training rewards are used to guide the learning agent."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6822470,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "b550e3e05701cbf6c76a8c71e91beb95f950b080",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply reinforcement learning methods to learn domain-specific heuristics for job shop scheduling. A repair-based scheduler starts with a critical-path schedule and incrementally repairs constraint violations with the goal of finding a short conflict-free schedule. The temporal difference algorithm TD(\u03bb) is applied to tram a neural network to learn a heuristic evaluation function over states. This evaluation function is used by a one-step lookahead search procedure to find good solutions to new scheduling problems. We evaluate this approach on synthetic problems and on problems from a NASA space shuttle pay load processing task. The evaluation function is trained on problems involving a small number of jobs and then tested on larger problems. The TD scheduler performs better than the best known existing algorithm for this task--Zwehen's iterative repair method based on simulated annealing. The results suggest that reinforcement learning can provide a new method for constructing high-performance scheduling systems."
            },
            "slug": "A-Reinforcement-Learning-Approach-to-job-shop-Zhang-Dietterich",
            "title": {
                "fragments": [],
                "text": "A Reinforcement Learning Approach to job-shop Scheduling"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Reinforcement learning methods are applied to learn domain-specific heuristics for job shop scheduling to suggest that reinforcement learning can provide a new method for constructing high-performance scheduling systems."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 169
                            }
                        ],
                        "text": "These programs enabled wide dissemination of new knowledge generated by the neural nets, resulting in great improvements in the overall caliber of human tournament play (Tesauro, 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 41
                            }
                        ],
                        "text": "Tesauro reported in a subsequent article (Tesauro, 2002) the results of an extensive rollout analysis of the move decisions and doubling decisions of TD-Gammon relative to top human players."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17990837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68310bbb8c005aea1d4f3b2719ac7e2479ac977b",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Programming-backgammon-using-self-teaching-neural-Tesauro",
            "title": {
                "fragments": [],
                "text": "Programming backgammon using self-teaching neural nets"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145726861"
                        ],
                        "name": "Ronald E. Parr",
                        "slug": "Ronald-E.-Parr",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Parr",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald E. Parr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11830414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c9e09c754773b13aa9a85c98d01ab30b6ced96f",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP) MDPs have been studied extensively and many methods are known for determining optimal courses of action or policies. The more realistic case where state information is only partially observable Partially Observable Markov Decision Processes (POMDPs) have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning meth ods a combination that was very effective in our test cases."
            },
            "slug": "Approximating-Optimal-Policies-for-Partially-Parr-Russell",
            "title": {
                "fragments": [],
                "text": "Approximating Optimal Policies for Partially Observable Stochastic Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Smooth Partially Observable Value Approximation (SPOVA) is introduced, a new approximation method that can quickly yield good approximations which can improve over time and can be combined with reinforcement learning meth ods a combination that was very effective in test cases."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47907087"
                        ],
                        "name": "Xiaohui Xie",
                        "slug": "Xiaohui-Xie",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924970"
                        ],
                        "name": "H. Seung",
                        "slug": "H.-Seung",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Seung",
                            "middleNames": [
                                "Sebastian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7453434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d12bd292d0b0b15469ce39fca9a6cf3a67317dd",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural networks are often trained by using the back propagation algorithm to compute the gradient of an objective function with respect to the synaptic strengths. For a biological neural network, such a gradient computation would be difficult to implement, because of the complex dynamics of intrinsic and synaptic conductances in neurons. Here we show that irregular spiking similar to that observed in biological neurons could be used as the basis for a learning rule that calculates a stochastic approximation to the gradient. The learning rule is derived based on a special class of model networks in which neurons fire spike trains with Poisson statistics. The learning is compatible with forms of synaptic dynamics such as short-term facilitation and depression. By correlating the fluctuations in irregular spiking with a reward signal, the learning rule performs stochastic gradient ascent on the expected reward. It is applied to two examples, learning the XOR computation and learning direction selectivity using depressing synapses. We also show in simulation that the learning rule is applicable to a network of noisy integrate-and-fire neurons."
            },
            "slug": "Learning-in-neural-networks-by-reinforcement-of-Xie-Seung",
            "title": {
                "fragments": [],
                "text": "Learning in neural networks by reinforcement of irregular spiking."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that irregular spiking similar to that observed in biological neurons could be used as the basis for a learning rule that calculates a stochastic approximation to the gradient."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. E, Statistical, nonlinear, and soft matter physics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806116"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5610275,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "9778c4b1b0401751566a7457ddc5c08f2e177ff2",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "According to a series of influential models, dopamine (DA) neurons signal reward prediction error using a temporal-difference (TD) algorithm. We address a problem not convincingly solved in these accounts: how to maintain a representation of cues that predict delayed consequences. Our new model uses a TD rule grounded in partially observable semi-Markov processes, a formalism that captures two largely neglected features of DA experiments: hidden state and temporal variability. Previous models predicted rewards using a tapped delay line representation of sensory inputs; we replace this with a more active process of inference about the underlying state of the world. The DA system can then learn to map these inferred states to reward predictions using TD. The new model can explain previously vexing data on the responses of DA neurons in the face of temporal variability. By combining statistical model-based learning with a physiologically grounded TD theory, it also brings into contact with physiology some insights about behavior that had previously been confined to more abstract psychological models."
            },
            "slug": "Timing-and-Partial-Observability-in-the-Dopamine-Daw-Courville",
            "title": {
                "fragments": [],
                "text": "Timing and Partial Observability in the Dopamine System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A TD rule grounded in partially observable semi-Markov processes is used, a formalism that captures two largely neglected features of DA experiments: hidden state and temporal variability and can explain previously vexing data on the responses of DA neurons in the face of temporal variability."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214276"
                        ],
                        "name": "J. Boyan",
                        "slug": "J.-Boyan",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Boyan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Boyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7799595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a8f4c3f0d317fd1f0bfe0b2e3e51aac893b8144",
            "isKey": false,
            "numCitedBy": 663,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show that the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce an entirely wrong policy. We then introduce Grow-Support, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization."
            },
            "slug": "Generalization-in-Reinforcement-Learning:-Safely-Boyan-Moore",
            "title": {
                "fragments": [],
                "text": "Generalization in Reinforcement Learning: Safely Approximating the Value Function"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Grow-Support is introduced, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization, and which is not robust, and in even very benign cases, may produce an entirely wrong policy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145042015"
                        ],
                        "name": "M. McCloskey",
                        "slug": "M.-McCloskey",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McCloskey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McCloskey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144529123"
                        ],
                        "name": "N. J. Cohen",
                        "slug": "N.-J.-Cohen",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Cohen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. J. Cohen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61019113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c213af6582c0d518a6e8e14217611c733eeb1ef1",
            "isKey": false,
            "numCitedBy": 2130,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Catastrophic-Interference-in-Connectionist-The-McCloskey-Cohen",
            "title": {
                "fragments": [],
                "text": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14742574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b11305f69641ecb8bd4a5e59cfebe41ad9ed989",
            "isKey": false,
            "numCitedBy": 843,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results, based on the TD() reinforcement learning algorithm (Sutton 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e., given only a raw description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set of hand-crafted features is added to the network's input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players."
            },
            "slug": "TD-Gammon,-a-Self-Teaching-Backgammon-Program,-Play-Tesauro",
            "title": {
                "fragments": [],
                "text": "TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 130
                            }
                        ],
                        "text": "Methods that we now see as related to policy gradients were actually some of the earliest to be studied in reinforcement learning (Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984; Williams, 1987, 1992) and in predecessor fields (Phansalkar and Thathachar, 1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 94
                            }
                        ],
                        "text": "5\u20136 Actor\u2013critic methods were among the earliest to be investigated in reinforcement learning (Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 30145758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0f2d0e9c57d268fc1d05ce657eaf64eaaeb323c7",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Adaptive-Optimal-Controller-for-Discrete-Time-Witten",
            "title": {
                "fragments": [],
                "text": "An Adaptive Optimal Controller for Discrete-Time Markov Environments"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543541"
                        ],
                        "name": "P. Auer",
                        "slug": "P.-Auer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Auer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152702481"
                        ],
                        "name": "P. Fischer",
                        "slug": "P.-Fischer",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fischer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207609497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
            "isKey": false,
            "numCitedBy": 5198,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support."
            },
            "slug": "Finite-time-Analysis-of-the-Multiarmed-Bandit-Auer-Cesa-Bianchi",
            "title": {
                "fragments": [],
                "text": "Finite-time Analysis of the Multiarmed Bandit Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145141118"
                        ],
                        "name": "D. Joel",
                        "slug": "D.-Joel",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Joel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779370"
                        ],
                        "name": "E. Ruppin",
                        "slug": "E.-Ruppin",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Ruppin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ruppin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 89
                            }
                        ],
                        "text": "7 This section, written with the help of Michael Littman, is based on Littman, Dean, and Kaelbling (1995). The phrase \u201ccurse of dimensionality\u201d is due to Bellman (1957)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2318353,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "871f07197135c5d09c5d65d90b02c7f0aa75c246",
            "isKey": false,
            "numCitedBy": 446,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Actor-critic-models-of-the-basal-ganglia:-new-and-Joel-Niv",
            "title": {
                "fragments": [],
                "text": "Actor-critic models of the basal ganglia: new anatomical and computational perspectives"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143928513"
                        ],
                        "name": "K. Unnikrishnan",
                        "slug": "K.-Unnikrishnan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Unnikrishnan",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Unnikrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32531594"
                        ],
                        "name": "K. P. Venugopal",
                        "slug": "K.-P.-Venugopal",
                        "structuredName": {
                            "firstName": "Kootala",
                            "lastName": "Venugopal",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. P. Venugopal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12752183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56b10282bce1c5943af786544f126aac4437dfaf",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a learning algorithm for neural networks, called Alopex. Instead of error gradient, Alopex uses local correlations between changes in individual weights and changes in the global error measure. The algorithm does not make any assumptions about transfer functions of individual neurons, and does not explicitly depend on the functional form of the error measure. Hence, it can be used in networks with arbitrary transfer functions and for minimizing a large class of error measures. The learning algorithm is the same for feedforward and recurrent networks. All the weights in a network are updated simultaneously, using only local computations. This allows complete parallelization of the algorithm. The algorithm is stochastic and it uses a temperature parameter in a manner similar to that in simulated annealing. A heuristic annealing schedule is presented that is effective in finding global minima of error surfaces. In this paper, we report extensive simulation studies illustrating these advantages and show that learning times are comparable to those for standard gradient descent methods. Feedforward networks trained with Alopex are used to solve the MONK's problems and symmetry problems. Recurrent networks trained with the same algorithm are used for solving temporal XOR problems. Scaling properties of the algorithm are demonstrated using encoder problems of different sizes and advantages of appropriate error measures are illustrated using a variety of problems."
            },
            "slug": "Alopex:-A-Correlation-Based-Learning-Algorithm-for-Unnikrishnan-Venugopal",
            "title": {
                "fragments": [],
                "text": "Alopex: A Correlation-Based Learning Algorithm for Feedforward and Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents a learning algorithm for neural networks, called Alopex, which uses local correlations between changes in individual weights and changes in the global error measure, and shows that learning times are comparable to those for standard gradient descent methods."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748374"
                        ],
                        "name": "R. Crites",
                        "slug": "R.-Crites",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Crites",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Crites"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14551518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e5a400e63e74f44d07be8b4742472c981ca5b8",
            "isKey": false,
            "numCitedBy": 635,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the application of reinforcement learning (RL) to the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are nonstationary due to changing passenger arrival rates. In addition, we use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility."
            },
            "slug": "Improving-Elevator-Performance-Using-Reinforcement-Crites-Barto",
            "title": {
                "fragments": [],
                "text": "Improving Elevator Performance Using Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Results in simulation surpass the best of the heuristic elevator control algorithms of which the author is aware and demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21889436"
                        ],
                        "name": "Geoffrey J. Gordon",
                        "slug": "Geoffrey-J.-Gordon",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey J. Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 42
                            }
                        ],
                        "text": "It is a variant of fitted value iteration (Gordon, 1999) adapted to Q-learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 85
                            }
                        ],
                        "text": "The convergence of FQI depends on properties of the function approximation algorithm (Gordon, 1999)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118115802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "578c9c14f4c3c5ece9caf9b4c1898f8a94461bd9",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the basic problems of machine learning is deciding how to act in an uncertain world. For example, if I want my robot to bring me a cup of coffee, it must be able to compute the correct sequence of electrical impulses to send to its motors to navigate from the coffee pot to my office. In fact, since the results of its actions are not completely predictable, it is not enough just to compute the correct sequence; instead the robot must sense and correct for deviations from its intended path. \nIn order for any machine learner to act reasonably in an uncertain environment, it must solve problems like the above one quickly and reliably. Unfortunately, the world is often so complicated that it is difficult or impossible to find the optimal sequence of actions to achieve a given goal. So, in order to scale our learners up to real-world problems, we usually must settle for approximate solutions. \nOne representation for a learner's environment and goals is a Markov decision process or MDP. MDPs allow us to represent actions that have probabilistic outcomes, and to plan for complicated, temporally-extended goals. An MDP consists of a set of states that the environment can be in, together with rules for how the environment can change state and for what the learner is supposed to do. \nOne way to approach a large MDP is to try to compute an approximation to its optimal state evaluation function, the function which tells us how much reward the learner can be expected to achieve if the world is in a particular state. If the approximation is good enough, we can use a shallow search to find a good action from most states. Researchers have tried many different ways to approximate evaluation functions. This thesis aims for a middle ground, between algorithms that don't scale well because they use an impoverished representation for the evaluation function and algorithms that we can't analyze because they use too complicated a representation."
            },
            "slug": "Approximate-solutions-to-markov-decision-processes-Gordon-Mitchell",
            "title": {
                "fragments": [],
                "text": "Approximate solutions to markov decision processes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This thesis aims for a middle ground, between algorithms that don't scale well because they use an impoverished representation for the evaluation function and algorithms that the authors can't analyze because they used too complicated a representation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850503"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706614"
                        ],
                        "name": "J. Connell",
                        "slug": "J.-Connell",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Connell",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Connell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 114918,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "9d612473d6bba6fb7b611b88d0b5f7fff6c17fdd",
            "isKey": false,
            "numCitedBy": 731,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Programming-of-Behavior-Based-Robots-Mahadevan-Connell",
            "title": {
                "fragments": [],
                "text": "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "What we call ``Kanerva coding\" was introduced by Kanerva (1988), as part of his more general idea of sparse distributed memory."
                    },
                    "intents": []
                }
            ],
            "corpusId": 57931704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcdb9bd64e3d7885c10938291153257b94f3df91",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nMotivated by the remarkable fluidity of memory the way in which items are pulled spontaneously and effortlessly from our memory by vague similarities to what is currently occupying our attention Sparse Distributed Memory presents a mathematically elegant theory of human long term memory. \nThe book, which is self contained, begins with background material from mathematics, computers, and neurophysiology; this is followed by a step by step development of the memory model. The concluding chapter describes an autonomous system that builds from experience an internal model of the world and bases its operation on that internal model. Close attention is paid to the engineering of the memory, including comparisons to ordinary computer memories. \nSparse Distributed Memory provides an overall perspective on neural systems. The model it describes can aid in understanding human memory and learning, and a system based on it sheds light on outstanding problems in philosophy and artificial intelligence. Applications of the memory are expected to be found in the creation of adaptive systems for signal processing, speech, vision, motor control, and (in general) robots. Perhaps the most exciting aspect of the memory, in its implications for research in neural networks, is that its realization with neuronlike components resembles the cortex of the cerebellum. \nPentti Kanerva is a scientist at the Research Institute for Advanced Computer Science at the NASA Ames Research Center and a visiting scholar at the Stanford Center for the Study of Language and Information. A Bradford Book."
            },
            "slug": "Sparse-Distributed-Memory-Kanerva",
            "title": {
                "fragments": [],
                "text": "Sparse Distributed Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Pentti Kanerva's Sparse Distributed Memory presents a mathematically elegant theory of human long term memory that resembles the cortex of the cerebellum, and provides an overall perspective on neural systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34565012"
                        ],
                        "name": "L. Booker",
                        "slug": "L.-Booker",
                        "structuredName": {
                            "firstName": "Lashon",
                            "lastName": "Booker",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Booker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60479414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0063d47a1885e841d9aad1f5dd14651f17231df",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "As research in artificial intelligence focuses on increasingly complex task domains, a key question to be resolved is how to design a system that can efficiently acquire knowledge and gracefully adapt its behavior in an uncertain environment. This dissertation argues that examining more closely the way animate systems cope with real-world environments can provide valuable insights about the structural requirements for intelligent behavior. \nAccordingly, a class of simulated environments is designed that embodies many of the important functional properties characteristic of natural environments. A new type of adaptive system is then defined that uses pattern-directed, rule-based processing to cope with uncertain information. As a rule-based system, the system presented here is notable in that several rules can be active at once and there are no fixed priorities determining the order in which rules can be activated. Moreover, the syntax of each rule is simple enough to make a powerful learning heuristic applicable--one that is provably more efficient than the techniques used in most other adaptive rule-based systems. \nA simple version of the adaptive system is implemented as a hypothetical organism having to locate resources and avoid noxious stimuli by generating temporal sequences of actions in a simulated environment. Simulation results show that the naive organism quickly acquires the knowledge required to function effectively. Further experiments show that the system is capable of discriminating a large class of schematic patterns; and, that prior learning experiences transfer to novel situations. \nThe results presented here demonstrate that activity in a collection of simple computational elements--operating in parallel and activated stochastically--can be orchestrated to produce reliable behavior in a challenging environment. The system touches on several issues related to cognitive functioning such as the generic representation of objects and the management of limited processing resources. These issues have been addressed in a way that is computationally feasible and that allows for rigorous testing."
            },
            "slug": "Intelligent-Behavior-as-an-Adaptation-to-the-Task-Booker",
            "title": {
                "fragments": [],
                "text": "Intelligent Behavior as an Adaptation to the Task Environment"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This dissertation argues that examining more closely the way animate systems cope with real-world environments can provide valuable insights about the structural requirements for intelligent behavior."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 104
                            }
                        ],
                        "text": "The roots of variable \u03b3 are in the work on options (Sutton, Precup, and Singh, 1999) and its precursors (Sutton, 1995a), becoming explicit in the GQ(\u03bb) paper (Maei and Sutton, 2010), which also introduced some of these recursive forms for the \u03bb-returns."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 84
                            }
                        ],
                        "text": "1 General value functions were first explicitly identified by Sutton and colleagues (Sutton, 1995a; Sutton et al., 2011; Modayil, White and Sutton, 2013)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11070703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b5db92ce2f86b2136fe7cf6a415fe1c0632a881",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "TD-Models:-Modeling-the-World-at-a-Mixture-of-Time-Sutton",
            "title": {
                "fragments": [],
                "text": "TD Models: Modeling the World at a Mixture of Time Scales"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145726861"
                        ],
                        "name": "Ronald E. Parr",
                        "slug": "Ronald-E.-Parr",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Parr",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald E. Parr"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53939299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2966ae949d1bc255bad11045fd0ff8eb5848cf5a",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation investigates the use of hierarchy and problem decomposition as a means of solving large, stochastic, sequential decision problems. These problems are framed as Markov decision problems (MDPs). The new technical content of this dissertation begins with a discussion of the concept of temporal abstraction. Temporal abstraction is shown to be equivalent to the transformation of a policy defined over a region of an MDP to an action in a semi-Markov decision problem (SMDP). Several algorithms are presented for performing this transformation efficiently. \nThis dissertation introduces the HAM for generating hierarchical, temporally abstract actions. This method permits the partial specification of abstract actions in a way that corresponds to an abstract plan or strategy. Abstract actions specified as HAMs can be optimally refined for new tasks by solving a reduced SMDP. The formal results show that traditional MDP algorithms can be used to optimally refine HAMs for new tasks. This can be achieved in much less time than it would take to learn a new policy for the task from scratch. \nHAMs complement some novel decomposition algorithms that are presented in this dissertation. These algorithms work by constructing a cache of policies for different regions of the MDP and then optimally combining the cached solution to produce a global solution that is within provable bounds of the optimal solution. \nTogether, the methods developed in this dissertation provide important tools for producing good policies for large MDPs. Unlike some ad-hoc methods, these methods provide strong formal guarantees. They use prior knowledge in a principled way, and they reduce larger MDPs into smaller ones while maintaining a well-defined relationship between the smaller problem and the larger problem."
            },
            "slug": "Hierarchical-control-and-learning-for-markov-Parr",
            "title": {
                "fragments": [],
                "text": "Hierarchical control and learning for markov decision processes"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This dissertation introduces the HAM for generating hierarchical, temporally abstract actions and shows that traditional MDP algorithms can be used to optimally refine HAMs for new tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 476312,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "66bdd8715fc0e9a110382a2b91477bea1adc25aa",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The methods of temporal differences (Samuel, 1959; Sutton, 1984, 1988) allow an agent to learn accurate predictions of stationary stochastic future outcomes. The learning is effectively stochastic approximation based on samples extracted from the process generating the agent's future.Sutton (1988) proved that for a special case of temporal differences, the expected values of the predictions converge to their correct values, as larger samples are taken, and Dayan (1992) extended his proof to the general case. This article proves the stronger result than the predictions of a slightly modified form of temporal difference learning converge with probability one, and shows how to quantify the rate of convergence."
            },
            "slug": "TD(\u03bb)-Converges-with-Probability-1-Dayan-Sejnowski",
            "title": {
                "fragments": [],
                "text": "TD(\u03bb) Converges with Probability 1"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article proves the stronger result than the predictions of a slightly modified form of temporal difference learning converge with probability one, and shows how to quantify the rate of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987887"
                        ],
                        "name": "Mark Derthick",
                        "slug": "Mark-Derthick",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Derthick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Derthick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 24
                            }
                        ],
                        "text": "The notions of momentum (Derthick, 1984), of Polyak-Ruppert averaging (Polyak, 1990;"
                    },
                    "intents": []
                }
            ],
            "corpusId": 61403722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92c61d5f45a8c0cee1ef2d638d1957a656061e5c",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Boltzmann Machines learn to model the structure of an environment by modifying internal weights. The algorithm used for changing a weight depends on collecting statistics about the behavior of the two units that the weight connects. The success and speed of the algorithm depends on the accuracy of the statistics, the size of the weight changes, and the way in which the accuracy of the machine's model varies as the weights are changed. This paper presents theoretical analysis and empirical results that can be used to select more effective parameters for the learning algorithm. The Boltzmann Machine learns to make its model of its environment correspond to the actual environment in which it is placed. An environment is a probability distribution of patterns over a subset of the units called the visible units. The environment can be specified explicitly as a list of ordered pairs, giving a pattern, V a' over the visible units, and a probability for the occurrence of the pattern. The machine's model is just the probability distribution it would produce over the visible units if it were allowed to run freely without any environmental input. This probability distribution is not stored directly. Instead, it is specified implicitly by the magnitude of the weights in the machine. For a machine architecture with v visible units, there are 2(v) possible patterns. A machine whose weights were all zero would implicitly specify an environment where each of these patterns had probability 2-v, since the energy of all states would be the same."
            },
            "slug": "Variations-on-the-Boltzmann-Machine-Learning-Derthick",
            "title": {
                "fragments": [],
                "text": "Variations on the Boltzmann Machine Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents theoretical analysis and empirical results that can be used to select more effective parameters for the learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2683521"
                        ],
                        "name": "M. Farries",
                        "slug": "M.-Farries",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Farries",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Farries"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595942"
                        ],
                        "name": "A. Fairhall",
                        "slug": "A.-Fairhall",
                        "structuredName": {
                            "firstName": "Adrienne",
                            "lastName": "Fairhall",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fairhall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3129244,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "f6ad684ae7007557a25bf8a0a722f57aaa4a0965",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 390,
            "paperAbstract": {
                "fragments": [],
                "text": "Spike timing-dependent synaptic plasticity (STDP) has emerged as the preferred framework linking patterns of pre- and postsynaptic activity to changes in synaptic strength. Although synaptic plasticity is widely believed to be a major component of learning, it is unclear how STDP itself could serve as a mechanism for general purpose learning. On the other hand, algorithms for reinforcement learning work on a wide variety of problems, but lack an experimentally established neural implementation. Here, we combine these paradigms in a novel model in which a modified version of STDP achieves reinforcement learning. We build this model in stages, identifying a minimal set of conditions needed to make it work. Using a performance-modulated modification of STDP in a two-layer feedforward network, we can train output neurons to generate arbitrarily selected spike trains or population responses. Furthermore, a given network can learn distinct responses to several different input patterns. We also describe in detail how this model might be implemented biologically. Thus our model offers a novel and biologically plausible implementation of reinforcement learning that is capable of training a neural population to produce a very wide range of possible mappings between synaptic input and spiking output."
            },
            "slug": "Reinforcement-learning-with-modulated-spike-timing-Farries-Fairhall",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning with modulated spike timing dependent synaptic plasticity."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This model offers a novel and biologically plausible implementation of reinforcement learning that is capable of training a neural population to produce a very wide range of possible mappings between synaptic input and spiking output."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214276"
                        ],
                        "name": "J. Boyan",
                        "slug": "J.-Boyan",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Boyan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Boyan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6988226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55c7cb8ca85c751f7a418ae06143d9f3473ce526",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "TD( ) is a popular family of algorithms for approximate policy evaluation in large MDPs. TD( ) works by incrementally updating the value function after each observed transition. It has two major drawbacks: it makes ine cient use of data, and it requires the user to manually tune a stepsize schedule for good performance. For the case of linear value function approximations and = 0, the Least-Squares TD (LSTD) algorithm of Bradtke and Barto (Bradtke and Barto, 1996) eliminates all stepsize parameters and improves data e ciency. This paper extends Bradtke and Barto's work in three signi cant ways. First, it presents a simpler derivation of the LSTD algorithm. Second, it generalizes from = 0 to arbitrary values of ; at the extreme of = 1, the resulting algorithm is shown to be a practical formulation of supervised linear regression. Third, it presents a novel, intuitive interpretation of LSTD as a model-based reinforcement learning technique."
            },
            "slug": "Least-Squares-Temporal-Difference-Learning-Boyan",
            "title": {
                "fragments": [],
                "text": "Least-Squares Temporal Difference Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a simpler derivation of the LSTD algorithm, which generalizes from = 0 to arbitrary values of ; at the extreme of = 1, the resulting algorithm is shown to be a practical formulation of supervised linear regression."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153352617"
                        ],
                        "name": "Richard Wheeler",
                        "slug": "Richard-Wheeler",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Wheeler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Wheeler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 30
                            }
                        ],
                        "text": "produced by statistical modeling carried out in sensory cortex rather than simpler representations based on raw sensory input. Ludvig, Sutton, and Kehoe (2008) found that TD learning with a microstimulus (MS) representation (Figure 14."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 48
                            }
                        ],
                        "text": "The chapter then presented a hypothesis, following Takahashi et al. (2008), about how the brain might implement an actor-critic algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 405,
                                "start": 30
                            }
                        ],
                        "text": "produced by statistical modeling carried out in sensory cortex rather than simpler representations based on raw sensory input. Ludvig, Sutton, and Kehoe (2008) found that TD learning with a microstimulus (MS) representation (Figure 14.2) fits the activity of dopamine neurons in the early-reward and other situations better than when a CSC representation is used. Pan, Schmidt, Wickens, and Hyland (2005) found that even with the CSC representation, prolonged eligibility traces improve the fit of the TD error to some aspects of dopamine neuron activity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 338,
                                "start": 36
                            }
                        ],
                        "text": "because they break the overall backing-up computation into smaller pieces\u2014those corresponding to individual transitions\u2014which then enables it to be focused more narrowly on the pieces that will have the largest impact. This idea was taken to what may be its logical limit in the \u201csmall backups\u201d introduced by van Seijen and Sutton (2013). These are backups along a single transition, like a sample backup, but based on the probability of the transition without sampling, as in a full backup."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 38
                            }
                        ],
                        "text": "Our treatment of the idea of discounting-aware importance sampling is based on the analysis of Sutton, Mahmood, Precup, and van Hasselt (2014). It has been worked out most fully to date by Mahmood (in preparation; Mahmood, van Hasselt, and Sutton, 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 399,
                                "start": 7
                            }
                        ],
                        "text": "\u201clearning with a critic\u201d instead of \u201clearning with a teacher.\u201d They analyzed this rule and showed how it could learn to play blackjack. This was an isolated foray into reinforcement learning by Widrow, whose contributions to supervised learning were much more influential. Our use of the term \u201ccritic\u201d is derived from Widrow, Gupta, and Maitra\u2019s paper. Buchanan, Mitchell, Smith, and Johnson (1978) independently used the term critic in the context of machine learning (see also Dietterich and Buchanan, 1984), but for them a critic is an expert system able to do more than evaluate performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 51
                            }
                        ],
                        "text": "4 Graybiel (2000) is a brief primer on the basal ganglia. The experiments mentioned that involve optogenetic activation of dopamine neurons were conducted by Tsai, Zhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg, Keiflin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge-Chang, Roorda, Vrontou, Sjulson, Li, Hirsh, and Miesenb\u00f6ck (2009)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 24748475,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2e6a0730f3bf906e3f4137db4ced6d630d594bca",
            "isKey": true,
            "numCitedBy": 70,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The principal contribution of this paper is a new result on the decentralized control of finite Markov chains with unknown transition probabilities and rewards. One decentralized decision maker is associated with each state in which two or more actions (decisions) are available. Each decision maker uses a simple learning scheme, requiring minimal information, to update its action choice. It is shown that, if updating is done in sufficiently small steps, the group will converge to the policy that maximizes the long-term expected reward per step. The analysis is based on learning in sequential stochastic games and on certain properties, derived in this paper, of ergodic Markov chains."
            },
            "slug": "Decentralized-learning-in-finite-Markov-chains-Wheeler-Narendra",
            "title": {
                "fragments": [],
                "text": "Decentralized learning in finite Markov chains"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that, if updating is done in sufficiently small steps, the group will converge to the policy that maximizes the long-term expected reward per step."
            },
            "venue": {
                "fragments": [],
                "text": "1985 24th IEEE Conference on Decision and Control"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682627"
                        ],
                        "name": "R. Korf",
                        "slug": "R.-Korf",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Korf",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Korf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11749385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fda10f6079156c4621fefc8b7cad72c1829ee94",
            "isKey": false,
            "numCitedBy": 1055,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Real-Time-Heuristic-Search-Korf",
            "title": {
                "fragments": [],
                "text": "Real-Time Heuristic Search"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32381927"
                        ],
                        "name": "Jan Storck",
                        "slug": "Jan-Storck",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Storck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Storck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6720319,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2547be25e1e07728aa0966a0354e90664816d15e",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "For an agent living in a nondeterministic Markov environment (NME), what is, in theory, the fastest way of acquiring information about its statistical properties? The answer is: to design \u201coptimal\u201d sequences of \u201cexperiments\u201d by performing action sequences that maximize expected information gain. This notion is implemented by combining concepts from information theory and reinforcement learning. Experiments show that the resulting method, reinforcement driven information acquisition, can explore certain NMEs much faster than conventional random exploration."
            },
            "slug": "REINFORCEMENT-DRIVEN-INFORMATION-ACQUISITION-IN-Storck-Hochreiter",
            "title": {
                "fragments": [],
                "text": "REINFORCEMENT DRIVEN INFORMATION ACQUISITION IN NONDETERMINISTIC ENVIRONMENTS"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments show that the resulting method, reinforcement driven information acquisition, can explore certain NMEs much faster than conventional random exploration."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924908"
                        ],
                        "name": "H. Seo",
                        "slug": "H.-Seo",
                        "structuredName": {
                            "firstName": "Hyojung",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7647153"
                        ],
                        "name": "D. Barraclough",
                        "slug": "D.-Barraclough",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Barraclough",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barraclough"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672498"
                        ],
                        "name": "Daeyeol Lee",
                        "slug": "Daeyeol-Lee",
                        "structuredName": {
                            "firstName": "Daeyeol",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daeyeol Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "Ludvig, Bellemare, and Pearson (2011) and Shah (2012) review reinforcement learning in the contexts of psychology and neuroscience."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14977865,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "ec80655fddd2be9e6f224bc590a17d57d08e5afb",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Although economic theories based on utility maximization account for a range of choice behaviors, utilities must be estimated through experience. Dynamics of this learning process may account for certain discrepancies between the predictions of economic theories and real choice behaviors of humans and other animals. To understand the neural mechanisms responsible for such adaptive decision making, we trained rhesus monkeys to play a simulated matching pennies game. Small but systematic deviations of the animal's behavior from the optimal strategy were consistent with the predictions of reinforcement learning theory. In addition, individual neurons in the dorsolateral prefrontal cortex (DLPFC) encoded 3 different types of signals that can potentially influence the animal's future choices. First, activity modulated by the animal's previous choices might provide the eligibility trace that can be used to attribute a particular outcome to its causative action. Second, activity related to the animal's rewards in the previous trials might be used to compute an average reward rate. Finally, activity of some neurons was modulated by the computer's choices in the previous trials and may reflect the process of updating the value functions. These results suggest that the DLPFC might be an important node in the cortical network of decision making."
            },
            "slug": "Dynamic-signals-related-to-choices-and-outcomes-in-Seo-Barraclough",
            "title": {
                "fragments": [],
                "text": "Dynamic signals related to choices and outcomes in the dorsolateral prefrontal cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results suggest that the DLPFC might be an important node in the cortical network of decision making, and activity of some neurons was modulated by the computer's choices in the previous trials and may reflect the process of updating the value functions."
            },
            "venue": {
                "fragments": [],
                "text": "Cerebral cortex"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2318089"
                        ],
                        "name": "Guanghua Hu",
                        "slug": "Guanghua-Hu",
                        "structuredName": {
                            "firstName": "Guanghua",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guanghua Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114968644"
                        ],
                        "name": "Yuqin Qiu",
                        "slug": "Yuqin-Qiu",
                        "structuredName": {
                            "firstName": "Yuqin",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuqin Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2483013"
                        ],
                        "name": "Liming Xiang",
                        "slug": "Liming-Xiang",
                        "structuredName": {
                            "firstName": "Liming",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liming Xiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2411167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3db0e4597c64576178d51410b1d7615a11e6f0dc",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of approximating the cost-to-go functions in reinforcement learning. By mapping the state implicitly into a feature space, we perform a simple algorithm in the feature space, which corresponds to a complex algorithm in the original state space. Two kernel-based reinforcement learning algorithms, the e -insensitive kernel based reinforcement learning (e \u2013 KRL) and the least squares kernel based reinforcement learning (LS-KRL) are proposed. An example shows that the proposed methods can deal effectively with the reinforcement learning problem without having to explore many states."
            },
            "slug": "Kernel-Based-Reinforcement-Learning-Hu-Qiu",
            "title": {
                "fragments": [],
                "text": "Kernel-Based Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Two kernel-based reinforcement learning algorithms, the e \u2013 KRL and the least squares kernel based reinforcement learning (LS-KRL) are proposed and an example shows that the proposed methods can deal effectively with the reinforcement learning problem without having to explore many states."
            },
            "venue": {
                "fragments": [],
                "text": "ICIC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740513"
                        ],
                        "name": "B. Buchanan",
                        "slug": "B.-Buchanan",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Buchanan",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Buchanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59327818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "041db552562217841768ef4669d2d7f4430ab1c0",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Buchanan, Mitchell, Smith, and Johnson (1978) described a general model of learning systems that included a component called the Critic. The task of the Critic was described as threefold: evaluation of the past actions of the performance element of the learning system, localization of credit and blame to particular portions of that performance element, and recommendation of possible improvements and modifications in the performance element. This article analyzes these three tasks in detail and surveys the methods that have been employed in existing learning systems to accomplish them. The principle method used to evaluate the performance element is to develop a global performance standard by (a) consulting an external source of knowledge, (b) consulting an internal source of knowledge, or (c) conducting deep search. Credit and blame have been localized by (a) asking an external knowledge source to do the localization, (b) factoring the global performance standard to produce a local performance standard, and (c) conducting controlled experiments on the performance element. Recommendations have been conmiunicated to the learning element using (a) local training instances, (b) correlation coefficients, and (c) partially-instantiated schemata."
            },
            "slug": "The-Role-of-the-Critic-in-Learning-Systems-Dietterich-Buchanan",
            "title": {
                "fragments": [],
                "text": "The Role of the Critic in Learning Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This article analyzes the three tasks of the Critic as threefold: evaluation of the past actions of the performance element of the learning system, localization of credit and blame to particular portions of that performance element, and recommendation of possible improvements and modifications in the performanceelement."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21889436"
                        ],
                        "name": "Geoffrey J. Gordon",
                        "slug": "Geoffrey-J.-Gordon",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey J. Gordon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14570927,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f36fa118e757ce917b7a03664768787d8b9bb62",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(0) and V(0); the latter algorithm was used in the well-known TD-Gammon program."
            },
            "slug": "Reinforcement-Learning-with-Function-Approximation-Gordon",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with Function Approximation Converges to a Region"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737497"
                        ],
                        "name": "Karl J. Friston",
                        "slug": "Karl-J.-Friston",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Friston",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl J. Friston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726111"
                        ],
                        "name": "G. Tononi",
                        "slug": "G.-Tononi",
                        "structuredName": {
                            "firstName": "Giulio",
                            "lastName": "Tononi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tononi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084582"
                        ],
                        "name": "G. Reeke",
                        "slug": "G.-Reeke",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Reeke",
                            "middleNames": [
                                "N."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Reeke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694232"
                        ],
                        "name": "O. Sporns",
                        "slug": "O.-Sporns",
                        "structuredName": {
                            "firstName": "Olaf",
                            "lastName": "Sporns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Sporns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1995280"
                        ],
                        "name": "G. Edelman",
                        "slug": "G.-Edelman",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Edelman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17148744,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "89794232987d3973109ba78327a4c989a6e4970b",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Value-dependent-selection-in-the-brain:-Simulation-Friston-Tononi",
            "title": {
                "fragments": [],
                "text": "Value-dependent selection in the brain: Simulation in a synthetic neural model"
            },
            "venue": {
                "fragments": [],
                "text": "Neuroscience"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60855956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a8aea51f5a911e0964d51ac764dc04d5900b7b7",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Strategy-Learning-with-Multilayer-Connectionist-Anderson",
            "title": {
                "fragments": [],
                "text": "Strategy Learning with Multilayer Connectionist Representations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806116"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12791556,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "d87af48568ace6fdbf9e8a3915950842c593451c",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "Although the responses of dopamine neurons in the primate midbrain are well characterized as carrying a temporal difference (TD) error signal for reward prediction, existing theories do not offer a credible account of how the brain keeps track of past sensory events that may be relevant to predicting future reward. Empirically, these shortcomings of previous theories are particularly evident in their account of experiments in which animals were exposed to variation in the timing of events. The original theories mispredicted the results of such experiments due to their use of a representational device called a tapped delay line. Here we propose that a richer understanding of history representation and a better account of these experiments can be given by considering TD algorithms for a formal setting that incorporates two features not originally considered in theories of the dopaminergic response: partial observability (a distinction between the animal's sensory experience and the true underlying state of the world) and semi-Markov dynamics (an explicit account of variation in the intervals between events). The new theory situates the dopaminergic system in a richer functional and anatomical context, since it assumes (in accord with recent computational theories of cortex) that problems of partial observability and stimulus history are solved in sensory cortex using statistical modeling and inference and that the TD system predicts reward using the results of this inference rather than raw sensory data. It also accounts for a range of experimental data, including the experiments involving programmed temporal variability and other previously unmodeled dopaminergic response phenomena, which we suggest are related to subjective noise in animals' interval timing. Finally, it offers new experimental predictions and a rich theoretical framework for designing future experiments."
            },
            "slug": "Representation-and-Timing-in-Theories-of-the-System-Daw-Courville",
            "title": {
                "fragments": [],
                "text": "Representation and Timing in Theories of the Dopamine System"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The new theory assumes (in accord with recent computational theories of cortex) that problems of partial observability and stimulus history are solved in sensory cortex using statistical modeling and inference and that the TD system predicts reward using the results of this inference rather than raw sensory data."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50844636"
                        ],
                        "name": "V. Konda",
                        "slug": "V.-Konda",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Konda",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Konda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207779694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac4af1df88e178386d782705acc159eaa0c3904a",
            "isKey": false,
            "numCitedBy": 1804,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Many complex decision making problems like scheduling in manufacturing systems, portfolio management in finance, admission control in communication networks etc., with clear and precise objectives, can be formulated as stochastic dynamic programming problems in which the objective of decision making is to maximize a single \u201coverall\u201d reward. In these formulations, finding an optimal decision policy involves computing a certain \u201cvalue function\u201d which assigns to each state the optimal reward one would obtain if the system was started from that state. This function then naturally prescribes the optimal policy, which is to take decisions that drive the system to states with maximum value. \nFor many practical problems, the computation of the exact value function is intractable, analytically and numerically, due to the enormous size of the state space. Therefore one has to resort to one of the following approximation methods to find a good sub-optimal policy: (1)\u00a0Approximate the value function. (2)\u00a0Restrict the search for a good policy to a smaller family of policies. \nIn this thesis, we propose and study actor-critic algorithms which combine the above two approaches with simulation to find the best policy among a parameterized class of policies. Actor-critic algorithms have two learning units: an actor and a critic. An actor is a decision maker with a tunable parameter. A critic is a function approximator. The critic tries to approximate the value function of the policy used by the actor, and the actor in turn tries to improve its policy based on the current approximation provided by the critic. Furthermore, the critic evolves on a faster time-scale than the actor. \nWe propose several variants of actor-critic algorithms. In all the variants, the critic uses Temporal Difference (TD) learning with linear function approximation. Some of the variants are inspired by a new geometric interpretation of the formula for the gradient of the overall reward with respect to the actor parameters. This interpretation suggests a natural set of basis functions for the critic, determined by the family of policies parameterized by the actor's parameters. We concentrate on the average expected reward criterion but we also show how the algorithms can be modified for other objective criteria. We prove convergence of the algorithms for problems with general (finite, countable, or continuous) state and decision spaces. \nTo compute the rate of convergence (ROC) of our algorithms, we develop a general theory of the ROC of two-time-scale algorithms and we apply it to study our algorithms. In the process, we study the ROC of TD learning and compare it with related methods such as Least Squares TD (LSTD). We study the effect of the basis functions used for linear function approximation on the ROC of TD. We also show that the ROC of actor-critic algorithms does not depend on the actual basis functions used in the critic but depends only on the subspace spanned by them and study this dependence. \nFinally, we compare the performance of our algorithms with other algorithms that optimize over a parameterized family of policies. We show that when only the \u201cnatural\u201d basis functions are used for the critic, the rate of convergence of the actor critic algorithms is the same as that of certain stochastic gradient descent algorithms. However, with appropriate additional basis functions for the critic, we show that our algorithms outperform the existing ones in terms of ROC. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Actor-Critic-Algorithms-Konda-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Actor-Critic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This thesis proposes and studies actor-critic algorithms which combine the above two approaches with simulation to find the best policy among a parameterized class of policies, and proves convergence of the algorithms for problems with general state and decision spaces."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390067049"
                        ],
                        "name": "R. O\u2019Reilly",
                        "slug": "R.-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "O\u2019Reilly",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. O\u2019Reilly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071345269"
                        ],
                        "name": "Michael J. Frank",
                        "slug": "Michael-J.-Frank",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Frank",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Frank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2159309"
                        ],
                        "name": "T. E. Hazy",
                        "slug": "T.-E.-Hazy",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hazy",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. E. Hazy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4324082"
                        ],
                        "name": "Brandon Watz",
                        "slug": "Brandon-Watz",
                        "structuredName": {
                            "firstName": "Brandon",
                            "lastName": "Watz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brandon Watz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17041002,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "25efb9b32951a3ed3006c4cab9f33146befc7824",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present their primary value learned value (PVLV) model for understanding the reward-predictive firing properties of dopamine (DA) neurons as an alternative to the temporal-differences (TD) algorithm. PVLV is more directly related to underlying biology and is also more robust to variability in the environment. The primary value (PV) system controls performance and learning during primary rewards, whereas the learned value (LV) system learns about conditioned stimuli. The PV system is essentially the Rescorla-Wagner/delta-rule and comprises the neurons in the ventral striatum/nucleus accumbens that inhibit DA cells. The LV system comprises the neurons in the central nucleus of the amygdala that excite DA cells. The authors show that the PVLV model can account for critical aspects of the DA firing data, making a number of clear predictions about lesion effects, several of which are consistent with existing data. For example, first- and second-order conditioning can be anatomically dissociated, which is consistent with PVLV and not TD. Overall, the model provides a biologically plausible framework for understanding the neural basis of reward learning."
            },
            "slug": "PVLV:-the-primary-value-and-learned-value-Pavlovian-O\u2019Reilly-Frank",
            "title": {
                "fragments": [],
                "text": "PVLV: the primary value and learned value Pavlovian learning algorithm."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The authors show that the PVLV model can account for critical aspects of the DA firing data, making a number of clear predictions about lesion effects, several of which are consistent with existing data."
            },
            "venue": {
                "fragments": [],
                "text": "Behavioral neuroscience"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696373"
                        ],
                        "name": "N. Chentanez",
                        "slug": "N.-Chentanez",
                        "structuredName": {
                            "firstName": "Nuttapong",
                            "lastName": "Chentanez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chentanez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6042628,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "8612afd4b9a2d52e8d0d3eec74dd80113456c632",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies."
            },
            "slug": "Intrinsically-Motivated-Learning-of-Hierarchical-of-Barto-Singh",
            "title": {
                "fragments": [],
                "text": "Intrinsically Motivated Learning of Hierarchical Collections of Skills"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650569"
                        ],
                        "name": "A. Klopf",
                        "slug": "A.-Klopf",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Klopf",
                            "middleNames": [
                                "Harry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Klopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 145091543,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "8049dc4c4e73f5a8da730a20a9a9820e31818640",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 166,
            "paperAbstract": {
                "fragments": [],
                "text": "A neuronal model of classical conditioning is proposed. The model is most easily described by contrasting it with a still influential neuronal model first analyzed by Hebb (1949). It is proposed that the Hebbian model be modified in three ways to yield a model more in accordance with animal learning phenomena. First, instead of correlating pre- and postsynaptic levels of activity, changes in pre- and postsynaptic levels of activity should be correlated to determine the changes in synaptic efficacy that represent learning. Second, instead of correlating approximately simultaneous pre- and postsynaptic signals, earlier changes in presynaptic signals should be correlated with later changes in postsynaptic signals. Third, a change in the efficacy of a synapse should be proportional to the current efficacy of the synapse, accounting for the initial positive acceleration in the S-shaped acquisition curves observed in animal learning. The resulting model, termed a drive-reinforcement model of single neuron function, suggests that nervous system activity can be understood in terms of two classes of neuronal signals: drives that are defined to be signal levels and reinforcers that are defined to be changes in signal levels. Defining drives and reinforcers in this way, in conjunction with the neuronal model, suggests a basis for a neurobiological theory of learning. The proposed neuronal model is an extension of the Sutton-Barto (1981) model, which in turn can be seen as a temporally refined extension of the Rescorla-Wagner (1972) model. It is shown that the proposed neuronal model predicts a wide range of classical conditioning phenomena, including delay and trace conditioning, conditioned and unconditioned stimulus duration and amplitude effects, partial reinforcement effects, interstimulus interval effects, second-order conditioning, conditioned inhibition, extinction, reacquisition effects, backward conditioning, blocking, overshadowing, compound conditioning, and discriminative stimulus effects. The neuronal model also eliminates some inconsistencies with the experimental evidence that occur with the Rescorla-Wagner and Sutton-Barto models. Implications of the neuronal model for animal learning theory, connectionist and neural network modeling, artificial intelligence, adaptive control theory, and adaptive signal processing are discussed. It is concluded that real-time learning mechanisms that do not require evaluative feedback from the environment are fundamental to natural intelligence and may have implications for artificial intelligence. Experimental tests of the model are suggested."
            },
            "slug": "A-neuronal-model-of-classical-conditioning-Klopf",
            "title": {
                "fragments": [],
                "text": "A neuronal model of classical conditioning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is concluded that real-time learning mechanisms that do not require evaluative feedback from the environment are fundamental to natural intelligence and may have implications for artificial intelligence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680510"
                        ],
                        "name": "T. Dean",
                        "slug": "T.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887699"
                        ],
                        "name": "Shieu-Hong Lin",
                        "slug": "Shieu-Hong-Lin",
                        "structuredName": {
                            "firstName": "Shieu-Hong",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shieu-Hong Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2843504,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "53958cff5f602a73ae8dd2512737e7beb0b60bbb",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with modeling planning problems involving uncertainty as discrete-time, finite-state stochastic automata Solving planning problems is reduced to computing policies for Markov decision processes. Classical methods for solving Markov decision processes cannot cope with the size of the state spaces for typical problems encountered in practice. As an alternative, we investigate methods that decompose global planning problems into a number of local problems solve the local problems separately and then combine the local solutions to generate a global solution. We present algorithms that decompose planning problems into smaller problems given an arbitrary partition of the state space. The local problems are interpreted as Markov decision processes and solutions to the local problems are interpreted as policies restricted to the subsets of the state space defined by the partition. One algorithm relies on constructing and solving an abstract version of the original decision problem. A second algorithm iteratively approximates parameters of the local problems to converge to an optimal solution. We show how properties of a specified partition affect the time and storage required for these algorithms."
            },
            "slug": "Decomposition-Techniques-for-Planning-in-Stochastic-Dean-Lin",
            "title": {
                "fragments": [],
                "text": "Decomposition Techniques for Planning in Stochastic Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper presents algorithms that decompose planning problems into smaller problems given an arbitrary partition of the state space and shows how properties of a specified partition affect the time and storage required for these algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155467859"
                        ],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 21
                            }
                        ],
                        "text": "Zhang and Dietterich (1995, 1996; Zhang, 1996) were motivated to apply reinforcement learning to job-shop scheduling because the design of domain-specific, heuristic algorithms can be very expensive and time-consuming."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59918844,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "8bc233c201a3cc09476aa4af02a97d926ada3db7",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation studies applying reinforcement learning algorithms to discover good domain-specific heuristics automatically for job-shop scheduling. It focuses on the NASA space shuttle payload processing problem. The problem involves scheduling a set of tasks to satisfy a set of temporal and resource constraints while also seeking to minimize the total length (makespan) of the schedule. \nThe approach described in the dissertation employs a repair-based scheduling problem space that starts with a critical-path schedule and incrementally repairs constraint violations with the goal of finding a short conflict-free schedule. The temporal difference (TD) learning algorithm TD($\\lambda$) is applied to train a neural network to learn a heuristic evaluation function for choosing repair actions over schedules. This learned evaluation function is used by a one-step lookahead search procedure to find solutions to new scheduling problems. \nSeveral important issues that affect the success and the efficiency of learning have been identified and deeply studied. These issues include schedule representation, network architectures, and learning strategies. A number of modifications to the TD($\\lambda$) algorithm are developed to improve learning performance. Learning is investigated based on both hand-engineered features and raw features. For learning from raw features, a time-delay neural network architecture is developed to extract features from irregular-length schedules. The learning approach is evaluated on synthetic problems and on problems from a NASA space shuttle payload processing task. The evaluation function is learned on small problems and then applied to solve larger problems. Both learning-based schedulers (using hand-engineered features and raw features respectively) perform better than the best existing algorithm for this task--Zweben's iterative repair method. \nIt is important to understand why TD learning works in this application. Several performance measures are employed to investigate learning behavior. We verified that TD learning works properly in capturing the evaluation function. It is concluded that TD learning along with a set of good features and a proper neural network is the key to this success. The success shows that reinforcement learning methods have the potential for quickly finding high-quality solutions to other combinatorial optimization problems."
            },
            "slug": "Reinforcement-learning-for-job-shop-scheduling-Zhang",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning for job shop scheduling"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This dissertation studies applying reinforcement learning algorithms to discover good domain-specific heuristics automatically for job-shop scheduling to show that reinforcement learning methods have the potential for quickly finding high-quality solutions to other combinatorial optimization problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809620"
                        ],
                        "name": "S. Lakshmivarahan",
                        "slug": "S.-Lakshmivarahan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Lakshmivarahan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lakshmivarahan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 361,
                                "start": 246
                            }
                        ],
                        "text": "The Tsetlin collection also includes studies of learning automata in team and game problems, which led to later work in this area using stochastic learning automata as described by Narendra and Thathachar (1974), Viswanathan and Narendra (1974), Lakshmivarahan and Narendra (1982), Narendra and Wheeler (1983), Narendra (1989), and Thathachar and Sastry (2002). Thathachar and Sastry (2011) is a more recent comprehensive account."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 246
                            }
                        ],
                        "text": "The Tsetlin collection also includes studies of learning automata in team and game problems, which led to later work in this area using stochastic learning automata as described by Narendra and Thathachar (1974), Viswanathan and Narendra (1974), Lakshmivarahan and Narendra (1982), Narendra and Wheeler (1983), Narendra (1989), and Thathachar and Sastry (2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122415672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a26d42bcb505c7294991a0e6c727ec5459290dd",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper extends recent results [Lakshmivarahan and Narendra, Math. Oper. Res., 6 (1981), pp. 379\u2013386] in two-person zero-sum sequential games in which the players use learning algorithms to update their strategies. It is assumed that neither player knows (i) the set of strategies available to the other player or (ii) the mixed strategy used by the other player or its pure realization at any stage. The outcome of the game depends on chance and the game is played sequentially. The distribution of the random outcome as a function of the pair of pure strategies chosen by the players is also, unknown to them. It is shown that if the players use a learning algorithm of, the reward-penalty type, with proper choice of certain parameters in the algorithm, the expected value of the mixed strategies for both players can be made arbitrarily close to optimal strategies."
            },
            "slug": "Learning-Algorithms-for-Two-Person-Zero-Sum-Games-A-Lakshmivarahan-Narendra",
            "title": {
                "fragments": [],
                "text": "Learning Algorithms for Two-Person Zero-Sum Stochastic Games with Incomplete Information: A Unified Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "If the players use a learning algorithm of the reward-penalty type, with proper choice of certain parameters in the algorithm, the expected value of the mixed strategies for both players can be made arbitrarily close to optimal strategies."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4319439"
                        ],
                        "name": "N. Mackintosh",
                        "slug": "N.-Mackintosh",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Mackintosh",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Mackintosh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17955262,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "9219d4cca4af0df78ac8d02ff1cb8fca0c6524a3",
            "isKey": false,
            "numCitedBy": 2577,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "According to theories of selective attention, learning about a stimulus depends on attending to that stimulus; this is represented in two-stage models by saying that subjects switch in analyzers as well as learning stimulusresponse associations. This assumption, however, is equally well represented in a formal model by the incorporation of a stimulus-specific learning-rate parameter, a, into the equations describing changes in the associative strength of stimuli. Theories of selective attention have also assumed (a) that subjects learn to attend to and ignore relevant and irrelevant stimuli (i.e., that a may increase or decrease depending on the correlation of a stimulus with reinforcement) and (b) that there is an inverse relationship between the probabilities of attending to different stimuli (i.e., that an increase in a to one stimulus is accompanied by a decrease in a to others). The first assumption is used to explain the phenomena of acquired distinctiveness and dimensional transfer, the second those of overshadowing and blocking. Although the first assumption is justified by the data, the second is not: Overshadowing and blocking are better explained by the choice of an appropriate rule for changing a, such that a decreases to stimuli that signal no change from the probability of reinforcement predicted by other stimuli."
            },
            "slug": "A-Theory-of-Attention:-Variations-in-the-of-Stimuli-Mackintosh",
            "title": {
                "fragments": [],
                "text": "A Theory of Attention: Variations in the Associability of Stimuli with Reinforcement"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Overshadowing and blocking are better explained by the choice of an appropriate rule for changing a, such that a decreases to stimuli that signal no change from the probability of reinforcement predicted by other stimuli."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145646162"
                        ],
                        "name": "Craig Boutilier",
                        "slug": "Craig-Boutilier",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Boutilier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig Boutilier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143654598"
                        ],
                        "name": "R. Dearden",
                        "slug": "R.-Dearden",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dearden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dearden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770007"
                        ],
                        "name": "M. Goldszmidt",
                        "slug": "M.-Goldszmidt",
                        "structuredName": {
                            "firstName": "Mois\u00e9s",
                            "lastName": "Goldszmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goldszmidt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6021318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3bfc039f870388491f6d89f8f2a88c0b117ee0c",
            "isKey": false,
            "numCitedBy": 455,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov decision processes (MDPs) have recently been applied to the problem of modeling decision-theoretic planning. While traditional methods for solving MDPs are often practical for small states spaces, their effectiveness for large AI planning problems is questionable. We present an algorithm, called structured policy Iteration (SPI), that constructs optimal policies without explicit enumeration of the state space. The algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm, but exploits the variable and prepositional independencies reflected in a temporal Bayesian network representation of MDPs. The principles behind SPI can be applied to any structured representation of stochastic actions, policies and value functions, and the algorithm itself can be used in conjunction with recent approximation methods."
            },
            "slug": "Exploiting-Structure-in-Policy-Construction-Boutilier-Dearden",
            "title": {
                "fragments": [],
                "text": "Exploiting Structure in Policy Construction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents an algorithm, called structured policy Iteration (SPI), that constructs optimal policies without explicit enumeration of the state space, and retains the fundamental computational steps of the commonly used modified policy iteration algorithm, but exploits the variable and prepositional independencies reflected in a temporal Bayesian network representation of MDPs."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2744613"
                        ],
                        "name": "N. Schmajuk",
                        "slug": "N.-Schmajuk",
                        "structuredName": {
                            "firstName": "Nestor",
                            "lastName": "Schmajuk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schmajuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39355814,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "3b25a530eabd0a39840c81becb980c3dc496975b",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-dynamics-of-adaptive-timing-and-temporal-Grossberg-Schmajuk",
            "title": {
                "fragments": [],
                "text": "Neural dynamics of adaptive timing and temporal discrimination during associative learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3252904"
                        ],
                        "name": "J. Andreae",
                        "slug": "J.-Andreae",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Andreae",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Andreae"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 70346544,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "be1558b852a05bf626e4736d4e12721707065913",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "STELLA:-A-scheme-for-a-learning-machine-Andreae",
            "title": {
                "fragments": [],
                "text": "STELLA: A scheme for a learning machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 132
                            }
                        ],
                        "text": "The history of ANNs as learning methods for classification or regression has passed through several stages: roughly, the Perceptron (Rosenblatt, 1962) and ADALINE (ADAptive LINear Element) (Widrow and Hoff, 1960) stage of learning by single-layer ANNs, the error-backpropagation stage (Werbos, 1974; LeCun, 1985; Parker, 1985; Rumelhart, Hinton, and Williams, 1986) of learning by multi-layer ANNs, and the current deep-learning stage with its emphasis on representation learning (e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62710001,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9b486c647916df9f8be0f8d4fc5c94c493bfaa80",
            "isKey": false,
            "numCitedBy": 1904,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light."
            },
            "slug": "PRINCIPLES-OF-NEURODYNAMICS.-PERCEPTRONS-AND-THE-OF-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons are reviewed, and some of the notation to be used in later sections are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368601"
                        ],
                        "name": "Doina Precup",
                        "slug": "Doina-Precup",
                        "structuredName": {
                            "firstName": "Doina",
                            "lastName": "Precup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doina Precup"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316271"
                        ],
                        "name": "Cosmin Paduraru",
                        "slug": "Cosmin-Paduraru",
                        "structuredName": {
                            "firstName": "Cosmin",
                            "lastName": "Paduraru",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cosmin Paduraru"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144394223"
                        ],
                        "name": "Anna Koop",
                        "slug": "Anna-Koop",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Koop",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Koop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 51685517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f99769ca3f0e3f41a239a1b58adf26bb930b9b2",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods. We develop the notion of a recognizer, a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections. We also consider target policies that are deviations from the state distribution of the behavior policy, such as potential temporally abstract options, which further reduces variance. This paper introduces recognizers and their potential advantages, then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence. Even though our algorithm is based on importance sampling, we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators."
            },
            "slug": "Off-policy-Learning-with-Options-and-Recognizers-Precup-Sutton",
            "title": {
                "fragments": [],
                "text": "Off-policy Learning with Options and Recognizers"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods is introduced and it is proved that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1363802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cacfb77e3dc8faa3cae25b3128f3b3c4c44fc266",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to solving nonlinear control problems is illustrated by means of a layered associative network composed of adaptive elements capable of reinforcement learning. The first layer adaptively develops a representation in terms of which the second layer can solve the problem linearly. The adaptive elements comprising the network employ a novel type of learning rule whose properties, we argue, are essential to the adaptive behavior of the layered network. The behavior of the network is illustrated by means of a spatial learning problem that requires the formation of nonlinear associations. We argue that this approach to nonlinearity can be extended to a large class of nonlinear control problems."
            },
            "slug": "Synthesis-of-nonlinear-control-surfaces-by-a-search-Barto-Anderson",
            "title": {
                "fragments": [],
                "text": "Synthesis of nonlinear control surfaces by a layered associative search network"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is argued that this approach to nonlinearity can be extended to a large class of nonlinear control problems."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214276"
                        ],
                        "name": "J. Boyan",
                        "slug": "J.-Boyan",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Boyan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Boyan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 616729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6aa631d44a7f2cbb3ab07f3b993f4e5c5c4150b",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "TD.\u03bb/ is a popular family of algorithms for approximate policy evaluation in large MDPs. TD.\u03bb/ works by incrementally updating the value function after each observed transition. It has two major drawbacks: it may make inefficient use of data, and it requires the user to manually tune a stepsize schedule for good performance. For the case of linear value function approximations and \u03bb = 0, the Least-Squares TD (LSTD) algorithm of Bradtke and Barto (1996, Machine learning, 22:1\u20133, 33\u201357) eliminates all stepsize parameters and improves data efficiency.This paper updates Bradtke and Barto's work in three significant ways. First, it presents a simpler derivation of the LSTD algorithm. Second, it generalizes from \u03bb = 0 to arbitrary values of \u03bb; at the extreme of \u03bb = 1, the resulting new algorithm is shown to be a practical, incremental formulation of supervised linear regression. Third, it presents a novel and intuitive interpretation of LSTD as a model-based reinforcement learning technique."
            },
            "slug": "Technical-Update:-Least-Squares-Temporal-Difference-Boyan",
            "title": {
                "fragments": [],
                "text": "Technical Update: Least-Squares Temporal Difference Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper updates Bradtke and Barto's work in three significant ways: first, it presents a simpler derivation of the LSTD algorithm; second, it generalizes from \u03bb = 0 to arbitrary values of \u03bb; at the extreme of \u03ba, the resulting new algorithm is shown to be a practical, incremental formulation of supervised linear regression."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40423197"
                        ],
                        "name": "R. Suri",
                        "slug": "R.-Suri",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Suri",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Suri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4133213"
                        ],
                        "name": "J. Bargas",
                        "slug": "J.-Bargas",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Bargas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bargas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795076"
                        ],
                        "name": "M. Arbib",
                        "slug": "M.-Arbib",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Arbib",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Arbib"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18895842,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "8ae26fddc83f3fbdca49f9b602640605c4582fb2",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 119,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-functions-of-striatal-dopamine-modulation-Suri-Bargas",
            "title": {
                "fragments": [],
                "text": "Modeling functions of striatal dopamine modulation in learning and planning"
            },
            "venue": {
                "fragments": [],
                "text": "Neuroscience"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720664"
                        ],
                        "name": "Pierre-Yves Oudeyer",
                        "slug": "Pierre-Yves-Oudeyer",
                        "structuredName": {
                            "firstName": "Pierre-Yves",
                            "lastName": "Oudeyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Yves Oudeyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791091"
                        ],
                        "name": "F. Kaplan",
                        "slug": "F.-Kaplan",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Kaplan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kaplan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2762516"
                        ],
                        "name": "V. Hafner",
                        "slug": "V.-Hafner",
                        "structuredName": {
                            "firstName": "Verena",
                            "lastName": "Hafner",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hafner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4464601,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c9ef00e1ce135cf6ae566c58e80a8a9f9f73e3cc",
            "isKey": false,
            "numCitedBy": 1101,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Exploratory activities seem to be intrinsically rewarding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Experimental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology"
            },
            "slug": "Intrinsic-Motivation-Systems-for-Autonomous-Mental-Oudeyer-Kaplan",
            "title": {
                "fragments": [],
                "text": "Intrinsic Motivation Systems for Autonomous Mental Development"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The mechanism of Intelligent Adaptive Curiosity is presented, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress, thus permitting autonomous mental development."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Evolutionary Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 62
                            }
                        ],
                        "text": "7 This section is largely based on Takahashi, Schoenbaum, and Niv (2008) and Niv (2009). Barto (1995) and Houk, Adams, and Barto (1995) first speculated about possible implementations of actor-critic algorithms in the basal ganglia."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 62
                            }
                        ],
                        "text": "7 This section is largely based on Takahashi, Schoenbaum, and Niv (2008) and Niv (2009)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2294833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "049c6719ac3470e03316d75b3d605b79eccd24e5",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new algorithm, Prioritized Sweeping, for e cient prediction and control of stochastic Markov systems. Incremental learning methods such as Temporal Di erencing and Qlearning have fast real time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of di erent stochastic optimal control problems. It successfully solves large state-space real time problems with which other methods have"
            },
            "slug": "Prioritized-Sweeping-:-Reinforcement-Learning-with-Moore-Atkeson",
            "title": {
                "fragments": [],
                "text": "Prioritized Sweeping : Reinforcement Learning with Less Data and Less Real Time"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Prioritized Sweeping successfully solves large state-space real time problems with which other methods have failed, and is compared with other reinforcement learning schemes for a number of stochastic optimal control problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778862"
                        ],
                        "name": "W. Whitt",
                        "slug": "W.-Whitt",
                        "structuredName": {
                            "firstName": "Ward",
                            "lastName": "Whitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Whitt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 166
                            }
                        ],
                        "text": ") There is now a fairly extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207235292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b33c0fbfa60c93aa28066563fd2566c26538c498",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A general procedure is presented for constructing and analyzing approximations of dynamic programming models. The models considered are the monotone contraction operator models of Denardo Denardo, E. V. 1967. Contraction mappings in the theory underlying dynamic programming. SIAM Rev.9 165--177., which include Markov decision processes and stochastic games with a criterion of discounted present value over an infinite horizon plus many finite-stage dynamic programs. The approximations are typically achieved by replacing the original state and action spaces by subsets. Tight bounds are obtained for the distances between the optimal return function in the original model and 1 the extension of the optimal return function in the approximate model and 2 the return function associated with the extension of an optimal policy in the approximate model. Conditions are also given under which the sequence of bounds associated with a sequence of approximating models converges to zero."
            },
            "slug": "Approximations-of-Dynamic-Programs,-I-Whitt",
            "title": {
                "fragments": [],
                "text": "Approximations of Dynamic Programs, I"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The models considered are the monotone contraction operator models of Denardo Denardo, E. V. 1967, which include Markov decision processes and stochastic games with a criterion of discounted present value over an infinite horizon plus many finite-stage dynamic programs."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6211302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a24508e65e599b5b20c33af96dbe7017d5caca37",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Probably the most important problem in machine learning is the preliminary biasing of a learner's hypothesis space so that it is small enough to ensure good generalisation from reasonable training sets, yet large enough that it contains a good solution to the problem being learnt. In this paper a mechanism for {\\em automatically} learning or biasing the learner's hypothesis space is introduced. It works by first learning an appropriate {\\em internal representation} for a learning environment and then using that representation to bias the learner's hypothesis space for the learning of future tasks drawn from the same environment. \nAn internal representation must be learnt by sampling from {\\em many similar tasks}, not just a single task as occurs in ordinary machine learning. It is proved that the number of examples $m$ {\\em per task} required to ensure good generalisation from a representation learner obeys $m = O(a+b/n)$ where $n$ is the number of tasks being learnt and $a$ and $b$ are constants. If the tasks are learnt independently ({\\em i.e.} without a common representation) then $m=O(a+b)$. It is argued that for learning environments such as speech and character recognition $b\\gg a$ and hence representation learning in these environments can potentially yield a drastic reduction in the number of examples required per task. It is also proved that if $n = O(b)$ (with $m=O(a+b/n)$) then the representation learnt will be good for learning novel tasks from the same environment, and that the number of examples required to generalise well on a novel task will be reduced to $O(a)$ (as opposed to $O(a+b)$ if no representation is used). \nIt is shown that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results."
            },
            "slug": "Learning-internal-representations-Baxter",
            "title": {
                "fragments": [],
                "text": "Learning internal representations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is proved that the number of examples required to ensure good generalisation from a representation learner obeys and that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2440747"
                        ],
                        "name": "R. French",
                        "slug": "R.-French",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "French",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. French"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2691726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2722b9e5ab8da95f03e578bb65879c452c105385",
            "isKey": false,
            "numCitedBy": 1084,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Catastrophic-forgetting-in-connectionist-networks-French",
            "title": {
                "fragments": [],
                "text": "Catastrophic forgetting in connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37814588"
                        ],
                        "name": "M. Puterman",
                        "slug": "M.-Puterman",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Puterman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Puterman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24313010"
                        ],
                        "name": "M. C. Shin",
                        "slug": "M.-C.-Shin",
                        "structuredName": {
                            "firstName": "Moon",
                            "lastName": "Shin",
                            "middleNames": [
                                "Chirl"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Shin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122017624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7b608eff19a85f85b34cda3ecb66d0a3e572583",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study a class of modified policy iteration algorithms for solving Markov decision problems. These correspond to performing policy evaluation by successive approximations. We discuss the relationship of these algorithms to Newton-Kantorovich iteration and demonstrate their covergence. We show that all of these algorithms converge at least as quickly as successive approximations and obtain estimates of their rates of convergence. An analysis of the computational requirements of these algorithms suggests that they may be appropriate for solving problems with either large numbers of actions, large numbers of states, sparse transition matrices, or small discount rates. These algorithms are compared to policy iteration, successive approximations, and Gauss-Seidel methods on large randomly generated test problems."
            },
            "slug": "Modified-Policy-Iteration-Algorithms-for-Discounted-Puterman-Shin",
            "title": {
                "fragments": [],
                "text": "Modified Policy Iteration Algorithms for Discounted Markov Decision Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A class of modified policy iteration algorithms for solving Markov decision problems correspond to performing policy evaluation by successive approximations and it is shown that all of these algorithms converge at least as quickly as successive approxIMations and estimates of their rates of convergence are obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 123
                            }
                        ],
                        "text": "The class of methods for solving optimal control problems by solving this equation came to be known as dynamic programming (Bellman, 1957a)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 24158615,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a4361a4bd93e207fb4cf263a63c24ead39cc2076",
            "isKey": false,
            "numCitedBy": 7710,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Little has been done in the study of these intriguing questions, and I do not wish to give the impression that any extensive set of ideas exists that could be called a \"theory.\" What is quite surprising, as far as the histories of science and philosophy are concerned, is that the major impetus for the fantastic growth of interest in brain processes, both psychological and physiological, has come from a device, a machine, the digital computer. In dealing with a human being and a human society, we enjoy the luxury of being irrational, illogical, inconsistent, and incomplete, and yet of coping. In operating a computer, we must meet the rigorous requirements for detailed instructions and absolute precision. If we understood the ability of the human mind to make effective decisions when confronted by complexity, uncertainty, and irrationality then we could use computers a million times more effectively than we do. Recognition of this fact has been a motivation for the spurt of research in the field of neurophysiology. The more we study the information processing aspects of the mind, the more perplexed and impressed we become. It will be a very long time before we understand these processes sufficiently to reproduce them. In any case, the mathematician sees hundreds and thousands of formidable new problems in dozens of blossoming areas, puzzles galore, and challenges to his heart's content. He may never resolve some of these, but he will never be bored. What more can he ask?"
            },
            "slug": "Dynamic-Programming-Bellman",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The more the authors study the information processing aspects of the mind, the more perplexed and impressed they become, and it will be a very long time before they understand these processes sufficiently to reproduce them."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17063561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ae6fbda6080f3ed6ec29a8d62a8c8941d7263d4",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Overcoming-Incomplete-Perception-with-Utile-Memory-McCallum",
            "title": {
                "fragments": [],
                "text": "Overcoming Incomplete Perception with Utile Distinction Memory"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145671619"
                        ],
                        "name": "R. Ratcliff",
                        "slug": "R.-Ratcliff",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18556305,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "591b52d24eb95f5ec3622b814bc91ac872acda9e",
            "isKey": false,
            "numCitedBy": 713,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer connectionist models of memory based on the encoder model using the backpropagation learning rule are evaluated. The models are applied to standard recognition memory procedures in which items are studied sequentially and then tested for retention. Sequential learning in these models leads to 2 major problems. First, well-learned information is forgotten rapidly as new information is learned. Second, discrimination between studied items and new items either decreases or is nonmonotonic as a function of learning. To address these problems, manipulations of the network within the multilayer model and several variants of the multilayer model were examined, including a model with prelearned memory and a context model, but none solved the problems. The problems discussed provide limitations on connectionist models applied to human memory and in tasks where information to be learned is not all available during learning."
            },
            "slug": "Connectionist-models-of-recognition-memory:-imposed-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Connectionist models of recognition memory: constraints imposed by learning and forgetting functions."
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The problems discussed provide limitations on connectionist models applied to human memory and in tasks where information to be learned is not all available during learning."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1960921"
                        ],
                        "name": "Razvan V. Florian",
                        "slug": "Razvan-V.-Florian",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Florian",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan V. Florian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14876236,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1debadf7e6c580a280919df9b9cef5bc52b50100",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The persistent modification of synaptic efficacy as a function of the relative timing of pre- and postsynaptic spikes is a phenomenon known as spike-timing-dependent plasticity (STDP). Here we show that the modulation of STDP by a global reward signal leads to reinforcement learning. We first derive analytically learning rules involving reward-modulated spike-timing-dependent synaptic and intrinsic plasticity, by applying a reinforcement learning algorithm to the stochastic spike response model of spiking neurons. These rules have several features common to plasticity mechanisms experimentally found in the brain. We then demonstrate in simulations of networks of integrate-and-fire neurons the efficacy of two simple learning rules involving modulated STDP. One rule is a direct extension of the standard STDP model (modulated STDP), and the other one involves an eligibility trace stored at each synapse that keeps a decaying memory of the relationships between the recent pairs of pre- and postsynaptic spike pairs (modulated STDP with eligibility trace). This latter rule permits learning even if the reward signal is delayed. The proposed rules are able to solve the XOR problem with both rate coded and temporally coded input and to learn a target output firing-rate pattern. These learning rules are biologically plausible, may be used for training generic artificial spiking neural networks, regardless of the neural model used, and suggest the experimental investigation in animals of the existence of reward-modulated STDP."
            },
            "slug": "Reinforcement-Learning-Through-Modulation-of-Florian",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning Through Modulation of Spike-Timing-Dependent Synaptic Plasticity"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the modulation of STDP by a global reward signal leads to reinforcement learning, and analytically learning rules involving reward-modulated spike-timing-dependent synaptic and intrinsic plasticity are derived, which may be used for training generic artificial spiking neural networks, regardless of the neural model used."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800855"
                        ],
                        "name": "A. Nedi\u0107",
                        "slug": "A.-Nedi\u0107",
                        "structuredName": {
                            "firstName": "Angelia",
                            "lastName": "Nedi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nedi\u0107"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9887595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7475a08f483de23d8af6f1a63b427ec7554f8c2e",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider policy evaluation algorithms within the context of infinite-horizon dynamic programming problems with discounted cost. We focus on discrete-time dynamic systems with a large number of states, and we discuss two methods, which use simulation, temporal differences, and linear cost function approximation. The first method is a new gradient-like algorithm involving least-squares subproblems and a diminishing stepsize, which is based on the \u03bb-policy iteration method of Bertsekas and Ioffe. The second method is the LSTD(\u03bb) algorithm recently proposed by Boyan, which for \u03bb=0 coincides with the linear least-squares temporal-difference algorithm of Bradtke and Barto. At present, there is only a convergence result by Bradtke and Barto for the LSTD(0) algorithm. Here, we strengthen this result by showing the convergence of LSTD(\u03bb), with probability 1, for every \u03bb \u2208 [0, 1]."
            },
            "slug": "Least-Squares-Policy-Evaluation-Algorithms-with-Nedi\u0107-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Least Squares Policy Evaluation Algorithms with Linear Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new gradient-like algorithm involving least-squares subproblems and a diminishing stepsize, which is based on the \u03bb-policy iteration method of Bertsekas and Ioffe is proposed, and the convergence of LSTD(\u03bb), with probability 1, for every \u03bb \u2208 [0, 1]."
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Event Dyn. Syst."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889692"
                        ],
                        "name": "S. I. Gallant",
                        "slug": "S.-I.-Gallant",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gallant",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. I. Gallant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5492125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d345bb0656e5bfede6bb963d861e592d761d5c55",
            "isKey": false,
            "numCitedBy": 739,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Network Learning and Expert Systems is the first book to present a unified and in-depth development of neural network learning algorithms and neural network expert systems. Especially suitable for students and researchers in computer science, engineering, and psychology, this text and reference provides a systematic development of neural network learning algorithms from a computational perspective, coupled with an extensive exploration of neural network expert systems which shows how the power of neural network learning can be harnessed to generate expert systems automatically. Features include a comprehensive treatment of the standard learning algorithms (with many proofs), along with much original research on algorithms and expert systems. Additional chapters explore constructive algorithms, introduce computational learning theory, and focus on expert system applications to noisy and redundant problems. For students there is a large collection of exercises, as well as a series of programming projects that lead to an extensive neural network software package. All of the neural network models examined can be implemented using standard programming languages on a microcomputer."
            },
            "slug": "Neural-network-learning-and-expert-systems-Gallant",
            "title": {
                "fragments": [],
                "text": "Neural network learning and expert systems"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This text and reference provides a systematic development of neural network learning algorithms from a computational perspective, coupled with an extensive exploration of Neural Network expert systems which shows how the power of neuralnetwork learning can be harnessed to generate expert systems automatically."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145324953"
                        ],
                        "name": "B. Seymour",
                        "slug": "B.-Seymour",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Seymour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Seymour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1346915,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a8eb5b829d20e75d208bb519a109dbeeaa2f8f90",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-misbehavior-of-value-and-the-discipline-of-the-Dayan-Niv",
            "title": {
                "fragments": [],
                "text": "The misbehavior of value and the discipline of the will"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762653"
                        ],
                        "name": "Chun-Shin Lin",
                        "slug": "Chun-Shin-Lin",
                        "structuredName": {
                            "firstName": "Chun-Shin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Shin Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109023611"
                        ],
                        "name": "Hyongsuk Kim",
                        "slug": "Hyongsuk-Kim",
                        "structuredName": {
                            "firstName": "Hyongsuk",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyongsuk Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34678347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7d851e184925792c1065138a56ce5631e491455",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A technique that integrates the cerebellar model articulation controller (CMAC) into a self-learning control scheme developed by A.G. Barto et al. (IEEE Trans. Syst. Man., Cybern., vol.SMC-13, p.834-46, Sept./Oct. 1983) is presented. Instead of reserving one input line (as a memory) for each quantized state, the integrated technique distributively stores learned information; this reduces the required memory and makes the self-learning control scheme applicable to problems of larger size. CMAC's capability with regard to information interpolation also helps improve the learning speed."
            },
            "slug": "CMAC-based-adaptive-critic-self-learning-control-Lin-Kim",
            "title": {
                "fragments": [],
                "text": "CMAC-based adaptive critic self-learning control"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Instead of reserving one input line (as a memory) for each quantized state, the integrated technique distributively stores learned information; this reduces the required memory and makes the self-learning control scheme applicable to problems of larger size."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49196452"
                        ],
                        "name": "P. Brouwer",
                        "slug": "P.-Brouwer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brouwer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brouwer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2329172,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "340a337665bbb9c5aea5a5ec1b33a6c09048cf9d",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "An associative memory system is presented which does not require a \u201cteacher\u201d to provide the desired associations. For each input key it conducts a search for the output pattern which optimizes an external payoff or reinforcement signal. The associative search network (ASN) combines pattern recognition and function optimization capabilities in a simple and effective way. We define the associative search problem, discuss conditions under which the associative search network is capable of solving it, and present results from computer simulations. The synthesis of sensory-motor control surfaces is discussed as an example of the associative search problem."
            },
            "slug": "Associative-search-network:-A-reinforcement-memory-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Associative search network: A reinforcement learning associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An associative memory system is presented which does not require a \u201cteacher\u201d to provide the desired associations and conducts a search for the output pattern which optimizes an external payoff or reinforcement signal."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743637"
                        ],
                        "name": "J. W. Silverstein",
                        "slug": "J.-W.-Silverstein",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Silverstein",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Silverstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52412616"
                        ],
                        "name": "Stephen A. Ritz",
                        "slug": "Stephen-A.-Ritz",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Ritz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen A. Ritz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109214855"
                        ],
                        "name": "Randall S. Jones",
                        "slug": "Randall-S.-Jones",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Jones",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randall S. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17526697,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "00c6914dab0fb75c0fe5c8d8ad57d726223b7d9b",
            "isKey": false,
            "numCitedBy": 937,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": "A previously proposed model for memory based on neurophysiolo gical considerations is reviewed. We assume that (a) nervous system activity is usefully represented as the set of simultaneous individual neuron activities in a group of neurons; (b) different memory traces make use of the same synapses; and (c) synapses associate two patterns of neural activity by incrementing synaptic connectivity proportionally to the product of pre- and postsynaptic activity, forming a matrix of synaptic connectivities. We extend this model by (a) introducing positive feedback of a set of neurons onto itself and (b) allowing the individual neurons to saturate. A hybrid model, partly analog and partly binary, arises. The system has certain characteristics reminiscent of analysis by distinctive features. Next, we apply the model to \"categorical perception.\" Finally, we discuss probability learning. The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy."
            },
            "slug": "Distinctive-features,-categorical-perception,-and-a-Anderson-Silverstein",
            "title": {
                "fragments": [],
                "text": "Distinctive features, categorical perception, and probability learning: some applications of a neural model"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy and applies the model to \"categorical perception\" and probability learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 771841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2dd697bbe99c2ec71c807580a00f7e723cc20ae",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Appropriate bias is widely viewed as the key to efficient learning and generalization. I present a new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience. The IDBD algorithm is developed for the case of a simple, linear learning system--the LMS or delta rule with a separate learning-rate parameter for each input. The IDBD algorithm adjusts the learning-rate parameters, which are an important form of bias for this system. Because bias in this approach is adapted based on previous learning experience, the appropriate test beds are drifting or non-stationary learning tasks. For particular tasks of this type, I show that the IDBD algorithm performs better than ordinary LMS and in fact finds the optimal learning rates. The IDBD algorithm extends and improves over prior work by Jacobs and by me in that it is fully incremental and has only a single free parameter. This paper also extends previous work by presenting a derivation of the IDBD algorithm as gradient descent in the space of learning-rate parameters. Finally, I offer a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation."
            },
            "slug": "Adapting-Bias-by-Gradient-Descent:-An-Incremental-Sutton",
            "title": {
                "fragments": [],
                "text": "Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience, and a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50675526"
                        ],
                        "name": "C. Person",
                        "slug": "C.-Person",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Person",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Person"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4324169,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "cda1c82b9d48b5cc94ae5bda33e864d95b3053dd",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "RECENT work has identified a neuron with widespread projections to odour processing regions of the honeybee brain whose activity represents the reward value of gustatory stimuli1,2. We have constructed a model of bee foraging in uncertain environments based on this type of neuron and a predictive form of hebbian synaptic plasticity. The model uses visual input from a simulated three-dimensional world and accounts for a wide range of experiments on bee learning during foraging, including risk aversion. The predictive model shows how neuromodulatory influences can be used to bias actions and control synaptic plasticity in a way that goes beyond standard correlational mechanisms. Although several behavioural models of conditioning in bees have been proposed3\u00967, this model is based on the neural substrate and was tested in a simulation of bee flight."
            },
            "slug": "Bee-foraging-in-uncertain-environments-using-Montague-Dayan",
            "title": {
                "fragments": [],
                "text": "Bee foraging in uncertain environments using predictive hebbian learning"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A model of bee foraging in uncertain environments based on a neuron with widespread projections to odour processing regions of the honeybee brain and a predictive form of hebbian synaptic plasticity is constructed, showing how neuromodulatory influences can be used to bias actions and control synaptic Plasticity in a way that goes beyond standard correlational mechanisms."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51966268"
                        ],
                        "name": "W. Estes",
                        "slug": "W.-Estes",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Estes",
                            "middleNames": [
                                "Kaye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Estes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 80
                            }
                        ],
                        "text": "In psychology, bandit problems have played roles in statistical learning theory (e.g., Bush and Mosteller, 1955; Estes, 1950)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 49018264,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c6e7f227db97cc95ef860333cd852832965907cd",
            "isKey": false,
            "numCitedBy": 559,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Improved experimental techniques for the study of conditioning and simple discrimination learning enable the present day investigator to obtain data which are sufficiently orderly and reproducible to support exact quantitative predictions of behavior. Analogy with other sciences suggests that full utilization of these techniques in the analysis of learning processes will depend to some extent upon a comparable refinement of theoretical concepts and methods. The necessary interplay between theory and experiment has been hindered, however, by the fact that none of the many current theories of learning commands general agreement among researchers. It seems likely that progress toward a common frame of reference will be slow so long as most theories are built around verbally defined hypothetical constructs which are not susceptible to unequivocal verification. While awaiting resolution of the many apparent disparities among competing theories, it may be advantageous to systematize well established empirical relationships at a peripheral, statistical level of analysis. The possibility of agreement on a theoretical framework, at least in certain intensively studied areas, may be maximized by defining concepts in terms of experimentally manipulable variables, and developing the consequences of assumptions by strict mathematical reasoning. This essay will introduce a series of"
            },
            "slug": "Toward-a-Statistical-Theory-of-Learning.-Estes",
            "title": {
                "fragments": [],
                "text": "Toward a Statistical Theory of Learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 115956069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5161939fe984f05a447845484cdac641c49322b8",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In contrast to recent work by Boyan and Moore, I have obtained excellent results applying conventional VFA methods to control tasks such as \u201cMountain Car\u201d and \u201cAcrobot\u201d. The difference in our results is due to the form of function approximator and the nature of the training. I used a local function approximator known as a CMAC\u2014essentially a linear learner using a sparse, coarse-coded representation of the state. Although not a complete solution, I argue that this approach solves the \u201ccurse of dimensionality\u201d as well as one can expect, and enables VFA systems to generalize as well as other artificial learning systems. Also, I used TD(\u03bb) and trained on state transitions from actual, experienced trajectories. A theorem by Dayan assures stability of TD(\u03bb) when trained using such trajectory distributions. Gordon and Tsitsiklis and Van Roy have recently shown that TD(\u03bb) can be unstable when trained with other distributions. Finally, I present a small compendium of results all showing much faster learning when \u03bb is slightly less than 1."
            },
            "slug": "On-The-Virtues-of-Linear-Learning-and-Trajectory-Sutton",
            "title": {
                "fragments": [],
                "text": "On The Virtues of Linear Learning and Trajectory Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is argued that this approach solves the \u201ccurse of dimensionality\u201d as as one can expect, and enables VFA systems to generalize as well as other artificial learning systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 109
                            }
                        ],
                        "text": "Some neuroscience models developed at this time are well interpreted in terms of temporaldifference learning (Hawkins and Kandel, 1984; Byrne, Gingrich, and Baxter, 1990; Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986; Friston et al., 1994), although in most cases there was no historical connection."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12082205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c65c239bf87cad222bf665b70dcf75db03ae72c",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A systematic study of the necessary and sufficient ingredients of a successful model of classical conditioning is presented. Models are constructed along the lines proposed by Gelperin, Hopfield, and Tank, who showed that many conditioning phenomena could be reproduced in a model using non-trivial distributed representations of the sensory stimuli. The additional phenomena of extinction and blocking are found to be obtainable by generalizing the Hebbian learning algorithm, rather than by additional complications in the hardware. The most successful algorithms have a minimal number of adjustable parameters, and require only local-time information about the level of postsynaptic activity. The proper behavior of these algorithms is verified by both simple analytic arguments and by direct numerical simulation. Certain detailed assumptions concerning the distributed sensory representations are also found to have a surprising degree of importance."
            },
            "slug": "Simple-neural-models-of-classical-conditioning-Tesauro",
            "title": {
                "fragments": [],
                "text": "Simple neural models of classical conditioning"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A systematic study of the necessary and sufficient ingredients of a successful model of classical conditioning is presented, and certain detailed assumptions concerning the distributed sensory representations are found to have a surprising degree of importance."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97306284"
                        ],
                        "name": "K. Fu",
                        "slug": "K.-Fu",
                        "structuredName": {
                            "firstName": "King-Sun",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 87
                            }
                        ],
                        "text": "3: One-dimensional Fourier cosine-basis features xi, i = 1, 2, 3, 4, for approximating functions over the interval [0, 1]. After Konidaris et al. (2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 184
                            }
                        ],
                        "text": "In the 1960s the terms \u201creinforcement\u201d and \u201creinforcement learning\u201d were used in the engineering literature for the first time to describe engineering uses of trial-and-error learning (e.g., Waltz and Fu, 1965; Mendel, 1966; Fu, 1970; Mendel and McClaren, 1970)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61407723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ab3822d80eed53380a86d797991a09a9ab566b3",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The basic concept of learning control is introduced. The following five learning schemes are briefly reviewed: 1) trainable controllers using pattern classifiers, 2) reinforcement learning control systems, 3) Bayesian estimation, 4) stochastic approximation, and 5) stochastic automata models. Potential applications and problems for further research in learning control are outlined."
            },
            "slug": "Learning-control-systems--Review-and-outlook-Fu",
            "title": {
                "fragments": [],
                "text": "Learning control systems--Review and outlook"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The basic concept of learning control is introduced, and the following five learning schemes are briefly reviewed: 1) trainable controllers using pattern classifiers, 2) reinforcement learning control systems, 3) Bayesian estimation, 4) stochastic approximation, and 5) Stochastic automata models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6304315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "812b49a877b98941f258f7c2bfc8e890963142bd",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Gain adaptation algorithms for neural networks typically adjust learning rates by monitoring the correlation between successive gradients. Here we discuss the limitations of this approach, and develop an alternative by extending Sutton''s work on linear systems to the general, nonlinear case. The resulting online algorithms are computationally little more expensive than other acceleration techniques, do not assume statistical independence between successive training patterns, and do not require an arbitrary smoothing parameter. In our benchmark experiments, they consistently outperform other acceleration methods, and show remarkable robustness when faced with non-i.i.d. sampling of the input space."
            },
            "slug": "Local-Gain-Adaptation-in-Stochastic-Gradient-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Local Gain Adaptation in Stochastic Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The limitations of this approach are discussed, and an alternative is developed by extending Sutton''s work on linear systems to the general, nonlinear case, and the resulting online algorithms are computationally little more expensive than other acceleration techniques, and do not assume statistical independence between successive training patterns."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14746271"
                        ],
                        "name": "A. Machado",
                        "slug": "A.-Machado",
                        "structuredName": {
                            "firstName": "Armando",
                            "lastName": "Machado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Machado"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13816662,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "607135569aedaded773ac3f036f5bff113194dba",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This study presents a dynamic model of how animals learn to regulate their behavior under time-based reinforcement schedules. The model assumes a serial activation of behavioral states during the interreinforcement interval, an associative process linking the states with the operant response, and a rule mapping the activation of the states and their associative strength onto response rate or probability. The model fits data sets from fixed-interval schedules, the peak procedure, mixed fixed-interval schedules, and the bisection of temporal intervals. The major difficulties of the model came from experiments that suggest that under some conditions animals may time 2 intervals independently and simultaneously."
            },
            "slug": "Learning-the-temporal-dynamics-of-behavior.-Machado",
            "title": {
                "fragments": [],
                "text": "Learning the temporal dynamics of behavior."
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A dynamic model of how animals learn to regulate their behavior under time-based reinforcement schedules and a rule mapping the activation of the states and their associative strength onto response rate or probability are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754233"
                        ],
                        "name": "E. Izhikevich",
                        "slug": "E.-Izhikevich",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Izhikevich",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Izhikevich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1754971,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "794b81bad2611cf09eb871f5c964fdfec4ed5b36",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "In Pavlovian and instrumental conditioning, reward typically comes seconds after reward-triggering actions, creating an explanatory conundrum known as \"distal reward problem\": How does the brain know what firing patterns of what neurons are responsible for the reward if 1) the patterns are no longer there when the reward arrives and 2) all neurons and synapses are active during the waiting period to the reward? Here, we show how the conundrum is resolved by a model network of cortical spiking neurons with spike-timing-dependent plasticity (STDP) modulated by dopamine (DA). Although STDP is triggered by nearly coincident firing patterns on a millisecond timescale, slow kinetics of subsequent synaptic plasticity is sensitive to changes in the extracellular DA concentration during the critical period of a few seconds. Random firings during the waiting period to the reward do not affect STDP and hence make the network insensitive to the ongoing activity-the key feature that distinguishes our approach from previous theoretical studies, which implicitly assume that the network be quiet during the waiting period or that the patterns be preserved until the reward arrives. This study emphasizes the importance of precise firing patterns in brain dynamics and suggests how a global diffusive reinforcement signal in the form of extracellular DA can selectively influence the right synapses at the right time."
            },
            "slug": "Solving-the-distal-reward-problem-through-linkage-Izhikevich",
            "title": {
                "fragments": [],
                "text": "Solving the distal reward problem through linkage of STDP and dopamine signaling"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This study emphasizes the importance of precise firing patterns in brain dynamics and suggests how a global diffusive reinforcement signal in the form of extracellular DA can selectively influence the right synapses at the right time."
            },
            "venue": {
                "fragments": [],
                "text": "BMC Neuroscience"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055652033"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "Jiirgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17874844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94db34f4b68189bfcba22beab33ee3b54f10b876",
            "isKey": false,
            "numCitedBy": 609,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a four-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an artificial nondeterministic environment demonstrates that the system can be superior to previous model-building control systems, which do not address the problem of modeling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model.<<ETX>>"
            },
            "slug": "Curious-model-building-control-systems-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Curious model-building control systems"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment, based on Watkins' Q-learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1991 IEEE International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4282090"
                        ],
                        "name": "S. Hampson",
                        "slug": "S.-Hampson",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Hampson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hampson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63985095,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "8b68cdf8d5835939abfed55399f4048db331aec3",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A neurally plausible model of adaptive behavior is developed in the Boolean domain. Behavior is modeled as the application of specific operators in response to patterns of internal and external features. Behavioral completeness requires that any stimulus (feature pattern) potentially be able to trigger any response (set of operators). Learning completeness requires that any stimulus-response mapping be learnable. Goal based behavior evaluation determines what the appropriate mapping is. The resulting problem is simple enough to be formally approached, but general enough to address a number of interesting issues. Appropriate structures and learning processes at the neuron and assembly level are developed. The problems of learning speed, noise rejection and shared memory are addressed in some detail."
            },
            "slug": "A-neural-model-of-adaptive-behavior-Hampson",
            "title": {
                "fragments": [],
                "text": "A neural model of adaptive behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A neurally plausible model of adaptive behavior is developed in the Boolean domain that is simple enough to be formally approached, but general enough to address a number of interesting issues."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7089976,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8302c2a57c98c08d634aa2212a87cd9009ecc2fe",
            "isKey": false,
            "numCitedBy": 562,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "An inverted pendulum is simulated as a control task with the goal of learning to balance the pendulum with no a priori knowledge of the dynamics. In contrast to other applications of neural networks to the inverted pendulum task, performance feedback is assumed to be unavailable on each step, appearing only as a failure signal when the pendulum falls or reaches the bounds of a horizontal track. To solve this task, the controller must deal with issues of delayed performance evaluation, learning under uncertainty, and the learning of nonlinear functions. Reinforcement and temporal-difference learning methods are presented that deal with these issues to avoid unstable conditions and balance the pendulum.<<ETX>>"
            },
            "slug": "Learning-to-control-an-inverted-pendulum-using-Anderson",
            "title": {
                "fragments": [],
                "text": "Learning to control an inverted pendulum using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Control Systems Magazine"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152128706"
                        ],
                        "name": "J. Pearce",
                        "slug": "J.-Pearce",
                        "structuredName": {
                            "firstName": "John M.",
                            "lastName": "Pearce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144298942"
                        ],
                        "name": "G. Hall",
                        "slug": "G.-Hall",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 434421,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "36303957caa2ebda4d04b6f25334d4fe9bf4b3cf",
            "isKey": false,
            "numCitedBy": 1168,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Several formal models of excitatory classical conditioning are reviewed. It is suggested that a central problem for all of them is the explanation of cases in which learning does not occur in spite of the fact that the conditioned stimulus is a signal for the reinforcer. We propose a new model that deals with this problem by specifying that certain procedures cause a conditioned stimulus (CS) to lose effectiveness; in particular, we argue that a CS will lose associability when its consequences are accurately predicted. In contrast to other current models, the effectiveness of the reinforcer remains constant throughout conditioning. The second part of the article presents a reformulation of the nature of the learning produced by inhibitory-conditioning procedures and a discussion of the way in which such learning can be accommodated within the model outlined for excitatory learning."
            },
            "slug": "A-model-for-Pavlovian-learning:-variations-in-the-Pearce-Hall",
            "title": {
                "fragments": [],
                "text": "A model for Pavlovian learning: variations in the effectiveness of conditioned but not of unconditioned stimuli."
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new model is proposed that deals with the explanation of cases in which learning does not occur in spite of the fact that the conditioned stimulus is a signal for the reinforcer by specifying that certain procedures cause a conditioned stimulus to lose effectiveness."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3349598,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
            "isKey": false,
            "numCitedBy": 3556,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "This article introduces a class of incremental learning procedures specialized for prediction \u2013 that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage."
            },
            "slug": "Learning-to-Predict-by-the-Methods-of-Temporal-Sutton",
            "title": {
                "fragments": [],
                "text": "Learning to Predict by the Methods of Temporal Differences"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This article introduces a class of incremental learning procedures specialized for prediction \u2013 that is, for using past experience with an incompletely known system to predict its future behavior \u2013 and proves their convergence and optimality for special cases and relation to supervised-learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13411,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 233
                            }
                        ],
                        "text": "In fact, training a network with k+1 hidden layers can actually result in poorer performance than training a network with k hidden layers, even though the deeper network can represent all the functions that the shallower network can (Bengio, 2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 2
                            }
                        ],
                        "text": ", Bengio, Courville, and Vincent, 2012; Goodfellow, Bengio, and Courville, 2016). Examples of the many books on ANNs are Haykin (1994), Bishop (1995), and Ripley (2007)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207178999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60ff004dde5c13ec53087872cfcdd12e85beb57",
            "isKey": false,
            "numCitedBy": 7558,
            "numCiting": 345,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks."
            },
            "slug": "Learning-Deep-Architectures-for-AI-Bengio",
            "title": {
                "fragments": [],
                "text": "Learning Deep Architectures for AI"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer modelssuch as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 66
                            }
                        ],
                        "text": "Details of this and related algorithms are provided in many texts (e.g., Widrow and Stearns, 1985; Bishop, 1995; Duda and Hart, 1973)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36887493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd4849072ca939e4e6a86a3d2b56b71593eafd18",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-Memory-Based-Dynamic-Programming-Peng",
            "title": {
                "fragments": [],
                "text": "Efficient Memory-Based Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Daw and Shohamy (2008) proposed that while dopamine signaling connects well to habitual, or modelfree, behavior, other processes are involved in goal-directed, or model-based, behavior."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16385268,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "767ed9b4c974411b46c445bd3e235515760ecc65",
            "isKey": false,
            "numCitedBy": 2048,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "A broad range of neural and behavioral data suggests that the brain contains multiple systems for behavioral choice, including one associated with prefrontal cortex and another with dorsolateral striatum. However, such a surfeit of control raises an additional choice problem: how to arbitrate between the systems when they disagree. Here, we consider dual-action choice systems from a normative perspective, using the computational theory of reinforcement learning. We identify a key trade-off pitting computational simplicity against the flexible and statistically efficient use of experience. The trade-off is realized in a competition between the dorsolateral striatal and prefrontal systems. We suggest a Bayesian principle of arbitration between them according to uncertainty, so each controller is deployed when it should be most accurate. This provides a unifying account of a wealth of experimental evidence about the factors favoring dominance by either system."
            },
            "slug": "Uncertainty-based-competition-between-prefrontal-Daw-Niv",
            "title": {
                "fragments": [],
                "text": "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work considers dual-action choice systems from a normative perspective, and suggests a Bayesian principle of arbitration between them according to uncertainty, so each controller is deployed when it should be most accurate."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35186838"
                        ],
                        "name": "J. Nie",
                        "slug": "J.-Nie",
                        "structuredName": {
                            "firstName": "Junhong",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16370420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6cbf0863880e89e56f35b778ec2c3619bd280c1",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the fundamental issues in the operation of a mobile communication system is the assignment of channels to cells and to calls. Since the number of channels allocated to a mobile communication system is limited, efficient utilization of these communication channels by using efficient channel assignment strategies is not only desirable but also imperative. This paper presents a novel approach to solving the dynamic channel assignment (DCA) problem by using a form of realtime reinforcement learning known as Q-learning in conjunction with neural network representation. Instead of relying on a known teacher, the system is designed to learn an optimal channel assignment policy by directly interacting with the mobile communication environment. The performance of the Q-learning-based DCA was examined by extensive simulation studies on a 49-cell mobile communication system under various conditions. Comparative studies with the fixed channel assignment (FCA) scheme and one of the best dynamic channel assignment strategies, MAXAVAIL, have revealed that the proposed approach is able to perform better than the FCA in various situations and capable of achieving a performance similar to that achieved by the MAXIAVIAL, but with a significantly reduced computational complexity."
            },
            "slug": "A-dynamic-channel-assignment-policy-through-Nie-Haykin",
            "title": {
                "fragments": [],
                "text": "A dynamic channel assignment policy through Q-learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel approach to solving the dynamic channel assignment (DCA) problem by using a form of realtime reinforcement learning known as Q-learning in conjunction with neural network representation, capable of achieving a performance similar to that achieved by the MAXIAVIAL, but with a significantly reduced computational complexity."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11622133"
                        ],
                        "name": "J. Hollerman",
                        "slug": "J.-Hollerman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Hollerman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hollerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 184
                            }
                        ],
                        "text": "However, at the later time when the reward is expected but omitted, the TD error is negative but dopamine neuron activity does not drop below baseline in the way the TD model predicts (Hollerman and Schultz, 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7785929,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "50cd666b67505616bee0515a594316e99fbb805d",
            "isKey": false,
            "numCitedBy": 1093,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Many behaviors are affected by rewards, undergoing long-term changes when rewards are different than predicted but remaining unchanged when rewards occur exactly as predicted. The discrepancy between reward occurrence and reward prediction is termed an 'error in reward prediction'. Dopamine neurons in the substantia nigra and the ventral tegmental area are believed to be involved in reward-dependent behaviors. Consistent with this role, they are activated by rewards, and because they are activated more strongly by unpredicted than by predicted rewards they may play a role in learning. The present study investigated whether monkey dopamine neurons code an error in reward prediction during the course of learning. Dopamine neuron responses reflected the changes in reward prediction during individual learning episodes; dopamine neurons were activated by rewards during early trials, when errors were frequent and rewards unpredictable, but activation was progressively reduced as performance was consolidated and rewards became more predictable. These neurons were also activated when rewards occurred at unpredicted times and were depressed when rewards were omitted at the predicted times. Thus, dopamine neurons code errors in the prediction of both the occurrence and the time of rewards. In this respect, their responses resemble the teaching signals that have been employed in particularly efficient computational learning models."
            },
            "slug": "Dopamine-neurons-report-an-error-in-the-temporal-of-Hollerman-Schultz",
            "title": {
                "fragments": [],
                "text": "Dopamine neurons report an error in the temporal prediction of reward during learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Dopamine neuron responses reflected the changes in reward prediction during individual learning episodes; dopamine neurons were activated by rewards during early trials, but activation was progressively reduced as performance was consolidated and rewards became more predictable."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806116"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15537575,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f8473ba18c05fc7efa000c799b925e3d242fb11e",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-theories-of-conditioning-in-a-changing-Courville-Daw",
            "title": {
                "fragments": [],
                "text": "Bayesian theories of conditioning in a changing world"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46856445"
                        ],
                        "name": "J. A. Bryson",
                        "slug": "J.-A.-Bryson",
                        "structuredName": {
                            "firstName": "Jr.",
                            "lastName": "Bryson",
                            "middleNames": [
                                "A.E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. A. Bryson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Chapman and Kaelbling (1991) and Tan (1991) adapted decision-tree methods for learning value functions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122869337,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "789db4f0277ee25685c414771b4a96fbc8300d69",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimal control had its origins in the calculus of variations in the 17th century. The calculus of variations was developed further in the 18th century by Euler and Lagrange and in the 19th century by Legendre, Jacobi, Hamilton, and Weierstrass. In the early 20th century, Bolza and Bliss put the final touches of rigor on the subject. In 1957, Bellman gave a new view of Hamilton-Jacobi theory which he called dynamic programming, essentially a nonlinear feedback control scheme. McShane (1939) and Pontryagin (1962) extended the calculus of variations to handle control variable inequality constraints, the latter enunciating his elegant maximum principle. The truly enabling element for use of optimal control theory was the digital computer, which became available commercially in the 1950s. In the 1980s research began, and continues today, on making optimal feedback logic more robust to variations in the plant and disturbance models; one element of this research is worst-case and H-infinity control, which developed out of differential game theory."
            },
            "slug": "Optimal-control-1950-to-1985-Bryson",
            "title": {
                "fragments": [],
                "text": "Optimal control-1950 to 1985"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293717"
                        ],
                        "name": "L. Abbott",
                        "slug": "L.-Abbott",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Abbott",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Abbott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14072069,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4eb35c07a0d6162ac09c0269b473ef464d920e0d",
            "isKey": false,
            "numCitedBy": 3290,
            "numCiting": 249,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory. The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site."
            },
            "slug": "Theoretical-Neuroscience:-Computational-and-of-Dayan-Abbott",
            "title": {
                "fragments": [],
                "text": "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143637056"
                        ],
                        "name": "P. Sastry",
                        "slug": "P.-Sastry",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Sastry",
                            "middleNames": [
                                "Shanti"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sastry"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118505797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "951cf0285a158bc11caa4711896c6f2d7b0e91b0",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction.- 1.1 Machine Intelligence and Learning.- 1.2 Learning Automata.- 1.3 The Finite Action Learning Automaton (FALA).- 1.3.1 The Automaton.- 1.3.2 The Random Environment.- 1.3.3 Operation of FALA.- 1.4 Some Classical Learning Algorithms.- 1.4.1 Linear Reward-Inaction (LR?I) Algorithm.- 1.4.2 Other Linear Algorithms.- 1.4.3 Estimator Algorithms.- 1.4.4 Simulation Results.- 1.5 The Discretized Probability FALA.- 1.5.1 DLR?I Algorithm.- 1.5.2 Discretized Pursuit Algorithm.- 1.6 The Continuous Action Learning Automaton (CALA).- 1.6.1 Analysis of the Algorithm.- 1.6.2 Simulation Results.- 1.6.3 Another Continuous Action Automaton.- 1.7 The Generalized Learning Automaton (GLA).- 1.7.1 Learning Algorithm.- 1.7.2 An Example.- 1.8 The Parameterized Learning Automaton (PLA).- 1.8.1 Learning Algorithm.- 1.9 Multiautomata Systems.- 1.10 Supplementary Remarks.- 2. Games of Learning Automata.- 2.1 Introduction.- 2.2 A Multiple Payoff Stochastic Game of Automata.- 2.2.1 The Learning Algorithm.- 2.3 Analysis of the Automata Game Algorithm.- 2.3.1 Analysis of the Approximating ODE.- 2.4 Game with Common Payoff.- 2.5 Games of FALA.- 2.5.1 Common Payoff Games of FALA.- 2.5.2 Pursuit Algorithm for a Team of FALA.- 2.5.3 Other Types of Games.- 2.6 Common Payoff Games of CALA.- 2.6.1 Stochastic Approximation Algorithms and CALA.- 2.7 Applications.- 2.7.1 System Identification.- 2.7.2 Learning Conjunctive Concepts.- 2.8 Discussion.- 2.9 Supplementary Remarks.- 3. Feedforward Networks.- 3.1 Introduction.- 3.2 Networks of FALA.- 3.3 The Learning Model.- 3.3.1 G-Environment.- 3.3.2 The Network.- 3.3.3 Network Operation.- 3.4 The Learning Algorithm.- 3.5 Analysis.- 3.6 Extensions.- 3.6.1 Other Network Structures.- 3.6.2 Other Learning Algorithms.- 3.7 Convergence to the Global Maximum.- 3.7.1 The Network.- 3.7.2 The Global Learning Algorithm.- 3.7.3 Analysis of the Global Algorithm.- 3.8 Networks of GLA.- 3.9 Discussion.- 3.10 Supplementary Remarks.- 4. Learning Automata for Pattern Classification.- 4.1 Introduction.- 4.2 Pattern Recognition.- 4.3 Common Payoff Game of Automata for PR.- 4.3.1 Pattern Classification with FALA.- 4.3.2 Pattern Classification with CALA.- 4.3.3 Simulations.- 4.4 Automata Network for Pattern Recognition.- 4.4.1 Simulations.- 4.4.2 Network of Automata for Learning Global Maximum.- 4.5 Decision Tree Classifiers.- 4.5.1 Learning Decision Trees using GLA and CALA.- 4.5.2 Learning Piece-wise Linear Functions.- 4.6 Discussion.- 4.7 Supplementary Remarks.- 5. Parallel Operation of Learning Automata.- 5.1 Introduction.- 5.2 Parallel Operation of FALA.- 5.2.1 Analysis.- 5.2.2 ?-optimality.- 5.2.3 Speed of Convergence and Module Size.- 5.2.4 Simulation Studies.- 5.3 Parallel Operation of CALA.- 5.4 Parallel Pursuit Algorithm.- 5.4.1 Simulation Studies.- 5.5 General Procedure.- 5.6 Parallel Operation of Games of FALA.- 5.6.1 Analysis.- 5.6.2 Common Payoff Game.- 5.7 Parallel Operation of Networks of FALA.- 5.7.1 Analysis.- 5.7.2 Modules of Parameterized Learning Automata (PLA).- 5.7.3 Modules of Generalized Learning Automata (GLA).- 5.7.4 Pattern Classification Example.- 5.8 Discussion.- 5.9 Supplementary Remarks.- 6. Some Recent Applications.- 6.1 Introduction.- 6.2 Supervised Learning of Perceptual Organization in Computer Vision.- 6.3 Distributed Control of Broadcast Communication Networks.- 6.4O ther Applications.- 6.5 Discussion.- Epilogue.- Appendices.- A The ODE Approach to Analysis of Learning Algorithms.- A.I Introduction.- A.2 Derivation of the ODE Approximation.- A.2.1 Assumptions.- A.2.2 Analysis.- A.3 Approximating ODEs for Some Automata Algorithms.- A.3.2 The CALA Algorithm.- A.3.3 Automata Team Algorithms.- A.4 Relaxing the Assumptions.- B Proofs of Convergence for Pursuit Algorithm.- B.1 Proof of Theorem 1.1.- B.2 Proof of Theorem 5.7.- C Weak Convergence and SDE Approximations.- C.I Introduction.- C.2 Weak Convergence.- C.3 Convergence to SDE.- C.3.1 Application to Global Algorithms.- C.4 Convergence to ODE.- References."
            },
            "slug": "Networks-of-Learning-Automata:-Techniques-for-Thathachar-Sastry",
            "title": {
                "fragments": [],
                "text": "Networks of Learning Automata: Techniques for Online Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work focuses on the development of a model for a parallel operation of the Finite Action Learning Automaton (FALA) and its applications in Pattern Classification and Decision Tree Classifiers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1985966"
                        ],
                        "name": "W. Lovejoy",
                        "slug": "W.-Lovejoy",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Lovejoy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lovejoy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121258294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b631510618b13c32a87ac134f11dcb22cec8c9a1",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A partially observed Markov decision process (POMDP) is a generalization of a Markov decision process that allows for incomplete information regarding the state of the system. The significant applied potential for such processes remains largely unrealized, due to an historical lack of tractable solution methodologies. This paper reviews some of the current algorithmic alternatives for solving discrete-time, finite POMDPs over both finite and infinite horizons. The major impediment to exact solution is that, even with a finite set of internal system states, the set of possible information states is uncountably infinite. Finite algorithms are theoretically available for exact solution of the finite horizon problem, but these are computationally intractable for even modest-sized problems. Several approximation methodologies are reviewed that have the potential to generate computationally feasible, high precision solutions."
            },
            "slug": "A-survey-of-algorithmic-methods-for-partially-Lovejoy",
            "title": {
                "fragments": [],
                "text": "A survey of algorithmic methods for partially observed Markov decision processes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Several approximation methodologies are reviewed that have the potential to generate computationally feasible, high precision solutions for solving discrete-time, finite POMDPs over both finite and infinite horizons."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801949"
                        ],
                        "name": "J. Deutsch",
                        "slug": "J.-Deutsch",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Deutsch",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deutsch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 110
                            }
                        ],
                        "text": "Results of functional imaging experiments with human subjects in the outcome-devaluation setting by Valentin, Dickinson, and O\u2019Doherty (2007) suggest that the orbitofrontal cortex (OFC) is an important component of goal-directed choice."
                    },
                    "intents": []
                }
            ],
            "corpusId": 144472653,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "a48c43586d4b8b51407a56acd74c0f23d40710de",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "When an animal solves a novel problem without trial and error, a psychologist tends to call the behaviour insightful. A relatively simple machine resting on new principles, capable of this and learning, has been constructed by the writer in order to demonstrate his tentative explanation of this and other aspects of animal behaviour. The first retains information and utilises it in accordance with the goal set. The second is a trolley which is guided by a pulse which the first part transmits; it also has bumpers which cause it reflexly to steer away from any obstacle and to turn out of corners by reversing one of the motors which drive each of the two main wheels separately; and i t has a light mounted on it which is thrown forward at a wide angle. The third part comprises photocells, which act as receptors and are attached to the walls of the maze which the trolley is required to learn."
            },
            "slug": "A-Machine-with-Insight-Deutsch",
            "title": {
                "fragments": [],
                "text": "A Machine with Insight"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38756714"
                        ],
                        "name": "K. Gurney",
                        "slug": "K.-Gurney",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gurney",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gurney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750570"
                        ],
                        "name": "T. Prescott",
                        "slug": "T.-Prescott",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Prescott",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Prescott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3544623"
                        ],
                        "name": "P. Redgrave",
                        "slug": "P.-Redgrave",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Redgrave",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Redgrave"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9863815,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "35034cf8114589ec2971a02e57e29a50f89b5680",
            "isKey": false,
            "numCitedBy": 368,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.\u2002In a companion paper a new functional architecture was proposed for the basal ganglia based on the premise that these brain structures play a central role in behavioural action selection. The current paper quantitatively describes the properties of the model using analysis and simulation. The decomposition of the basal ganglia into selection and control pathways is supported in several ways. First, several elegant features are exposed \u2013 capacity scaling, enhanced selectivity and synergistic dopamine modulation \u2013 which might be expected to exist in a well designed action selection mechanism. The discovery of these features also lends support to the computational premise of selection that underpins our model. Second, good matches between model globus pallidus external segment output and globus pallidus internal segment and substantia nigra reticulata area output, and neurophysiological data, have been found which are indicative of common architectural features in the model and biological basal ganglia. Third, the behaviour of the model as a signal selection mechanism has parallels with some kinds of action selection observed in animals under various levels of dopaminergic modulation."
            },
            "slug": "A-computational-model-of-action-selection-in-the-of-Gurney-Prescott",
            "title": {
                "fragments": [],
                "text": "A computational model of action selection in the basal ganglia. II. Analysis and simulation of behaviour"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 185
                            }
                        ],
                        "text": "6 Automatic methods for adapting the step-size parameter include RMSprop (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2015), stochastic meta-descent methods such as DeltaBar-Delta (Jacobs, 1988), its incremental generalization (Sutton, 1992b, c; Mahmood et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9947500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9ef2995e8e1bd57a74343073219364811c2ace0",
            "isKey": false,
            "numCitedBy": 1988,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Increased-rates-of-convergence-through-learning-Jacobs",
            "title": {
                "fragments": [],
                "text": "Increased rates of convergence through learning rate adaptation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 117
                            }
                        ],
                        "text": "Based on TD-Gammon\u2019s success and further analysis, the best human players now play these positions as TD-Gammon does (Tesauro, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6023746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ed59f49c1bb7de06cfa2a9467d5efb535103277",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome."
            },
            "slug": "Temporal-difference-learning-and-TD-Gammon-Tesauro",
            "title": {
                "fragments": [],
                "text": "Temporal difference learning and TD-Gammon"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731282"
                        ],
                        "name": "Benjamin Van Roy",
                        "slug": "Benjamin-Van-Roy",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Van Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7353554,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ceee9569717991607b399d9a6890f1dcb9541ac0",
            "isKey": false,
            "numCitedBy": 1441,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "We present new results about the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of a Markov chain using linear function approximators. The algorithm we analyze performs on-line updating of a parameter vector during a single endless trajectory of an aperiodic irreducible finite state Markov chain. Results include convergence (with probability 1), a characterization of the limit of convergence, and a bound on the resulting approximation error. In addition to establishing new and stronger results than those previously available, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning. Furthermore, we discuss the implications of two counter-examples with regards to the Significance of on-line updating and linearly parameterized function approximators."
            },
            "slug": "Analysis-of-Temporal-Diffference-Learning-with-Tsitsiklis-Roy",
            "title": {
                "fragments": [],
                "text": "Analysis of Temporal-Diffference Learning with Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "New results about the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of a Markov chain using linear function approximators are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2439060"
                        ],
                        "name": "C. Buhusi",
                        "slug": "C.-Buhusi",
                        "structuredName": {
                            "firstName": "Catalin",
                            "lastName": "Buhusi",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buhusi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2744613"
                        ],
                        "name": "N. Schmajuk",
                        "slug": "N.-Schmajuk",
                        "structuredName": {
                            "firstName": "Nestor",
                            "lastName": "Schmajuk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schmajuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35353200,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cb914e7c6613f33125b73f35356c35972cb2a582",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Timing-in-simple-conditioning-and-occasion-setting:-Buhusi-Schmajuk",
            "title": {
                "fragments": [],
                "text": "Timing in simple conditioning and occasion setting: a neural network approach"
            },
            "venue": {
                "fragments": [],
                "text": "Behavioural Processes"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44560099,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "99fa74130727064680a2f27f78f1bc72131a73fb",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "Some forms of synaptic plasticity depend on the temporal coincidence of presynaptic activity and postsynaptic response. This requirement is consistent with the Hebbian, or correlational, type of learning rule used in many neural network models. Recent evidence suggests that synaptic plasticity may depend in part on the production of a membrane permeant-diffusible signal so that spatial volume may also be involved in correlational learning rules. This latter form of synaptic change has been called volume learning. In both Hebbian and volume learning rules, interaction among synaptic inputs depends on the degree of coincidence of the inputs and is otherwise insensitive to their exact temporal order. Conditioning experiments and psychophysical studies have shown, however, that most animals are highly sensitive to the temporal order of the sensory inputs. Although these experiments assay the behavior of the entire animal or perceptual system, they raise the possibility that nervous systems may be sensitive to temporally ordered events at many spatial and temporal scales. We suggest here the existence of a new class of learning rule, called a predictive Hebbian learning rule, that is sensitive to the temporal ordering of synaptic inputs. We show how this predictive learning rule could act at single synaptic connections and through diffuse neuromodulatory systems."
            },
            "slug": "The-predictive-brain:-temporal-coincidence-and-in-Montague-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The predictive brain: temporal coincidence and temporal order in synaptic learning mechanisms."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new class of learning rule is suggested, called a predictive Hebbian learning rule, that is sensitive to the temporal ordering of synaptic inputs and shown how this predictive learning rule could act at single synaptic connections and through diffuse neuromodulatory systems."
            },
            "venue": {
                "fragments": [],
                "text": "Learning & memory"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153691324"
                        ],
                        "name": "Xin Wang",
                        "slug": "Xin-Wang",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11674229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e27c86fee8f59acbebf157a1a86e0f42c6ec1278",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We present three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning. One formulation is based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves. All formulations attempt to minimize the number of support vectors while fitting the data. Experiments in a difficult, synthetic maze problem show that all three formulations give excellent performance, but the advantage formulation is much easier to train. Unlike policy gradient methods, the kernel methods described here can easily adjust the complexity of the function approximator to fit the complexity of the value function."
            },
            "slug": "Batch-Value-Function-Approximation-via-Support-Dietterich-Wang",
            "title": {
                "fragments": [],
                "text": "Batch Value Function Approximation via Support Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning are presented, one based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702229"
                        ],
                        "name": "C. Fiorillo",
                        "slug": "C.-Fiorillo",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Fiorillo",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fiorillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3315497"
                        ],
                        "name": "P. Tobler",
                        "slug": "P.-Tobler",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Tobler",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tobler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2363255,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "91bf62ef88c5f1308b060ae684923c8bf4db004a",
            "isKey": false,
            "numCitedBy": 1867,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Uncertainty is critical in the measure of information and in assessing the accuracy of predictions. It is determined by probability P, being maximal at P = 0.5 and decreasing at higher and lower probabilities. Using distinct stimuli to indicate the probability of reward, we found that the phasic activation of dopamine neurons varied monotonically across the full range of probabilities, supporting past claims that this response codes the discrepancy between predicted and actual reward. In contrast, a previously unobserved response covaried with uncertainty and consisted of a gradual increase in activity until the potential time of reward. The coding of uncertainty suggests a possible role for dopamine signals in attention-based learning and risk-taking behavior."
            },
            "slug": "Discrete-Coding-of-Reward-Probability-and-by-Fiorillo-Tobler",
            "title": {
                "fragments": [],
                "text": "Discrete Coding of Reward Probability and Uncertainty by Dopamine Neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Using distinct stimuli to indicate the probability of reward, it was found that the phasic activation of dopamine neurons varied monotonically across the full range of probabilities, supporting past claims that this response codes the discrepancy between predicted and actual reward."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14540458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b18833db0de9393d614d511e60821a1504fc6cd1",
            "isKey": false,
            "numCitedBy": 883,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris."
            },
            "slug": "A-Natural-Policy-Gradient-Kakade",
            "title": {
                "fragments": [],
                "text": "A Natural Policy Gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work provides a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space and shows drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2004600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82479b544924ee734fb22f9dce78aace0f90cd3c",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Issues involved in implementing robot learning for a challenging dynamic task are explored in this article, using a case study from robot juggling. We use a memory-based local modeling approach (locally weighted regression) to represent a learned model of the task to be performed. Statistical tests are given to examine the uncertainty of a model, to optimize its prediction quality, and to deal with noisy and corrupted data. We develop an exploration algorithm that explicitly deals with prediction accuracy requirements during exploration. Using all these ingredients in combination with methods from optimal control, our robot achieves fast real-time learning of the task within 40 to 100 trials.<<ETX>>"
            },
            "slug": "Robot-juggling:-implementation-of-memory-based-Schaal-Atkeson",
            "title": {
                "fragments": [],
                "text": "Robot juggling: implementation of memory-based learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A memory-based local modeling approach (locally weighted regression) is used to represent a learned model of the task to be performed, and an exploration algorithm is developed that explicitly deals with prediction accuracy requirements during exploration."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Control Systems"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47200559"
                        ],
                        "name": "John Rust",
                        "slug": "John-Rust",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Rust",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Rust"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 26
                            }
                        ],
                        "text": "At about the same time as Samuel's work, Bellman and Dreyfus (1959) proposed using function approximation methods with DP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 166
                            }
                        ],
                        "text": ") There is now a fairly extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 473,
                                "start": 248
                            }
                        ],
                        "text": "Dynamic programming has been extensively developed in the last four decades, including extensions to partially observable MDPs (surveyed by Lovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), approximation methods (surveyed by Rust, 1996), and asynchronous methods (Bertsekas, 1982, 1983). Many excellent modern treatments of dynamic programming are available (e.g., Bertsekas, 1995; Puterman, 1994; Ross, 1983; and Whittle, 1982, 1983). Bryson (1996) provides a detailed authoritative history of optimal control."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12984000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67de670898ebe2c982dab0429704370cd8d9a327",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 266,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-dynamic-programming-in-economics-Rust",
            "title": {
                "fragments": [],
                "text": "Numerical dynamic programming in economics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51704,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7307228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a7de0669fd835b2efcab97c7d3dc28ea7a1e6a3",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that states of a dynamical system can be usefully represented by multi-step, action-conditional predictions of future observations. State representations that are grounded in data in this way may be easier to learn, generalize better, and be less dependent on accurate prior models than, for example, POMDP state representations. Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear specialization of the predictive approach with the state representations used in POMDPs and in k-order Markov models. Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls). We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model."
            },
            "slug": "Predictive-Representations-of-State-Littman-Sutton",
            "title": {
                "fragments": [],
                "text": "Predictive Representations of State"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls) and it is shown that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3252904"
                        ],
                        "name": "J. Andreae",
                        "slug": "J.-Andreae",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Andreae",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Andreae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31541291"
                        ],
                        "name": "P. Cashin",
                        "slug": "P.-Cashin",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Cashin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cashin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 126
                            }
                        ],
                        "text": "This system included an internal model of the world and, later, an \u201cinternal monologue\u201d to deal with problems of hidden state (Andreae, 1969a)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60797800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c6fcb9003001bf6a02328a642a089e2b3737d58",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-learning-machine-with-monologue-Andreae-Cashin",
            "title": {
                "fragments": [],
                "text": "A learning machine with monologue"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404817"
                        ],
                        "name": "J. Holland",
                        "slug": "J.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58781161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4279db68b16e20fbc56f9d41980a950191d30a",
            "isKey": false,
            "numCitedBy": 38743,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)"
            },
            "slug": "Adaptation-in-natural-and-artificial-systems-Holland",
            "title": {
                "fragments": [],
                "text": "Adaptation in natural and artificial systems"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Names of founding work in the area of Adaptation and modiication, which aims to mimic biological optimization, and some (Non-GA) branches of AI."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696373"
                        ],
                        "name": "N. Chentanez",
                        "slug": "N.-Chentanez",
                        "structuredName": {
                            "firstName": "Nuttapong",
                            "lastName": "Chentanez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chentanez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7241207,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "12d6fde053e2c7174a76fe1bbdb97dd039a3b662",
            "isKey": false,
            "numCitedBy": 708,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy."
            },
            "slug": "Intrinsically-Motivated-Reinforcement-Learning-Singh-Barto",
            "title": {
                "fragments": [],
                "text": "Intrinsically Motivated Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731421"
                        ],
                        "name": "Xi-Ren Cao",
                        "slug": "Xi-Ren-Cao",
                        "structuredName": {
                            "firstName": "Xi-Ren",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi-Ren Cao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21babbb7816abc59e07245786040b99d70562140",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-learning-and-optimization-A-approach-Cao",
            "title": {
                "fragments": [],
                "text": "Stochastic learning and optimization - A sensitivity-based approach"
            },
            "venue": {
                "fragments": [],
                "text": "Annu. Rev. Control."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5151405"
                        ],
                        "name": "R. Herrnstein",
                        "slug": "R.-Herrnstein",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Herrnstein",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Herrnstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 257
                            }
                        ],
                        "text": "Thorndike later modified the law to better account for accumulating data on animal learning (such as differences between the effects of reward and punishment), and the law in its various forms has generated considerable controversy among learning theorists (e.g., see Gallistel, 2005; Herrnstein, 1970; Kimble, 1961, 1967; Mazur, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10657442,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "41c627ff4d1a46846515765402fdcdbb7487ff7f",
            "isKey": false,
            "numCitedBy": 2709,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Experiments on single, multiple, and concurrent schedules of reinforcement find various correlations between the rate of responding and the rate or magnitude of reinforcement. For concurrent schedules (i.e., simultaneous choice procedures), there is matching between the relative frequencies of responding and reinforcement; for multiple schedules (i.e., successive discrimination procedures), there are contrast effects between responding in each component and reinforcement in the others; and for single schedules, there are a host of increasing monotonic relations between the rate of responding and the rate of reinforcement. All these results, plus several others, can be accounted for by a coherent system of equations, the most general of which states that the absolute rate of any response is proportional to its associated relative reinforcement."
            },
            "slug": "On-the-law-of-effect.-Herrnstein",
            "title": {
                "fragments": [],
                "text": "On the law of effect."
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "Experiments on single, multiple, and concurrent schedules of reinforcement find various correlations between the rate of responding and the rate or magnitude of reinforcement, which can be accounted for by a coherent system of equations."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the experimental analysis of behavior"
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390067049"
                        ],
                        "name": "R. O\u2019Reilly",
                        "slug": "R.-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "O\u2019Reilly",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. O\u2019Reilly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071345269"
                        ],
                        "name": "Michael J. Frank",
                        "slug": "Michael-J.-Frank",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Frank",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Frank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8912485,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "71b93e4fd006c09ad127d27e963021fcbdbd95d8",
            "isKey": false,
            "numCitedBy": 909,
            "numCiting": 157,
            "paperAbstract": {
                "fragments": [],
                "text": "The prefrontal cortex has long been thought to subserve both working memory (the holding of information online for processing) and executive functions (deciding how to manipulate working memory and perform processing). Although many computational models of working memory have been developed, the mechanistic basis of executive function remains elusive, often amounting to a homunculus. This article presents an attempt to deconstruct this homunculus through powerful learning mechanisms that allow a computational model of the prefrontal cortex to control both itself and other brain areas in a strategic, task-appropriate manner. These learning mechanisms are based on subcortical structures in the midbrain, basal ganglia, and amygdala, which together form an actor-critic architecture. The critic system learns which prefrontal representations are task relevant and trains the actor, which in turn provides a dynamic gating mechanism for controlling working memory updating. Computationally, the learning mechanism is designed to simultaneously solve the temporal and structural credit assignment problems. The model's performance compares favorably with standard backpropagation-based temporal learning mechanisms on the challenging 1-2-AX working memory task and other benchmark working memory tasks."
            },
            "slug": "Making-Working-Memory-Work:-A-Computational-Model-O\u2019Reilly-Frank",
            "title": {
                "fragments": [],
                "text": "Making Working Memory Work: A Computational Model of Learning in the Prefrontal Cortex and Basal Ganglia"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article presents an attempt to deconstruct this homunculus through powerful learning mechanisms that allow a computational model of the prefrontal cortex to control both itself and other brain areas in a strategic, task-appropriate manner."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35062651"
                        ],
                        "name": "M. Corbin",
                        "slug": "M.-Corbin",
                        "structuredName": {
                            "firstName": "Malcolm",
                            "lastName": "Corbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Corbin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62584854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9eed0f983e871f8259c25e05c6184b031b9bda00",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-operators-and-automatic-adaptive-controllers:-Witten-Corbin",
            "title": {
                "fragments": [],
                "text": "Human operators and automatic adaptive controllers: A comparative study on a particular control task"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2488500"
                        ],
                        "name": "P. Apicella",
                        "slug": "P.-Apicella",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Apicella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Apicella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50337944"
                        ],
                        "name": "T. Ljungberg",
                        "slug": "T.-Ljungberg",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Ljungberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ljungberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7903097,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "8163c4ccfad54c63aa840b25a3a377f39001d2e4",
            "isKey": false,
            "numCitedBy": 1255,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The present investigation had two aims: (1) to study responses of dopamine neurons to stimuli with attentional and motivational significance during several steps of learning a behavioral task, and (2) to study the activity of dopamine neurons during the performance of cognitive tasks known to be impaired after lesions of these neurons. Monkeys that had previously learned a simple reaction time task were trained to perform a spatial delayed response task via two intermediate tasks. During the learning of each new task, a total of 25% of 76 dopamine neurons showed phasic responses to the delivery of primary liquid reward, whereas only 9% of 163 neurons responded to this event once task performance was established. This produced an average population response during but not after learning of each task. Reward responses during learning were significantly more numerous and pronounced in area A10, as compared to areas A8 and A9. Dopamine neurons also showed phasic responses to the two conditioned stimuli. These were the instruction cue, which was the first stimulus in each trial and indicated the target of the upcoming arm movement (58% of 76 neurons during and 44% of 163 neurons after learning), and the trigger stimulus, which was a conditioned incentive stimulus predicting reward and eliciting a saccadic eye movement and an arm reaching movement (38% of neurons during and 40% after learning). None of the dopamine neurons showed sustained activity in the delay between the instruction and trigger stimuli that would resemble the activity of neurons in dopamine terminal areas, such as the striatum and frontal cortex. Thus, dopamine neurons respond phasically to alerting external stimuli with behavioral significance whose detection is crucial for learning and performing delayed response tasks. The lack of sustained activity suggests that dopamine neurons do not encode representational processes, such as working memory, expectation of external stimuli or reward, or preparation of movement. Rather, dopamine neurons are involved with transient changes of impulse activity in basic attentional and motivational processes underlying learning and cognitive behavior."
            },
            "slug": "Responses-of-monkey-dopamine-neurons-to-reward-and-Schultz-Apicella",
            "title": {
                "fragments": [],
                "text": "Responses of monkey dopamine neurons to reward and conditioned stimuli during successive steps of learning a delayed response task"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Dopamine neurons respond phasically to alerting external stimuli with behavioral significance whose detection is crucial for learning and performing delayed response tasks."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123721998,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f6a140e441dc62dc9b10fab690e32ae8a609e507",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-apparent-conflict-between-estimation-and-survey-Witten",
            "title": {
                "fragments": [],
                "text": "The apparent conflict between estimation and control\u2014a survey of the two-armed bandit problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145913312"
                        ],
                        "name": "P. Doyle",
                        "slug": "P.-Doyle",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Doyle",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doyle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34296841"
                        ],
                        "name": "J. Snell",
                        "slug": "J.-Snell",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Snell",
                            "middleNames": [
                                "Laurie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Snell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Duff (1995) showed how it is possible to learn Gittins indices for bandit problems through reinforcement learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 115
                            }
                        ],
                        "text": "The soap bubble example is a classical Dirichlet problem whose Monte Carlo solution was first proposed by Kakutani (1945; see Hersh and Griego, 1969; Doyle and Snell, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 119671461,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6d67a36f5fae72da77cdfa4c69c92b34ce27a9f4",
            "isKey": false,
            "numCitedBy": 1583,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability theory, like much of mathematics, is indebted to physics as a source of problems and intuition for solving these problems. Unfortunately, the level of abstraction of current mathematics often makes it difficult for anyone but an expert to appreciate this fact. In this work we will look at the interplay of physics and mathematics in terms of an example where the mathematics involved is at the college level. The example is the relation between elementary electric network theory and random walks. Central to the work will be Polya\u2019s beautiful theorem that a random walker on an infinite street network in d-dimensional space is bound to return to the starting point when d = 2, but has a positive probability of escaping to infinity without returning to the starting point when d \u2265 3. Our goal will be to interpret this theorem as a statement about electric networks, and then to prove the theorem using techniques from classical electrical theory. The techniques referred to go back to Lord Rayleigh, who introduced them in connection with an investigation of musical instruments. The analog of Polya\u2019s theorem in this connection is that wind instruments are possible in our three-dimensional world, but are not possible in Flatland (Abbott [1]). The connection between random walks and electric networks has been recognized for some time (see Kakutani [12], Kemeny, Snell, and"
            },
            "slug": "Random-walks-and-electric-networks-Doyle-Snell",
            "title": {
                "fragments": [],
                "text": "Random walks and electric networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The goal will be to interpret Polya\u2019s beautiful theorem that a random walker on an infinite street network in d-dimensional space is bound to return to the starting point when d = 2, but has a positive probability of escaping to infinity without returning to the Starting Point when d \u2265 3, and to prove the theorem using techniques from classical electrical theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6909281,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "334d339ce1b2f8f19cd71779320b66e44e95fffd",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a local learning rule in which Hebbian learning is conditional on an incorrect prediction of a reinforcement signal. We propose a biological interpretation of such a framework and display its utility through examples in which the reinforcement signal is cast as the delivery of a neuromodulator to its target. Three examples are presented which illustrate how this framework can be applied to the development of the oculomotor system."
            },
            "slug": "Using-Aperiodic-Reinforcement-for-Directed-During-Montague-Dayan",
            "title": {
                "fragments": [],
                "text": "Using Aperiodic Reinforcement for Directed Self-Organization During Development"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A biological interpretation of a local learning rule in which Hebbian learning is conditional on an incorrect prediction of a reinforcement signal is proposed and its utility is displayed through examples in which the reinforcement signal was cast as the delivery of a neuromodulator to its target."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720664"
                        ],
                        "name": "Pierre-Yves Oudeyer",
                        "slug": "Pierre-Yves-Oudeyer",
                        "structuredName": {
                            "firstName": "Pierre-Yves",
                            "lastName": "Oudeyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Yves Oudeyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791091"
                        ],
                        "name": "F. Kaplan",
                        "slug": "F.-Kaplan",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Kaplan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kaplan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6933296,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e8077729ef723d71a0b7492f2f271ae413b5a4d5",
            "isKey": false,
            "numCitedBy": 566,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Intrinsic motivation, centrally involved in spontaneous exploration and curiosity, is a crucial concept in developmental psychology. It has been argued to be a crucial mechanism for open-ended cognitive development in humans, and as such has gathered a growing interest from developmental roboticists in the recent years. The goal of this paper is threefold. First, it provides a synthesis of the different approaches of intrinsic motivation in psychology. Second, by interpreting these approaches in a computational reinforcement learning framework, we argue that they are not operational and even sometimes inconsistent. Third, we set the ground for a systematic operational study of intrinsic motivation by presenting a formal typology of possible computational approaches. This typology is partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation. We argue that this kind of computational typology might be useful for opening new avenues for research both in psychology and developmental robotics."
            },
            "slug": "What-is-Intrinsic-Motivation-A-Typology-of-Oudeyer-Kaplan",
            "title": {
                "fragments": [],
                "text": "What is Intrinsic Motivation? A Typology of Computational Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper sets the ground for a systematic operational study of intrinsic motivation by presenting a formal typology of possible computational approaches, partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation."
            },
            "venue": {
                "fragments": [],
                "text": "Frontiers Neurorobotics"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145189997"
                        ],
                        "name": "A. Dickinson",
                        "slug": "A.-Dickinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Dickinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dickinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9208483,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "1abda55b890345dfffd14ed9f6763077b6109ed1",
            "isKey": false,
            "numCitedBy": 1019,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The study of animal behaviour has been dominated by two general models. According to the mechanistic stimulus-response model, a particular behaviour is either an innate or an acquired habit which is simply triggered by the appropriate stimulus. By contrast, the teleological model argues that, at least, some activities are purposive actions controlled by the current value of their goals through knowledge about the instrumental relations between the actions and their consequences. The type of control over any particular behaviour can be determined by a goal revaluation procedure. If the animal's performance changes appropriately following an alteration in the value of the goal or reward without further experience of the instrumental relationship, the behaviour should be regarded as a purposive action. On the other hand, the stimulus-response model is more appropriate for an activity whose performance is autonomous of the current value of the goal. By using this assay, we have found that a simple food-rewarded activity is sensitive to reward devaluation in rats following limited but not extended training. The development of this behavioural autonomy with extended training appears to depend not upon the amount of training per se, but rather upon the fact that the animal no longer experiences the correlation between variations in performance and variations in the associated consequences during overtraining. In agreement with this idea, limited exposure to an instrumental relationship that arranges a low correlation between performance and reward rates also favours the development of behavioural autonomy. Thus, the same activity can be either an action or a habit depending upon the type of training it has received."
            },
            "slug": "Actions-and-habits:-the-development-of-behavioural-Dickinson",
            "title": {
                "fragments": [],
                "text": "Actions and habits: the development of behavioural autonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that a simple food-rewarded activity is sensitive to reward devaluation in rats following limited but not extended training, and limited exposure to an instrumental relationship that arranges a low correlation between performance and reward rates also favours the development of behavioural autonomy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075363006"
                        ],
                        "name": "W. Pan",
                        "slug": "W.-Pan",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153900859"
                        ],
                        "name": "R. Schmidt",
                        "slug": "R.-Schmidt",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144329674"
                        ],
                        "name": "J. Wickens",
                        "slug": "J.-Wickens",
                        "structuredName": {
                            "firstName": "Jeffery",
                            "lastName": "Wickens",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wickens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2473829"
                        ],
                        "name": "B. Hyland",
                        "slug": "B.-Hyland",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Hyland",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hyland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7507612,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "2b80930ce9f57c31f8d33613d9e1c1119333dff3",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Behavioral conditioning of cue-reward pairing results in a shift of midbrain dopamine (DA) cell activity from responding to the reward to responding to the predictive cue. However, the precise time course and mechanism underlying this shift remain unclear. Here, we report a combined single-unit recording and temporal difference (TD) modeling approach to this question. The data from recordings in conscious rats showed that DA cells retain responses to predicted reward after responses to conditioned cues have developed, at least early in training. This contrasts with previous TD models that predict a gradual stepwise shift in latency with responses to rewards lost before responses develop to the conditioned cue. By exploring the TD parameter space, we demonstrate that the persistent reward responses of DA cells during conditioning are only accurately replicated by a TD model with long-lasting eligibility traces (nonzero values for the parameter \u03bb) and low learning rate (\u03b1). These physiological constraints for TD parameters suggest that eligibility traces and low per-trial rates of plastic modification may be essential features of neural circuits for reward learning in the brain. Such properties enable rapid but stable initiation of learning when the number of stimulus-reward pairings is limited, conferring significant adaptive advantages in real-world environments."
            },
            "slug": "Dopamine-Cells-Respond-to-Predicted-Events-during-Pan-Schmidt",
            "title": {
                "fragments": [],
                "text": "Dopamine Cells Respond to Predicted Events during Classical Conditioning: Evidence for Eligibility Traces in the Reward-Learning Network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the persistent reward responses of DA cells during conditioning are only accurately replicated by a TD model with long-lasting eligibility traces (nonzero values for the parameter \u03bb) and low learning rate (\u03b1), suggesting that eligibility traces and low per-trial rates of plastic modification may be essential features of neural circuits for reward learning in the brain."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 166
                            }
                        ],
                        "text": ") There is now a fairly extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120252299,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "774c9d82e4a0c7a8e54d9be971730fb98ec4c084",
            "isKey": false,
            "numCitedBy": 1488,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A powerful and usable class of methods for numerically approximating the solutions to optimal stochastic control problems for diffusion, reflected diffusion, or jump-diffusion models is discussed. The basic idea involves uconsistent approximation of the model by a Markov chain, and then solving an appropriate optimization problem for the Murkoy chain model. A general method for obtaining a useful approximation is given. All the standard classes of cost functions can be handled here, for illustrative purposes, discounted and average cost per unit time problems with both reflecting and nonreflecting diffusions are concentrated on. Both the drift and the variance can be controlled. Owing to its increasing importance and to lack of material on numerical methods, an application to the control of queueing and production systems in heavy traffic is developed in detail. The methods of proof of convergence are relatively simple, using only some basic ideas in the theory of weak convergence of a sequence of probabi..."
            },
            "slug": "Numerical-Methods-for-Stochastic-Control-Problems-Kushner",
            "title": {
                "fragments": [],
                "text": "Numerical Methods for Stochastic Control Problems in Continuous Time"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2726891"
                        ],
                        "name": "R. Rescorla",
                        "slug": "R.-Rescorla",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Rescorla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rescorla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 207
                            }
                        ],
                        "text": "This model, called the TD model of classical conditioning, or just the TD model, extends what is arguably the most widely-known and most influential model of classical conditioning: the RescorlaWagner model (Rescorla and Wagner, 1972)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 51139715,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "afaf65883ff75cc19926f61f181a687927789ad1",
            "isKey": false,
            "numCitedBy": 6196,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In several recent papers (Rescorla, 1969; Wagner, 1969a, 1969b) we have entertained similar theories of Pavlovian conditioning. The separate statements have in [act differed more in the language of their expression than in their substance. The major intent of the present paper is to explicate a more precise version of the form of theory involved, and to indicate how it may be usefully applied to a variety of phenomena in\u00b7 volving asso('iative learning. The impetus for a new theoretical model is not generally a new datum which dearly disconfirms existing theory. It is more likely to be the accumulation of a salient pattern of data, separate portions of which may be ade(luately handled by separate existing theories, but which apo pears to invite a more integrated theoretical account. Such, at least, is the better description of the background of the present work. In the sections which follow we will first describe certain data from our laboratories which exemplify the kind of observations which have encouraged the present theorizing. The theory will then be presented in sufficient detail to show how it may be applied to experimental situations involving a variety of Pavlovian conditioning arrangements. Finally, we will briefly discuss the theory in relationship to more conventional approaches."
            },
            "slug": "A-theory-of-Pavlovian-conditioning-:-Variations-in-Rescorla",
            "title": {
                "fragments": [],
                "text": "A theory of Pavlovian conditioning : Variations in the effectiveness of reinforcement and nonreinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 151
                            }
                        ],
                        "text": "We restrict attention to discrete time to keep things as simple as possible, even though many of the ideas can be extended to the continuous-time case (e.g., see Bertsekas and Tsitsiklis, 1996; Werbos, 1992; Doya, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1170136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25b813ee578fa34be7c13c78f1a35865bbafa519",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A continuous-time, continuous-state version of the temporal difference (TD) algorithm is derived in order to facilitate the application of reinforcement learning to real-world control tasks and neurobiological modeling. An optimal nonlinear feedback control law was also derived using the derivatives of the value function. The performance of the algorithms was tested in a task of swinging up a pendulum with limited torque. Both the \"critic\" that specifies the paths to the upright position and the \"actor\" that works as a nonlinear feedback controller were successfully implemented by radial basis function (RBF) networks."
            },
            "slug": "Temporal-Difference-Learning-in-Continuous-Time-and-Doya",
            "title": {
                "fragments": [],
                "text": "Temporal Difference Learning in Continuous Time and Space"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A continuous-time, continuous-state version of the temporal difference algorithm is derived in order to facilitate the application of reinforcement learning to real-world control tasks and neurobiological modeling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2751811"
                        ],
                        "name": "S. McClure",
                        "slug": "S.-McClure",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "McClure",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McClure"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11701048,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "4cb91ed7603e74f4b6d10e640f5db682405c278e",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-computational-substrate-for-incentive-salience-McClure-Daw",
            "title": {
                "fragments": [],
                "text": "A computational substrate for incentive salience"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145141118"
                        ],
                        "name": "D. Joel",
                        "slug": "D.-Joel",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Joel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7161073,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "e1625352c7b6e275ec9a63984a5af25a98032671",
            "isKey": false,
            "numCitedBy": 922,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "RationaleDopamine neurotransmission has long been known to exert a powerful influence over the vigor, strength, or rate of responding. However, there exists no clear understanding of the computational foundation for this effect; predominant accounts of dopamine\u2019s computational function focus on a role for phasic dopamine in controlling the discrete selection between different actions and have nothing to say about response vigor or indeed the free-operant tasks in which it is typically measured.ObjectivesWe seek to accommodate free-operant behavioral tasks within the realm of models of optimal control and thereby capture how dopaminergic and motivational manipulations affect response vigor.MethodsWe construct an average reward reinforcement learning model in which subjects choose both which action to perform and also the latency with which to perform it. Optimal control balances the costs of acting quickly against the benefits of getting reward earlier and thereby chooses a best response latency.ResultsIn this framework, the long-run average rate of reward plays a key role as an opportunity cost and mediates motivational influences on rates and vigor of responding. We review evidence suggesting that the average reward rate is reported by tonic levels of dopamine putatively in the nucleus accumbens.ConclusionsOur extension of reinforcement learning models to free-operant tasks unites psychologically and computationally inspired ideas about the role of tonic dopamine in striatum, explaining from a normative point of view why higher levels of dopamine might be associated with more vigorous responding."
            },
            "slug": "Tonic-dopamine:-opportunity-costs-and-the-control-Niv-Daw",
            "title": {
                "fragments": [],
                "text": "Tonic dopamine: opportunity costs and the control of response vigor"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The extension of reinforcement learning models to free-operant tasks unites psychologically and computationally inspired ideas about the role of tonic dopamine in striatum, explaining from a normative point of view why higher levels of dopamine might be associated with more vigorous responding."
            },
            "venue": {
                "fragments": [],
                "text": "Psychopharmacology"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47191491"
                        ],
                        "name": "M. Duff",
                        "slug": "M.-Duff",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Duff",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Duff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7314425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "936fcbdbf013b0308a68d0c35c6f1a89d461184b",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Q-Learning-for-Bandit-Problems-Duff",
            "title": {
                "fragments": [],
                "text": "Q-Learning for Bandit Problems"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064887053"
                        ],
                        "name": "Christopher D. Adams",
                        "slug": "Christopher-D.-Adams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Adams",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Adams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1014,
                                "start": 6
                            }
                        ],
                        "text": "Houk, Adams, and Barto (1995) suggested how TD learning and the actor\u2013critic architecture might map onto the anatomy, physiology, and molecular mechanism of the basal ganglia. Doya and Sejnowski (1998) extended their earlier paper on a model of birdsong learning (Doya and Sejnowski, 1995) by including a TD-like error identified with dopamine to reinforce the selection of auditory input to be memorized. O\u2019Reilly and Frank (2006) and O\u2019Reilly, Frank, Hazy, and Watz (2007) argued that phasic dopamine signals are RPEs but not TD errors. In support of their theory they cited results with variable interstimulus intervals that do not match predictions of a simple TD model, as well as the observation that higher-order conditioning beyond second-order conditioning is rarely observed, while TD learning is not so limited. Dayan and Niv (2008) discussed \u201cthe good, the bad, and the ugly\u201d of how reinforcement learning theory and the reward prediction error hypothesis align with experimental data. Glimcher (2011) reviewed the empirical findings that support the reward prediction error hypothesis and emphasized the significance of the hypothesis for contemporary neuroscience."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 60
                            }
                        ],
                        "text": "An early important experiment of this type was conducted by Adams and Dickinson (1981). They trained rats via instrumental conditioning until the rats energetically pressed a lever for sucrose pellets in a training chamber."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 6
                            }
                        ],
                        "text": "Houk, Adams, and Barto (1995) suggested how TD learning and the actor\u2013critic architecture might map onto the anatomy, physiology, and molecular mechanism of the basal ganglia. Doya and Sejnowski (1998) extended their earlier paper on a model of birdsong learning (Doya and Sejnowski, 1995) by including a TD-like error identified with dopamine to reinforce the selection of auditory input to be memorized."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 6
                            }
                        ],
                        "text": "Houk, Adams, and Barto (1995) suggested how TD learning and the actor\u2013critic architecture might map onto the anatomy, physiology, and molecular mechanism of the basal ganglia."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 432,
                                "start": 6
                            }
                        ],
                        "text": "Houk, Adams, and Barto (1995) suggested how TD learning and the actor\u2013critic architecture might map onto the anatomy, physiology, and molecular mechanism of the basal ganglia. Doya and Sejnowski (1998) extended their earlier paper on a model of birdsong learning (Doya and Sejnowski, 1995) by including a TD-like error identified with dopamine to reinforce the selection of auditory input to be memorized. O\u2019Reilly and Frank (2006) and O\u2019Reilly, Frank, Hazy, and Watz (2007) argued that phasic dopamine signals are RPEs but not TD errors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 844,
                                "start": 6
                            }
                        ],
                        "text": "Houk, Adams, and Barto (1995) suggested how TD learning and the actor\u2013critic architecture might map onto the anatomy, physiology, and molecular mechanism of the basal ganglia. Doya and Sejnowski (1998) extended their earlier paper on a model of birdsong learning (Doya and Sejnowski, 1995) by including a TD-like error identified with dopamine to reinforce the selection of auditory input to be memorized. O\u2019Reilly and Frank (2006) and O\u2019Reilly, Frank, Hazy, and Watz (2007) argued that phasic dopamine signals are RPEs but not TD errors. In support of their theory they cited results with variable interstimulus intervals that do not match predictions of a simple TD model, as well as the observation that higher-order conditioning beyond second-order conditioning is rarely observed, while TD learning is not so limited. Dayan and Niv (2008) discussed \u201cthe good, the bad, and the ugly\u201d of how reinforcement learning theory and the reward prediction error hypothesis align with experimental data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1332,
                                "start": 0
                            }
                        ],
                        "text": "Adams and Dickinson concluded that the injected rats associated lever pressing with consequent nausea by means of a cognitive map linking lever pressing to pellets, and pellets to nausea. Hence, in the extinction trials, the rats \u201cknew\u201d that the consequences of pressing the lever would be something they did not want, and so they reduced their lever-pressing right from the start. The important point is that they reduced lever-pressing without ever having experienced lever-pressing directly followed by being sick: no lever was present when they were made sick. They seemed able to combine knowledge of the outcome of a behavioral choice (pressing the lever will be followed by getting a pellet) with the reward value of the outcome (pellets are to be avoided) and hence could alter their behavior accordingly. Not every psychologist agrees with this \u201ccognitive\u201d account of this kind of experiment, and it is not the only possible way to explain these results, but the model-based planning explanation is widely accepted. Nothing prevents an agent from using both model-free and model-based algorithms, and there are good reasons for using both. We know from our own experience that with enough repetition, goaldirected behavior tends to turn into habitual behavior. Experiments show that this happens for rats too. Adams (1982) conducted an experiment to see if extended training would convert goal-directed behavior into habitual behavior."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 54
                            }
                        ],
                        "text": "To the best of our knowledge, Barto (1995a) and Houk, Adams, and Barto (1995) first speculated about possible implementations of actor\u2013critic algorithms in the basal ganglia."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3714,
                                "start": 0
                            }
                        ],
                        "text": "Adams and Dickinson concluded that the injected rats associated lever pressing with consequent nausea by means of a cognitive map linking lever pressing to pellets, and pellets to nausea. Hence, in the extinction trials, the rats \u201cknew\u201d that the consequences of pressing the lever would be something they did not want, and so they reduced their lever-pressing right from the start. The important point is that they reduced lever-pressing without ever having experienced lever-pressing directly followed by being sick: no lever was present when they were made sick. They seemed able to combine knowledge of the outcome of a behavioral choice (pressing the lever will be followed by getting a pellet) with the reward value of the outcome (pellets are to be avoided) and hence could alter their behavior accordingly. Not every psychologist agrees with this \u201ccognitive\u201d account of this kind of experiment, and it is not the only possible way to explain these results, but the model-based planning explanation is widely accepted. Nothing prevents an agent from using both model-free and model-based algorithms, and there are good reasons for using both. We know from our own experience that with enough repetition, goaldirected behavior tends to turn into habitual behavior. Experiments show that this happens for rats too. Adams (1982) conducted an experiment to see if extended training would convert goal-directed behavior into habitual behavior. He did this by comparing the effect of outcome devaluation on rats that experienced different amounts of training. If extended training made the rats less sensitive to devaluation compared to rats that received less training, this would be evidence that extended training made the behavior more habitual. Adams\u2019 experiment closely followed the Adams and Dickinson (1981) experiment just described. Simplifying a bit, rats in one group were trained until they made 100 rewarded lever-presses, and rats in the other group\u2014the overtrained group\u2014were trained until they made 500 rewarded lever-presses. After this training, the reward value of the pellets was decreased (using lithium chloride injections) for rats in both groups. Then both groups of rats were given a session of extinction training. Adams\u2019 question was whether devaluation would effect the rate of lever-pressing for the overtrained rats less than it would for the non-overtrained rats, which would be evidence that extended training reduces sensitivity to outcome devaluation. It turned out that devaluation strongly decreased the lever-pressing rate of the non-overtrained rats. For the overtrained rats, in contrast, devaluation had little effect on their lever-pressing; in fact, if anything, it made it more vigorous. (The full experiment included control groups showing that the different amounts of training did not by themselves significantly effect lever-pressing rates after learning.) This result suggested that while the non-overtrained rats were acting in a goal-directed manner sensitive to their knowledge of the outcome of their actions, the overtrained rats had developed a lever-pressing habit. Viewing this and other results like it from a computational perspective provides insight as to why one might expect animals to behave habitually in some circumstances, in a goal-directed way in others, and why they shift from one mode of control to another as they continue to learn. While animals undoubtedly use algorithms that do not exactly match those we have presented in this book, one can gain insight into animal behavior by considering the tradeoffs that various reinforcement learning algorithms imply. An idea developed by computational neuroscientists Daw, Niv, and Dayan (2005) is that animals use both model-free and model-based processes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 144453967,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3e475fdd47780d48667e58607ba152ad40ed89f2",
            "isKey": true,
            "numCitedBy": 528,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In five experiments hungry rats were trained to make a lever press response for a sucrose reinforcer. That sucrose was subsequently devalued by conditioning a food-aversion to it, and the ability of the rats to integrate knowledge about the instrumental contingency with that gained from aversion training was assessed in an extinction test. Experiment I showed successful integration following limited but not extended instrumental training. Experiment II suggested that the crucial factor was the spacing of training; successful integration was seen after massed but not distributed training. The third experiment implicated distributed experience with the reinforcer, rather than distributed response practice, in failures of integration. Experiment IV showed that if the distribution of food-aversion learning was dissimilar to that of instrumental training then a failure of integration could result; this finding was able to account for the distribution of training effects seen in previous studies, but not the effect of extended training. Experiment V replicated the extended training effect seen in Experiment I, and provided evidence that this may reflect the degree of exposure to the reinforcer rather than the extent of response practice."
            },
            "slug": "Variations-in-the-Sensitivity-of-Instrumental-to-Adams",
            "title": {
                "fragments": [],
                "text": "Variations in the Sensitivity of Instrumental Responding to Reinforcer Devaluation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2980808"
                        ],
                        "name": "A. Gelperin",
                        "slug": "A.-Gelperin",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Gelperin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelperin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516142"
                        ],
                        "name": "D. Tank",
                        "slug": "D.-Tank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tank",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141274099,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "58cc1683ec449657442b510dbdf05712d97c9657",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We wish to understand the neuronal computations performed on sensory inputs that result in the categorization of those inputs, their storage as memory states, and their associative combination. Dur experimental and theoretical work is focused on the neuronal computations performed on odor and taste inputs in the CNS of Limax maximus, a terrestrial mollusk convenient for behavioral, neuro\u00ad physiologieal, and neurochemical experiments (Gelperin, 1983). The questions posed in this specific system are designed to illuminate issues of learning and mem\u00ad ory storage with panphyletic generality. Several aspects of the learning behavior of Limax have encouraged us to attempt a unique synthesis of the approaches of behavioral biology, neurobiology, and neural modeling. While Limax displays many of the learning phenomena of higher organisms, including primates, it accomplishes these learning tasks using onlyabout 10,000 cells in its CNS. We are using behavioral experiments to define the major types of learning exhibited by Limax and for each type of learning to establish the critical interevent timing relations that allow learning to occur. Neu\u00ad rophysiological and neurochemical experiments delimit the areas of CNS necessary for learning and describe the types of cellular elements and synaptic interactions available for modification during learning. The neural modeling asks how a collec\u00ad tion of relatively simple neurons might interact synaptically to collectively accom\u00ad plish the learning tasks. The model avoids assumptions about precise anatomical details, instead asking questions about the computational consequences of a simple set of rules governing synaptic interactions."
            },
            "slug": "The-Logic-of-Limax-Learning-Gelperin-Hopfield",
            "title": {
                "fragments": [],
                "text": "The Logic of Limax Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work aims to understand the neuronal computations performed on sensory inputs that result in the categorization of those inputs, their storage as memory states, and their associative combination in the CNS of Limax maximus, a terrestrial mollusk convenient for behavioral, neuro\u00ad physiologieal, and neurochemical experiments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3254214"
                        ],
                        "name": "G. Monahan",
                        "slug": "G.-Monahan",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Monahan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Monahan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 123582406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1b800aacf850a18e6504c693b4c5d33b8b3ac32",
            "isKey": false,
            "numCitedBy": 600,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper surveys models and algorithms dealing with partially observable Markov decision processes. A partially observable Markov decision process (POMDP) is a generalization of a Markov decision process which permits uncertainty regarding the state of a Markov process and allows for state information acquisition. A general framework for finite state and action POMDP's is presented. Next, there is a brief discussion of the development of POMDP's and their relationship with other decision processes. A wide range of models in such areas as quality control, machine maintenance, internal auditing, learning, and optimal stopping are discussed within the POMDP-framework. Lastly, algorithms for computing optimal solutions to POMDP's are presented."
            },
            "slug": "State-of-the-Art\u2014A-Survey-of-Partially-Observable-Monahan",
            "title": {
                "fragments": [],
                "text": "State of the Art\u2014A Survey of Partially Observable Markov Decision Processes: Theory, Models, and Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A wide range of models in such areas as quality control, machine maintenance, internal auditing, learning, and optimal stopping are discussed within the POMDP-framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145189997"
                        ],
                        "name": "A. Dickinson",
                        "slug": "A.-Dickinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Dickinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dickinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 162
                            }
                        ],
                        "text": "Sometimes in psychology reinforcement refers to the process of producing lasting changes in behavior, whether the changes strengthen or weaken a behavior pattern (Mackintosh, 1983)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39540883,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d7ed2b8913c1cbb3b364071ed1bbefc4c92f90da",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This is rightly regarded as the last word in lea~=iing theory but, as Mackintosh emphasizes on p. 2, lee/rning theory of this particular kind 'no \"anger occupies the exalted position it once held among the various fields of psychology'. The the-o~y applies most directly to rats and pigeons re-,.eiving signal-reinforcer pairings in Skinner boxes (co.,a.(titioned emotional response procedures for rats; autoshaping for pigeons): in the preface Mackintosh notes that he has 'eschewed discussion' of anything directly concerned with schedules of reinforcement , and said little or nothing about naturalistic topics such as imprinting, song learning , navigation or intelligence in animal species. That is because the theoretical aim of the book is to establish certain laws of association which make up \"one possible view of the nature of condition-ing'. One cannot do justice to this view briefly, but Mackintosh first adopts a version of two-factor theory, that is he accepts an operational and functional distinction between classical and instrumental conditioning (p. 41); then he advocates a stimulus-substi~r.ution theory of c!assical conditioning , in which a CS elicits responses by activating a representation of a UCS, but only according to its own sensory properties (pp. 68-70); puts forward a '!'oimamai~ ,.h~ory of instrumental conditioning, in which an animal must infer from previous associations between lever pressing and food that it might be a good idea to press the lever again (pp. 110-112); argues for the theoretical symmetry of re~ard and pmfishment (pp. 126-131); just about (I think) accepts the two-iactor tileory of avoidance learning (pp. 155-170); discusses various laws of association in terms of the adequacy or otherwise of the Rescoda-Wagner single-equation model (pp. 171-239); and in a final short chapter sets off the phenomena of discrimination learning as calling upon processes 'not normally studied in simple conditioning experiments' (p. 273) and 'outside the scope of standard theories of condi-tioning' (p. 271). l?y comparison with his enormously ~uccessful The Psychology of Animal Learning, Maekintosh's present book is theoretically tighter and more succinct. The major theoretical change seems to me to be a slight firming up of the classical/instrumental distinction. Within the areas covered, Macldntosh is so encyclopaedically knowledgeable, and au fait with the merits and failings of all conceivable theoretical positions, as to be quite abc~vc criticism. One can only lament, for the purposes of this journal, that the learning theories of the present Cambridge school make so little contact with \u2026"
            },
            "slug": "Conditioning-and-associative-learning.-Dickinson",
            "title": {
                "fragments": [],
                "text": "Conditioning and associative learning."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Compared with his enormously ~uccessful The Psychology of Animal Learning, Maekintosh's present book is theoretically tighter and more succinct, and there is a slight firming up of the classical/instrumental distinction."
            },
            "venue": {
                "fragments": [],
                "text": "British medical bulletin"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3061441"
                        ],
                        "name": "L. Saksida",
                        "slug": "L.-Saksida",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Saksida",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saksida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32092023"
                        ],
                        "name": "Scott M. Raymond",
                        "slug": "Scott-M.-Raymond",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Raymond",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott M. Raymond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806116"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1572356,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "7bd3ae59740463b9d2730645874dc7aa3bfec1ab",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Shaping-robot-behavior-using-principles-from-Saksida-Raymond",
            "title": {
                "fragments": [],
                "text": "Shaping robot behavior using principles from instrumental conditioning"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32831607"
                        ],
                        "name": "R. Yee",
                        "slug": "R.-Yee",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Yee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Yee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46796200"
                        ],
                        "name": "S. Saxena",
                        "slug": "S.-Saxena",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Saxena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Saxena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195972"
                        ],
                        "name": "P. Utgoff",
                        "slug": "P.-Utgoff",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Utgoff",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Utgoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15217827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "241b50e5a6009beaf194f824795ed88c0c6611f4",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a technique for improving problem-solving performance by creating concepts that allow problem states to be evaluated through an efficient recognition process. A temporal-difference (TD) method is used to bootstrap a collection of useful concepts by backing up evaluations from recognized states to their predecessors. This procedure is combined with explanation- based generalization (EBG) and goal regression to use knowledge of the problem domain to help generalize the new concept definitions. This maintains the efficiency of using the concepts and accelerates the learning process in comparison to knowledge-free approaches. Also, because the learned definitions may describe negative conditions, it becomes possible to use EBG to explain why some instance is not an example of a concept. The learning technique has been elaborated for minimax gameplaying and tested on a Tic-Tat-Toe system, T2. Given only concepts defining the end-game states and constrained to a two-ply search bound, experiments show that T2 learns concepts for achieving near-perfect play. T2's total searching time, including concept recognition, is within acceptable performance limits while perfect play without the concepts requires searches taking well over 100 times longer than T2's."
            },
            "slug": "Explaining-Temporal-Differences-to-Create-Useful-Yee-Saxena",
            "title": {
                "fragments": [],
                "text": "Explaining Temporal Differences to Create Useful Concepts for Evaluating States"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A temporal-difference method is used to bootstrap a collection of useful concepts by backing up evaluations from recognized states to their predecessors and this procedure is combined with explanation- based generalization and goal regression to use knowledge of the problem domain to help generalize the new concept definitions."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650569"
                        ],
                        "name": "A. Klopf",
                        "slug": "A.-Klopf",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Klopf",
                            "middleNames": [
                                "Harry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Klopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17852070,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "a418c03feca1abc2ebaf4d8b2f0ab26839b342ca",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper was motivated, in part, by Price's recent (1975) tutorial comparing human and computer visual systems. While I found his comparisons interesting, a fundamental question continued to trouble me, as it has now for a number of years. Namely, does artificial intelligence (AI) really bear any fundamentally important relationship to natural intelligence? Or are the two sufficiently different so that a comparison of human and computer visual systems, for example, might be of the same significance as a comparison of locomotion in four-legged animals and four-wheeled vehicles?"
            },
            "slug": "A-comparison-of-natural-and-artificial-intelligence-Klopf",
            "title": {
                "fragments": [],
                "text": "A comparison of natural and artificial intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper was motivated, in part, by Price's recent (1975) tutorial comparing human and computer visual systems and found his comparisons interesting."
            },
            "venue": {
                "fragments": [],
                "text": "SGAR"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145057514"
                        ],
                        "name": "H. B. McMahan",
                        "slug": "H.-B.-McMahan",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "McMahan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B. McMahan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21889436"
                        ],
                        "name": "Geoffrey J. Gordon",
                        "slug": "Geoffrey-J.-Gordon",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey J. Gordon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2538740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0399ee4d162f4167430142efb451a730572ca21",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of computing the optimal value function for a Markov decision process with positive costs. Computing this function quickly and accurately is a basic step in many schemes for deciding how to act in stochastic environments. There are efficient algorithms which compute value functions for special types of MDPs: for deterministic MDPs with S states and A actions, Dijkstra's algorithm runs in time O(AS log S). And, in single-action MDPs (Markov chains), standard linear-algebraic algorithms find the value function in time O(S3), or faster by taking advantage of sparsity or good conditioning. Algorithms for solving general MDPs can take much longer: we are not aware of any speed guarantees better than those for comparably-sized linear programs. We present a family of algorithms which reduce to Dijkstra's algorithm when applied to deterministic MDPs, and to standard techniques for solving linear equations when applied to Markov chains. More importantly, we demonstrate experimentally that these algorithms perform well when applied to MDPs which \"almost\" have the required special structure."
            },
            "slug": "Fast-Exact-Planning-in-Markov-Decision-Processes-McMahan-Gordon",
            "title": {
                "fragments": [],
                "text": "Fast Exact Planning in Markov Decision Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A family of algorithms which reduce to Dijkstra's algorithm when applied to deterministic MDPs, and to standard techniques for solving linear equations when appliedto Markov chains are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICAPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 220093382,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "12b9019f99a315a137400389ee7c6faa4cceef35",
            "isKey": false,
            "numCitedBy": 7610,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control."
            },
            "slug": "A-Neural-Substrate-of-Prediction-and-Reward-Schultz-Dayan",
            "title": {
                "fragments": [],
                "text": "A Neural Substrate of Prediction and Reward"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Findings in this work indicate that dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events can be understood through quantitative theories of adaptive optimizing control."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680510"
                        ],
                        "name": "T. Dean",
                        "slug": "T.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2080151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ae91735c6e5f4ab44bfa95bd144663f057d1935",
            "isKey": false,
            "numCitedBy": 536,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning. In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms. We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly. To encourage future research, we sketch some alternative methods of analysis that rely on the structure of MDPs."
            },
            "slug": "On-the-Complexity-of-Solving-Markov-Decision-Littman-Dean",
            "title": {
                "fragments": [],
                "text": "On the Complexity of Solving Markov Decision Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly and to encourage future research, some alternative methods of analysis are sketched that rely on the structure of M DPs."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 130812,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "aeeb7bebd5f6bd01b2c88054bfd282a01f48b32e",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic findings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine."
            },
            "slug": "How-fast-to-work:-Response-vigor,-motivation-and-Niv-Daw",
            "title": {
                "fragments": [],
                "text": "How fast to work: Response vigor, motivation and tonic dopamine"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An RL framework for free-operant behavior is developed, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073142"
                        ],
                        "name": "R. Legenstein",
                        "slug": "R.-Legenstein",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Legenstein",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Legenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2733044"
                        ],
                        "name": "Dejan Pecevski",
                        "slug": "Dejan-Pecevski",
                        "structuredName": {
                            "firstName": "Dejan",
                            "lastName": "Pecevski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dejan Pecevski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6861063,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "41c51488f87d4249e5962771672d6f16dd3f723b",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Reward-modulated spike-timing-dependent plasticity (STDP) has recently emerged as a candidate for a learning rule that could explain how local learning rules at single synapses support behaviorally relevant adaptive changes in complex networks of spiking neurons. However the potential and limitations of this learning rule could so far only be tested through computer simulations. This article provides tools for an analytic treatment of reward-modulated STDP, which allow us to predict under which conditions reward-modulated STDP will be able to achieve a desired learning effect. In particular, we can produce in this way a theoretical explanation and a computer model for a fundamental experimental finding on biofeedback in monkeys (reported in [1])."
            },
            "slug": "Theoretical-Analysis-of-Learning-with-Plasticity-Legenstein-Pecevski",
            "title": {
                "fragments": [],
                "text": "Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "These tools are provided to predict under which conditions reward-modulated STDP will be able to achieve a desired learning effect and to produce a theoretical explanation and a computer model for a fundamental experimental finding on biofeedback in monkeys."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719882"
                        ],
                        "name": "J. Houk",
                        "slug": "J.-Houk",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Houk",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Houk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40468115"
                        ],
                        "name": "Joel L. Davis",
                        "slug": "Joel-L.-Davis",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Davis",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel L. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3899043"
                        ],
                        "name": "D. Beiser",
                        "slug": "D.-Beiser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beiser",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beiser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 24
                            }
                        ],
                        "text": "(1994), Hampson (1989), Houk et al. (1995), Montague et al. (1996), and Schultz et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 24
                            }
                        ],
                        "text": "(1994), Hampson (1989), Houk et al. (1995), Montague et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 141411992,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "188030804d0e5816762cca70e12cea9f18e0a182",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Dopamine Neurons, Organization of Strtosomal Modules, Mechanism of Responsiveness to Predictors of Reinforcement, Correspondence with the Theory of Adaptive Critics, Learning to Predict Primary Reinforcement, Learning Earlier Predictors of Reinforcement, Relation to the Actor-Critic Architecture, More Realistic Assumptions, Summary, Acknowledgments, References"
            },
            "slug": "A-Model-of-How-the-Basal-Ganglia-Generate-and-Use-Houk-Davis",
            "title": {
                "fragments": [],
                "text": "A Model of How the Basal Ganglia Generate and Use Neural Signals That Predict Reinforcement"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This chapter contains sections titled: Introduction, Dopamine Neurons, Organization of Strtosomal Modules, Mechanism of Responsiveness to Predictors of Reinforcement, Correspondence with the Theory of Adaptive Critics, and Relation to the Actor-Critic Architecture."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2261353"
                        ],
                        "name": "T. Perkins",
                        "slug": "T.-Perkins",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Perkins",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Perkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368601"
                        ],
                        "name": "Doina Precup",
                        "slug": "Doina-Precup",
                        "structuredName": {
                            "firstName": "Doina",
                            "lastName": "Precup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doina Precup"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5189486,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f25e01d5c5c39ce3cab13933f59ceb6a258836f3",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a \"policy improvement operator\" to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces e-soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy. To our knowledge, this is the first convergence result for any form of approximate policy iteration under similar computational-resource assumptions."
            },
            "slug": "A-Convergent-Form-of-Approximate-Policy-Iteration-Perkins-Precup",
            "title": {
                "fragments": [],
                "text": "A Convergent Form of Approximate Policy Iteration"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is proved that if the policy improvement operator produces e-soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145763319"
                        ],
                        "name": "J. Staddon",
                        "slug": "J.-Staddon",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Staddon",
                            "middleNames": [
                                "E.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Staddon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 144783938,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "5a602d697f93b659e4ef7ccf7324d64a57166391",
            "isKey": false,
            "numCitedBy": 485,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. The evolution, development and modification of behaviour 2. Variation and selection of behaviour 3. Direct orientation and feedback 4. Operant behaviour 5. Reward and punishment 6. Feeding regulation: a model motivational system 7. The optimal allocation of behaviour 8. Choice and decision rules 9. Foraging and behavioural ecology 10. Stimulus control and cognition 11. Stimulus control and performance 12. Memory and temporal control 13. Learning I: the acquisition of knowledge 14. Learning II: the guidance of action References Indexes."
            },
            "slug": "Adaptive-behavior-and-learning-Staddon",
            "title": {
                "fragments": [],
                "text": "Adaptive behavior and learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The evolution, development and modification of behaviour, including feeding regulation, and Variation and selection of behaviour are studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095777"
                        ],
                        "name": "P. Parks",
                        "slug": "P.-Parks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Parks",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Parks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144806174"
                        ],
                        "name": "J. Militzer",
                        "slug": "J.-Militzer",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Militzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Militzer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 118
                            }
                        ],
                        "text": "Extensive studies have been made of the effect of different displacement vectors on the generalization of tile coding (Parks and Militzer, 1991; An, 1991; An, Miller and Parks, 1991; Miller, Glanz and Carter, 1991), assessing their homegeneity and tendency toward diagonal artifacts like those seen for the (1, 1) displacement vectors."
                    },
                    "intents": []
                }
            ],
            "corpusId": 115622523,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "20bc2fa075a19f0f2638956dcf0bbdb6555ffe9a",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improved-Allocation-of-Weights-for-Associative-in-Parks-Militzer",
            "title": {
                "fragments": [],
                "text": "Improved Allocation of Weights for Associative Memory Storage in Learning Control Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054180658"
                        ],
                        "name": "J. Christensen",
                        "slug": "J.-Christensen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Christensen",
                            "middleNames": [
                                "Peter",
                                "Reus"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Christensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682627"
                        ],
                        "name": "R. Korf",
                        "slug": "R.-Korf",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Korf",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Korf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 43
                            }
                        ],
                        "text": "3-5 Truncated TD methods were developed by Cichosz (1995) and van Seijen (2016)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 12
                            }
                        ],
                        "text": "The work of Cichosz (1995) and particularly van Seijen (2016) showed that they are actually completely practical algorithms."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17099383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a698fff13d9eea16b7e7a84c9ce023ba0a695d6a",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a characterization of heuristic evaluation functions which unifies their treatment in single-agent problems and two-person games. The central result is that a useful heuristic function is one which determines the outcome of a search and is invariant along a solution path. This local characterization of heuristics can be used to predict die effectiveness of given heuristics and to automatically learn useful heuristic functions for problems. In one experiment, a set of relative weights for the different chess pieces was automatically learned."
            },
            "slug": "A-Unified-Theory-of-Heuristic-Evaluation-Functions-Christensen-Korf",
            "title": {
                "fragments": [],
                "text": "A Unified Theory of Heuristic Evaluation Functions and its Application to Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A characterization of heuristic evaluation functions is presented which unifies their treatment in single-agent problems and two-person games and shows that a useful heuristic function is one which determines the outcome of a search and is invariant along a solution path."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 12
                            }
                        ],
                        "text": "Chapman and Kaelbling (1991) and Tan (1991) adapted decision-tree methods for learning value functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 384,
                                "start": 123
                            }
                        ],
                        "text": "Reinforcement comparison methods were extensively developed by Sutton (1984) and further refined by Williams (1986, 1992), Kaelbling (1993), and Dayan (1991). These authors analyzed many variations of the idea including various eligibility terms that may significantly improve performance. Perhaps the earliest use of reinforcement comparison was by Barto, Sutton, and Brouwer (1981)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 123
                            }
                        ],
                        "text": "Reinforcement comparison methods were extensively developed by Sutton (1984) and further refined by Williams (1986, 1992), Kaelbling (1993), and Dayan (1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 131
                            }
                        ],
                        "text": "For additional general coverage of reinforcement learning, we refer the reader to the books by Bertsekas and Tsitsiklis (1996) and Kaelbling (1993). Two special issues of the journal Machine Learning focus on reinforcement learning: Sutton (1992) and Kaelbling (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 131
                            }
                        ],
                        "text": "For additional general coverage of reinforcement learning, we refer the reader to the books by Bertsekas and Tsitsiklis (1996) and Kaelbling (1993). Two special issues of the journal Machine Learning focus on reinforcement learning: Sutton (1992) and Kaelbling (1996). Useful surveys are provided by Barto (1995), Kaelbling, Littman, and Moore (1996), and Keerthi and Ravindran (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 123
                            }
                        ],
                        "text": "Reinforcement comparison methods were extensively developed by Sutton (1984) and further refined by Williams (1986, 1992), Kaelbling (1993), and Dayan (1991). These authors analyzed many variations of the idea including various eligibility terms that may significantly improve performance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15784464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4c6240b68e97d6f3b9bc67a701f10e49a1b1dab",
            "isKey": true,
            "numCitedBy": 232,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hierarchical-Learning-in-Stochastic-Domains:-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Hierarchical Learning in Stochastic Domains: Preliminary Results"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7587835,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3abac8d1bf1a6c69805e8aa6f0335b66f39ca999",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Successes with expert systems and other specialized systems have revived hopes for factory automation and productivity growth. A full realization of these potentials will require conscious effort to overcome obsolete rigidities, to develop unified and adaptive methods for integrating complex systems, and to increase our understanding of these systems (understanding which is vital to human productivity in developing software). How adaptive systems may be built and understood by extending control theory and statistics is discussed. Adaptive systems, like human infants, are less agile than young monkeys but have something important to contribute as they mature. It argues that the old dream of understanding intelligence in generalized terms, permitting a unified understanding of adaptive systems and of the human mind, was not incorrect; rather, the early attempts in that direction failed because they did not make full use of research possibilities in statistics, control theory, and numerical analysis (many of which are still unexploited) and were limited by hardware costs which are now coming down. A basic adaptive system derived from this approach fits the fundamental, qualitative empirical facts of human brain physiology in some detail (unlike the usual \"general neuron models,\" which rarely even discriminate between basic components of the brain), and offers opportunities for further research; it can even translate certain fundamental ideas of Freud into something more mathematical and scientific. The mathematics of the basic system, and the fit to the brain, are described in detail."
            },
            "slug": "Building-and-Understanding-Adaptive-Systems:-A-to-Werbos",
            "title": {
                "fragments": [],
                "text": "Building and Understanding Adaptive Systems: A Statistical/Numerical Approach to Factory Automation and Brain Research"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that the old dream of understanding intelligence in generalized terms, permitting a unified understanding of adaptive systems and of the human mind, was not incorrect; rather, the early attempts failed because they did not make full use of research possibilities in statistics, control theory, and numerical analysis and were limited by hardware costs which are now coming down."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732302"
                        ],
                        "name": "J. Koza",
                        "slug": "J.-Koza",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Koza",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koza"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31978081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "505c58c2c100e7512b7f7d906a9d4af72f6e8415",
            "isKey": false,
            "numCitedBy": 14069,
            "numCiting": 245,
            "paperAbstract": {
                "fragments": [],
                "text": "Background on genetic algorithms, LISP, and genetic programming hierarchical problem-solving introduction to automatically-defined functions - the two-boxes problem problems that straddle the breakeven point for computational effort Boolean parity functions determining the architecture of the program the lawnmower problem the bumblebee problem the increasing benefits of ADFs as problems are scaled up finding an impulse response function artificial ant on the San Mateo trail obstacle-avoiding robot the minesweeper problem automatic discovery of detectors for letter recognition flushes and four-of-a-kinds in a pinochle deck introduction to biochemistry and molecular biology prediction of transmembrane domains in proteins prediction of omega loops in proteins lookahead version of the transmembrane problem evolutionary selection of the architecture of the program evolution of primitives and sufficiency evolutionary selection of terminals evolution of closure simultaneous evolution of architecture, primitive functions, terminals, sufficiency, and closure the role of representation and the lens effect. Appendices: list of special symbols list of special functions list of type fonts default parameters computer implementation annotated bibliography of genetic programming electronic mailing list and public repository."
            },
            "slug": "Genetic-programming-on-the-programming-of-computers-Koza",
            "title": {
                "fragments": [],
                "text": "Genetic programming - on the programming of computers by means of natural selection"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This book discusses the evolution of architecture, primitive functions, terminals, sufficiency, and closure, and the role of representation and the lens effect in genetic programming."
            },
            "venue": {
                "fragments": [],
                "text": "Complex adaptive systems"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47328071"
                        ],
                        "name": "P. Glimcher",
                        "slug": "P.-Glimcher",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Glimcher",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Glimcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 142624465,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0f30c266ecd40fc4716b30845876d98ddb417b8a",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decisions,-Uncertainty,-and-the-Brain:-The-Science-Glimcher",
            "title": {
                "fragments": [],
                "text": "Decisions, Uncertainty, and the Brain: The Science of Neuroeconomics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715339"
                        ],
                        "name": "D. Goldberg",
                        "slug": "D.-Goldberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 38613589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e62d1345b340d5fda3b092c460264b9543bc4b5",
            "isKey": false,
            "numCitedBy": 58106,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \n \nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required."
            },
            "slug": "Genetic-Algorithms-in-Search-Optimization-and-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms in Search Optimization and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This book brings together the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2576707"
                        ],
                        "name": "S. Lane",
                        "slug": "S.-Lane",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Lane",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911003"
                        ],
                        "name": "D. Handelman",
                        "slug": "D.-Handelman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Handelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Handelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144233163"
                        ],
                        "name": "J. Gelfand",
                        "slug": "J.-Gelfand",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Gelfand",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gelfand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14859163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fc47fbc6665d07d9d97ca185e206c89393f3ba",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The cerebellar model articulation controller (CMAC) neural network is capable of learning nonlinear functions extremely quickly due to the local nature of its weight updating. The rectangular shape of CMAC receptive field functions, however, produces discontinuous (staircase) function approximations without inherent analytical derivatives. The ability to learn both functions and function derivatives is important for the development of many online adaptive filter, estimation, and control algorithms. It is shown that use of B-spline receptive field functions in conjunction with more general CMAC weight addressing schemes allows higher-order CMAC neural networks to be developed that can learn both functions and function derivatives. This also allows hierarchical and multilayer CMAC network architectures to be constructed that can be trained using standard error back-propagation learning techniques.<<ETX>>"
            },
            "slug": "Theory-and-development-of-higher-order-CMAC-neural-Lane-Handelman",
            "title": {
                "fragments": [],
                "text": "Theory and development of higher-order CMAC neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that use of B-spline receptive field functions in conjunction with more general CMAC weight addressing schemes allows higher-order CMAC neural networks to be developed that can learn both functions and function derivatives."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Control Systems"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6635519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4abd4e51705e74f1739bd3a1e47ac10e45f6468b",
            "isKey": false,
            "numCitedBy": 1171,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multidimensional function (that is, solving the problem of hypersurface reconstruction). From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions. These networks are not only equivalent to generalized splines but are also closely related to the classical radial basis functions used for interpolation tasks and to several pattern recognition and neural network algorithms. They also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage."
            },
            "slug": "Regularization-Algorithms-for-Learning-That-Are-to-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 794,
                                "start": 87
                            }
                        ],
                        "text": "Among the further developments beyond what we cover here are natural-gradient methods (Amari, 1998; Kakade, 2002, Peters, Vijayakumar and Schaal, 2005; Peters and Schall, 2008; Park, Kim and Kang, 2005; Bhatnagar, Sutton, Ghavamzadeh and Lee, 2009; see Grondman, Busoniu, Lopes and Babuska, 2012), and deterministic policy gradient (Silver et al., 2014). Major applications include acrobatic helicopter autopilots and AlphaGo (see Section 16.6). Our presentation in this chapter is based primarily on that by Sutton, McAllester, Singh, and Mansour (2000, see also Sutton, Singh, and McAllester, 2000), who introduced the term \u201cpolicy gradient methods.\u201d A useful overview is provided by Bhatnagar et al. (2009). One of the earliest related works is by Aleksandrov, Sysoyev, and Shemeneva (1968). Thomas (2014) first realized that the factor of \u03b3, as specified in the boxed algorithms of this chapter, was needed in the case of discounted episodic problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 993,
                                "start": 47
                            }
                        ],
                        "text": "That paper also introduced the TD(\u03bb) algorithm and proved some of its convergence properties. As we were finalizing our work on the actor\u2013critic architecture in 1981, we discovered a paper by Ian Witten (1977) which appears to be the earliest publication of a temporal-difference learning rule. He proposed the method that we now call tabular TD(0) for use as part of an adaptive controller for solving MDPs. Witten\u2019s work was a descendant of Andreae\u2019s early experiments with STeLLA and other trial-and-error learning systems. Thus, Witten\u2019s 1977 paper spanned both major threads of reinforcement learning research\u2014trial-and-error learning and optimal control\u2014while making a distinct early contribution to temporal-difference learning. The temporal-difference and optimal control threads were fully brought together in 1989 with Chris Watkins\u2019s development of Q-learning. This work extended and integrated prior work in all three threads of reinforcement learning research. Paul Werbos (1987) contributed to this integration by arguing for the convergence of trial-and-error learning and dynamic programming since 1977."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 86
                            }
                        ],
                        "text": "Among the further developments beyond what we cover here are natural-gradient methods (Amari, 1998; Kakade, 2002, Peters, Vijayakumar and Schaal, 2005; Peters and Schall, 2008; Park, Kim and Kang, 2005; Bhatnagar, Sutton, Ghavamzadeh and Lee, 2009; see Grondman, Busoniu, Lopes and Babuska, 2012), and deterministic policy gradient (Silver et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 47
                            }
                        ],
                        "text": "That paper also introduced the TD(\u03bb) algorithm and proved some of its convergence properties. As we were finalizing our work on the actor\u2013critic architecture in 1981, we discovered a paper by Ian Witten (1977) which appears to be the earliest publication of a temporal-difference learning rule."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 710,
                                "start": 87
                            }
                        ],
                        "text": "Among the further developments beyond what we cover here are natural-gradient methods (Amari, 1998; Kakade, 2002, Peters, Vijayakumar and Schaal, 2005; Peters and Schall, 2008; Park, Kim and Kang, 2005; Bhatnagar, Sutton, Ghavamzadeh and Lee, 2009; see Grondman, Busoniu, Lopes and Babuska, 2012), and deterministic policy gradient (Silver et al., 2014). Major applications include acrobatic helicopter autopilots and AlphaGo (see Section 16.6). Our presentation in this chapter is based primarily on that by Sutton, McAllester, Singh, and Mansour (2000, see also Sutton, Singh, and McAllester, 2000), who introduced the term \u201cpolicy gradient methods.\u201d A useful overview is provided by Bhatnagar et al. (2009). One of the earliest related works is by Aleksandrov, Sysoyev, and Shemeneva (1968)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 114
                            }
                        ],
                        "text": "3 The reward prediction error hypothesis of dopamine neuron activity is most prominently discussed by Schultz, Dayan, and Montague (1997). The hypothesis was first explicitly put forward by Montague, Dayan, and Sejnowski (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 114
                            }
                        ],
                        "text": "3 The reward prediction error hypothesis of dopamine neuron activity is most prominently discussed by Schultz, Dayan, and Montague (1997). The hypothesis was first explicitly put forward by Montague, Dayan, and Sejnowski (1996). As they stated the hypothesis, it referred to reward prediction errors (RPEs) but not specifically to TD errors; however, their development of the hypothesis made it clear that they were referring to TD errors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "7 Early work on using estimates of the upper confidence bound to select actions was done by Lai and Robbins (1985), Kaelbling (1993b), and Agrawal (1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207585383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a767a341364de1f75bea85e0b12ba7d3586a461",
            "isKey": true,
            "numCitedBy": 2730,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed."
            },
            "slug": "Natural-Gradient-Works-Efficiently-in-Learning-Amari",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Works Efficiently in Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802148"
                        ],
                        "name": "S. Gelly",
                        "slug": "S.-Gelly",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Gelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2917313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e7aa6d3c4272eb867419a4e88a4c064887e20b4",
            "isKey": false,
            "numCitedBy": 555,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The UCT algorithm learns a value function online using sample-based search. The TD(\u03bb) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 x 9 Go program. Each technique significantly improves MoGo's playing strength."
            },
            "slug": "Combining-online-and-offline-knowledge-in-UCT-Gelly-Silver",
            "title": {
                "fragments": [],
                "text": "Combining online and offline knowledge in UCT"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work considers three approaches for combining offline and online value functions in the UCT algorithm, and combines these algorithms in MoGo, the world's strongest 9 x 9 Go program, where each technique significantly improves MoGo's playing strength."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145880436"
                        ],
                        "name": "Xin Xu",
                        "slug": "Xin-Xu",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057038649"
                        ],
                        "name": "Tao Xie",
                        "slug": "Tao-Xie",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46570618"
                        ],
                        "name": "D. Hu",
                        "slug": "D.-Hu",
                        "structuredName": {
                            "firstName": "Dewen",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143434185"
                        ],
                        "name": "Xicheng Lu",
                        "slug": "Xicheng-Lu",
                        "structuredName": {
                            "firstName": "Xicheng",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xicheng Lu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8603944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ecd2d5b89d474c529bef401298fc0c6730cfd50c",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods have attracted many research interests recently since by utilizing Mercer kernels, non-linear and non-parametric versions of conventional supervised or unsupervised learning algorithms can be implemented and usually better generalization abilities can be obtained. However, kernel methods in reinforcement learning have not been popularly studied in the literature. In this paper, we present a novel kernel-based least-squares temporal-difference (TD) learning algorithm called KLS-TD(\u03bb), which can be viewed as the kernel version or nonlinear form of the previous linear LS-TD(\u03bb) algorithms. By introducing kernel-based nonlinear mapping, the KLS-TD(\u03bb) algorithm is superior to conventional linear TD(\u03bb) algorithms in value function prediction or policy evaluation problems with nonlinear value functions. Furthermore, in KLS-TD(\u03bb), the eligibility traces in kernel-based TD learning are derived to make use of data more efficiently, which is different from the recent work on Gaussian Processes in reinforcement learning. Experimental results on a typical value-function learning prediction problem of a Markov chain demonstrate the effectiveness of the proposed method."
            },
            "slug": "Kernel-Least-Squares-Temporal-Difference-Learning-Xu-Xie",
            "title": {
                "fragments": [],
                "text": "Kernel Least-Squares Temporal Difference Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a novel kernel-based least-squares temporal-difference (TD) learning algorithm called KLS-TD(\u03bb), which can be viewed as the kernel version or nonlinear form of the previous linear LS- TD(\u03bb) algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2787918,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "eb2f539a17487db2c93785214da2fc7a67a57840",
            "isKey": false,
            "numCitedBy": 592,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Quantitative-Results-Concerning-the-Utility-of-Minton",
            "title": {
                "fragments": [],
                "text": "Quantitative Results Concerning the Utility of Explanation-based Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388954491"
                        ],
                        "name": "Joshua W. Brown",
                        "slug": "Joshua-W.-Brown",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Brown",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua W. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145218384"
                        ],
                        "name": "D. Bullock",
                        "slug": "D.-Bullock",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bullock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bullock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8604570,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "2bf5bdb70412b5eac283f6b50b829d12470a98e7",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "After classically conditioned learning, dopaminergic cells in the substantia nigra pars compacta (SNc) respond immediately to unexpected conditioned stimuli (CS) but omit formerly seen responses to expected unconditioned stimuli, notably rewards. These cells play an important role in reinforcement learning. A neural model explains the key neurophysiological properties of these cells before, during, and after conditioning, as well as related anatomical and neurophysiological data about the pedunculopontine tegmental nucleus (PPTN), lateral hypothalamus, ventral striatum, and striosomes. The model proposes how two parallel learning pathways from limbic cortex to the SNc, one devoted to excitatory conditioning (through the ventral striatum, ventral pallidum, and PPTN) and the other to adaptively timed inhibitory conditioning (through the striosomes), control SNc responses. The excitatory pathway generates CS-induced excitatory SNc dopamine bursts. The inhibitory pathway prevents dopamine bursts in response to predictable reward-related signals. When expected rewards are not received, striosomal inhibition of SNc that is unopposed by excitation results in a phasic drop in dopamine cell activity. The adaptively timed inhibitory learning uses an intracellular spectrum of timed responses that is proposed to be similar to adaptively timed cellular mechanisms in the hippocampus and cerebellum. These mechanisms are proposed to include metabotropic glutamate receptor-mediated Ca2+ spikes that occur with different delays in striosomal cells. A dopaminergic burst in concert with a Ca2+ spike is proposed to potentiate inhibitory learning. The model provides a biologically predictive alternative to temporal difference conditioning models and explains substantially more data than alternative models."
            },
            "slug": "How-the-Basal-Ganglia-Use-Parallel-Excitatory-and-Brown-Bullock",
            "title": {
                "fragments": [],
                "text": "How the Basal Ganglia Use Parallel Excitatory and Inhibitory Learning Pathways to Selectively Respond to Unexpected Rewarding Cues"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A Neural model of dopaminergic cells in the substantia nigra pars compacta provides a biologically predictive alternative to temporal difference conditioning models and explains substantially more data than alternative models."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646034"
                        ],
                        "name": "C. Gallistel",
                        "slug": "C.-Gallistel",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gallistel",
                            "middleNames": [
                                "Randy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gallistel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34009497,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "cf8303457e225bbf8c736e18503aa3d2e0984c6a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deconstructing-the-law-of-effect-Gallistel",
            "title": {
                "fragments": [],
                "text": "Deconstructing the law of effect"
            },
            "venue": {
                "fragments": [],
                "text": "Games Econ. Behav."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157847390"
                        ],
                        "name": "Patchigolla Kiran Kumar",
                        "slug": "Patchigolla-Kiran-Kumar",
                        "structuredName": {
                            "firstName": "Patchigolla",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patchigolla Kiran Kumar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Interval estimation methods are due to Lai (1987) and Kaelbling (1993)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 119620718,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9b1a16c102d033765455933f342cd1c19a7bd3e9",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Some results in discrete-time stochastic adaptive control are surveyed. The survey divides itself into two parts\u2014Bayesian and non-Bayesian adaptive control. In the former area, the problems of converting an incompletely observed system into a completely observed one, multi-armed bandit processes, Bayesian adaptive control of Markov chains and Bayesian adaptive control of linear systems are exposed and surveyed. In the latter area, non-Bayesian adaptive control of Markov chains and the self-tuning regulator are dealt with. Proofs are given, where appropriate, to illustrate the methods involved."
            },
            "slug": "A-Survey-of-Some-Results-in-Stochastic-Adaptive-Kumar",
            "title": {
                "fragments": [],
                "text": "A Survey of Some Results in Stochastic Adaptive Control"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Some results in discrete-time stochastic adaptive control are surveyed and Bayesian and non-Bayesian adaptive control issues are dealt with."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145091317"
                        ],
                        "name": "W. Arthur",
                        "slug": "W.-Arthur",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Arthur",
                            "middleNames": [
                                "Brian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Arthur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 934,
                                "start": 135
                            }
                        ],
                        "text": "One goal of this research was to study artificial agents that act more like real people than do traditional idealized economic agents (Arthur, 1991). This approach expanded to the study of reinforcement learning in the context of game theory. Although reinforcement learning in economics developed largely independently of the early work in artificial intelligence, reinforcement learning and game theory is a topic of current interest in both fields, but one that is beyond the scope of this book. Camerer (2003) discusses the reinforcement learning tradition in economics, and Now\u00e9 et al. (2012) provide an overview of the subject from the point of view of multi-agent extensions to the approach that we introduce in this book. Reinforcement learning and game theory is a much different subject from reinforcement learning used in programs to play tic-tac-toe, checkers, and other recreational games. See, for example, Szita (2012) for an overview of this aspect of reinforcement learning and games."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Arthur Samuel (1959) was the first to propose and implement a learning method that included temporal-difference ideas, as part of his celebrated checkers-playing program."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 134
                            }
                        ],
                        "text": "One goal of this research was to study artificial agents that act more like real people than do traditional idealized economic agents (Arthur, 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 514,
                                "start": 135
                            }
                        ],
                        "text": "One goal of this research was to study artificial agents that act more like real people than do traditional idealized economic agents (Arthur, 1991). This approach expanded to the study of reinforcement learning in the context of game theory. Although reinforcement learning in economics developed largely independently of the early work in artificial intelligence, reinforcement learning and game theory is a topic of current interest in both fields, but one that is beyond the scope of this book. Camerer (2003) discusses the reinforcement learning tradition in economics, and Now\u00e9 et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 598,
                                "start": 135
                            }
                        ],
                        "text": "One goal of this research was to study artificial agents that act more like real people than do traditional idealized economic agents (Arthur, 1991). This approach expanded to the study of reinforcement learning in the context of game theory. Although reinforcement learning in economics developed largely independently of the early work in artificial intelligence, reinforcement learning and game theory is a topic of current interest in both fields, but one that is beyond the scope of this book. Camerer (2003) discusses the reinforcement learning tradition in economics, and Now\u00e9 et al. (2012) provide an overview of the subject from the point of view of multi-agent extensions to the approach that we introduce in this book."
                    },
                    "intents": []
                }
            ],
            "corpusId": 152684405,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "428d7dcca59ad16b377b167aa9074f9278852186",
            "isKey": true,
            "numCitedBy": 326,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Most economists accept that there are limits to the reasoning abilities of human beings-that human rationality is bounded. The question is how to model economic choices made under these limits. Where, between perfect rationality and its complete absence, are we to set the \"dial of rationality,\" and how do we build this dial setting in to our theoretical models? One approach to this problem is to lay down axioms or assumptions that suppose limits to economic agents' computational ability or memory, and investigate their consequences. This is useful, but it begs the question of how humans actually behave. A different approach (the one I suggest here) is to develop theoretical economic agents that act and choose in the way actual humans do. We could do this by representing agents as using parametrized decision algorithms, and choose and calibrate these algorithms so that the agents' behavior matches real human behavior observed in the same decision context. Theoretical models using these \"calibrated agents\" would then, we could claim, furnish predictions based on actual rather than idealized behavior. It is unlikely there exists some yet-to-bedefined decision algorithm, some \"model of man,\" that would represent human behavior in all economic problems-an algorithm whose parameters would constitute universal constants of human behavior. Different contexts of decision making in the economy call for different actions; and an algorithm calibrated to reproduce human learning in a search problem might differ from one that reproduces strategic-choice behavior. We would likely need a repertoire of calibrated algorithms to cover the various contexts that might arise. Nevertheless, for a particular context of decision making, calibrating theoretical behavior to match human behavior would allow us to ask questions that are not answerable at present under the assumption of either perfect rationality or idealized learning. We might want to know whether a given neoclassical model with human agents represented by \"calibrated agents\" will result in some standard asymptotic patterna rational-expectations equilibrium, say. We might ask whether agents calibrated to learn as humans do converge to some form of optimality, or interactively to a Nash equilibrium.' And we might want to study the speed of adaptation in a particular economic model with human agents represented by calibrated agents. What would it mean to calibrate an algorithm to \"reproduce\" human behavior? The object would be algorithmic behavior that reproduces statistically the characteristics of human choice, including the distinctive errors or departures from rationality that hutDiscussants: Ken Binmore, University of Michigan; Drew Fudenberg, MIT; John Geanakoplos, Yale University."
            },
            "slug": "Designing-Economic-Agents-that-Act-Like-Human-A-to-Arthur",
            "title": {
                "fragments": [],
                "text": "Designing Economic Agents that Act Like Human Agents: A Behavioral Approach to Bounded Rationality"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1346015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28be6c2ed074a7fa63818a1730b04219d8a01c02",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The method of temporal differences (TD) is one way of making consistent predictions about the future. This paper uses some analysis of Watkins (1989) to extend a convergence theorem due to Sutton (1988) from the case which only uses information from adjacent time steps to that involving information from arbitrary ones.It also considers how this version of TD behaves in the face of linearly dependent representations for states\u2014demonstrating that it still converges, but to a different answer from the least mean squares algorithm. Finally it adapts Watkins' theorem that Q-learning, his closely related prediction and action learning method, converges with probability one, to demonstrate this strong form of convergence for a slightly modified version of TD."
            },
            "slug": "The-convergence-of-TD(\u03bb)-for-general-\u03bb-Dayan",
            "title": {
                "fragments": [],
                "text": "The convergence of TD(\u03bb) for general \u03bb"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Watkins' theorem that Q-learning, his closely related prediction and action learning method, converges with probability one is adapted to demonstrate this strong form of convergence for a slightly modified version of TD."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60847433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d181a84258bd1249768bfaaa580be5dd1338c596",
            "isKey": false,
            "numCitedBy": 590,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "THESIS. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. Dynamic Feedback, Statistical Estimation, and Systems Optimization: General Techniques. The Multivariate ARMA(1,1) Model: Its Significance and Estimation. Simulation Studies of Techniques of Time--Series Analysis. General Applications of These Ideas: Practical Hazards and New Possibilities. Nationalism and Social Communications: A Test Case for Mathematical Approaches. APPLICATIONS AND EXTENSIONS. Forms of Backpropagation for Sensitivity Analysis, Optimization, and Neural Networks. Backpropagation Through Time: What It Does and How to Do It. Neurocontrol: Where It Is Going and Why It Is Crucial. Neural Networks and the Human Mind: New Mathematics Fits Humanistic Insight. Index."
            },
            "slug": "The-Roots-of-Backpropagation:-From-Ordered-to-and-Werbos",
            "title": {
                "fragments": [],
                "text": "The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book discusses forms of Backpropagation for Sensitivity Analysis, Optimization, and Neural Networks, and the importance of the Multivariate ARMA(1,1) Model in this regard."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48072291"
                        ],
                        "name": "V. V. Valentin",
                        "slug": "V.-V.-Valentin",
                        "structuredName": {
                            "firstName": "Vivian",
                            "lastName": "Valentin",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. V. Valentin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145189997"
                        ],
                        "name": "A. Dickinson",
                        "slug": "A.-Dickinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Dickinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dickinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101096038"
                        ],
                        "name": "J. O'Doherty",
                        "slug": "J.-O'Doherty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "O'Doherty",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. O'Doherty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2994735,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "7892d324abf4455f247063fb07343f604a5a1353",
            "isKey": false,
            "numCitedBy": 470,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Instrumental conditioning is considered to involve at least two distinct learning systems: a goal-directed system that learns associations between responses and the incentive value of outcomes, and a habit system that learns associations between stimuli and responses without any link to the outcome that that response engendered. Lesion studies in rodents suggest that these two distinct components of instrumental conditioning may be mediated by anatomically distinct neural systems. The aim of the present study was to determine the neural substrates of the goal-directed component of instrumental learning in humans. Nineteen human subjects were scanned with functional magnetic resonance imaging while they learned to choose instrumental actions that were associated with the subsequent delivery of different food rewards (tomato juice, chocolate milk, and orange juice). After training, one of these foods was devalued by feeding the subject to satiety on that food. The subjects were then scanned again, while being re-exposed to the instrumental choice procedure (in extinction). We hypothesized that regions of the brain involved in goal-directed learning would show changes in their activity as a function of outcome devaluation. Our results indicate that neural activity in one brain region in particular, the orbitofrontal cortex, showed a strong modulation in its activity during selection of a devalued compared with a nondevalued action. These results suggest an important contribution of orbitofrontal cortex in guiding goal-directed instrumental choices in humans."
            },
            "slug": "Determining-the-Neural-Substrates-of-Goal-Directed-Valentin-Dickinson",
            "title": {
                "fragments": [],
                "text": "Determining the Neural Substrates of Goal-Directed Learning in the Human Brain"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Investigation of the neural substrates of the goal-directed component of instrumental learning in humans indicates that neural activity in one brain region in particular, the orbitofrontal cortex, showed a strong modulation in its activity during selection of a devalued compared with a nondevalued action."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13928975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c980493673bad9bcb888dd18788d2b4b3ecb7a2e",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Sutton's Dyna framework provides a novel and computationally appealing way to integrate learning, planning, and reacting in autonomous agents. Examined here is a class of strategies designed to enhance the learning and planning power of Dyna systems by increasing their computational efficiency. The benefit of using these strategies is demonstrated on some simple abstract learning tasks."
            },
            "slug": "Efficient-Learning-and-Planning-Within-the-Dyna-Peng-Williams",
            "title": {
                "fragments": [],
                "text": "Efficient Learning and Planning Within the Dyna Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A class of strategies designed to enhance the learning and planning power of Dyna systems by increasing their computational efficiency are examined."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157702467"
                        ],
                        "name": "V. Kumar",
                        "slug": "V.-Kumar",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2597670"
                        ],
                        "name": "L. Kanal",
                        "slug": "L.-Kanal",
                        "structuredName": {
                            "firstName": "Laveen",
                            "lastName": "Kanal",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kanal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59702717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fa02258b3703ab98cfc57f0885de9e43de06e23",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the composite decision process (CDP), a general model for discrete optimization problems. Using certain relationships between formal grammars, AND/OR graphs, and game trees, it is shown that a large number of search problems in artificial intelligence can be formulated in terms of this model. Two general classes of algorithms are discussed, and it is shown that most of the existing search algorithms to solve problems represented by this model fall into one of two categories. This approach to formulating and solving discrete optimization problems makes it possible to view several search procedures in a unified manner and clarifies the relationships among them. The approach also aids in synthesizing new variations and generalizations of existing search procedures."
            },
            "slug": "The-CDP:-A-unifying-formulation-for-heuristic-and-Kumar-Kanal",
            "title": {
                "fragments": [],
                "text": "The CDP: A unifying formulation for heuristic search, dynamic programming, and branch-and-bound"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper presents the composite decision process (CDP), a general model for discrete optimization problems, and it is shown that a large number of search problems in artificial intelligence can be formulated in terms of this model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259367"
                        ],
                        "name": "J. Albus",
                        "slug": "J.-Albus",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Albus",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Albus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 753,
                                "start": 52
                            }
                        ],
                        "text": "4 Tile coding, including hashing, was introduced by Albus (1971, 1981). He described it in terms of his \u201ccerebellar model articulator controller,\u201d or CMAC, as tile coding is sometimes known in the literature. The term \u201ctile coding\u201d was new to the first edition of this book, though the idea of describing CMAC in these terms is taken from Watkins (1989). Tile coding has been used in many reinforcement learning systems (e.g., Shewchuk and Dean, 1990; Lin and Kim, 1991; Miller, Scalera, and Kim, 1994; Sofge and White, 1992; Tham, 1994; Sutton, 1996; Watkins, 1989) as well as in other types of learning control systems (e.g., Kraft and Campagna, 1990; Kraft, Miller, and Dietz, 1992). This section draws heavily on the work of Miller and Glanz (1996). General software for tile coding is available on the web in several languages (e."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 354,
                                "start": 52
                            }
                        ],
                        "text": "4 Tile coding, including hashing, was introduced by Albus (1971, 1981). He described it in terms of his \u201ccerebellar model articulator controller,\u201d or CMAC, as tile coding is sometimes known in the literature. The term \u201ctile coding\u201d was new to the first edition of this book, though the idea of describing CMAC in these terms is taken from Watkins (1989). Tile coding has been used in many reinforcement learning systems (e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7157466,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "baa317e95f76f9286c91b1cc8250665d6aaab7fa",
            "isKey": false,
            "numCitedBy": 2395,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theory-of-Cerebellar-Function-Albus",
            "title": {
                "fragments": [],
                "text": "A Theory of Cerebellar Function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9219592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b502694a8e23d3b9ab08fa7a0c6d5e82bd1f066",
            "isKey": false,
            "numCitedBy": 1689,
            "numCiting": 403,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper surveys locally weighted learning, a form of lazy learning and memory-based learning, and focuses on locally weighted linear regression. The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning fit parameters, interference between old and new data, implementing locally weighted learning efficiently, and applications of locally weighted learning. A companion paper surveys how locally weighted learning can be used in robot learning and control."
            },
            "slug": "Locally-Weighted-Learning-Atkeson-Moore",
            "title": {
                "fragments": [],
                "text": "Locally Weighted Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning fit parameters, and applications of locally weighted learning."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence Review"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4605464"
                        ],
                        "name": "W. McCulloch",
                        "slug": "W.-McCulloch",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "McCulloch",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. McCulloch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50314979"
                        ],
                        "name": "W. Pitts",
                        "slug": "W.-Pitts",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Pitts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Pitts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15619658,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "090c5a5df345ab60c41d6de02b3e366e1a27cf43",
            "isKey": false,
            "numCitedBy": 6084,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed."
            },
            "slug": "A-logical-calculus-of-the-ideas-immanent-in-nervous-McCulloch-Pitts",
            "title": {
                "fragments": [],
                "text": "A logical calculus of the ideas immanent in nervous activity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023560"
                        ],
                        "name": "G. Pagnoni",
                        "slug": "G.-Pagnoni",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Pagnoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pagnoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40634049"
                        ],
                        "name": "C. Zink",
                        "slug": "C.-Zink",
                        "structuredName": {
                            "firstName": "Caroline",
                            "lastName": "Zink",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Zink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763536"
                        ],
                        "name": "G. Berns",
                        "slug": "G.-Berns",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Berns",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Berns"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6997636,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "ad70487d0b2bd2f1af61d54b0e443c6ea968bac2",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The mesolimbic dopaminergic system has long been known to be involved in the processing of rewarding stimuli, although recent evidence from animal research has suggested a more specific role of signaling errors in the prediction of rewards. We tested this hypothesis in humans, using functional magnetic resonance imaging (fMRI) and an operant conditioning paradigm for the discrete delivery of small quantities of fruit juice, along with a control experiment in which juice was substituted with a neutral visual stimulus. A local estimation of the activity in the ventral striatum showed a significant differentiation when the juice was withheld at the expected time of delivery; this finding was not replicated in the case of visual stimulation, providing evidence for time-locked processing of reward prediction errors in human ventral striatum."
            },
            "slug": "Activity-in-human-ventral-striatum-locked-to-errors-Pagnoni-Zink",
            "title": {
                "fragments": [],
                "text": "Activity in human ventral striatum locked to errors of reward prediction"
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731761"
                        ],
                        "name": "M. Hauskrecht",
                        "slug": "M.-Hauskrecht",
                        "structuredName": {
                            "firstName": "Milos",
                            "lastName": "Hauskrecht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hauskrecht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735986"
                        ],
                        "name": "N. Meuleau",
                        "slug": "N.-Meuleau",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Meuleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Meuleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680510"
                        ],
                        "name": "T. Dean",
                        "slug": "T.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145646162"
                        ],
                        "name": "Craig Boutilier",
                        "slug": "Craig-Boutilier",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Boutilier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig Boutilier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1180912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "036373f17e5e47bcadc289e6c57d61cf5e08fe3d",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current models that combine both primitive actions and macro-actions and leave the state space unchanged, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macroactions as local policies that act in certain regions of state space, and by restricting states in the abstract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss several ways in which macro-actions can be generated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational overhead of macro-action generation."
            },
            "slug": "Hierarchical-Solution-of-Markov-Decision-Processes-Hauskrecht-Meuleau",
            "title": {
                "fragments": [],
                "text": "Hierarchical Solution of Markov Decision Processes using Macro-actions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A hierarchical model is proposed (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space, and is shown to justify the computational overhead of macro-action generation."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101096038"
                        ],
                        "name": "J. O'Doherty",
                        "slug": "J.-O'Doherty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "O'Doherty",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. O'Doherty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737497"
                        ],
                        "name": "Karl J. Friston",
                        "slug": "Karl-J.-Friston",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Friston",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl J. Friston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31528641"
                        ],
                        "name": "H. Critchley",
                        "slug": "H.-Critchley",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Critchley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Critchley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2231343"
                        ],
                        "name": "R. Dolan",
                        "slug": "R.-Dolan",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Dolan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14455400,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "75e2da1eb176214026605d1d260d370e443e9942",
            "isKey": false,
            "numCitedBy": 1365,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-Difference-Models-and-Reward-Related-in-O'Doherty-Dayan",
            "title": {
                "fragments": [],
                "text": "Temporal Difference Models and Reward-Related Learning in the Human Brain"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2669803"
                        ],
                        "name": "K. Berridge",
                        "slug": "K.-Berridge",
                        "structuredName": {
                            "firstName": "Kent",
                            "lastName": "Berridge",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Berridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32016909"
                        ],
                        "name": "T. Robinson",
                        "slug": "T.-Robinson",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Robinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Robinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11959878,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "f8a7bddcc781545ec7305181733085b79d0ade50",
            "isKey": false,
            "numCitedBy": 3761,
            "numCiting": 489,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-the-role-of-dopamine-in-reward:-hedonic-or-Berridge-Robinson",
            "title": {
                "fragments": [],
                "text": "What is the role of dopamine in reward: hedonic impact, reward learning, or incentive salience?"
            },
            "venue": {
                "fragments": [],
                "text": "Brain Research Reviews"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731282"
                        ],
                        "name": "Benjamin Van Roy",
                        "slug": "Benjamin-Van-Roy",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Van Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2202186,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4a5a283e178f187e64d0c1019314d0a6e1d4ca96",
            "isKey": false,
            "numCitedBy": 471,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a methodological framework and present a few different ways in which dynamic programming and compact representations can be combined to solve large scale stochastic control problems. In particular, we develop algorithms that employ two types of feature-based compact representations; that is, representations that involve feature extraction and a relatively simple approximation architecture. We prove the convergence of these algorithms and provide bounds on the approximation error. As an example, one of these algorithms is used to generate a strategy for the game of Tetris. Furthermore, we provide a counter-example illustrating the difficulties of integrating compact representations with dynamic programming, which exemplifies the shortcomings of certain simple approaches."
            },
            "slug": "Feature-based-methods-for-large-scale-dynamic-Tsitsiklis-Roy",
            "title": {
                "fragments": [],
                "text": "Feature-based methods for large scale dynamic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A methodological framework is developed and algorithms that employ two types of feature-based compact representations; that is, representations that involve feature extraction and a relatively simple approximation architecture are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 34th IEEE Conference on Decision and Control"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32712371"
                        ],
                        "name": "P. Calabresi",
                        "slug": "P.-Calabresi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Calabresi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Calabresi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6772922"
                        ],
                        "name": "B. Picconi",
                        "slug": "B.-Picconi",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Picconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Picconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761674"
                        ],
                        "name": "A. Tozzi",
                        "slug": "A.-Tozzi",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Tozzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tozzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3591916"
                        ],
                        "name": "M. Filippo",
                        "slug": "M.-Filippo",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Filippo",
                            "middleNames": [
                                "Di"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Filippo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 372,
                                "start": 156
                            }
                        ],
                        "text": "setting by Valentin, Dickinson, and O\u2019Doherty (2007) suggest that the orbitofrontal cortex (OFC) is an important component of goal-directed choice, Rangel, Camerer, and Montague (2008) and Rangel and Hare (2010) reviewed findings from the perspective of neuroeconomics about how the brain makes goal-directed decisions. Pezzulo, van der Meer, Lansink, and Pennartz (2014) reviewed the neuroscience of internally generated sequences and presented a model of how these mechanisms might be components of model-based planning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 546,
                                "start": 156
                            }
                        ],
                        "text": "setting by Valentin, Dickinson, and O\u2019Doherty (2007) suggest that the orbitofrontal cortex (OFC) is an important component of goal-directed choice, Rangel, Camerer, and Montague (2008) and Rangel and Hare (2010) reviewed findings from the perspective of neuroeconomics about how the brain makes goal-directed decisions. Pezzulo, van der Meer, Lansink, and Pennartz (2014) reviewed the neuroscience of internally generated sequences and presented a model of how these mechanisms might be components of model-based planning. Daw and Shohamy (2008) proposed that while dopamine signaling connects well to habitual, or model-free, behavior, other processes are involved in goal-directed, or model-based, behavior."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 156
                            }
                        ],
                        "text": "setting by Valentin, Dickinson, and O\u2019Doherty (2007) suggest that the orbitofrontal cortex (OFC) is an important component of goal-directed choice, Rangel, Camerer, and Montague (2008) and Rangel and Hare (2010) reviewed findings from the perspective of neuroeconomics about how the brain makes goal-directed decisions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 19202543,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "33990e68f77dbd32f8b4f430f7f6ee62cae10993",
            "isKey": false,
            "numCitedBy": 719,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dopamine-mediated-regulation-of-corticostriatal-Calabresi-Picconi",
            "title": {
                "fragments": [],
                "text": "Dopamine-mediated regulation of corticostriatal synaptic plasticity"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 138
                            }
                        ],
                        "text": "that we and colleagues accomplished was directed toward showing that reinforcement learning and supervised learning were indeed different (Barto, Sutton, and Brouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9296323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1140bddcd08706767375a3af6076a632b38c964",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In a previous paper we defined the associative search problem and presented a system capable of solving it under certain conditions. In this paper we interpret a spatial learning problem as an associative search task and describe the behavior of an adaptive network capable of solving it. This example shows how naturally the associative search problem can arise and permits the search, association, and generalization properties of the adaptive network to bee clearly illustrated."
            },
            "slug": "Landmark-learning:-An-illustration-of-associative-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Landmark learning: An illustration of associative search"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper interprets a spatial learning problem as an associative search task and describes the behavior of an adaptive network capable of solving it and permits the search, association, and generalization properties of the adaptive network to bee clearly illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9194763"
                        ],
                        "name": "T. Hesterberg",
                        "slug": "T.-Hesterberg",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Hesterberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hesterberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6552928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47ba51a5913a6f2be8ab874a5f2d246735c89614",
            "isKey": false,
            "numCitedBy": 1675,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The strength of this book is in bringing together advanced Monte Carlo (MC) methods developed in many disciplines. This intent is clear from the outset: \u201cMany researchers in different scienti\u008e c areas have contributed to its development: : : communications among researchers in these \u008e elds are very limited. It is therefore desirable to develop a relatively general framework in which scientists in every \u008e eld: : : can compare their Monte Carlo techniques and learn from one another.\u201d Throughout the book are examples of techniques invented, or reinvented, in different \u008e elds that may be applied elsewhere. This is occasionally embarrassing to those of us who are statisticians. Consider this statement: \u201cUsing the HMC to solve statistical inference problems was \u008e rst introduced by Neil (1996). This effort was only 10 years behind that in physics and theoretical chemistry. In contrast, statisticians were 40 years late in using the Metropolis algorithm.\u201d The book serves \u201cthree audiences: researchers specializing in the study of Monte Carlo algorithms; scientists who are interested in using advanced Monte Carlo techniques; and graduate students...second-year graduate-level course on Monte Carlo methods.\u201d Chapter 1 gives an overview and a variety of applications. These include the Ising model, molecular structure simulation, bioinformatics, target tracking, hypothesis testing for astronomical observations, Bayesian inference of multilevel models, missing-data problems. Chapter 2 covers basic MC methods and begins sequential methods, including exact sampling for chain-structured models, and sequential importance sampling and rejection control, with applications in solving a linear system, missing data, and populations genetics. Chapter 3 expands on sequential methods. The common thread is that each observation from a multivariate distribution is generated sequentially from approximate conditional distributions. The ratio between the joint density (of dimensions generated so far) and the approximation is an importance sampling weight and is a martingale; for high-dimensional problems, this tends to diverge, with most observations having weights near 0 and a few having high weight. Remedies include a variety of pruning and enrichment (also known as Russian roulette and splitting) and resampling techniques. Applications include growing a polymer, missing data, nonlinear \u008e ltering, and (in Chap. 4) molecular simulation, population genetics, motif patterns in DNA sequences, counting 0\u20131 tables with \u008e xed margins, parametric Bayes analysis, approximating permanents, target tracking, and digital communications. Chapter 5 introduces Markov chain Monte Carlo (MCMC) methods, with Metropolis\u2013Hastings and a number of generalizations, including multipoint, reversible jumping, and dynamic weighting rules. Chapters 6\u20138 treat MCMC methods based on the Gibbs sampler, including data augmentation, cluster algorithms, partial resampling, slice sampler, metropolized Gibbs, hit-and-run, random-ray, collapsing and grouping, the Swendsen\u2013Wang algorithm as data augmentation, transformation groups, and generalized Gibbs. Applications include Gaussian random \u008e elds, texture synthesis Bayesian probit regression, stochastic differential equations, hierarchical Bayes, \u008e nding motifs in protein or DNA sequences, Ising and Potts models, inference with multivariate t distributions, and parameter expansion for data augmentation. Chapter 9 considers hybrid MC and a connection to molecular dynamics algorithms used in structural biology and theoretical chemistry. Also covered are some strategies for improving ef\u008e ciency, including surrogate transition, window, and multipoint methods, and applications in Bayesian analysis and stochastic volatility. Chapters 10 and 11 discuss recent methods for ef\u008e cient MC sampling, including temperature-based methods (simulated tempering, parallel tempering, and simulated annealing), reweighting methods (umbrella sampling and multicanonical sampling) and evolution-based methods (adaptive direction sampling and conjugate gradient MC). Chapters 12 and 13 cover theory for Markov chains and their convergence rates. The book focuses on relatively more dif\u008e cult MC applications where \u201cdirectly generating independent samples from the target distribution \u008f is not feasible.\u201d It omits discussion of some relatively simple MC techniques that are valuable in applications where direct generation is feasible and which could be adapted for other applications; e.g. strati\u008e ed sampling (the \u201cstrati\u008e ed sampling\u201d technique discussed here is unusual and of limited value) post-strati\u008e cation, and defensive mixture designs in importance sampling (Hesterberg 1995). The treatment of importance sampling (IS) could be improved. The book describes the original motivation for IS\u2014focusing attention on \u201cimportant\u201d regions\u2014then indicates:"
            },
            "slug": "Monte-Carlo-Strategies-in-Scientific-Computing-Hesterberg",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Strategies in Scientific Computing"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The strength of this book is in bringing together advanced Monte Carlo methods developed in many disciplines, including the Ising model, molecular structure simulation, bioinformatics, target tracking, hypothesis testing for astronomical observations, Bayesian inference of multilevel models, missing-data problems."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698218"
                        ],
                        "name": "P. Marbach",
                        "slug": "P.-Marbach",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Marbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Marbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9620988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f608268033a797a38047575e6b4de65899eedd5f",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a simulation-based algorithm for optimizing the average reward in a Markov reward process that depends on a set of parameters. As a special case, the method applies to Markov decision processes where optimization takes place within a parametrized set of policies. The algorithm involves the simulation of a single sample path, and can be implemented online. A convergence result (with probability 1) is provided."
            },
            "slug": "Simulation-based-optimization-of-Markov-reward-Marbach-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Simulation-based optimization of Markov reward processes"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A simulation-based algorithm for optimizing the average reward in a Markov reward process that depends on a set of parameters where optimization takes place within a parametrized set of policies is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731421"
                        ],
                        "name": "Xi-Ren Cao",
                        "slug": "Xi-Ren-Cao",
                        "structuredName": {
                            "firstName": "Xi-Ren",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi-Ren Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145251852"
                        ],
                        "name": "Han-Fu Chen",
                        "slug": "Han-Fu-Chen",
                        "structuredName": {
                            "firstName": "Han-Fu",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han-Fu Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14696906,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f76a76747699db19d469587d6d66ae72e958c43b",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Two fundamental concepts and quantities, realization factors and performance potentials, are introduced for Markov processes. The relations among these two quantities and the group inverse of the infinitesimal generator are studied. It is shown that the sensitivity of the steady-state performance with respect to the change of the infinitesimal generator can be easily calculated by using either of these three quantities and that these quantities can be estimated by analyzing a single sample path of a Markov process. Based on these results, algorithms for estimating performance sensitivities on a single sample path of a Markov process can be proposed. The potentials in this paper are defined through realization factors and are shown to be the same as those defined by Poisson equations. The results provide a uniform framework of perturbation realization for infinitesimal perturbation analysis (IPA) and non-IPA approaches to the sensitivity analysis of steady-state performance; they also provide a theoretical background for the PA algorithms developed in recent years."
            },
            "slug": "Perturbation-realization,-potentials,-and-analysis-Cao-Chen",
            "title": {
                "fragments": [],
                "text": "Perturbation realization, potentials, and sensitivity analysis of Markov processes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results provide a uniform framework of perturbation realization for infinitesimal perturbations analysis (IPA) and non-IPA approaches to the sensitivity analysis of steady-state performance; they also provide a theoretical background for the PA algorithms developed in recent years."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Autom. Control."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042818"
                        ],
                        "name": "D. Aberdeen",
                        "slug": "D.-Aberdeen",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Aberdeen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Aberdeen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40565216"
                        ],
                        "name": "Jin Yu",
                        "slug": "Jin-Yu",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10105225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e4ced680344a6e8ded1396567d9342fafcef196",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems. We improve its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products. In our experiments the resulting algorithms outperform previously employed online stochastic, offline conjugate, and natural policy gradient methods."
            },
            "slug": "Fast-Online-Policy-Gradient-Learning-with-SMD-Gain-Schraudolph-Aberdeen",
            "title": {
                "fragments": [],
                "text": "Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work improves its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products that outperform previously employed online stochastics, offline conjugate, and natural policy gradient methods."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650569"
                        ],
                        "name": "A. Klopf",
                        "slug": "A.-Klopf",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Klopf",
                            "middleNames": [
                                "Harry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Klopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As we outlined in Chapter 1, the idea of TD learning has its early roots in animal learning psychology and artificial intelligence, most notably the work of Samuel (1959) and Klopf (1972). Samuel's work is described as a case study in Section 11 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The idea of a stimulus trace serving exclusively for credit assignment is apparently due to Klopf (1972), who proposed that a neuron's synapses become ``eligible\" under certain conditions for modification if, and when, reinforcement arrives later at the neuron."
                    },
                    "intents": []
                }
            ],
            "corpusId": 56108868,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0c1accd2ef7218534a1726a8de7d6e7c14271a75",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : A new theory of intelligent adaptive systems is proposed. The theory provides a single unifying framework within which the neurophysiological, psychological, and sociological properties of living adaptive systems can be understood. Furthermore, the theory offers a new basis for the synthesis of machines possessing adaptive intelligence. The proposed theory is of a heterostatic type. That is to say, it is a theory which assumes that living adaptive systems seek, as their primary goal, a maximal condition (heterostasis) , rather than assuming that the primary goal is a steadystate condition (homeostasis). It is further assumed that the heterostatic nature of animals, including man, derives from the heterostatic nature of neurons. The postulate that the neuron is a heterostat (that is, a maximizer) is a generalization of a more specific postulate, namely, that the neuron is a hedonist. This latter postulate is interpreted strictly in terms of physical variables, yielding the heterostatic neuronal model that is the basis for the detailed development of the theory."
            },
            "slug": "Brain-Function-and-Adaptive-Systems:-A-Heterostatic-Klopf",
            "title": {
                "fragments": [],
                "text": "Brain Function and Adaptive Systems: A Heterostatic Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The theory provides a single unifying framework within which the neurophysiological, psychological, and sociological properties of living adaptive systems can be understood and offers a new basis for the synthesis of machines possessing adaptive intelligence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783006"
                        ],
                        "name": "J. Byrne",
                        "slug": "J.-Byrne",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Byrne",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Byrne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31562636"
                        ],
                        "name": "K. Gingrich",
                        "slug": "K.-Gingrich",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gingrich",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gingrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777069"
                        ],
                        "name": "D. A. Baxter",
                        "slug": "D.-A.-Baxter",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Baxter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. A. Baxter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141846860,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "410af288c634164a88d3a7051a94be677027fba1",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computational-Capabilities-of-Single-Neurons:-to-of-Byrne-Gingrich",
            "title": {
                "fragments": [],
                "text": "Computational Capabilities of Single Neurons: Relationship to Simple Forms of Associative and Nonassociative Learning in Aplysia"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2520096"
                        ],
                        "name": "R\u00e9mi Coulom",
                        "slug": "R\u00e9mi-Coulom",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Coulom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Coulom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 66
                            }
                        ],
                        "text": "7 The central ideas of Monte Carlo Tree Search were introduced by Coulom (2006) and by Kocsis and Szepesv\u00e1ri (2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 66
                            }
                        ],
                        "text": "7 The central ideas of Monte Carlo Tree Search were introduced by Coulom (2006) and by Kocsis and Szepesv\u00e1ri (2006). David Silver contributed to the ideas and presentation in this section."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16724115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5d64db0dcd088a9db3480aecf52a3f96dc1499b",
            "isKey": false,
            "numCitedBy": 1109,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to minmax as the number of simulations grows. This approach provides a finegrained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9 \u00d7 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament."
            },
            "slug": "Efficient-Selectivity-and-Backup-Operators-in-Tree-Coulom",
            "title": {
                "fragments": [],
                "text": "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte- carlo phase is presented, that provides finegrained control of the tree growth, at the level of individual simulations, and allows efficient selectivity."
            },
            "venue": {
                "fragments": [],
                "text": "Computers and Games"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 133
                            }
                        ],
                        "text": "He and Barto refined these ideas and developed a psychological model of classical conditioning based on temporal-difference learning (Sutton and Barto, 1981a; Barto and Sutton, 1982)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28329561,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "b419910ab2914fcb2cf75bc66708146125ae686c",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Simulation-of-anticipatory-responses-in-classical-a-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Simulation of anticipatory responses in classical conditioning by a neuron-like adaptive element"
            },
            "venue": {
                "fragments": [],
                "text": "Behavioural Brain Research"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819261"
                        ],
                        "name": "S. Bradtke",
                        "slug": "S.-Bradtke",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bradtke",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bradtke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 35
                            }
                        ],
                        "text": "7 LSTD is due to Bradtke and Barto (see Bradtke, 1993, 1994; Bradtke and Barto, 1996; Bradtke, Ydstie, and Barto, 1994), and was further developed by Boyan (2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10316699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e1b9de83a9e2f53bcb3f27e18d349fd63b40fa",
            "isKey": false,
            "numCitedBy": 524,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce two new temporal diffence (TD) algorithms based on the theory of linear least-squares function approximation. We define an algorithm we call Least-Squares TD (LS TD) for which we prove probability-one convergence when it is used with a function approximator linear in the adjustable parameters. We then define a recursive version of this algorithm, Recursive Least-Squares TD (RLS TD). Although these new TD algorithms require more computation per time-step than do Sutton's TD(\u03bb) algorithms, they are more efficient in a statistical sense because they extract more information from training experiences. We describe a simulation experiment showing the substantial improvement in learning rate achieved by RLS TD in an example Markov prediction problem. To quantify this improvement, we introduce theTD error variance of a Markov chain, \u03c9TD, and experimentally conclude that the convergence rate of a TD algorithm depends linearly on \u03c9TD. In addition to converging more rapidly, LS TD and RLS TD do not have control parameters, such as a learning rate parameter, thus eliminating the possibility of achieving poor performance by an unlucky choice of parameters."
            },
            "slug": "Linear-Least-Squares-algorithms-for-temporal-Bradtke-Barto",
            "title": {
                "fragments": [],
                "text": "Linear Least-Squares algorithms for temporal difference learning"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Two new temporal diffence algorithms based on the theory of linear least-squares function approximation, LS TD and RLS TD, are introduced and prove probability-one convergence when it is used with a function approximator linear in the adjustable parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32589991"
                        ],
                        "name": "P. E. An",
                        "slug": "P.-E.-An",
                        "structuredName": {
                            "firstName": "Pak",
                            "lastName": "An",
                            "middleNames": [
                                "Cheung",
                                "Edgar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. E. An"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 67149543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efe838313137a8bf4ea8edfd4ec9562ff675c08a",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The standard CMAC has been shown to have fast learning computation as a result of modular receptive field placement, rectangular receptive field shape and a simple weight adaptation algorithm. The standard CMAC, however, suffers from slow convergence at some critical frequency due to the rectangular receptive field shape. A linearly-tapered field, which requires a uniform placement, was used in this research. The receptive field placement of the standard CMAC becomes less uniform locally for a larger receptive field width. This dissertation suggests a new field placement which is more uniform without extra computation. Results show that the slow convergence at the critical frequency is eliminated, and the interaction of the linearly-tapered field with the new placement achieves more accurate function approximation. A theoretical bound on the receptive field width as a function of the input dimension is proposed if a uniform placement is to be achieved. Also, a procedure for adapting receptive field density to minimize the weight usage for a given approximation accuracy is suggested."
            },
            "slug": "An-improved-multi-dimensional-CMAC-neural-network:-An",
            "title": {
                "fragments": [],
                "text": "An improved multi-dimensional CMAC neural network: Receptive field function and placement"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results show that the slow convergence at the critical frequency is eliminated, and the interaction of the linearly-tapered field with the new placement achieves more accurate function approximation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5883542"
                        ],
                        "name": "E. Thorndike",
                        "slug": "E.-Thorndike",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Thorndike",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Thorndike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 227
                            }
                        ],
                        "text": "When first placed in a puzzle box, with food visible outside, all but a few of Thorndike\u2019s cats displayed \u201cevident signs of discomfort\u201d and extraordinarily vigorous activity \u201cto strive instinctively to escape from confinement\u201d (Thorndike, 1898)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 143644299,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "15364a187e0a926b9d2035f331370afbbb02fd3a",
            "isKey": false,
            "numCitedBy": 1263,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "II T his monograph is an attempt at an explanation of the nature of the process of association in the animal mind. Inasmuch as there have been no extended researches of a character similar to the present one either in subject-matter or experimental method, it is necessary to explain briefly its standpoint. Our knowledge of the mental life of animals equals in the main our knowledge of their sense-powers, of their instincts or reactions performed without experience, and of their reactions which are built up by experience. Confining our attention to the latter we find it the opinion of the better observers and analysts that these reactions can all be explained by the ordinary associative processes without aid from abstract, conceptual, inferential thinking. These associative processes then, as present in ani-mals' minds and as displayed in their acts, are my subject-matter. Any one familiar in even a general way with the literature of comparative psychology will recall that this part of the field has received faulty and unsuccessful treatment. The careful, minute, and solid knowledge of the sense-organs of animals finds no counterpart in the realm of associations and habits. We do not know how delicate or how complex or how permanent are the possible associations of any given group of animals. And although one would be rash who said that our present equipment of facts about instincts was sufficient or that our theories about it were surely sound, yet our notion of what occurs when a chick grabs a worm are luminous and infallible compared to our notion of what happens when a kitten runs into the house at the familiar call. The reasons that they have satisfied us as well as they have is just that they are so vague. We say that the kitten associates the sound 'kitty kitty' with the experience of nice milk to drink, which does very well for a common-sense answer. It also suffices as a rebuke to those who would have the kitten ratiocinate about the matter, but it fails to tell what real mental content is present. Does the kitten feel \"sound of call, memory-image of milk in a saucer in the kitchen, thought of running into the house, a feeling, finally, of 'I will run in'?\" Does he perhaps feel only the sound of the bell and an impulse to run in, similar in quality to the impulses which make \u2026"
            },
            "slug": "Animal-intelligence:-An-experimental-study-of-the-Thorndike",
            "title": {
                "fragments": [],
                "text": "Animal intelligence: An experimental study of the associative processes in animals."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1898
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34748552"
                        ],
                        "name": "Chee-Seng Chow",
                        "slug": "Chee-Seng-Chow",
                        "structuredName": {
                            "firstName": "Chee-Seng",
                            "lastName": "Chow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chee-Seng Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122807122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a321926b30f0f0c38ff2d52a0e5b5ff91a752138",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The numerical solution of discrete-time stationary infinite-horizon discounted stochastic control problems is considered for the case where the state space is continuous and the problem is to be solved approximately, within a desired accuracy. After a discussion of problem discretization, the authors introduce a multigrid version of the successive approximation algorithm that proceeds 'one way' from coarse to fine grids, and analyze its computational requirements as a function of the desired accuracy and of the discount factor. They also study the effects of a certain mixing (ergodicity) condition on the algorithm's performance. It is shown that the one-way multigrid algorithm improves upon the complexity of its single-grid variant and is, in a certain sense, optimal. >"
            },
            "slug": "An-optimal-one-way-multigrid-algorithm-for-control-Chow-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "An optimal one-way multigrid algorithm for discrete-time stochastic control"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that the one-way multigrid algorithm improves upon the complexity of its single-grid variant and is, in a certain sense, optimal."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152512161"
                        ],
                        "name": "JOHN W. Moore",
                        "slug": "JOHN-W.-Moore",
                        "structuredName": {
                            "firstName": "JOHN W.",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JOHN W. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34915397"
                        ],
                        "name": "J. Desmond",
                        "slug": "J.-Desmond",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Desmond",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Desmond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362247"
                        ],
                        "name": "N. Berthier",
                        "slug": "N.-Berthier",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Berthier",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Berthier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3364790"
                        ],
                        "name": "D. E. Blazis",
                        "slug": "D.-E.-Blazis",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Blazis",
                            "middleNames": [
                                "E.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. E. Blazis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 126
                            }
                        ],
                        "text": "There followed several other influential psychological models of classical conditioning based on temporal-difference learning (e.g., Klopf, 1988; Moore et al., 1986; Sutton and Barto, 1987, 1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 26100434,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b5d16f0d097091d48a15b355b615f89ead4abc85",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Simulation-of-the-classically-conditioned-membrane-Moore-Desmond",
            "title": {
                "fragments": [],
                "text": "Simulation of the classically conditioned nictitating membrane response by a neuron-like adaptive element: Response topography, neuronal firing, and interstimulus intervals"
            },
            "venue": {
                "fragments": [],
                "text": "Behavioural Brain Research"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096070"
                        ],
                        "name": "J. Shewchuk",
                        "slug": "J.-Shewchuk",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Shewchuk",
                            "middleNames": [
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shewchuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680510"
                        ],
                        "name": "T. Dean",
                        "slug": "T.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61570364,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aca7bc48be39efb8671347daa03f5ed60224751c",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive control problems in which the control law changes over time are considered. Such problems arise in robotics applications in which unanticipated variations in sensors, effectors, and the work environment change the desired input/output behavior of the controller. The problems are characterized in terms of learning an input/output function, and algorithms are presented for quick learning of such time-varying functions. The techniques presented are particularly effective for problems with input spaces of high dimensionality. The authors discuss why many existing algorithms are unsuitable for this type of problem and propose additional techniques for reducing the dimensionality of input spaces.<<ETX>>"
            },
            "slug": "Toward-learning-time-varying-functions-with-high-Shewchuk-Dean",
            "title": {
                "fragments": [],
                "text": "Toward learning time-varying functions with high input dimensionality"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Algorithms are presented for quick learning of time-varying functions in adaptive control problems in terms of learning an input/output function, and they are particularly effective for problems with input spaces of high dimensionality."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 5th IEEE International Symposium on Intelligent Control 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 118
                            }
                        ],
                        "text": "A particularly effective method for reducing overfitting by deep ANNs is the dropout method introduced by Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov (2014). During training, units are randomly removed from the network (dropped out) along with their connections."
                    },
                    "intents": []
                }
            ],
            "corpusId": 50027191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3106e66537a0c8f53278e553bcb38f0b0992ec0e",
            "isKey": false,
            "numCitedBy": 1243,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a \u0302 network of simple computing elements and some entities to be represented, the most straightforward scheme is to use one computing element for each entity. This is called a local representation. It is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains. This report describes a different type of representation that is less familiar and harder to think about than local representations. Each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities. The strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer, but rather in the efficiency with which it makes use of the processing abilities of networks of simple, neuron-like computing elements. Every representational scheme has its good and bad points. Distributed representations are no exception. Some desirable properties like content-addressable memory and automatic generalization arise very naturally from the use of patterns of activity as representations. Other properties, like the ability to temporarily store a large set of arbitrary associations, are much harder to achieve. The best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind. ^This research was supported by a grant from the System Development Foundation. I thank Jim Anderson, Dave Ackley Dana Ballard, Francis Crick, Scott Fahlman, Jerry Feldman, Christopher Longuet-Higgins, Don Norman, Terry Sejnowski, and Tim Shallice for helpful discussions. Jay McClelland and Dave Rumelhart helped me refine and rewrite many of the ideas presented here A substantially revised version of this report will appear as a chapter by Hinton, McClelland and Rumelhart in Parallel Distributed Processing: Explorations in the micro-structure of cognition, edited by McClelland and Rumelhart)"
            },
            "slug": "Distributed-Representations-Hinton-McClelland",
            "title": {
                "fragments": [],
                "text": "Distributed Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This report describes a different type of representation that is less familiar and harder to think about than local representations, which makes use of the processing abilities of networks of simple, neuron-like computing elements."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065631"
                        ],
                        "name": "R. Kalaba",
                        "slug": "R.-Kalaba",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kalaba",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kalaba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16881113"
                        ],
                        "name": "B. Kotkin",
                        "slug": "B.-Kotkin",
                        "structuredName": {
                            "firstName": "Bella",
                            "lastName": "Kotkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kotkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119838553,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "20f96bb603df56b569eef7b2d2fc23f9b9cf5fc5",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "In principle, this equation can be solved computationally using the same technique that applies so well to (1.3). In practice (see [1] for a discussion), questions of time and accuracy arise. There are a number of ways of circumventing these difficulties, among which the Lagrange multiplier plays a significant role. In this series of papers, we wish to present a number of applications of a new, simple and quite powerful method, that of polynomial approximation. We shall begin with a discussion of the allocation process posed in the foregoing paragraphs and continue, in subsequent papers, with a treatment of realistic trajectory and"
            },
            "slug": "Polynomial-approximation\u2014a-new-computational-in-Bellman-Kalaba",
            "title": {
                "fragments": [],
                "text": "Polynomial approximation\u2014a new computational technique in dynamic programming: Allocation processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13724949"
                        ],
                        "name": "G. Cziko",
                        "slug": "G.-Cziko",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Cziko",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cziko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 120
                            }
                        ],
                        "text": "Despite this, the Law of Effect\u2014in one form or another\u2014is widely regarded as a basic principle underlying much behavior (e.g., Hilgard and Bower, 1975; Dennett, 1978; Campbell, 1960; Cziko, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 82659779,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0c49199a74a84b67d649a795591ac3b529733453",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 The need for selection: puzzles of fit. Part 2 The achievements of selection: the fit of biological structures the emergence of instinct the immune system - selection by the enemy brain evolution and development - the selection of neurons and synapses. Part 3 The promise of selection: the origin and growth of human knowledge the adaptive modification of behaviour adapted behaviour as the control of perception the development and functioning of thought cultural knowledge as the evolution of tradition, technology and science the evolution, acquisition and use of language education - the provision and transmission of truth, or the selectionist growth of fallible knowledge?. Part 4 The use of selection: evolutionary computing - selection within silicon the artificial selection of organisms and molecules. Part 5 The universality of selection: from providence through instruction to selection - a well-travelled road universal selection theory - the second Darwinian revolution appendix - the trouble with miracles."
            },
            "slug": "Without-Miracles:-Universal-Selection-Theory-and-Cziko",
            "title": {
                "fragments": [],
                "text": "Without Miracles: Universal Selection Theory and the Second Darwinian Revolution"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The need for selection: puzzles of fit, the use of selection: evolutionary computing - selection within silicon the artificial selection of organisms and molecules and the universality of selection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076400"
                        ],
                        "name": "B. Caprile",
                        "slug": "B.-Caprile",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Caprile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caprile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10243731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "898c01de58eb3b8e790b60e0fe0db2230d88f15b",
            "isKey": false,
            "numCitedBy": 698,
            "numCiting": 152,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multi-dimensional function. We develop a theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that we call Generalized Radial Basis Functions (GRBF). GRBF networks are not only equivalent to generalized splines, but are also closely related to several pattern recognition methods and neural network algorithms. The paper introduces several extensions and applications of the technique and discusses intriguing analogies with neurobiological data."
            },
            "slug": "Extensions-of-a-Theory-of-Networks-for-and-Learning-Girosi-Poggio",
            "title": {
                "fragments": [],
                "text": "Extensions of a Theory of Networks for Approximation and Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that is called Generalized Radial Basis Functions (GRBF), which is not only equivalent to generalized splines, but is closely related to several pattern recognition methods and neural network algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67024781"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Sejnowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 224172,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "2811534b332206223188687e214ebbde0f2f294d",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a theoretical framework that shows how mesencephalic dopamine systems could distribute to their targets a signal that represents information about future expectations. In particular, we show how activity in the cerebral cortex can make predictions about future receipt of reward and how fluctuations in the activity levels of neurons in diffuse dopamine systems above and below baseline levels would represent errors in these predictions that are delivered to cortical and subcortical targets. We present a model for how such errors could be constructed in a real brain that is consistent with physiological results for a subset of dopaminergic neurons located in the ventral tegmental area and surrounding dopaminergic neurons. The theory also makes testable predictions about human choice behavior on a simple decision-making task. Furthermore, we show that, through a simple influence on synaptic plasticity, fluctuations in dopamine release can act to change the predictions in an appropriate manner."
            },
            "slug": "A-framework-for-mesencephalic-dopamine-systems-on-Montague-Dayan",
            "title": {
                "fragments": [],
                "text": "A framework for mesencephalic dopamine systems based on predictive Hebbian learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A theoretical framework is developed that shows how mesencephalic dopamine systems could distribute to their targets a signal that represents information about future expectations and shows that, through a simple influence on synaptic plasticity, fluctuations in dopamine release can act to change the predictions in an appropriate manner."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682627"
                        ],
                        "name": "R. Korf",
                        "slug": "R.-Korf",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Korf",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Korf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118776956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c02365d67b65f6ccb840b5f2eda6f892674a502",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Path-finding search occurs in the presence of various sources of knowledge such as heuristic evaluation functions, subgoals, macro-operators, and abstractions. For a given source of knowledge, we explore optimal algorithms in terms of time, space, and cost of solution path, and quantify their performance. In the absence of any knowledge, a depth-first iterative-deepening algorithm is asymptotically optimal in time and space among minimal-cost exponential tree searches. A heuristic version of the algorithm, called iterative-deepening-A*, is shown to be optimal in the same sense over heuristic searches. For solving large problems, more powerful knowledge sources, such as subgoals, macro-operators, and abstraction are required, usually at the cost of suboptimal solutions. Subgoals often drastically reduce problem solving complexity, depending upon the subgoal distance, a new measure of problem difficulty. Macro-operators store solutions to previously solved subproblems in order to speed-up solutions to new problems, and are subject to a multiplicative time-space tradeoff. Finally, an analysis of abstraction concludes that abstraction hierarchies can reduce exponential problems to linear complexity."
            },
            "slug": "Optimal-path-finding-algorithms*-Korf",
            "title": {
                "fragments": [],
                "text": "Optimal path-finding algorithms*"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An analysis of abstraction concludes that abstraction hierarchies can reduce exponential problems to linear complexity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50337944"
                        ],
                        "name": "T. Ljungberg",
                        "slug": "T.-Ljungberg",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Ljungberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ljungberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2488500"
                        ],
                        "name": "P. Apicella",
                        "slug": "P.-Apicella",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Apicella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Apicella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18024404,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "5c9bae1a3d1cdd5753a89f2c2355d62d5eafef9b",
            "isKey": false,
            "numCitedBy": 982,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Previous studies have shown that dopamine (DA) neurons respond to stimuli of behavioral significance, such as primary reward and conditioned stimuli predicting reward and eliciting behavioral reactions. The present study investigated how these responses develop and vary when the behavioral significance of stimuli changes during different stages of learning. Impulses from DA neurons were recorded with movable microelectrodes from areas A8, A9, and A10 in two awake monkeys during the successive acquisition of two behavioral tasks. Impulses of DA neurons were distinguished from other neurons by their long duration (1.8-5.0 ms) and low spontaneous frequency (0.5-7.0 imp/s). 2. In the first task, animals learned to reach in a small box in front of them when it opened visibly and audibly. Before conditioning, DA neurons were activated the first few times that the empty box opened and animals reacted with saccadic eye movements. Neuronal and behavioral responses disappeared on repeated stimulus presentation. Thus neuronal responses were related to the novelty of an unexpected stimulus eliciting orienting behavior. 3. Subsequently, the box contained a small morsel of apple in one out of six trials. Animals reacted with ocular saccades to nearly every box opening and reached out when the morsel was present. One-third of 49 neurons were phasically activated by every door opening. The response was stronger when food was present. Thus DA neurons responded simultaneously to the sight of primary food reward and to the conditioned stimulus associated with reward. 4. When the box contained a morsel of apple on every trial, animals regularly reacted with target-directed eye and arm movements, and the majority of 76 DA neurons responded to door opening. The same neurons lacked responses to a light not associated with task performance that was illuminated at the position of the food box in alternate sessions, thus demonstrating specificity for the behavioral significance of stimuli. 5. The second task employed the operant conditioning of a reaction time situation in which animals reached from a resting key toward a lever when a small light was illuminated. DA neurons lacked responses to the unconditioned light. During task acquisition lasting 2-3 days, one-half of 25 DA neurons were phasically activated when a drop of liquid reward was delivered for reinforcing the reaching movement. In contrast, neurons were not activated when reward was delivered at regular intervals (2.5-3.5 s) but a task was not performed.(ABSTRACT TRUNCATED AT 400 WORDS)"
            },
            "slug": "Responses-of-monkey-dopamine-neurons-during-of-Ljungberg-Apicella",
            "title": {
                "fragments": [],
                "text": "Responses of monkey dopamine neurons during learning of behavioral reactions."
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "D dopamine neurons responded simultaneously to the sight of primary food reward and to the conditioned stimulus associated with reward, thus demonstrating specificity for the behavioral significance of stimuli."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706614"
                        ],
                        "name": "J. Connell",
                        "slug": "J.-Connell",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Connell",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Connell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 86
                            }
                        ],
                        "text": "The recycling robot example was inspired by the cancollecting robot built by Jonathan Connell (1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 30858265,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "016492fd13554c557e5d10bdbdee85e45255f50b",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes a working autonomous mobile robot whose only goal is to collect and return empty soda cans. It operates in an unmodified office environment occupied by moving people. The robot is controlled by a collection of over 40 independent ``behaviors'''' distributed over a loosely coupled network of 24 processors. Together this ensemble helps the robot locate cans with its laser rangefinder, collect them with its on-board manipulator, and bring them home using a compass and an array of proximity sensors. We discuss the advantages of using such a multi-agent control system and show how to decompose the required tasks into component activities. We also examine the benefits and limitations of spatially local, stateless, and independent computation by the agents."
            },
            "slug": "A-colony-architecture-for-an-artificial-creature-Connell",
            "title": {
                "fragments": [],
                "text": "A colony architecture for an artificial creature"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A working autonomous mobile robot whose only goal is to collect and return empty soda cans operates in an unmodified office environment occupied by moving people and is controlled by a collection of over 40 independent ``behaviors'' distributed over a loosely coupled network of 24 processors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2211151"
                        ],
                        "name": "Colin Camerer",
                        "slug": "Colin-Camerer",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Camerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Camerer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 8
                            }
                        ],
                        "text": "Rangel, Camerer, and Montague (2008) and Rangel and Hare (2010) reviewed findings from the perspective of neuroe-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 8
                            }
                        ],
                        "text": "Rangel, Camerer, and Montague (2008) review many of the outstanding issues involving habitual, goal-directed, and Pavlovian modes of control."
                    },
                    "intents": []
                }
            ],
            "corpusId": 142793094,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "02954dd24ddd69650ad9db634ed945c27a892a76",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Game theory, the formalized study of strategy, began in the 1940s by asking how emotionless geniuses should play games, but ignored until recently how average people with emotions and limited foresight actually play games. This book marks the first substantial and authoritative effort to close this gap. Colin Camerer, one of the field's leading figures, uses psychological principles and hundreds of experiments to develop mathematical theories of reciprocity, limited strategizing, and learning, which help predict what real people and companies do in strategic situations. Unifying a wealth of information from ongoing studies in strategic behavior, he takes the experimental science of behavioral economics a major step forward. He does so in lucid, friendly prose. \n \n Behavioral game theory has three ingredients that come clearly into focus in this book: mathematical theories of how moral obligation and vengeance affect the way people bargain and trust each other; a theory of how limits in the brain constrain the number of steps of \"I think he thinks . . .\" reasoning people naturally do; and a theory of how people learn from experience to make better strategic decisions. Strategic interactions that can be explained by behavioral game theory include bargaining, games of bluffing as in sports and poker, strikes, how conventions help coordinate a joint activity, price competition and patent races, and building up reputations for trustworthiness or ruthlessness in business or life. \n \n While there are many books on standard game theory that address the way ideally rational actors operate,\u00a0Behavioral Game Theory\u00a0stands alone in blending experimental evidence and psychology in a mathematical theory of normal strategic behavior. It is must reading for anyone who seeks a more complete understanding of strategic thinking, from professional economists to scholars and students of economics, management studies, psychology, political science, anthropology, and biology."
            },
            "slug": "Behavioral-Game-Theory:-Experiments-in-Strategic-Camerer",
            "title": {
                "fragments": [],
                "text": "Behavioral Game Theory: Experiments in Strategic Interaction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1825117"
                        ],
                        "name": "M. Hammer",
                        "slug": "M.-Hammer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hammer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 65
                            }
                        ],
                        "text": "The model is based on research by Hammer, Menzel, and colleagues (Hammer and Menzel, 1995; Hammer, 1997) showing that the neuromodulator octopamine acts as a reinforcement signal in the honeybee."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17006209,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "9d53a5d317bea1ba62d1c2b5c5e99222f9bf8fbe",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-neural-basis-of-associative-reward-learning-in-Hammer",
            "title": {
                "fragments": [],
                "text": "The neural basis of associative reward learning in honeybees"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40358971"
                        ],
                        "name": "E. Tolman",
                        "slug": "E.-Tolman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Tolman",
                            "middleNames": [
                                "Chace"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tolman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 321
                            }
                        ],
                        "text": "Latent learning is most closely associated with the psychologist Edward Tolman, who interpreted this result, and others like it, as showing that animals could learn a \u201ccognitive map of the environment\u201d in the absence of rewards or penalties, and that they could use the map later when they were motivated to reach a goal (Tolman, 1948)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 42496633,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "e84d660ebf894e2fb85d7b27985bfbf07b9acd22",
            "isKey": false,
            "numCitedBy": 5558,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I shall devote the body of this paper to a description of experiments with rats. But I shall also attempt in a few words at the close to indicate the significance of these findings on rats for the clinical behavior of men. Most of the rat investigations, which I shall report, were carried out in the Berkeley laboratory. But I shall also include, occasionally, accounts of the behavior of non-Berkeley rats who obviously have misspent their lives in out-of-State laboratories. Furthermore, in reporting our Berkeley experiments I shall have to omit a very great many. The ones I shall talk about were carried out by graduate students (or underpaid research assistants) who, supposedly, got some of their ideas from me. And a few, though a very few, were even carried out by me myself. Let me begin by presenting diagrams for a couple of typical mazes, an alley maze and an elevated maze. In the typical experiment a hungry rat is put at the entrance of the maze (alley or elevated), and wanders about through the various true path segments and blind alleys until he finally comes to the food box and eats. This is repeated (again in the typical experiment) one trial every 24 hours and the animal tends to make fewer and fewer errors (that is, blindalley entrances) and to take less and less time between start and goal-box until finally he is entering no blinds at all and running in a very few seconds from start to goal. The results are usually presented in the form of average curves of blind-entrances, or of seconds from start to finish, for groups of rats. All students agree as to the facts. They disagree, however, on theory and explanation. (1) First, there is a school of animal psychologists which believes that the maze behavior of rats is a matter of mere simple stimulus-response connections. Learning, according to them, consists in the strengthening of some of these connections and in the weakening of others. According to this \u2018stimulus-response\u2019 school the rat in progressing down the maze is helplessly responding to a succession of external stimulisights, sounds, smells, pressures, etc. impinging upon his external sense organs-plus internal stimuli coming from the viscera and from the skeletal muscles. Figure 1: Plan of maze 14-Unit T-Alley Maze. (From M.H. Elliot, The effect of change of reward on the maze performance of rats. University of California Publications in Psychology, 1928, 4, 20.)"
            },
            "slug": "Cognitive-maps-in-rats-and-men.-Tolman",
            "title": {
                "fragments": [],
                "text": "Cognitive maps in rats and men."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Most of the rat investigations, which I shall report, were carried out in the Berkeley laboratory, and a few, though a very few, were even carried out by me myself."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1948
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222423"
                        ],
                        "name": "A. Gopnik",
                        "slug": "A.-Gopnik",
                        "structuredName": {
                            "firstName": "Alison",
                            "lastName": "Gopnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gopnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3058012"
                        ],
                        "name": "C. Glymour",
                        "slug": "C.-Glymour",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Glymour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Glymour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124398"
                        ],
                        "name": "D. Sobel",
                        "slug": "D.-Sobel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sobel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sobel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3693228"
                        ],
                        "name": "T. Kushnir",
                        "slug": "T.-Kushnir",
                        "structuredName": {
                            "firstName": "Tamar",
                            "lastName": "Kushnir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kushnir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143752796"
                        ],
                        "name": "D. Danks",
                        "slug": "D.-Danks",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Danks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Danks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 809336,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "792695c436fd0148a71e7f2830ea5bac7938b014",
            "isKey": false,
            "numCitedBy": 965,
            "numCiting": 161,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors outline a cognitive and computational account of causal learning in children. They propose that children use specialized cognitive systems that allow them to recover an accurate \"causal map\" of the world: an abstract, coherent, learned representation of the causal relations among events. This kind of knowledge can be perspicuously understood in terms of the formalism of directed graphical causal models, or Bayes nets. Children's causal learning and inference may involve computations similar to those for learning causal Bayes nets and for predicting with them. Experimental results suggest that 2- to 4-year-old children construct new causal maps and that their learning is consistent with the Bayes net formalism."
            },
            "slug": "A-theory-of-causal-learning-in-children:-causal-and-Gopnik-Glymour",
            "title": {
                "fragments": [],
                "text": "A theory of causal learning in children: causal maps and Bayes nets."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results suggest that 2- to 4-year-old children construct new causal maps and that their learning is consistent with the Bayes net formalism."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34908205"
                        ],
                        "name": "Levente Kocsis",
                        "slug": "Levente-Kocsis",
                        "structuredName": {
                            "firstName": "Levente",
                            "lastName": "Kocsis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Levente Kocsis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40868287"
                        ],
                        "name": "Csaba Szepesvari",
                        "slug": "Csaba-Szepesvari",
                        "structuredName": {
                            "firstName": "Csaba",
                            "lastName": "Szepesvari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Csaba Szepesvari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15184765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e635d81a617d1239232a9c9a11a196c53dab8240",
            "isKey": false,
            "numCitedBy": 2632,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives."
            },
            "slug": "Bandit-Based-Monte-Carlo-Planning-Kocsis-Szepesvari",
            "title": {
                "fragments": [],
                "text": "Bandit Based Monte-Carlo Planning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new algorithm is introduced, UCT, that applies bandit ideas to guide Monte-Carlo planning and is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2813699"
                        ],
                        "name": "A. Griffith",
                        "slug": "A.-Griffith",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Griffith",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griffith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8391141,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "77f3a837fce24d23fa64b4725c7fabc3c86ac6c2",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Comparison-and-Evaluation-of-Three-Machine-as-to-Griffith",
            "title": {
                "fragments": [],
                "text": "A Comparison and Evaluation of Three Machine Learning Procedures as Applied to the Game of Checkers"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 117
                            }
                        ],
                        "text": "The mapping was learned via supervised learning from one of the data sets by means of a random forest (RF) algorithm (Breiman, 2001)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 89141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986",
            "isKey": false,
            "numCitedBy": 65209,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression."
            },
            "slug": "Random-Forests-Breiman",
            "title": {
                "fragments": [],
                "text": "Random Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the forest, and are also applicable to regression."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776101"
                        ],
                        "name": "A. Redish",
                        "slug": "A.-Redish",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redish",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16503204,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "ea98f21a7cace5d9f5387d2c5a10e5304d8a9a6d",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Addictive drugs have been hypothesized to access the same neurophysiological mechanisms as natural learning systems. These natural learning systems can be modeled through temporal-difference reinforcement learning (TDRL), which requires a reward-error signal that has been hypothesized to be carried by dopamine. TDRL learns to predict reward by driving that reward-error signal to zero. By adding a noncompensable drug-induced dopamine increase to a TDRL model, a computational model of addiction is constructed that over-selects actions leading to drug receipt. The model provides an explanation for important aspects of the addiction literature and provides a theoretic view-point with which to address other aspects."
            },
            "slug": "Addiction-as-a-Computational-Process-Gone-Awry-Redish",
            "title": {
                "fragments": [],
                "text": "Addiction as a Computational Process Gone Awry"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A computational model of addiction is constructed that over-selects actions leading to drug receipt and provides an explanation for important aspects of the addiction literature and provides a theoretic view-point with which to address other aspects."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783471"
                        ],
                        "name": "W. Levy",
                        "slug": "W.-Levy",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Levy",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32835180"
                        ],
                        "name": "O. Steward",
                        "slug": "O.-Steward",
                        "structuredName": {
                            "firstName": "Oswald",
                            "lastName": "Steward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Steward"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16184572,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "397265325909045dbe8706b56c80d2293798ee2f",
            "isKey": false,
            "numCitedBy": 793,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-contiguity-requirements-for-long-term-in-Levy-Steward",
            "title": {
                "fragments": [],
                "text": "Temporal contiguity requirements for long-term associative potentiation/depression in the hippocampus"
            },
            "venue": {
                "fragments": [],
                "text": "Neuroscience"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 993440,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "6fc241221e6d2b92f752d7d88d2d2313f9c643f3",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A widely used class of models for stochastic systems is hidden Markov models. Systems that can be modeled by hidden Markov models are a proper subclass of linearly dependent processes, a class of stochastic systems known from mathematical investigations carried out over the past four decades. This article provides a novel, simple characterization of linearly dependent processes, called observable operator models. The mathematical properties of observable operator models lead to a constructive learning algorithm for the identification of linearly dependent processes. The core of the algorithm has a time complexity of O (N + nm3), where N is the size of training data, n is the number of distinguishable outcomes of observations, and m is model state-space dimension."
            },
            "slug": "Observable-Operator-Models-for-Discrete-Stochastic-Jaeger",
            "title": {
                "fragments": [],
                "text": "Observable Operator Models for Discrete Stochastic Time Series"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel, simple characterization of linearly dependent processes, called observable operator models, is provided, which leads to a constructive learning algorithm for the identification of linially dependent processes."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37094858,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "1a6eacdbf4e881a91cf6b76a9f70f53ccc290ae1",
            "isKey": false,
            "numCitedBy": 947,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with the problem of constructing a computing routine or \u201cprogram\u201d for a modern general purpose computer which will enable it to play chess. Although perhaps of no practical importance, the question is of theoretical interest, and it is hoped that a satisfactory solution of this problem will act as a wedge in attacking other problems of a similar nature and of greater significance. Some possibilities in this direction are:- \n \n(1) \n \nMachines for designing filters, equalizers, etc. \n \n \n \n \n(2) \n \nMachines for designing relay and switching circuits. \n \n \n \n \n(3) \n \nMachines which will handle routing of telephone calls based on the individual circumstances rather than by fixed patterns. \n \n \n \n \n(4) \n \nMachines for performing symbolic (non-numerical) mathematical operations. \n \n \n \n \n(5) \n \nMachines capable of translating from one language to another. \n \n \n \n \n(6) \n \nMachines for making strategic decisions in simplified military operations. \n \n \n \n \n(7) \n \nMachines capable of orchestrating a melody. \n \n \n \n \n(8) \n \nMachines capable of logical deduction."
            },
            "slug": "Programming-a-computer-for-playing-chess-Shannon",
            "title": {
                "fragments": [],
                "text": "Programming a computer for playing chess"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper is concerned with the problem of constructing a computing routine or \u201cprogram\u201d for a modern general purpose computer which will enable it to play chess."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686406"
                        ],
                        "name": "R. Clark",
                        "slug": "R.-Clark",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Clark",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2890841"
                        ],
                        "name": "L. Squire",
                        "slug": "L.-Squire",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Squire",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Squire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 22186860,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "5e4022c0720363c90e6cbecf146a4a5736e891f4",
            "isKey": false,
            "numCitedBy": 841,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Classical conditioning of the eye-blink response, perhaps the best studied example of associative learning in vertebrates, is relatively automatic and reflexive, and with the standard procedure (simple delay conditioning), it is intact in animals with hippocampal lesions. In delay conditioning, a tone [the conditioned stimulus (CS)] is presented just before an air puff to the eye [the unconditioned stimulus (US)]. The US is then presented, and the two stimuli coterminate. In trace conditioning, a variant of the standard paradigm, a short interval (500 to 1000 ms) is interposed between the offset of the CS and the onset of the US. Animals with hippocampal lesions fail to acquire trace conditioning. Amnesic patients with damage to the hippocampal formation and normal volunteers were tested on two versions of delay conditioning and two versions of trace conditioning and then assessed for the extent to which they became aware of the temporal relationship between the CS and the US. Amnesic patients acquired delay conditioning at a normal rate but failed to acquire trace conditioning. For normal volunteers, awareness was unrelated to successful delay conditioning but was a prerequisite for successful trace conditioning. Trace conditioning is hippocampus dependent because, as in other tasks of declarative memory, conscious knowledge must be acquired across the training session. Trace conditioning may provide a means for studying awareness in nonhuman animals, in the context of current ideas about multiple memory systems and the function of the hippocampus."
            },
            "slug": "Classical-conditioning-and-brain-systems:-the-role-Clark-Squire",
            "title": {
                "fragments": [],
                "text": "Classical conditioning and brain systems: the role of awareness."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 17
                            }
                        ],
                        "text": "What we call the Bellman equation for v\u2217 was first introduced by Richard Bellman (1957a), who called it the \u201cbasic functional equation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 734,
                                "start": 21
                            }
                        ],
                        "text": "It suffers from what Bellman called \u201cthe curse of dimensionality,\u201d meaning that its computational requirements grow exponentially with the number of state variables, but it is still far more efficient and more widely applicable than any other general method. Dynamic programming has been extensively developed since the late 1950s, including extensions to partially observable MDPs (surveyed by Lovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), approximation methods (surveyed by Rust, 1996), and asynchronous methods (Bertsekas, 1982, 1983). Many excellent modern treatments of dynamic programming are available (e.g., Bertsekas, 2005, 2012; Puterman, 1994; Ross, 1983; and Whittle, 1982, 1983). Bryson (1996) provides an authoritative history of optimal control."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 109
                            }
                        ],
                        "text": "and of a value function, or \u201coptimal return function,\u201d to define a functional equation, now often called the Bellman equation. The class of methods for solving optimal control problems by solving this equation came to be known as dynamic programming (Bellman, 1957a). Bellman (1957b) also introduced the discrete stochastic version of the optimal control problem known as Markovian decision processes (MDPs), and Ronald Howard (1960) devised the policy iteration method for MDPs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 320,
                                "start": 161
                            }
                        ],
                        "text": "In statistics, bandit problems fall under the heading \u201csequential design of experiments,\u201d introduced by Thompson (1933, 1934) and Robbins (1952), and studied by Bellman (1956). Berry and Fristedt (1985) provide an extensive treatment of bandit problems from the perspective of statistics. Narendra and Thathachar (1989) treat bandit problems from the engineering perspec-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 161
                            }
                        ],
                        "text": "In statistics, bandit problems fall under the heading \u201csequential design of experiments,\u201d introduced by Thompson (1933, 1934) and Robbins (1952), and studied by Bellman (1956). Berry and Fristedt (1985) provide an extensive treatment of bandit problems from the perspective of statistics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Bellman (1956) was the first to show how dynamic programming could be used to compute the optimal balance between exploration and exploitation within a Bayesian formulation of the problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 62
                            }
                        ],
                        "text": "(It is possible that these ideas of Shannon\u2019s also influenced Bellman, but we know of no evidence for this.) Minsky (1961) extensively discussed Samuel\u2019s work in his \u201cSteps\u201d paper, suggesting the connection to secondary reinforcement theories, both natural"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 0
                            }
                        ],
                        "text": "Bellman (1956) was the first to show how dynamic programming could be used to compute the optimal balance between exploration and exploitation within a Bayesian formulation of the problem. The survey by Kumar (1985) provides a good discussion of Bayesian and nonBayesian approaches to these problems."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122573583,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "910d3158af44ba3fc8a92ec5afd13c9ae930eff9",
            "isKey": true,
            "numCitedBy": 195,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : THE PROBLEM OF DETERMINING AN OPTIMAL TESTING POLICY WHERE ONE SIMULTANEOUSLY GAINS AND LEARNS FOR THE CASE WHERE THE OUTCOME OF ONE CHOICE IS KNOWN AND THE OTHER IS SUBJECT TO A KNOWN A PRIORI DISTRIBUTION IS CONSIDERED. Results of Johnson and Karlin, P-328, are obtained in a different way and extended. The methods used are applicable to more general processes."
            },
            "slug": "A-PROBLEM-IN-THE-SEQUENTIAL-DESIGN-OF-EXPERIMENTS-Bellman",
            "title": {
                "fragments": [],
                "text": "A PROBLEM IN THE SEQUENTIAL DESIGN OF EXPERIMENTS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43501924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77808d64daa93b0c1babc58d8dcf5c4ce34c2683",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new procedure for inferring the structure of a finitestate automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments. Our procedure uses a new representation for FSA's, based on the notion of equivalence between testa. We call the number of such equivalence classes the diversity of the automaton; the diversity may be as small as the logarithm of the number of states of the automaton. The size of our representation of the FSA, and the running time of our procedure (in some case provably, in others conjecturally) is polynomial in the diversity and ln(1/e), where e is a given upper bound on the probability that our procedure returns an incorrect result. (Since our procedure uses randomization to perform experiments, there is a certain controllable chance that it will return an erroneous result.) We also present some evidence for the practical efficiency of our approach. For example, our procedure is able to infer the structure of an automaton based on Rubik's Cube (which has approximately 1019 states) in about 2 minutes on a DEC Micro Vax. This automaton is many orders of magnitude larger than possible with previous techniques, which would require time proportional at least to the number of global states. (Note that in this example, only a small fraction (10-14) of the global states were even visited.)"
            },
            "slug": "Diversity-Based-Inference-of-Finite-Automata-Rivest-Schapire",
            "title": {
                "fragments": [],
                "text": "Diversity-Based Inference of Finite Automata (Extended Abstract)"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new procedure for inferring the structure of a finitestate automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments, based on the notion of equivalence between testa."
            },
            "venue": {
                "fragments": [],
                "text": "FOCS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1472007,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4c1b2433b8205e8dc489000b7788466ca5ca14ea",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a finite-state Markov decision problem and establish the convergence of a special case of optimistic policy iteration that involves Monte Carlo estimation of Q-values, in conjunction with greedy policy selection. We provide convergence results for a number of algorithmic variations, including one that involves temporal difference learning (bootstrapping) instead of Monte Carlo estimation. We also indicate some extensions that either fail or are unlikely to go through."
            },
            "slug": "On-the-Convergence-of-Optimistic-Policy-Iteration-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "On the Convergence of Optimistic Policy Iteration"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A finite-state Markov decision problem is considered and the convergence of a special case of optimistic policy iteration that involves Monte Carlo estimation of Q-values, in conjunction with greedy policy selection is established."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60492634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "638df1b831feb3647a9bf5496780b38890573d4d",
            "isKey": false,
            "numCitedBy": 5436,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "gineering, computer science, operations research, and applied mathematics. It is essentially a self-contained work, with the development of the material occurring in the main body of the text and excellent appendices on linear algebra and analysis, graph theory, duality theory, and probability theory and Markov chains supporting it. The introduction discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later. After the introduction, the text is organized in two parts: synchronous algorithms and asynchronous algorithms. The discussion of synchronous algorithms comprises four chapters, with Chapter 2 presenting both direct methods (converging to the exact solution within a finite number of steps) and iterative methods for linear"
            },
            "slug": "Parallel-and-Distributed-Computation:-Numerical-Bertsekas-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Parallel and Distributed Computation: Numerical Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940495"
                        ],
                        "name": "M. Thielscher",
                        "slug": "M.-Thielscher",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Thielscher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thielscher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 9
                            }
                        ],
                        "text": "See also Glimcher and Fehr (2013). The text on computational and mathematical modeling in neuroscience by Dayan and Abbott (2001) includes reinforcement learning\u2019s role in these approaches."
                    },
                    "intents": []
                }
            ],
            "corpusId": 23241902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f5a8b82c9b020c1c7d7f11c89205f1b7e142ad9",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "General game players are computer systems able to play strategy games based solely on formal game descriptions supplied at \"runtime\" (n other words, they don't know the rules until the game starts). Unlike specialized game players, such as Deep Blue, general game players cannot rely on algorithms designed in advance for specific games; they must discover such algorithms themselves. General game playing expertise depends on intelligence on the part of the game player and not just intelligence of the programmer of the game player. GGP is an interesting application in its own right. It is intellectually engaging and more than a little fun. But it is much more than that. It provides a theoretical framework for modeling discrete dynamic systems and defining rationality in a way that takes into account problem representation and complexities like incompleteness of information and resource bounds. It has practical applications in areas where these features are important, e.g., in business and law. More fundamentally, it raises questions about the nature of intelligence and serves as a laboratory in which to evaluate competing approaches to artificial intelligence. This book is an elementary introduction to General Game Playing (GGP). (1) It presents the theory of General Game Playing and leading GGP technologies. (2) It shows how to create GGP programs capable of competing against other programs and humans. (3) It offers a glimpse of some of the real-world applications of General Game Playing."
            },
            "slug": "General-Game-Playing-Thielscher",
            "title": {
                "fragments": [],
                "text": "General Game Playing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This book presents the theory of General Game Playing and leading GGP technologies, and shows how to create GGP programs capable of competing against other programs and humans."
            },
            "venue": {
                "fragments": [],
                "text": "K\u00fcnstliche Intell."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 87
                            }
                        ],
                        "text": "Doya and Sejnowski (1998) extended their earlier paper on a model of birdsong learning (Doya and Sejnowski, 1995) by including a TD-like error identified with dopamine to reinforce the selection of auditory input to be memorized."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6223123,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "582b12885316187a33c8f901bf6382cf0a8d4367",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Songbirds learn to imitate a tutor song through auditory and motor learning. We have developed a theoretical framework for song learning that accounts for response properties of neurons that have been observed in many of the nuclei that are involved in song learning. Specifically, we suggest that the anterior forebrain pathway, which is not needed for song production in the adult but is essential for song acquisition, provides synaptic perturbations and adaptive evaluations for syllable vocalization learning. A computer model based on reinforcement learning was constructed that could replicate a real zebra finch song with 90% accuracy based on a spectrographic measure. The second generation of the birdsong model replicated the tutor song with 96% accuracy."
            },
            "slug": "A-Novel-Reinforcement-Model-of-Birdsong-Learning-Doya-Sejnowski",
            "title": {
                "fragments": [],
                "text": "A Novel Reinforcement Model of Birdsong Vocalization Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is suggested that the anterior forebrain pathway, which is not needed for song production in the adult but is essential for song acquisition, provides synaptic perturbations and adaptive evaluations for syllable vocalization learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615848"
                        ],
                        "name": "Rajesh P. N. Rao",
                        "slug": "Rajesh-P.-N.-Rao",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Rao",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajesh P. N. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1215146,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "659a8118b1c061576f64f1f0237c5f15435194a9",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "A spike-timing-dependent Hebbian mechanism governs the plasticity of recurrent excitatory synapses in the neocortex: synapses that are activated a few milliseconds before a postsynaptic spike are potentiated, while those that are activated a few milliseconds after are depressed. We show that such a mechanism can implement a form of temporal difference learning for prediction of input sequences. Using a biophysical model of a cortical neuron, we show that a temporal difference rule used in conjunction with dendritic backpropagating action potentials reproduces the temporally asymmetric window of Hebbian plasticity observed physiologically. Furthermore, the size and shape of the window vary with the distance of the synapse from the soma. Using a simple example, we show how a spike-timing-based temporal difference learning rule can allow a network of neocortical neurons to predict an input a few milliseconds before the input's expected arrival."
            },
            "slug": "Spike-Timing-Dependent-Hebbian-Plasticity-as-Rao-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Spike-Timing-Dependent Hebbian Plasticity as Temporal Difference Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Using a biophysical model of a cortical neuron, it is shown that a temporal difference rule used in conjunction with dendritic backpropagating action potentials reproduces the temporally asymmetric window of Hebbian plasticity observed physiologically."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819261"
                        ],
                        "name": "S. Bradtke",
                        "slug": "S.-Bradtke",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bradtke",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bradtke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885776"
                        ],
                        "name": "B. Ydstie",
                        "slug": "B.-Ydstie",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Ydstie",
                            "middleNames": [
                                "Erik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ydstie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14505901,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "c0720cba764469053b3fea2c3f6adb8127106490",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present the stability and convergence results for dynamic programming-based reinforcement learning applied to linear quadratic regulation (LQR). The specific algorithm we analyze is based on Q-learning and it is proven to converge to an optimal controller provided that the underlying system is controllable and a particular signal vector is persistently excited. This is the first convergence result for DP-based reinforcement learning algorithms for a continuous problem."
            },
            "slug": "Adaptive-linear-quadratic-control-using-policy-Bradtke-Ydstie",
            "title": {
                "fragments": [],
                "text": "Adaptive linear quadratic control using policy iteration"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "The stability and convergence results for dynamic programming-based reinforcement learning applied to linear quadratic regulation (LQR) are presented and the specific algorithm is based on Q-learning and it is proven to converge to an optimal controller."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 American Control Conference - ACC '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31993898,
            "fieldsOfStudy": [
                "Mathematics",
                "Art"
            ],
            "id": "3bb5a439a0d610a7eac68f73068cdd278b8c9775",
            "isKey": false,
            "numCitedBy": 21002,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Neal",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion and will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1988233"
                        ],
                        "name": "M. Roesch",
                        "slug": "M.-Roesch",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Roesch",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Roesch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5118743"
                        ],
                        "name": "Donna J. Calu",
                        "slug": "Donna-J.-Calu",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Calu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donna J. Calu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4192235"
                        ],
                        "name": "G. Schoenbaum",
                        "slug": "G.-Schoenbaum",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Schoenbaum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schoenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7122570,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "77c1f6f24afc317364e5e55f03f219bc5cc967ee",
            "isKey": false,
            "numCitedBy": 569,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The dopamine system is thought to be involved in making decisions about reward. Here we recorded from the ventral tegmental area in rats learning to choose between differently delayed and sized rewards. As expected, the activity of many putative dopamine neurons reflected reward prediction errors, changing when the value of the reward increased or decreased unexpectedly. During learning, neural responses to reward in these neurons waned and responses to cues that predicted reward emerged. Notably, this cue-evoked activity varied with size and delay. Moreover, when rats were given a choice between two differently valued outcomes, the activity of the neurons initially reflected the more valuable option, even when it was not subsequently selected."
            },
            "slug": "Dopamine-neurons-encode-the-better-option-in-rats-Roesch-Calu",
            "title": {
                "fragments": [],
                "text": "Dopamine neurons encode the better option in rats deciding between differently delayed or sized rewards"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Recordings from the ventral tegmental area in rats learning to choose between differently delayed and sized rewards showed that when rats were given a choice between two differently valued outcomes, the activity of the neurons initially reflected the more valuable option, even when it was not subsequently selected."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153352617"
                        ],
                        "name": "Richard Wheeler",
                        "slug": "Richard-Wheeler",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Wheeler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Wheeler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 51
                            }
                        ],
                        "text": "4 Graybiel (2000) is a brief primer on the basal ganglia. The experiments mentioned that involve optogenetic activation of dopamine neurons were conducted by Tsai, Zhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg, Keiflin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge-Chang, Roorda, Vrontou, Sjulson, Li, Hirsh, and Miesenb\u00f6ck (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 90
                            }
                        ],
                        "text": "7 Gradient bandit algorithms are a special case of the gradient-based reinforcement learning algorithms introduced by Williams (1992), and that later developed into the actor\u2013critic and policy-gradient algorithms that we treat later in this book."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 39
                            }
                        ],
                        "text": "3 Gradient-descent methods for minimizing mean-squared error in supervised learning are well known. Widrow and Hoff (1960) introduced the least-meansquare (LMS) algorithm, which is the prototypical incremental gradientdescent algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 55
                            }
                        ],
                        "text": "3 The optimality of the TD algorithm under batch training was established by Sutton (1988). Illuminating this result is Barnard\u2019s (1993) derivation of the TD algorithm as a combination of one step of an incremental method for learning a model of the Markov chain and one step of a method for computing predictions from the model."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13203533,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "f390b0bf0cc15eddb1dd24b759c425400647782c",
            "isKey": true,
            "numCitedBy": 25,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A sequential stochastic game among an arbitrary number of players in which all players' payoffs are identical is analyzed. The players are unaware that they are in a game and hence they have no knowledge of other players' strategies or the payoff structure. At each instant the players use a simple learning algorithm to update their mixed strategy choices based entirely on the response of a random environment. It is shown that the expected change in each player's payoff is nonnegative at every instant, so that the group improves its performance monotonically. This result appears to have important implications in decentralized decision-making in large complex systems."
            },
            "slug": "An-N-player-sequential-stochastic-game-with-payoffs-Narendra-Wheeler",
            "title": {
                "fragments": [],
                "text": "An N-player sequential stochastic game with identical payoffs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the expected change in each player's payoff is nonnegative at every instant, so that the group improves its performance monotonically, which appears to have important implications in decentralized decision-making in large complex systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2858469,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "547ce5daf623353afa1be87e8c6978fa545dc0a7",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "In addition to the goal of acquiring a precise description of the acoustic environment, central auditory processing also provides useful information for animal behaviors, such as navigation and communication. Singing is a learned behavior of male songbirds for protecting territories and attracting females (Konishi, 1985; Catchpole and Slater, 1995). It has been experimentally shown that singing behavior depends on auditory information in two ways. First, the phonetic features of a bird\u2019s song depends on the bird\u2019s auditory experience during a limited period after birth. Second, the development of songs of a juvenile bird depends on the auditory feedback of its own vocalization."
            },
            "slug": "A-Computational-Model-of-Birdsong-Learning-by-and-Doya-Sejnowski",
            "title": {
                "fragments": [],
                "text": "A Computational Model of Birdsong Learning by Auditory Experience and Auditory Feedback"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In addition to the goal of acquiring a precise description of the acoustic environment, central auditory processing also provides useful information for animal behaviors, such as navigation and communication."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153444195"
                        ],
                        "name": "R. Viswanathan",
                        "slug": "R.-Viswanathan",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Viswanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Viswanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41147296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b32d0557b0bcc0db48902f758be5d97e0e76ca1b",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The collective behavior of variable-structure stochastic automata in competitive game situations is investigated. It is demonstrated that when the automata use optimal or ?-optimal reinforcement schemes, the Von Neumann value is achieved for games against nature as well as for two-player zero-sum games having a saddle point. Computer simulations of the latter games also indicate that in the absenice of a saddle point the value of the game oscillates about the Von Neunmann value in mixed strategies."
            },
            "slug": "Games-of-Stochastic-Automata-Viswanathan-Narendra",
            "title": {
                "fragments": [],
                "text": "Games of Stochastic Automata"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Narendra and Thathachar (1989) treat bandit problems from the engineering perspective, providing a good discussion of the various theoretical traditions that have focussed on them."
                    },
                    "intents": []
                }
            ],
            "corpusId": 42185255,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "44eeb93197dcf2e7bf4ed9172a82f81de9c05365",
            "isKey": false,
            "numCitedBy": 1597,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the combination of knowledge and actions, someone can improve their skill and ability. It will lead them to live and work much better. This is why, the students, workers, or even employers should have reading habit for books. Any book will give certain knowledge to take all benefits. This is what this learning automata an introduction tells you. It will add more knowledge of you to life and work better. Try it and prove it."
            },
            "slug": "Learning-automata-an-introduction-Narendra-Thathachar",
            "title": {
                "fragments": [],
                "text": "Learning automata - an introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "From the combination of knowledge and actions, someone can improve their skill and ability and this learning automata an introduction tells you that any book will give certain knowledge to take all benefits."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145100736"
                        ],
                        "name": "J. Cross",
                        "slug": "J.-Cross",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cross",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Dayan (2002) commented that this would require an error as in Sutton and Barto\u2019s (1981) early model of classical conditioning and not a true TD error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 130
                            }
                        ],
                        "text": "This work began in 1973 with the application of Bush and Mosteller\u2019s learning theory to a collection of classical economic models (Cross, 1973)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 153894147,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "72c94dc1d35507df0d0c5f175536a42a2e7abbdc",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic learning theory, 242. \u2014 Probability matching: an example, 244. \u2014 The model, 247. \u2014 Alternative states of the market: a monopoly model, 252. \u2014 A simple market model, 254. \u2014 Further properties of the single-firm model, 256. \u2014 The dynamics of adjustment, 258. \u2014 The firm in disequilibrium, 261. \u2014 A final note: rules of thumb, 264."
            },
            "slug": "A-Stochastic-Learning-Model-of-Economic-Behavior-Cross",
            "title": {
                "fragments": [],
                "text": "A Stochastic Learning Model of Economic Behavior"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3315497"
                        ],
                        "name": "P. Tobler",
                        "slug": "P.-Tobler",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Tobler",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tobler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702229"
                        ],
                        "name": "C. Fiorillo",
                        "slug": "C.-Fiorillo",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Fiorillo",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fiorillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13789954,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "5fa71f1ae1b44b59b0a2d26b4ae950a512e5ef07",
            "isKey": false,
            "numCitedBy": 1140,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "It is important for animals to estimate the value of rewards as accurately as possible. Because the number of potential reward values is very large, it is necessary that the brain's limited resources be allocated so as to discriminate better among more likely reward outcomes at the expense of less likely outcomes. We found that midbrain dopamine neurons rapidly adapted to the information provided by reward-predicting stimuli. Responses shifted relative to the expected reward value, and the gain adjusted to the variance of reward value. In this way, dopamine neurons maintained their reward sensitivity over a large range of reward values."
            },
            "slug": "Adaptive-Coding-of-Reward-Value-by-Dopamine-Neurons-Tobler-Fiorillo",
            "title": {
                "fragments": [],
                "text": "Adaptive Coding of Reward Value by Dopamine Neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that midbrain dopamine neurons rapidly adapted to the information provided by reward-predicting stimuli and maintained their reward sensitivity over a large range of reward values."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153409932"
                        ],
                        "name": "Adam Johnson",
                        "slug": "Adam-Johnson",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776101"
                        ],
                        "name": "A. Redish",
                        "slug": "A.-Redish",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redish",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 116
                            }
                        ],
                        "text": "6 Early work on using estimates of the upper confidence bound to select actions was done by Lai and Robbins (1985), Kaelbling (1993b), and Agrawal (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 190
                            }
                        ],
                        "text": "When a rat pauses at a choice point in a maze, the representation of space in the hippocampus sweeps forward (and not backwards) along the possible paths the animal can take from that point (Johnson and Redish, 2007)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14934527,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "a960e46e69d9794ae287c7ca6d51ea2618c80e19",
            "isKey": false,
            "numCitedBy": 786,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural ensembles were recorded from the CA3 region of rats running on T-based decision tasks. Examination of neural representations of space at fast time scales revealed a transient but repeatable phenomenon as rats made a decision: the location reconstructed from the neural ensemble swept forward, first down one path and then the other. Estimated representations were coherent and preferentially swept ahead of the animal rather than behind the animal, implying it represented future possibilities rather than recently traveled paths. Similar phenomena occurred at other important decisions (such as in recovery from an error). Local field potentials from these sites contained pronounced theta and gamma frequencies, but no sharp wave frequencies. Forward-shifted spatial representations were influenced by task demands and experience. These data suggest that the hippocampus does not represent space as a passive computation, but rather that hippocampal spatial processing is an active process likely regulated by cognitive mechanisms."
            },
            "slug": "Neural-Ensembles-in-CA3-Transiently-Encode-Paths-of-Johnson-Redish",
            "title": {
                "fragments": [],
                "text": "Neural Ensembles in CA3 Transiently Encode Paths Forward of the Animal at a Decision Point"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The data suggest that the hippocampus does not represent space as a passive computation, but rather that hippocampal spatial processing is an active process likely regulated by cognitive mechanisms."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In describing how a computer could be programmed to play chess, Shannon (1950b) suggested using an evaluation function that took into account the long-term advantages and disadvantages of a chess position."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Shannon (1950b) suggested that a function could be used by a chess-playing program to decide whether a move M in position P is worth exploring."
                    },
                    "intents": []
                }
            ],
            "corpusId": 45034253,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "83d4d801ff410961b72bc72b57418600d2ab0454",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "For centuries philosophers and scientists have speculated about whether or not the human brain is essentially a machine. Could a machine be designed that would be capable of \u201cthinking\u201d? During the past decade several large-scale electronic computing machines have been constructed which are capable of something very close to the reasoning process. These new computers were designed primarily to carry out purely numerical calculations. They perform automatically a long sequence of additions, multiplications, and other arithmetic operations at a rate of thousands per second. The basic design of these machines is so general and flexible, however, that they can be adapted to work symbolically with elements representing words, propositions, or other conceptual entities."
            },
            "slug": "A-chess-playing-machine.-Shannon",
            "title": {
                "fragments": [],
                "text": "A chess-playing machine."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The basic design of these large-scale electronic computing machines is so general and flexible, however, that they can be adapted to work symbolically with elements representing words, propositions, or other conceptual entities."
            },
            "venue": {
                "fragments": [],
                "text": "Scientific American"
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144767"
                        ],
                        "name": "H. Breiter",
                        "slug": "H.-Breiter",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Breiter",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Breiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144141544"
                        ],
                        "name": "I. Aharon",
                        "slug": "I.-Aharon",
                        "structuredName": {
                            "firstName": "Itzhak",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3683465"
                        ],
                        "name": "D. Kahneman",
                        "slug": "D.-Kahneman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kahneman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kahneman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143619169"
                        ],
                        "name": "A. Dale",
                        "slug": "A.-Dale",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Dale",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983138"
                        ],
                        "name": "P. Shizgal",
                        "slug": "P.-Shizgal",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Shizgal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shizgal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2688773,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "375863eeec6f83445ce6176a60055e06325310e6",
            "isKey": false,
            "numCitedBy": 1329,
            "numCiting": 218,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Functional-Imaging-of-Neural-Responses-to-and-of-Breiter-Aharon",
            "title": {
                "fragments": [],
                "text": "Functional Imaging of Neural Responses to Expectancy and Experience of Monetary Gains and Losses"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30710621"
                        ],
                        "name": "J. Randl\u00f8v",
                        "slug": "J.-Randl\u00f8v",
                        "structuredName": {
                            "firstName": "Jette",
                            "lastName": "Randl\u00f8v",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Randl\u00f8v"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816463"
                        ],
                        "name": "P. Alstr\u00f8m",
                        "slug": "P.-Alstr\u00f8m",
                        "structuredName": {
                            "firstName": "Preben",
                            "lastName": "Alstr\u00f8m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Alstr\u00f8m"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28257125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d8f6219fbd2da14d8d55562dcedf43fe671d0e3",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present and solve a real-world problem of learning to drive a bicycle. We solve the problem by online reinforcement learning using the Sarsa(A)-algorithm. Then we solve the composite problem of learning to balance a bicycle and then drive to 'It goal. In our approach the reinforcement function is independent of the task the agent tries to learn to solve."
            },
            "slug": "Learning-to-Drive-a-Bicycle-Using-Reinforcement-and-Randl\u00f8v-Alstr\u00f8m",
            "title": {
                "fragments": [],
                "text": "Learning to Drive a Bicycle Using Reinforcement Learning and Shaping"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The real-world problem of learning to drive a bicycle is presented and the problem is solved by online reinforcement learning using the Sarsa(A)-algorithm and the reinforcement function is independent of the task the agent tries to solve."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 13
                            }
                        ],
                        "text": "9) is due to Bridle (1990). This rule appears to have been first proposed by Luce (1959)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18865663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "830ccb44084d9d6cdcb70d623df5012ae4835142",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multilayer perceptron to provide mathematically correct probability distributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mutual Information training, which has been used successfully to improve the performance of hidden Markov models for speech recognition. If the network is specially constructed to perform the recognition computations of a given kind of stochastic model based classifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'."
            },
            "slug": "Training-Stochastic-Model-Recognition-Algorithms-as-Bridle",
            "title": {
                "fragments": [],
                "text": "Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown that once the output layer of a multilayer perceptron is modified to provide mathematically correct probability distributions, and the usual squared error criterion is replaced with a probability-based score, the result is equivalent to Maximum Mutual Information training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731282"
                        ],
                        "name": "Benjamin Van Roy",
                        "slug": "Benjamin-Van-Roy",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Van Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5760976,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2d6177244636f892c1620e3e5c2870c5e3902b55",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a variant of temporal-difference learning that approximates average and differential costs of an irreducible aperiodic Markov chain. Approximations are comprised of linear combinations of fixed basis functions whose weights are incrementally updated during a single endless trajectory of the Markov chain. We present results concerning convergence and the limit of convergence. We also provide a bound on the resulting approximation error that exhibits an interesting dependence on the \"mixing time\" of the Markov chain. The results parallel previous work by the authors (1997), involving approximations of discounted cost-to-go."
            },
            "slug": "Average-cost-temporal-difference-learning-Tsitsiklis-Roy",
            "title": {
                "fragments": [],
                "text": "Average cost temporal-difference learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A variant of temporal-difference learning that approximates average and differential costs of an irreducible aperiodic Markov chain and provides a bound on the resulting approximation error that exhibits an interesting dependence on the \"mixing time\" of the Markov Chain."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 36th IEEE Conference on Decision and Control"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38946003"
                        ],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 209404771,
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "id": "a5a48f3876da907f3be47c146f31af58ec041a86",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Job-shop scheduling is an important task for manufacturing industries. We are interested in the particular task of scheduling payload processing for NASA's space shuttle program. This paper summarizes our previous work on formulating this task for solution by the reinforcement learning algorithm TD(\u03bb). A shortcoming of this previous work was its reliance on hand-engineered input features. This paper shows how to extend the time-delay neural network (TDNN) architecture to apply it to irregular-length schedules. Experimental tests show that this TDNN-TD(\u03bb) network can match the performance of our previous hand-engineered system. The tests also show that both neural network approaches significantly outperform the best previous (non-learning) solution to this problem in terms of the quality of the resulting schedules and the number of search steps required to construct them."
            },
            "slug": "High-Performance-Job-Shop-Scheduling-With-A-TD(\u03bb)-Zhang-Dietterich",
            "title": {
                "fragments": [],
                "text": "High-Performance Job-Shop Scheduling With A Time-Delay TD(\u03bb) Network"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental tests show that this TDNN-TD(\u03bb) network can match the performance of the previous hand-engineered system, and both neural network approaches significantly outperform the best previous (non-learning) solution to this problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1995"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1381206897"
                        ],
                        "name": "P. An",
                        "slug": "P.-An",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "An",
                            "middleNames": [
                                "Edger"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. An"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50472018"
                        ],
                        "name": "W. Miller",
                        "slug": "W.-Miller",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Miller",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095777"
                        ],
                        "name": "P. Parks",
                        "slug": "P.-Parks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Parks",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Parks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 119
                            }
                        ],
                        "text": "Nevertheless, extensive studies have been made of graded response functions such as RBFs in the context of tile coding (An, 1991; Miller et al., 1991; An et al., 1991; Lane, Handelman and Gelfand, 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60758156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d128edc658d5aceaffbaa38dc2846a2a732a637",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Design-Improvements-in-Associative-Memories-for-An-Miller",
            "title": {
                "fragments": [],
                "text": "Design Improvements in Associative Memories for Cerebellar Model Articulation Controllers (CMAC)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145215525"
                        ],
                        "name": "R. Romo",
                        "slug": "R.-Romo",
                        "structuredName": {
                            "firstName": "Ranulfo",
                            "lastName": "Romo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Romo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43295234,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "4c3cebe580fd59c6ea01e2d2f695e8880419d3e4",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Previous studies have shown that midbrain dopamine (DA) neurons in monkeys respond to external stimuli that are used to initiate behavioral reactions. In the present study, we investigated to what extent changes in neuronal activity would occur when behavioral acts are generated internally or whether they would depend solely on external stimuli. 2. Monkeys performed self-initiated arm movements from a resting key into a covered, food-containing box at a self-chosen moment and without external preparatory or triggering signals. In a second task, the arm movement was triggered by rapid opening of the door of the food box. This stimulus was either audible and visible or only audible to the animal. Impulses of DA neurons were recorded with movable microelectrodes from the pars compacta of substantia nigra (area A9) and areas A8 and A10 and were discriminated from those of other neurons by their long duration (1.5-5.0 ms) and low spontaneous frequency (0.5-8.5 imp/s). 3. The activity of 12% of 104 DA neurons increased slowly and moderately up to 1,500 ms before the onset of individual self-initiated arm movements. Median increases amounted to 91% over background discharge rate. A further 16% of DA neurons were activated together with the onset of muscle activity and during the movement. 4. During self-initiated movements, a nonhabituating, phasic burst of impulses occurred when the monkey's hand touched a morsel of food inside the box. This response was seen in 84% of 154 neurons on the contralateral side, with median onset latency of 65 ms and duration of 160 ms. A comparable percentage of neurons responded to ipsilateral touch with similar latency and duration. 5. The touch response during self-initiated movements was absent, both on the contra- and ipsilateral sides, when the animal's hand touched the bare wire normally holding the food, when touching nonfood objects, or during tactile exploration of the empty interior of the food box. Thus responses appeared to be related to the appetitive properties of the object being touched rather than the object itself. 6. In the task employing stimulus-triggered movements, 77% of 86 DA neurons discharged a burst of impulses in response to door opening but entirely failed to respond to the touch of food in the box. The response to door opening in this task was similar to the touch response during self-initiated movements in the same neurons in terms of latency, duration, and magnitude.(ABSTRACT TRUNCATED AT 400 WORDS)"
            },
            "slug": "Dopamine-neurons-of-the-monkey-midbrain:-of-to-arm-Romo-Schultz",
            "title": {
                "fragments": [],
                "text": "Dopamine neurons of the monkey midbrain: contingencies of responses to active touch during self-initiated arm movements."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Response to door opening in this task was similar to the touch response during self-initiated movements in the same neurons in terms of latency, duration, and magnitude, and responses appeared to be related to the appetitive properties of the object being touched rather than the object itself."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101096038"
                        ],
                        "name": "J. O'Doherty",
                        "slug": "J.-O'Doherty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "O'Doherty",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. O'Doherty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37816170"
                        ],
                        "name": "J. Schultz",
                        "slug": "J.-Schultz",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2202967"
                        ],
                        "name": "R. Deichmann",
                        "slug": "R.-Deichmann",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Deichmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Deichmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737497"
                        ],
                        "name": "Karl J. Friston",
                        "slug": "Karl-J.-Friston",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Friston",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl J. Friston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2231343"
                        ],
                        "name": "R. Dolan",
                        "slug": "R.-Dolan",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Dolan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43507282,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "45d3a2454ae38657c38429d2abbb3e65898d43cf",
            "isKey": false,
            "numCitedBy": 1946,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Instrumental conditioning studies how animals and humans choose actions appropriate to the affective structure of an environment. According to recent reinforcement learning models, two distinct components are involved: a \u201ccritic,\u201d which learns to predict future reward, and an \u201cactor,\u201d which maintains information about the rewarding outcomes of actions to enable better ones to be chosen more frequently. We scanned human participants with functional magnetic resonance imaging while they engaged in instrumental conditioning. Our results suggest partly dissociable contributions of the ventral and dorsal striatum, with the former corresponding to the critic and the latter corresponding to the actor."
            },
            "slug": "Dissociable-Roles-of-Ventral-and-Dorsal-Striatum-in-O'Doherty-Dayan",
            "title": {
                "fragments": [],
                "text": "Dissociable Roles of Ventral and Dorsal Striatum in Instrumental Conditioning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work scanned human participants with functional magnetic resonance imaging while they engaged in instrumental conditioning to suggest partly dissociable contributions of the ventral and dorsal striatum to the critic and the actor."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 210
                            }
                        ],
                        "text": "However an ANN with a single hidden layer having a large enough finite number of sigmoid units can approximate any continuous function on a compact region of the network\u2019s input space to any degree of accuracy (Cybenko, 1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3958369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da1dda34ecc96263102181448c94ec7d645d085",
            "isKey": false,
            "numCitedBy": 6387,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110975"
                        ],
                        "name": "A. S. Klyubin",
                        "slug": "A.-S.-Klyubin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Klyubin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. S. Klyubin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799704"
                        ],
                        "name": "D. Polani",
                        "slug": "D.-Polani",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Polani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Polani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718528"
                        ],
                        "name": "Chrystopher L. Nehaniv",
                        "slug": "Chrystopher-L.-Nehaniv",
                        "structuredName": {
                            "firstName": "Chrystopher",
                            "lastName": "Nehaniv",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chrystopher L. Nehaniv"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8868123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cc0699a4a99132b7aecddb7f2e37d731e36bb43",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The classical approach to using utility functions suffers from the drawback of having to design and tweak the functions on a case by case basis. Inspired by examples from the animal kingdom, social sciences and games we propose empowerment, a rather universal function, defined as the information-theoretic capacity of an agent's actuation channel. The concept applies to any sensorimotor apparatus. Empowerment as a measure reflects the properties of the apparatus as long as they are observable due to the coupling of sensors and actuators via the environment. Using two simple experiments we also demonstrate how empowerment influences sensor-actuator evolution"
            },
            "slug": "Empowerment:-a-universal-agent-centric-measure-of-Klyubin-Polani",
            "title": {
                "fragments": [],
                "text": "Empowerment: a universal agent-centric measure of control"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Inspired by examples from the animal kingdom, social sciences and games, empowerment is proposed, a rather universal function, defined as the information-theoretic capacity of an agent's actuation channel that applies to any sensorimotor apparatus."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Congress on Evolutionary Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32511959"
                        ],
                        "name": "P. Schweitzer",
                        "slug": "P.-Schweitzer",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Schweitzer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Schweitzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153926850"
                        ],
                        "name": "A. Seidmann",
                        "slug": "A.-Seidmann",
                        "structuredName": {
                            "firstName": "Avi",
                            "lastName": "Seidmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Seidmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 166
                            }
                        ],
                        "text": ") There is now a fairly extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121615527,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dcc3022b37afef00004686e508d305aaee9e12ca",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-polynomial-approximations-in-Markovian-Schweitzer-Seidmann",
            "title": {
                "fragments": [],
                "text": "Generalized polynomial approximations in Markovian decision processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390156766"
                        ],
                        "name": "C. Padoa-Schioppa",
                        "slug": "C.-Padoa-Schioppa",
                        "structuredName": {
                            "firstName": "Camillo",
                            "lastName": "Padoa-Schioppa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Padoa-Schioppa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143815538"
                        ],
                        "name": "J. Assad",
                        "slug": "J.-Assad",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Assad",
                            "middleNames": [
                                "A"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Assad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4374326,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "235e638c8e49dd45953fe6465231bc2c1137bf17",
            "isKey": false,
            "numCitedBy": 1363,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Economic choice is the behaviour observed when individuals select one among many available options. There is no intrinsically \u2018correct\u2019 answer: economic choice depends on subjective preferences. This behaviour is traditionally the object of economic analysis and is also of primary interest in psychology. However, the underlying mental processes and neuronal mechanisms are not well understood. Theories of human and animal choice have a cornerstone in the concept of \u2018value\u2019. Consider, for example, a monkey offered one raisin versus one piece of apple: behavioural evidence suggests that the animal chooses by assigning values to the two options. But where and how values are represented in the brain is unclear. Here we show that, during economic choice, neurons in the orbitofrontal cortex (OFC) encode the value of offered and chosen goods. Notably, OFC neurons encode value independently of visuospatial factors and motor responses. If a monkey chooses between A and B, neurons in the OFC encode the value of the two goods independently of whether A is presented on the right and B on the left, or vice versa. This trait distinguishes the OFC from other brain areas in which value modulates activity related to sensory or motor processes. Our results have broad implications for possible psychological models, suggesting that economic choice is essentially choice between goods rather than choice between actions. In this framework, neurons in the OFC seem to be a good candidate network for value assignment underlying economic choice."
            },
            "slug": "Neurons-in-the-orbitofrontal-cortex-encode-economic-Padoa-Schioppa-Assad",
            "title": {
                "fragments": [],
                "text": "Neurons in the orbitofrontal cortex encode economic value"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Neurons in the orbitofrontal cortex (OFC) encode the value of offered and chosen goods during economic choice, suggesting that economic choice is essentially choice between goods rather than choice between actions."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152512161"
                        ],
                        "name": "JOHN W. Moore",
                        "slug": "JOHN-W.-Moore",
                        "structuredName": {
                            "firstName": "JOHN W.",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JOHN W. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11467984"
                        ],
                        "name": "K. Stickney",
                        "slug": "K.-Stickney",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stickney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stickney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 12
                            }
                        ],
                        "text": "tween learning and motivation. Wise (2004) provides an overview of reinforcement learning and its relation to motivation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 181
                            }
                        ],
                        "text": "The Tsetlin collection also includes studies of learning automata in team and game problems, which led to later work in this area using stochastic learning automata as described by Narendra and Thathachar (1974), Viswanathan and Narendra (1974), Lakshmivarahan and Narendra (1982), Narendra and Wheeler (1983), Narendra (1989), and Thathachar and Sastry (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 5
                            }
                        ],
                        "text": "Nothing prevents an agent from using both model-free and model-based algorithms, and there are good reasons for using both. We know from our own experience that with enough repetition, goal-directed behavior tends to turn into habitual behavior. Experiments show that this happens for rats too. Adams (1982) conducted an experiment to see if extended training would convert goal-directed behavior into habitual behavior."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 51
                            }
                        ],
                        "text": "10 Research on the behavior of reinforcement learning agents in team and game problems has a long history roughly occurring in three phases. To the best or our knowledge, the first phase began with investigations by the Russian mathematician and physicist M. L. Tsetlin. A collection of his work was published as Tsetlin (1973) after his death in 1966."
                    },
                    "intents": []
                }
            ],
            "corpusId": 144497396,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "addbcdceadbe8e97e46fe8d8ed121721c7356ae7",
            "isKey": true,
            "numCitedBy": 173,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The function of the hippocampus in conditioning is portrayed in terms of an extension of Mackintosh\u2019s (1975) attention theory, which describes the evolution of the salience (associability) of each stimulus in the situation, including the context, and its predictive associative relationship to itself and all other stimuli. In terms of the model, the hippocampus is essential for computations that reduce salience when a stimulus is presented in the context of other stimuli that are better predictors of events. The model is applied to the phenomena of latent inhibition and blocking."
            },
            "slug": "Erratum-to:-Formation-of-attentional-associative-in-Moore-Stickney",
            "title": {
                "fragments": [],
                "text": "Erratum to: Formation of attentional-associative networks in real time: Role of the hippocampus and implications for conditioning"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The function of the hippocampus in conditioning is portrayed in terms of an extension of Mackintosh\u2019s (1975) attention theory, which describes the evolution of the salience of each stimulus in the situation, including the context, and its predictive associative relationship to itself and all other stimuli."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064887053"
                        ],
                        "name": "Christopher D. Adams",
                        "slug": "Christopher-D.-Adams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Adams",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Adams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145189997"
                        ],
                        "name": "A. Dickinson",
                        "slug": "A.-Dickinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Dickinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dickinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 51
                            }
                        ],
                        "text": "The first experiment of this type was conducted by Adams and Dickinson (1981). They trained rats via instrumental conditioning until the rats energetically pressed a lever for sucrose pellets in a training chamber."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 39
                            }
                        ],
                        "text": "Adams\u2019 experiment closely followed the Adams and Dickinson (1981) experiment just described."
                    },
                    "intents": []
                }
            ],
            "corpusId": 143282163,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "0db8e55a0b6ab914e155c55695d839be73c16ded",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In two experiments, hungry rats were given instrumental lever-press training for an appetitive reinforcer and, in addition, were exposed to another type of food which was not contingent on lever pressing. In the first experiment, exposure to each type of food was on separate days, whereas in the second experiment rats were exposed to each type of food in strict alternation within each session. Subsequently, a food aversion was conditioned to the reinforcer for the experimental group and to the non-contingent food for the control group. In both experiments, animals with an aversion to the reinforcer responded less in an extinction test than animals with an aversion to the non-contingent food. Subsequent reacquisition tests confirmed that the aversion to the non-contingent food in the control group was of comparable strength with that to the reinforcer in the experimental group. The results were discussed in terms of whether the reinforcer is encoded in the associative structure set up by exposure to an instrumental contingency."
            },
            "slug": "Instrumental-Responding-following-Reinforcer-Adams-Dickinson",
            "title": {
                "fragments": [],
                "text": "Instrumental Responding following Reinforcer Devaluation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3466790"
                        ],
                        "name": "J. Donahoe",
                        "slug": "J.-Donahoe",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Donahoe",
                            "middleNames": [
                                "W"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Donahoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13467345"
                        ],
                        "name": "J. Burgos",
                        "slug": "J.-Burgos",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Burgos",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Burgos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17269056,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "4f2690580166d83b8c4d859a198c649ab3542486",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "Revaluation refers to phenomena in which the strength of an operant is altered by reinforcer-related manipulations that take place outside the conditioning situation in which the operant was selected. As an example, if lever pressing is acquired using food as a reinforcer and food is later paired with an aversive stimulus, the frequency of lever pressing decreases when subsequently tested. Associationist psychology infers from such findings that conditioning produces a response-outcome (i.e., reinforcer) association and that the operant decreased in strength because pairing the reinforcer with the aversive stimulus changed the value of the outcome. Here, we present an approach to the interpretation of these and related findings that employs neural network simulations grounded in the experimental analysis of behavior and neuroscience. In so doing, we address some general issues regarding the relations among behavior analysis, neuroscience, and associationism."
            },
            "slug": "Behavior-analysis-and-revaluation.-Donahoe-Burgos",
            "title": {
                "fragments": [],
                "text": "Behavior analysis and revaluation."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An approach to the interpretation of reinforcer-related manipulations and related findings is presented that employs neural network simulations grounded in the experimental analysis of behavior and neuroscience and addresses some general issues regarding the relations among behavior analysis, neuroscience, and associationism."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the experimental analysis of behavior"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2757153"
                        ],
                        "name": "R. Agrawal",
                        "slug": "R.-Agrawal",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Agrawal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 139
                            }
                        ],
                        "text": "6 Early work on using estimates of the upper confidence bound to select actions was done by Lai and Robbins (1985), Kaelbling (1993b), and Agrawal (1995). The UCB algorithm we present here is called UCB1 in the literature and was first developed by Auer, Cesa-Bianchi and Fischer (2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120313529,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8597ed8596c08ba2ba7bc8da3e9546749d6f4f7b",
            "isKey": false,
            "numCitedBy": 574,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a non-Bayesian infinite horizon version of the multi-armed bandit problem with the objective of designing simple policies whose regret increases slowly with time. In their seminal work on this problem, Lai and Robbins had obtained a O(log n) lower bound on the regret with a constant that depends on the Kullback\u2013Leibler number. They also constructed policies for some specific families of probability distributions (including exponential families) that achieved the lower bound. In this paper we construct index policies that depend on the rewards from each arm only through their sample mean. These policies are computationally much simpler and are also applicable much more generally. They achieve a O(log n) regret with a constant that is also based on the Kullback\u2013Leibler number. This constant turns out to be optimal for one-parameter exponential families; however, in general it is derived from the optimal one via a \u2018contraction' principle. Our results rely entirely on a few key lemmas from the theory of large deviations."
            },
            "slug": "Sample-mean-based-index-policies-by-O(log-n)-regret-Agrawal",
            "title": {
                "fragments": [],
                "text": "Sample mean based index policies by O(log n) regret for the multi-armed bandit problem"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper constructs index policies that depend on the rewards from each arm only through their sample mean, and achieves a O(log n) regret with a constant that is based on the Kullback\u2013Leibler number."
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Applied Probability"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085276"
                        ],
                        "name": "Alexander Balke",
                        "slug": "Alexander-Balke",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Balke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Balke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 133
                            }
                        ],
                        "text": "For example, it is closely related to the idea of \u201cinterventions\u201d and \u201ccounterfactuals\u201d in probabalistic graphical (Bayesian) models (e.g., Pearl, 1995; Balke and Pearl, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 15
                            }
                        ],
                        "text": ", Pearl, 1995; Balke and Pearl, 1994). Off-policy methods using importance sampling have a long history and yet still are not well understood. Weighted importance sampling, which is also sometimes called normalized importance sampling (e.g., Koller and Friedman, 2009), is discussed by Rubinstein (1981), Hesterberg (1988), Shelton (2001), and Liu (2001) among others."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9735547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f48b2ba8c3b8e8f0aa3d61e3f30c5c66997c7ab",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Counterfactual-Probabilities:-Computational-Bounds-Balke-Pearl",
            "title": {
                "fragments": [],
                "text": "Counterfactual Probabilities: Computational Methods, Bounds and Applications"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145648751"
                        ],
                        "name": "H. Robbins",
                        "slug": "H.-Robbins",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Robbins",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Robbins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15556973,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b77aa53b45f6aa1873780d0fa7aad50efb422458",
            "isKey": false,
            "numCitedBy": 1803,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Until recently, statistical theory has been restricted to the design and analysis of sampling experiments in which the size and composition of the samples are completely determined before the experimentation begins. The reasons for this are partly historical, dating back to the time when the statistician was consulted, if at all, only after the experiment was over, and partly intrinsic in the mathematical difficulty of working with anything but a fixed number of independent random variables. A major advance now appears to be in the making with the creation of a theory of the sequential design of experiments, in which the size and composition of the samples are not fixed in advance but are functions of the observations themselves."
            },
            "slug": "Some-aspects-of-the-sequential-design-of-Robbins",
            "title": {
                "fragments": [],
                "text": "Some aspects of the sequential design of experiments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48166426"
                        ],
                        "name": "U. Frey",
                        "slug": "U.-Frey",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Frey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38254647"
                        ],
                        "name": "R. Morris",
                        "slug": "R.-Morris",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Morris",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Morris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4339789,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c02e20c1682703f85694197af442cead47bd89ba",
            "isKey": false,
            "numCitedBy": 1511,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Repeated stimulation of hippocampal neurons can induce an immediate and prolonged increase in synaptic strength that is called long-term potentiation (LTP)\u2014the primary cellular model of memory in the mammalian brain1. An early phase of LTP (lasting less than three hours) can be dissociated from late-phase LTP by using inhibitors of transcription and translation2\u20138. Because protein synthesis occurs mainly in the cell body9\u201312, whereas LTP is input-specific, the question arises of how the synapse specificity of late LTP is achieved without elaborate intracellular protein trafficking. We propose that LTP initiates the creation of a short-lasting protein-synthesis-independent 'synaptic tag' at the potentiated synapse which sequesters the relevant protein(s) to establish late LTP. In support of this idea, we now show that weak tetanic stimulation, which ordinarily leads only to early LTP, or repeated tetanization in the presence of protein-synthesis inhibitors, each results in protein-synthesis-dependent late LTP, provided repeated tetanization has already been applied at another input to the same population of neurons. The synaptic tag decays in less than three hours. These findings indicate that the persistence of LTP depends not only on local events during its induction, but also on the prior activity of the neuron."
            },
            "slug": "Synaptic-tagging-and-long-term-potentiation-Frey-Morris",
            "title": {
                "fragments": [],
                "text": "Synaptic tagging and long-term potentiation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that weak tetanic stimulation, which ordinarily leads only to early LTP, or repeated tetanization in the presence of protein-Synthesis inhibitors, each results in protein-synthesis-dependent late LTP; this indicates that the persistence of LTP depends not only on local events during its induction, but also on the prior activity of the neuron."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21889436"
                        ],
                        "name": "Geoffrey J. Gordon",
                        "slug": "Geoffrey-J.-Gordon",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey J. Gordon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 2
                            }
                        ],
                        "text": "4 Graybiel (2000) is a brief primer on the basal ganglia."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9336700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc93d7c552b498a4b9ffadf6c07c597ad222dc44",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the reinforcement learning problem, motivate algorithms which seek an approximation to the Q function, and present new convergence results for two such algorithms."
            },
            "slug": "Stable-Fitted-Reinforcement-Learning-Gordon",
            "title": {
                "fragments": [],
                "text": "Stable Fitted Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "The reinforcement learning problem is described, algorithms which seek an approximation to the Q function are motivated, and new convergence results for two such algorithms are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6073397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0b1d1e2a99c4d9c67750c231b3883e5976717d4",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Described here is sparse distributed memory (SDM) as a neural-net associative memory. It is characterized by two weight matrices and by a large internal dimension - the number of hidden units is much larger than the number of input or output units. The first matrix, A, is fixed and possibly random, and the second matrix, C, is modifiable. The SDM is compared and contrasted to (1) computer memory, (2) correlation-matrix memory, (3) feet-forward artificial neural network, (4) cortex of the cerebellum, (5) Marr and Albus models of the cerebellum, and (6) Albus' cerebellar model arithmetic computer (CMAC). Several variations of the basic SDM design are discussed: the selected-coordinate and hyperplane designs of Jaeckel, the pseudorandom associative neural memory of Hassoun, and SDM with real-valued input variables by Prager and Fallside. SDM research conducted mainly at the Research Institute for Advanced Computer Science (RIACS) in 1986-1991 is highlighted."
            },
            "slug": "Sparse-distributed-memory-and-related-models-Kanerva",
            "title": {
                "fragments": [],
                "text": "Sparse distributed memory and related models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Several variations of the basic SDM design are discussed: the selected-coordinate and hyperplane designs of Jaeckel, the pseudorandom associative neural memory of Hassoun, and SDM with real-valued input variables by Prager and Fallside."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59694629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa277dfe3645463a25432282563fca4891d846ea",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting \u201eSensitivity Analysis Methods for Nonlinear Systems\u201c from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461."
            },
            "slug": "Applications-of-advances-in-nonlinear-sensitivity-Werbos",
            "title": {
                "fragments": [],
                "text": "Applications of advances in nonlinear sensitivity analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost, including the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16675221"
                        ],
                        "name": "K. J. Craik",
                        "slug": "K.-J.-Craik",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Craik",
                            "middleNames": [
                                "J.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. J. Craik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 153
                            }
                        ],
                        "text": "The authors were also strongly influenced by psychological studies of latent learning (Tolman, 1932) and by psychological views of the nature of thought (e.g., Galanter and Gerstenhaber, 1956; Craik, 1943; Campbell, 1960; Dennett, 1978)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 41364251,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "f69d0ee05bb1f5a352fc5e939a725425f6f5b72d",
            "isKey": false,
            "numCitedBy": 1422,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most fundamental properties of thought is its power of predicting events. This gives it immense adaptive and constructive significance as noted by Dewey and other pragmatists. It enables us, for. instance, to design bridges with a sufficient factor of safety instead of building them haphazard and waiting to see whether they collapse, and to predict consequences of recondite physical or chemical processes whose value may often be more theoretical than practical. In all these cases the process of thought, reduced to its simplest terms, is as follows: a man observes some external event or process and arrives at some 'conclusion' or 'prediction' expressed in words or numbers that 'mean' or refer to or describe some external event or process which comes to pass if the man's reasoning was correct. During the process of reasoning, he may also have availed himself of words or numbers. Here there are three essential processes:"
            },
            "slug": "The-nature-of-explanation-Craik",
            "title": {
                "fragments": [],
                "text": "The nature of explanation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1943
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 88
                            }
                        ],
                        "text": "Convergence with probability 1 was proved by several researchers at about the same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis, 1994; Gurvits, Lin, and Hanson, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4449439,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "621c03cd67b0b7bac665f3c7887481b4b42f269c",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available."
            },
            "slug": "Asynchronous-Stochastic-Approximation-and-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Asynchronous Stochastic Approximation and Q-Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, is studied to establish its convergence under conditions more general than previously available."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2290651"
                        ],
                        "name": "G. Peterson",
                        "slug": "G.-Peterson",
                        "structuredName": {
                            "firstName": "Gail",
                            "lastName": "Peterson",
                            "middleNames": [
                                "Beaton"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Peterson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34687176,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "273bf4ff0b43e376a94073603af9e000ffb30111",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the seminal studies of response differentiation by the method of successive approximation detailed in chapter 8 of The Behavior of Organisms (1938), B. F. Skinner never actually shaped an operant response by hand until a memorable incident of startling serendipity on the top floor of a flour mill in Minneapolis in 1943. That occasion appears to have been a genuine eureka experience for Skinner, causing him to appreciate as never before the significance of reinforcement mediated by biological connections with the animate social environment, as opposed to purely mechanical connections with the inanimate physical environment. This insight stimulated him to coin a new term (shaping), and also led directly to a shift in his perspective on verbal behavior from an emphasis on antecedents and molecular topographical details to an emphasis on consequences and more molar, functional properties in which the social dyad inherent to the shaping process became the definitive property of verbal behavior. Moreover, the insight seems to have emboldened Skinner to explore the greater implications of his behaviorism for human behavior writ large, an enterprise that characterized the bulk of his post-World War II scholarship."
            },
            "slug": "A-day-of-great-illumination:-B.-F.-Skinner's-of-Peterson",
            "title": {
                "fragments": [],
                "text": "A day of great illumination: B. F. Skinner's discovery of shaping."
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Despite the seminal studies of response differentiation by the method of successive approximation detailed in chapter 8 of The Behavior of Organisms, B. F. Skinner never actually shaped an operant response by hand until a memorable incident of startling serendipity on the top floor of a flour mill in Minneapolis in 1943, causing him to appreciate as never before the significance of reinforcement mediated by biological connections with the animate social environment."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the experimental analysis of behavior"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38647333"
                        ],
                        "name": "J. Reynolds",
                        "slug": "J.-Reynolds",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Reynolds",
                            "middleNames": [
                                "N.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reynolds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144329674"
                        ],
                        "name": "J. Wickens",
                        "slug": "J.-Wickens",
                        "structuredName": {
                            "firstName": "Jeffery",
                            "lastName": "Wickens",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wickens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17453150,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c368cda2ab89f37935c2b6977c1509b62d9d212a",
            "isKey": false,
            "numCitedBy": 518,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dopamine-dependent-plasticity-of-corticostriatal-Reynolds-Wickens",
            "title": {
                "fragments": [],
                "text": "Dopamine-dependent plasticity of corticostriatal synapses"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802807"
                        ],
                        "name": "G. DeJong",
                        "slug": "G.-DeJong",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "DeJong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. DeJong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145963824"
                        ],
                        "name": "M. Spong",
                        "slug": "M.-Spong",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Spong",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Spong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53932171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "362891a2110a7cc4e7b378c237c26fbbf5a65860",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In our approach, intelligent control integrates the symbolic reasoning of artificial intelligence (AI) into the control procedure. We describe how this can be accomplished by a straightforward abstraction of the conventional notion of the control model. We apply the techniques to the problem of swing-up control of the Acrobot. The resulting explanation-based control strategy is compared with two more-conventionally-derived control strategies. We briefly discuss the strengths and weaknesses of the new approach."
            },
            "slug": "Swinging-up-the-Acrobot:-an-example-of-intelligent-DeJong-Spong",
            "title": {
                "fragments": [],
                "text": "Swinging up the Acrobot: an example of intelligent control"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work describes how intelligent control can be accomplished by a straightforward abstraction of the conventional notion of the control model by applying the techniques to the problem of swing-up control of the Acrobot."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 American Control Conference - ACC '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145215525"
                        ],
                        "name": "R. Romo",
                        "slug": "R.-Romo",
                        "structuredName": {
                            "firstName": "Ranulfo",
                            "lastName": "Romo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Romo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46062741,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "8991a362622590832faf2f06a23a73296e40a8f8",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "1. This study investigates the behavioral conditions in which dopamine (DA) neurons of substantia nigra and adjoining areas A8 and A10 respond with impulses to visual and auditory trigger stimuli eliciting immediate arm- and eye-movement reactions. 2. In a formal task, the rapid opening of the door of a small, food-containing box located at eye level ahead of the animal served as visible and audible trigger stimulus. Most DA neurons on the contralateral side responded to this stimulus with a short burst of impulses with median onset latency of 50 ms and duration of 90 ms (75% of 164 neurons). Similar responses were seen in a comparable fraction of DA neurons during ipsilateral task performance, suggesting that responses were not specific for the limb being used. 3. When the sensory components of the door opening stimulus were separated, DA neurons typically responded in a similar manner to the moving visual stimulus of the opening door, the low-intensity sliding noise of the opening door, and the 1-kHz sound of 90-92 dB intensity emitted from a distant source at the onset of door opening. Responses to each component alone were lower in magnitude than to all three together. 4. In a variation of the task, a neighboring, identical food box opened in random alternation with the other box but without permitting animals to reach out (asymmetric, direct-reaction go/no-go task). With each sensory component, DA neurons typically responded both to opening of go and no-go boxes. Responses were enhanced when stimuli elicited limb movements in go trials. 5. Monkeys reacted to door opening with target-directed saccadic eye movements in the majority of both go and no-go trials. Neuronal responses were equally present during the occasional absence of eye movements. Thus responses were not specific for the initiation of individual arm or eye movements. 6. Neuronal responses were absent when the same stimuli occurred outside of the behavioral task with target-direct arm and eye movements lacking. This shows that responses were not of purely sensory nature but were related to the capacity of the stimulus for eliciting behavioral reactions. 7. In a variation of the go/no-go task, an instruction light illuminated 2-3 s before door opening prepared the animal to perform the reaching movement on door opening or to refrain from moving (asymmetric, instruction-dependent go/no-go task).(ABSTRACT TRUNCATED AT 400 WORDS)"
            },
            "slug": "Dopamine-neurons-of-the-monkey-midbrain:-of-to-Schultz-Romo",
            "title": {
                "fragments": [],
                "text": "Dopamine neurons of the monkey midbrain: contingencies of responses to stimuli eliciting immediate behavioral reactions."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Responses to visual and auditory trigger stimuli eliciting immediate arm- and eye-movement reactions were related to the capacity of the stimulus for eliciting behavioral reactions."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84288907"
                        ],
                        "name": "K. Breland",
                        "slug": "K.-Breland",
                        "structuredName": {
                            "firstName": "Keller",
                            "lastName": "Breland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Breland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97237825"
                        ],
                        "name": "M. Breland",
                        "slug": "M.-Breland",
                        "structuredName": {
                            "firstName": "Marian",
                            "lastName": "Breland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Breland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 51818837,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5b2587316b6753979e4da4bae76c3933a9e95884",
            "isKey": false,
            "numCitedBy": 1022,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "THERE seems to be a continuing realization by psychologists that perhaps the white rat cannot reveal everything there is to know about behavior. Among the voices raised on this topic, Beach (1950) has emphasized the necessity of widening the range of species subjected to experimental techniques and conditions. However, psychologists as a whole do not seem to be heeding these admonitions, as Whalen (1961) has pointed out. Perhaps this reluctance is due in part to some dark precognition of what they might find in such investigations, for the ethologists Lorenz (1950, p. 233) and Tinbergen (1951, p. 6) have warned that if psychologists are to understand and predict the behavior of organisms, it is essential that they become thoroughly familiar with the instinctive behavior patterns of each new species they essay to study. Of course, the Watsonian or neobehavioristically oriented experimenter is apt to consider \"instinct\" an ugly word. He tends to class it with Hebb's (1960) other \"seditious notions\" which were discarded in the behavioristic revolution, and he may have some premonition that he will encounter this bete noir in extending the range of species and situations studied. We can assure him that his apprehensions are well grounded. In our attempt to extend a behavioristically oriented approach to the engineering control of animal behavior by operant conditioning techniques, we have fought a running battle with the seditious notion of instinct. It might be of some interest to the psychologist to know how the battle is going and to learn something about the nature of the adversary he is likely to meet if and when he tackles new species in new learning situations. Our first report (Breland & Breland, 1951) in the American Psychologist, concerning our experiences in controlling animal behavior, was wholly affirmative and optimistic, saying in essence that the principles derived from the laboratory could be applied to the extensive control of behavior"
            },
            "slug": "The-misbehavior-of-organisms.-Breland-Breland",
            "title": {
                "fragments": [],
                "text": "The misbehavior of organisms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15045008,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f22ebe96b8dd10fbf3fcfd7c4882d7a3ead4b5c",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This tutorial gives a basic yet rigorous introduction to observable operator models (OOMs). OOMs are a recently discovered class of models of stochastic processes. They are mathematically simple in that they require only concepts from elementary linear algebra. The linear algebra nature gives rise to an efficient, consistent, unbiased, constructive learning procedure for estimating models from empirical data. The tutorial describes in detail the mathematical foundations and the practical use of OOMs for identifying and predicting discrete-time, discrete-valued processes, both for output-only and input-output systems. key words: stochastic time series, system identification, observable operator models"
            },
            "slug": "Discrete-time,-discrete-valued-observable-operator-Jaeger",
            "title": {
                "fragments": [],
                "text": "Discrete-time, discrete-valued observable operator models: a tutorial"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111849083"
                        ],
                        "name": "W. R. Thompson",
                        "slug": "W.-R.-Thompson",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Thompson",
                            "middleNames": [
                                "Robin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. R. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 124674478,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e02d1d482ea7a51ede5a0babd45ab3d4344a8e13",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. If in an accepted sense, P is the probability that one method of treatment, T,, is better than a rival, T2, we may develop a system of apportionment such that the proportionate use of T1 is f(p), a monotolle increasing functioni, rather than make no discriminationi at all up to a certaini poinit and then finally entirely reject one or the other. The only paper * which has so far appeared in his field, as far as I am aware, is one by myself in a recent issue of Biometrilca. In this paper I have considered the case of choice between two such rival treatments,t and for symmetry suggested that f (QI f(p) where Q = 1 P. Then the riski of assignmenit to T1 when it is not the better is Q f(p), while the correspondinig risk for T2 is P f.(Q Accordingly, I suggested further that we set f(p) P, which is a necessary and sufficient conditioni that these two risks be equal. Their sum, the total qrisk, is then 2PQ. A special case was considered wherein the result of use of Ti at any given trial is either success or failure, the probability of failure being an unkniownl, pi, a priori (independenitly for i 1, , kc) equally likely to lie in either of any two equal intervals in the possible range, (0, 1). It is further assumed that for a given Ti we have an experience of exactly ni indepenidenit trials, the number of su?ccesses being si and of failu-res being ri = si-; and the probability of obtaininig such a sample is"
            },
            "slug": "On-the-Theory-of-Apportionment-Thompson",
            "title": {
                "fragments": [],
                "text": "On the Theory of Apportionment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1935
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35268,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88114252"
                        ],
                        "name": "R. Wise",
                        "slug": "R.-Wise",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Wise",
                            "middleNames": [
                                "A"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wise"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9604689,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "0222c7dd4562330a2304087d6a669a0b48bc93ae",
            "isKey": false,
            "numCitedBy": 2872,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "The hypothesis that dopamine is important for reward has been proposed in a number of forms, each of which has been challenged. Normally, rewarding stimuli such as food, water, lateral hypothalamic brain stimulation and several drugs of abuse become ineffective as rewards in animals given performance-sparing doses of dopamine antagonists. Dopamine release in the nucleus accumbens has been linked to the efficacy of these unconditioned rewards, but dopamine release in a broader range of structures is implicated in the 'stamping-in' of memory that attaches motivational importance to otherwise neutral environmental stimuli."
            },
            "slug": "Dopamine,-learning-and-motivation-Wise",
            "title": {
                "fragments": [],
                "text": "Dopamine, learning and motivation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Dopamine release in the nucleus accumbens has been linked to the efficacy of these unconditioned rewards, but dopamine release in a broader range of structures is implicated in the 'stamping-in' of memory that attaches motivational importance to otherwise neutral environmental stimuli."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1381910617"
                        ],
                        "name": "B. Van Roy",
                        "slug": "B.-Van-Roy",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Van Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Van Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116691029"
                        ],
                        "name": "Y. Lee",
                        "slug": "Y.-Lee",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11475801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2a5e8d9b0d0a9657aad850f5747941025f71d29",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss an application of neuro-dynamic programming techniques to the optimization of retailer inventory systems. We describe a specific case study involving a model with thirty-three state variables. The enormity of this state space renders classical algorithms of dynamic programming inapplicable. We compare the performance of solutions generated by neuro-dynamic programming algorithms to that delivered by optimized s-type (\"order-up-to\") policies. We are able to generate control strategies substantially superior, reducing inventory costs by approximately ten percent."
            },
            "slug": "A-neuro-dynamic-programming-approach-to-retailer-Roy-Bertsekas",
            "title": {
                "fragments": [],
                "text": "A neuro-dynamic programming approach to retailer inventory management"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work compares the performance of solutions generated by neuro-dynamic programming algorithms to that delivered by optimized s-type (\"order-up-to\") policies and is able to generate control strategies substantially superior, reducing inventory costs by approximately ten percent."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 36th IEEE Conference on Decision and Control"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2547681"
                        ],
                        "name": "S. Dreyfus",
                        "slug": "S.-Dreyfus",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Dreyfus",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dreyfus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44189702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d7d120409cb3551f191ef32dcbdc4add3eb1ba8",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper indicates some ways in which the theory of approximation can be used to increase the range of present day computers. Although the primary interest is in applying these techniques to the functional equations occurring in the theory of dynamic programming. These same methods are applicable, and even more readily, to the classical functional equations of mathematical physics. The objective of the paper is to trade additional computing time, which is expensive, for additional memory capacity, which does not exist."
            },
            "slug": "FUNCTIONAL-APPROXIMATIONS-AND-DYNAMIC-PROGRAMMING-Bellman-Dreyfus",
            "title": {
                "fragments": [],
                "text": "FUNCTIONAL APPROXIMATIONS AND DYNAMIC PROGRAMMING"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Some ways in which the theory of approximation can be used to increase the range of present day computers are indicated, and even more readily, to the classical functional equations of mathematical physics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763536"
                        ],
                        "name": "G. Berns",
                        "slug": "G.-Berns",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Berns",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Berns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2751811"
                        ],
                        "name": "S. McClure",
                        "slug": "S.-McClure",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "McClure",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McClure"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023560"
                        ],
                        "name": "G. Pagnoni",
                        "slug": "G.-Pagnoni",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Pagnoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pagnoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15507067,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "13fc5cdb0a17f78128b04db7a867bd0883f6194f",
            "isKey": false,
            "numCitedBy": 689,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Certain classes of stimuli, such as food and drugs, are highly effective in activating reward regions. We show in humans that activity in these regions can be modulated by the predictability of the sequenced delivery of two mildly pleasurable stimuli, orally delivered fruit juice and water. Using functional magnetic resonance imaging, the activity for rewarding stimuli in both the nucleus accumbens and medial orbitofrontal cortex was greatest when the stimuli were unpredictable. Moreover, the subjects' stated preference for either juice or water was not directly correlated with activity in reward regions but instead was correlated with activity in sensorimotor cortex. For pleasurable stimuli, these findings suggest that predictability modulates the response of human reward regions, and subjective preference can be dissociated from this response."
            },
            "slug": "Predictability-Modulates-Human-Brain-Response-to-Berns-McClure",
            "title": {
                "fragments": [],
                "text": "Predictability Modulates Human Brain Response to Reward"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "For pleasurable stimuli, these findings suggest that predictability modulates the response of human reward regions, and subjective preference can be dissociated from this response."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753432"
                        ],
                        "name": "J. Schneider",
                        "slug": "J.-Schneider",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Schneider",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schneider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3362723"
                        ],
                        "name": "Kangkang Deng",
                        "slug": "Kangkang-Deng",
                        "structuredName": {
                            "firstName": "Kangkang",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kangkang Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15830203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8486d1be0250a83b447d8ee75043359bcef486f",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Locally weighted polynomial regression (LWPR) is a popular instance-based algorithm for learning continuous non-linear mappings. For more than two or three inputs and for more than a few thousand datapoints the computational expense of predictions is daunting. We discuss drawbacks with previous approaches to dealing with this problem, and present a new algorithm based on a multiresolution search of a quicklyconstructible augmented kd-tree. Without needing to rebuild the tree, we can make fast predictions with arbitrary local weighting functions, arbitrary kernel widths and arbitrary queries. The paper begins with a new, faster, algorithm for exact LWPR predictions. Next we introduce an approximation that achieves up to a two-ordersof-magnitude speedup with negligible accuracy losses. Increasing a certain approximation parameter achieves greater speedups still, but with a correspondingly larger accuracy degradation. This is nevertheless useful during operations such as the early stages of model selection and locating optima of a tted surface. We also show how the approximations can permit real-time query-speci c optimization of the kernel width. We conclude with a brief discussion of potential extensions for tractable instance-based learning on datasets that are too large to t in a computer's main memory. 1 Locally Weighted Polynomial Regression Locally weighted polynomial regression (LWPR) is a form of instance-based (a.k.a memory-based) algorithm for learning continuous non-linear mappings from real-valued input vectors to real-valued output vectors. It is particularly appropriate for learning complex highly non-linear functions of up to about 30 inputs from noisy data. Popularized in the statistics literature in the past decades (Cleveland and Delvin, 1988; Grosse, 1989; Atkeson et al., 1997a) it is enjoying increasing use in applications such as learning robot dynamics (Moore, 1992; Schaal and Atkeson, 1994) and learning process models. Both classical and Bayesian linear regression analysis tools can be extended to work in the locally weighted framework (Hastie and Tibshirani, 1990), providing con dence intervals on predictions, on gradient estimates and on noise estimates|all important when a learned mapping is to be used by a controller (Atkeson et al., 1997b; Schneider, 1997). Let us review LWPR. We begin with linear regression on one input and one output. Global linear regression (left of Figure 1) nds the line that minimizes the sum squared residuals. If this is represented as"
            },
            "slug": "Efficient-Locally-Weighted-Polynomial-Regression-Moore-Schneider",
            "title": {
                "fragments": [],
                "text": "Efficient Locally Weighted Polynomial Regression Predictions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new, faster, algorithm is presented based on a multiresolution search of a quicklyconstructible augmented kd-tree to make exact LWPR predictions, and an approximation that achieves up to a two-orders ofmagnitude speedup with negligible accuracy losses is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143736701"
                        ],
                        "name": "Milind Tambe",
                        "slug": "Milind-Tambe",
                        "structuredName": {
                            "firstName": "Milind",
                            "lastName": "Tambe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Milind Tambe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48603437"
                        ],
                        "name": "A. Newell",
                        "slug": "A.-Newell",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Newell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Newell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749322"
                        ],
                        "name": "P. Rosenbloom",
                        "slug": "P.-Rosenbloom",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Rosenbloom",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rosenbloom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17657705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "079e7921f9116ce13025a2ae53e3d05571a74b7c",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "Soar is an architecture for a system that is intended to be capable of general intelligence. Chunking, a simple experience-based learning mechanism, is Soar's only learning mechanism. Chunking creates new items of information, called chunks, based on the results of problem-solving and stores them in the knowledge base. These chunks are accessed and used in appropriate later situations to avoid the problem-solving required to determine them. It is already well-established that chunking improves performance in Soar when viewed in terms of the subproblems required and the number of steps within a subproblem. However, despite the reduction in number of steps, sometimes there may be a severe degradation in the total run time. This problem arises due toexpensive chunks, i.e., chunks that require a large amount of effort in accessing them from the knowledge base. They pose a major problem for Soar, since in their presence, no guarantees can be given about Soar's performance.In this article, we establish that expensive chunks exist and analyze their causes. We use this analysis to propose a solution for expensive chunks. The solution is based on the notion of restricting the expressiveness of the representational language to guarantee that the chunks formed will require only a limited amount of accessing effort. We analyze the tradeoffs involved in restricting expressiveness and present some empirical evidence to support our analysis."
            },
            "slug": "The-problem-of-expensive-chunks-and-its-solution-by-Tambe-Newell",
            "title": {
                "fragments": [],
                "text": "The problem of expensive chunks and its solution by restricting expressiveness"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article establishes that expensive chunks exist and analyzes their causes and proposes a solution based on the notion of restricting the expressiveness of the representational language to guarantee that the chunks formed will require only a limited amount of accessing effort."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117200472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5558a34dfd1dbb572895664d38fca04029a99cb",
            "isKey": false,
            "numCitedBy": 2933,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule. Great Britain."
            },
            "slug": "Radial-Basis-Functions,-Multi-Variable-Functional-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed, leading naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329664"
                        ],
                        "name": "T. Crow",
                        "slug": "T.-Crow",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Crow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Crow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4197049,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "eef94be5d2413d194be31a26ba11112468251f87",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "MOST theories of learning assume some change in the synaptic conductivity of cortical pathways1,2. Facilitation by use alone, however, will not account for the most characteristic feature of learning\u2014that what is learned are those motor responses which lead to a satisfactory or adaptive state of affairs for the organism3\u20135."
            },
            "slug": "Cortical-Synapses-and-Reinforcement:-a-Hypothesis-Crow",
            "title": {
                "fragments": [],
                "text": "Cortical Synapses and Reinforcement: a Hypothesis"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Facilitation by use alone will not account for the most characteristic feature of learning\u2014that what is learned are those motor responses which lead to a satisfactory or adaptive state of affairs for the organism."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33889400"
                        ],
                        "name": "Cynara C. Wu",
                        "slug": "Cynara-C.-Wu",
                        "structuredName": {
                            "firstName": "Cynara",
                            "lastName": "Wu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cynara C. Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 322786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e76f4b497245eb85fe57146d2b2528d9963f51d",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the approximate solution of discrete optimization problems using procedures that are capable of magnifying the effectiveness of any given heuristic algorithm through sequential application. In particular, we embed the problem within a dynamic programming framework, and we introduce several types of rollout algorithms, which are related to notions of policy iteration. We provide conditions guaranteeing that the rollout algorithm improves the performance of the original heuristic algorithm. The method is illustrated in the context of a machine maintenance and repair problem."
            },
            "slug": "Rollout-Algorithms-for-Combinatorial-Optimization-Bertsekas-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Rollout Algorithms for Combinatorial Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This work considers the approximate solution of discrete optimization problems using procedures that are capable of magnifying the effectiveness of any given heuristic algorithm through sequential application and introduces several types of rollout algorithms, which are related to notions of policy iteration."
            },
            "venue": {
                "fragments": [],
                "text": "J. Heuristics"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16862344"
                        ],
                        "name": "R. Thouless",
                        "slug": "R.-Thouless",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Thouless",
                            "middleNames": [
                                "Henry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Thouless"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 302
                            }
                        ],
                        "text": "Woodworth the idea of trial-and-error learning goes as far back as the 1850s to Alexander Bain\u2019s discussion of learning by \u201cgroping and experiment\u201d and more explicitly to the British ethologist and psychologist Conway Lloyd Morgan\u2019s 1894 use of the term to describe his observations of animal behavior (Woodworth, 1938)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4105761,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "35e6a239f68ce764b531ea3d1eb9cd7d036b65e5",
            "isKey": false,
            "numCitedBy": 1360,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "TWENTY years ago it was still possible to get the main results of the application of experiment to the problems of psychology within a reasonably small volume, and several excellent text-books were produced for that time. The rapid increase in the amount of published experimental work has made all such books out of date, and the teacher of experimental psychology has been forced to use seriously incomplete text-books and to supplement them in his teaching by his own knowledge of the technical journals. The task of making a single volume representive of the present position of experimental psychology seemed too laborious for accomplishment. This, however, has now been done by Prof. R. S. Woodworth, and his book will be welcomed by all students of the subject.Experimental PsychologyBy Robert S. Woodworth. Pp. xi + 889. (London: Methuen and Co., Ltd., 1938.) 18s. net."
            },
            "slug": "Experimental-Psychology-Thouless",
            "title": {
                "fragments": [],
                "text": "Experimental Psychology"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1939
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5650509,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "f074d6094585cd9916062f9ceb06f8021e8166a8",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In cellular telephone systems, an important problem is to dynamically allocate the communication resource (channels) so as to maximize service in a stochastic caller environment. This problem is naturally formulated as a dynamic programming problem and we use a reinforcement learning (RL) method to find dynamic channel allocation policies that are better than previous heuristic solutions. The policies obtained perform well for a broad variety of call traffic patterns. We present results on a large cellular system with approximately 4949 states."
            },
            "slug": "Reinforcement-Learning-for-Dynamic-Channel-in-Singh-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work uses a reinforcement learning method to find dynamic channel allocation policies that are better than previous heuristic solutions and results are presented on a large cellular system with approximately 4949 states."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3099409"
                        ],
                        "name": "E. Maguire",
                        "slug": "E.-Maguire",
                        "structuredName": {
                            "firstName": "Eleanor",
                            "lastName": "Maguire",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Maguire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 97
                            }
                        ],
                        "text": "The hippocampus may also be a critical component of our human ability to imagine new experiences (Hassabis and Maguire, 2007; \u00d3lafsd\u00f3ttir, Barry, Saleem, Hassabis, and Spiers, 2105)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13939288,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d068d12947b36e4777c317f71f93176f0bbc443d",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deconstructing-episodic-memory-with-construction-Hassabis-Maguire",
            "title": {
                "fragments": [],
                "text": "Deconstructing episodic memory with construction"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057580732"
                        ],
                        "name": "R. Hawkins",
                        "slug": "R.-Hawkins",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hawkins",
                            "middleNames": [
                                "X.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hawkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5177815"
                        ],
                        "name": "E. Kandel",
                        "slug": "E.-Kandel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Kandel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kandel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 110
                            }
                        ],
                        "text": "Some neuroscience models developed at this time are well interpreted in terms of temporal-difference learning (Hawkins and Kandel, 1984; Byrne, Gingrich, and Baxter, 1990; Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986; Friston et al., 1994), although in most cases there was no historical connection."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11015428,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "43369b75cc3ef7df87eb049872428cf5d11d1706",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent studies indicate that the cellular mechanism underlying classical conditioning of the Aplysia siphon withdrawal reflex is an extension of the mechanism underlying sensitization. This finding suggests that the mechanisms of yet higher forms of learning may similarly be based on the mechanisms of these simple forms of learning. We illustrate this hypothesis by showing how several higher order features of classical conditioning, including generalization, extinction, second-order conditioning, blocking, and the effect of contingency, can be accounted for by combinations of the cellular processes that underlie habituation, sensitization, and classical conditioning in Aplysia."
            },
            "slug": "Is-there-a-cell-biological-alphabet-for-simple-of-Hawkins-Kandel",
            "title": {
                "fragments": [],
                "text": "Is there a cell-biological alphabet for simple forms of learning?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work illustrates this hypothesis by showing how several higher order features of classical conditioning, including generalization, extinction, second-order conditioning, blocking, and the effect of contingency, can be accounted for by combinations of the cellular processes that underlie habituation, sensitization, and classical conditioning in Aplysia."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34584634"
                        ],
                        "name": "B. Abramson",
                        "slug": "B.-Abramson",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Abramson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Abramson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 638,
                                "start": 3
                            }
                        ],
                        "text": "10 Abramson\u2019s (1990) expected-outcome model is a rollout algorithm applied to two-person games in which the play of both simulated players is random. He argued that even with random play, it is a \u201cpowerful heuristic\u201d that is \u201cprecise, accurate, easily estimable, efficiently calculable, and domain-independent.\u201d Tesauro and Galperin (1997) demonstrated the effectiveness of rollout algorithms for improving the play of backgammon programs, adopting the term \u201crollout\u201d from its use in evaluating backgammon positions by playing out positions with different randomly generating sequences of dice rolls. Bertsekas, Tsitsiklis, and Wu (1997) examine rollout algorithms applied to combinatorial optimization problems, and Bertsekas (2013) surveys their use in discrete deterministic optimization problems, remarking that they are \u201coften surprisingly effective."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 340,
                                "start": 3
                            }
                        ],
                        "text": "10 Abramson\u2019s (1990) expected-outcome model is a rollout algorithm applied to two-person games in which the play of both simulated players is random. He argued that even with random play, it is a \u201cpowerful heuristic\u201d that is \u201cprecise, accurate, easily estimable, efficiently calculable, and domain-independent.\u201d Tesauro and Galperin (1997) demonstrated the effectiveness of rollout algorithms for improving the play of backgammon programs, adopting the term \u201crollout\u201d from its use in evaluating backgammon positions by playing out positions with different randomly generating sequences of dice rolls."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 3
                            }
                        ],
                        "text": "10 Abramson\u2019s (1990) expected-outcome model is a rollout algorithm applied to two-person games in which the play of both simulated players is random."
                    },
                    "intents": []
                }
            ],
            "corpusId": 27401254,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "fc3577a26c3be6ca5fe1213e8b5cfeb2502a09e1",
            "isKey": true,
            "numCitedBy": 149,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The expected-outcome model, in which the proper evaluation of a game-tree node is the expected value of the game's outcome given random play from that node on, is proposed. Expected outcome is considered in its ideal form, where it is shown to be a powerful heuristic. The ability of a simple random sampler that estimates expected outcome to outduel a standard Othello evaluator is demonstrated. The sampler is combined with a linear regression procedure to produce efficient expected-outcome estimators. Overall, the expected-outcome model of two-player games is shown to be precise, accurate, easily estimable, efficiently calculable, and domain-independent. >"
            },
            "slug": "Expected-Outcome:-A-General-Model-of-Static-Abramson",
            "title": {
                "fragments": [],
                "text": "Expected-Outcome: A General Model of Static Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Overall, the expected-outcome model of two-player games is shown to be precise, accurate, easily estimable, efficiently calculable, and domain-independent."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754424"
                        ],
                        "name": "J. Walrand",
                        "slug": "J.-Walrand",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Walrand",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Walrand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127648991"
                        ],
                        "name": "F. F",
                        "slug": "F.-F",
                        "structuredName": {
                            "firstName": "Farrel",
                            "lastName": "F",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. F"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 2
                            }
                        ],
                        "text": ", Bertsekas (2005), Ross (1983), White (1969), and Whittle (1982, 1983)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 160
                            }
                        ],
                        "text": "We designed the book to be used as a text in a one-semester course, perhaps supplemented by readings from the literature or by a more mathematical text such as Bertsekas and Tsitsiklis (1996) or Szepesv\u00e1ri (2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 0
                            }
                        ],
                        "text": "Bertsekas and Tsitsiklis (1989) provide excellent coverage of these variations and their performance differences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 111
                            }
                        ],
                        "text": "3\u20134 This material falls under the general heading of stochastic iterative algorithms, which is well covered by Bertsekas and Tsitsiklis (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 160
                            }
                        ],
                        "text": "We designed the book to be used as a text in a one-semester course, perhaps supplemented by readings from the literature or by a more mathematical text such as Bertsekas and Tsitsiklis (1996) or Szepesv\u00e1ri (2010). This book can also be used as part of a broader course on machine learning, artificial intelligence, or neural networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 0
                            }
                        ],
                        "text": "Bertsekas and Tsitsiklis (1996), Bertsekas (2012), and Sugiyama et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 113
                            }
                        ],
                        "text": ", Puterman, 1994) and from the point of view of reinforcement learning (Mahadevan, 1996; Tadepalli and Ok, 1994; Bertsekas and Tsitiklis, 1996; Tsitsiklis and Van Roy, 1999). The algorithm described here is the onpolicy analog of the \u201cR-learning\u201d algorithm introduced by Schwartz (1993). The name R-learning was probably meant to be the alphabetic successor to Q-learning, but we prefer to think of it as a reference to the learning of differential or relative values."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 105
                            }
                        ],
                        "text": "An analysis showing how value iteration can be made to find an optimal policy in finite time is given by Bertsekas (1987)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 0
                            }
                        ],
                        "text": "Bertsekas and Tsitsiklis (1996), Bertsekas (2012), and Sugiyama et al. (2013) present the state of the art in function approximation in reinforcement learning."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14129654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3bc022120f2622b1427807494d3163f40255b6a",
            "isKey": true,
            "numCitedBy": 107,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "with two snitch-over levels for a class of IY/G/I queuing systems Pratin P. Varaiya (!W68-SM78-F'XO), for a photograph and biography. with variable arrival and sewice rate. \" Srochasrrc Processes and see this issue. p. 655. At present. he holds the position sity of California. Berkeley. His research interests of Lecturer in the Faculty of Computer Science. Technion-Israel In-are in communication nerworks. stochatic control. and decentralized stitute of Technology. Haifa. s>stems. Ahstruct-We consider distributed algorithms for salting dynamic programming problems whereby several processors participate simultaneously in the computation while maintaining coordination by information ea-change via communication links. A model of asynchronous distributed computation is developed which requires very weak assumptions on the ordering of computations, the timing of information exchange. the amount of local information needed at each computation node. and the initial conditions for the algorithm. The class of problems considered is very broad and includes shortest path problems. and finite and infinite horizon stochastic optimal control problems. When specialized to a shortest path problem the algorithm reduces to the algorithm originall) implemented for routing of messages in the ARPANET. R ECENT advances in microcomputer technology have intensified interest in distributed computation schemes. Aside from modular expandability. other potential advantages of such schemes are a reduction in computation time for solving a given problem due to parallelism of computation. and elimination of the need to communicate problem data available at geographically dispersed data collection points to a computation center. The first advantage is of crucial importance in real-time applications where problem solution time can be an implementation bottleneck. The second advantage manifests itself for example in applications involving communication networks where there is a natural decentralization of problem data acquisition. The structure of dynamic programming naturally lends itself well to distributed computation since it involves calculations that to a great extent can be camed out in parallel. In fact it is trikial to devise simple schemes taking advantage of this structure whereby the calculation involved in each iteration of the standard form of the algorithm is simply shared by several processors. Such schemes require a certain degree of synchronization in that all processors must complete their assigned portion of the computation before a new iteration can begin. As a result complex protocols for algorithm initiation and processor synchronization may be necessary, and the speed of computation is limited to that of the slowest processor. These drawbacks motivate distributed algorithms whereby \u2026"
            },
            "slug": "Distributed-Dynamic-Programming-Walrand-F.",
            "title": {
                "fragments": [],
                "text": "Distributed Dynamic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 34,
                "text": "A model of asynchronous distributed computation is developed which requires very weak assumptions on the ordering of computations, the timing of information exchange, and the initial conditions for the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145141118"
                        ],
                        "name": "D. Joel",
                        "slug": "D.-Joel",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Joel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10400817,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "a0f741ecea8970da08aa0ab8f770aea08d861616",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-normative-perspective-on-motivation-Niv-Joel",
            "title": {
                "fragments": [],
                "text": "A normative perspective on motivation"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60805494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c3337861b56120ff0b5b956f5c4e1084973bb45",
            "isKey": false,
            "numCitedBy": 1356,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "If you really want to be smarter, reading can be one of the lots ways to evoke and realize. Many people who like reading will have more knowledge and experiences. Reading can be a way to gain information from economics, politics, science, fiction, literature, religion, and many others. As one of the part of book categories, dynamic programming deterministic and stochastic models always becomes the most wanted book. Many people are absolutely searching for this book. It means that many love to read this kind of book."
            },
            "slug": "Dynamic-Programming:-Deterministic-and-Stochastic-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming: Deterministic and Stochastic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "As one of the part of book categories, dynamic programming deterministic and stochastic models always becomes the most wanted book."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784519"
                        ],
                        "name": "Peter Norvig",
                        "slug": "Peter-Norvig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Norvig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Norvig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46211846,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "bae3eda9605700b14237f4d04652ab6759c68eef",
            "isKey": false,
            "numCitedBy": 1862,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach"
            },
            "slug": "Artificial-intelligence-a-modern-approach,-2nd-Russell-Norvig",
            "title": {
                "fragments": [],
                "text": "Artificial intelligence - a modern approach, 2nd Edition"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "Artificial IntelligenceArtificial intelligence: A Modern Approach 2Nd Ed, eBook, Global Edition."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in artificial intelligence"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735186"
                        ],
                        "name": "Margaret E. Connell",
                        "slug": "Margaret-E.-Connell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Connell",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret E. Connell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195972"
                        ],
                        "name": "P. Utgoff",
                        "slug": "P.-Utgoff",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Utgoff",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Utgoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15527323,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "0efeee1a3802701bcb9c49f3d8b267ed0fd2b86d",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to learning to control a dynamic physical system. The approach has been implemented in a program named CART, and applied to a simple physical system studied previously by several researchers. Experiments illustrate that a control method is learned in about 16 trials, an improvement over previous learning programs."
            },
            "slug": "Learning-to-control-a-dynamic-physical-system-Connell-Utgoff",
            "title": {
                "fragments": [],
                "text": "Learning to control a dynamic physical system"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments illustrate that a control method is learned in about 16 trials, an improvement over previous learning programs."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Intell."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35126179"
                        ],
                        "name": "P. Agre",
                        "slug": "P.-Agre",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Agre",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Agre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "The example of Phil\u2019s breakfast in this chapter was inspired by Agre (1988). We direct the reader to Chapter 6 for references to the kind of temporal-difference method we used in the tic-tac-toe example."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 302
                            }
                        ],
                        "text": "1 The overall view of planning and learning presented here has developed gradually over a number of years, in part by the authors (Sutton, 1990, 1991a, 1991b; Barto, Bradtke, and Singh, 1991, 1995; Sutton and Pinette, 1985; Sutton and Barto, 1981b); it has been strongly influenced by Agre and Chapman (1990; Agre 1988), Bertsekas and Tsitsiklis (1989), Singh (1993), and others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 361,
                                "start": 8
                            }
                        ],
                        "text": "Experts agree that the major stumbling block to creating stronger-than-amateur Go programs is the difficulty of defining an adequate position evaluation function. A good evaluation function allows search to be truncated at a feasible depth by providing relatively easy-to-compute predictions of what deeper search would likely yield. According to M\u00fcller (2002): \u201cNo simple yet reasonable evaluation function will ever be found for Go."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28607732,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "88ca1abdb94d6fe559ab9be2ebd948b662698166",
            "isKey": false,
            "numCitedBy": 333,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational theories of action have generally understood the organized nature of human activity through the construction and execution of plans. By consigning the phenomena of contingency and improvisation to peripheral roles, this view has led to impractical technical proposals. As an alternative, I suggest that contingency is a central feature of everyday activity and that improvisation is the central kind of human activity. I also offer a computational model of certain aspects of everyday routine activity based on an account of improvised activity called {\\it running arguments} and an account of representation for situated agents called {\\it deictic representation}."
            },
            "slug": "The-dynamic-structure-of-everyday-life-Agre",
            "title": {
                "fragments": [],
                "text": "The dynamic structure of everyday life"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work suggests that contingency is a central feature of everyday activity and that improvisation is the central kind of human activity and offers a computational model of certain aspects of everyday routine activity based on an account of improvised activity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61148665,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3764fb7db442dffb6c5e03ac376e8e117f5172e9",
            "isKey": false,
            "numCitedBy": 284,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A review is presented of the field of neuroengineering as a whole, highlighting the importance of neurocontrol and neuroidentification. Then a description is given of the five major architectures in use today in neurocontrol (in robotics, in particular) and a few areas for future research. The author concludes with comments on neuroidentification.<<ETX>>"
            },
            "slug": "Neural-networks-for-control-and-system-Werbos",
            "title": {
                "fragments": [],
                "text": "Neural networks for control and system identification"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A review is presented of the field of neuroengineering as a whole, highlighting the importance of neurocontrol and neuroidentification, and of the five major architectures in use today in neurocontrol (in robotics, in particular)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 28th IEEE Conference on Decision and Control,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4848734"
                        ],
                        "name": "D. Bernoulli",
                        "slug": "D.-Bernoulli",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bernoulli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bernoulli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 535,
                                "start": 84
                            }
                        ],
                        "text": "8 explores the space of collective actions because the output of each unit, being a Bernoulli-logistic unit, probabilistically depends on the weighted sum of its input vector\u2019s components. The weighted sum biases firing probability up or down, but there is always variability. Because each unit uses a REINFORCE policy gradient algorithm (Chapter 13), each unit adjusts its weights with the goal of maximizing the average reward rate it experiences while stochastically exploring its own action space. One can show, as Williams (1992) did, that a team of Bernoulli-logistic REINFORCE units implements a policy gradient algorithm as a whole with respect to average rate of the team\u2019s common reward signal, where the actions are the collective actions of the team."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9165746,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "ef512f15aa1c294c7575aa329a7fecd80be6633b",
            "isKey": false,
            "numCitedBy": 2668,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "EVER SINCE mathematicians first began to study the measurement of risk there has been general agreement on the following proposition: Expected values are computed by multiplying each possible gain by the number of ways in which it can occur, and then dividing the sum of these products by the total number of possible cases where, in this theory, the consideration of cases which are all of the same probability is insisted upon. If this rule be accepted, what remains to be done within the framework of this theory amounts to the enumeration of all alternatives, their breakdown into equi-probable cases and, finally, their insertion into corresponding classifications\u2026"
            },
            "slug": "Exposition-of-a-New-Theory-on-the-Measurement-of-Bernoulli",
            "title": {
                "fragments": [],
                "text": "Exposition of a New Theory on the Measurement of Risk"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113802893"
                        ],
                        "name": "W. A. Clark",
                        "slug": "W.-A.-Clark",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Clark",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. A. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34099369"
                        ],
                        "name": "B. Farley",
                        "slug": "B.-Farley",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Farley",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Farley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 166
                            }
                        ],
                        "text": "But their interests soon shifted from trial-and-error learning to generalization and pattern recognition, that is, from reinforcement learning to supervised learning (Clark and Farley, 1955)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16523575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015296655fba530e3accdc2b05e44d727a946f79",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A self-organizing system reported upon earlier is briefly described. Two further experiments to determine its properties have been carried out. The first demonstrates that self-organization still takes place even if the input patterns are subjected to considerable random variation. The second experiment indicates that, after organization with the usual fixed patterns, the system classifies other input patterns statistically according to a simple preponderance criterion. Significance of this result as a generalization in pattern recognition is discussed. Some remarks are made on methods of simulation of such systems and their relation to computer design."
            },
            "slug": "Generalization-of-pattern-recognition-in-a-system-Clark-Farley",
            "title": {
                "fragments": [],
                "text": "Generalization of pattern recognition in a self-organizing system"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The first experiment demonstrates that self-organization still takes place even if the input patterns are subjected to considerable random variation, and the second indicates that the system classifies other input patterns statistically according to a simple preponderance criterion."
            },
            "venue": {
                "fragments": [],
                "text": "AFIPS '55 (Western)"
            },
            "year": 1955
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60862320,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f3a217c11175f2cf904b2f7f6378b7ade176f2d0",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Why should wait for some days to get or receive the associative memory a system theoretical approach book that you order? Why should you take it if you can get the faster one? You can find the same book that you order right here. This is it the book that you can receive directly after purchasing. This associative memory a system theoretical approach is well known book in the world, of course many people will try to own it. Why don't you become the first? Still confused with the way?"
            },
            "slug": "Associative-memory.-A-system-theoretical-approach-Kohonen",
            "title": {
                "fragments": [],
                "text": "Associative memory. A system-theoretical approach"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This associative memory a system theoretical approach is well known book in the world, of course many people will try to own it and this is it the book that you can receive directly after purchasing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10283818,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c2a1a3211d50cfafd47b73c8fdea6ad401132587",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "This article discusses several important phenomena wherein present behavior depends on how temporal and geometrical relationships among past events are influenced by competitive feedback."
            },
            "slug": "A-neural-model-of-attention,-reinforcement-and-Grossberg",
            "title": {
                "fragments": [],
                "text": "A neural model of attention, reinforcement and discrimination learning."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This article discusses several important phenomena wherein present behavior depends on how temporal and geometrical relationships among past events are influenced by competitive feedback."
            },
            "venue": {
                "fragments": [],
                "text": "International review of neurobiology"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2162203764"
                        ],
                        "name": "H. Yin",
                        "slug": "H.-Yin",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Yin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202953"
                        ],
                        "name": "B. Knowlton",
                        "slug": "B.-Knowlton",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Knowlton",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Knowlton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14179477,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "5ad3351e343d6e10a772e3a3392be5c912a00e01",
            "isKey": false,
            "numCitedBy": 2033,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": "Many organisms, especially humans, are characterized by their capacity for intentional, goal-directed actions. However, similar behaviours often proceed automatically, as habitual responses to antecedent stimuli. How are goal-directed actions transformed into habitual responses? Recent work combining modern behavioural assays and neurobiological analysis of the basal ganglia has begun to yield insights into the neural basis of habit formation."
            },
            "slug": "The-role-of-the-basal-ganglia-in-habit-formation-Yin-Knowlton",
            "title": {
                "fragments": [],
                "text": "The role of the basal ganglia in habit formation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Recent work combining modern behavioural assays and neurobiological analysis of the basal ganglia has begun to yield insights into the neural basis of habit formation."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262347"
                        ],
                        "name": "A. Turing",
                        "slug": "A.-Turing",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Turing",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Turing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14636783,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "2d5673caa9e6af3a7b82a43f19ee920992db07ad",
            "isKey": false,
            "numCitedBy": 4504,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose to consider the question, \u201cCan machines think?\u201d\u2663 This should begin with definitions of the meaning of the terms \u201cmachine\u201d and \u201cthink\u201d. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words \u201cmachine\u201d and \u201cthink\u201d are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, \u201cCan machines think?\u201d is to be sought in a statistical survey such as a Gallup poll."
            },
            "slug": "Computing-Machinery-and-Intelligence-Turing",
            "title": {
                "fragments": [],
                "text": "Computing Machinery and Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The question, \u201cCan machines think?\u201d is considered, and the question is replaced by another, which is closely related to it and is expressed in relatively unambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7991309"
                        ],
                        "name": "A. Samuel",
                        "slug": "A.-Samuel",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Samuel",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 7
                            }
                        ],
                        "text": "Arthur Samuel (1959) was the first to propose and implement a learning method that included temporal-difference ideas, as part of his celebrated checkers-playing program."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 619,
                                "start": 7
                            }
                        ],
                        "text": "Arthur Samuel (1959) was the first to propose and implement a learning method that included temporal-difference ideas, as part of his celebrated checkers-playing program. Samuel made no reference to Minsky's work or to possible connections to animal learning. His inspiration apparently came from Claude Shannon's (1950a) suggestion that a computer could be programmed to use an evaluation function to play chess, and that it might be able to to improve its play by modifying this function online. (It is possible that these ideas of Shannon also influenced Bellman, but we know of no evidence for this.) Minsky (1961) extensively discussed Samuel's work in his ``Steps\" paper, suggesting the connection to secondary reinforcement theories, natural and artificial."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 47
                            }
                        ],
                        "text": "205 were used for learning value functions was Samuel's checkers player (1959, 1967). Samuel followed Shannon's (1950b) suggestion that a value function did not have to be exact to be a useful guide to selecting moves in a game and that it might be approximated by linear combination of features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 7
                            }
                        ],
                        "text": "Arthur Samuel (1959) was the first to propose and implement a learning method that included temporal-difference ideas, as part of his celebrated checkers-playing program. Samuel made no reference to Minsky's work or to possible connections to animal learning. His inspiration apparently came from Claude Shannon's (1950a) suggestion that a computer could be programmed to use an evaluation function to play chess, and that it might be able to to improve its play by modifying this function online."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 530,
                                "start": 136
                            }
                        ],
                        "text": "104 To the best of our knowledge, the first connection between DP and reinforcement learning was made by Minsky (1961) in commenting on Samuel's checkers player. In a footnote, Minsky mentioned that it is possible to apply DP to problems in which Samuel's backing-up process can be handled in closed analytic form. This remark may have mislead artificial intelligence researchers into believing that DP was restricted to analytically tractable problems and therefore largely irrelevant to artificial intelligence. Andreae (1969b) also mentioned DP in the context of reinforcement learning, specifically policy iteration, although he did not make specific connections between DP and learning algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1029,
                                "start": 136
                            }
                        ],
                        "text": "104 To the best of our knowledge, the first connection between DP and reinforcement learning was made by Minsky (1961) in commenting on Samuel's checkers player. In a footnote, Minsky mentioned that it is possible to apply DP to problems in which Samuel's backing-up process can be handled in closed analytic form. This remark may have mislead artificial intelligence researchers into believing that DP was restricted to analytically tractable problems and therefore largely irrelevant to artificial intelligence. Andreae (1969b) also mentioned DP in the context of reinforcement learning, specifically policy iteration, although he did not make specific connections between DP and learning algorithms. Werbos (1977) suggested an approach to approximating DP called ``heuristic dynamic programming\" that emphasizes gradient-descent methods for continuous-state problems (Werbos, 1982, 1987, 1988, 1989,1992). These methods are closely related to the reinforcement learning algorithms that we discuss in this book. Watkins (1989) was explicit in connecting reinforcement learning to DP, characterizing a class of reinforcement learning methods as ``incremental dynamic programming."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "Fairly good amateur opponents characterized it as ``tricky but beatable\" (Samuel, 1959)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 717,
                                "start": 136
                            }
                        ],
                        "text": "104 To the best of our knowledge, the first connection between DP and reinforcement learning was made by Minsky (1961) in commenting on Samuel's checkers player. In a footnote, Minsky mentioned that it is possible to apply DP to problems in which Samuel's backing-up process can be handled in closed analytic form. This remark may have mislead artificial intelligence researchers into believing that DP was restricted to analytically tractable problems and therefore largely irrelevant to artificial intelligence. Andreae (1969b) also mentioned DP in the context of reinforcement learning, specifically policy iteration, although he did not make specific connections between DP and learning algorithms. Werbos (1977) suggested an approach to approximating DP called ``heuristic dynamic programming\" that emphasizes gradient-descent methods for continuous-state problems (Werbos, 1982, 1987, 1988, 1989,1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2126705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772",
            "isKey": true,
            "numCitedBy": 3041,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called \u201calpha-beta\u201d pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the"
            },
            "slug": "Some-Studies-in-Machine-Learning-Using-the-Game-of-Samuel",
            "title": {
                "fragments": [],
                "text": "Some Studies in Machine Learning Using the Game of Checkers"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method and to permit the program to look ahead to a much greater depth than it otherwise could do."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730396"
                        ],
                        "name": "M. Zweben",
                        "slug": "M.-Zweben",
                        "structuredName": {
                            "firstName": "Monte",
                            "lastName": "Zweben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zweben"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153052322"
                        ],
                        "name": "Eugene Davis",
                        "slug": "Eugene-Davis",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2958927"
                        ],
                        "name": "B. Daun",
                        "slug": "B.-Daun",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Daun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Daun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741150"
                        ],
                        "name": "Michael Deale",
                        "slug": "Michael-Deale",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Deale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Deale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 203
                            }
                        ],
                        "text": "For clues about how to do this, they looked to an existing optimization approach to SSPP, in fact, the one actually in use by NASA at the time of their research: the iterative repair method developed by Zweben et al.\\ (1994). The starting point for the search is a critical path schedule, a schedule that meets the temporal constraints but ignores the resource constraints."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9249115,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "9840b7a2c9088a8ee4506e0bf49300055352a1ae",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The GERRY scheduling and rescheduling system being applied to coordinate Space Shuttle ground processing is described. The system uses constraint-based iterative repair, a technique that starts with a complete but possibly flawed schedule and iteratively improves it by using constraint knowledge within repair heuristics. The tradeoff between the informedness and the computational cost of several repair heuristics is explored. It is shown empirically that some knowledge can greatly improve the convergence speed of a repair-based system, but that too much knowledge can overwhelm a system and result in degraded performance. >"
            },
            "slug": "Scheduling-and-rescheduling-with-iterative-repair-Zweben-Davis",
            "title": {
                "fragments": [],
                "text": "Scheduling and rescheduling with iterative repair"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown empirically that some knowledge can greatly improve the convergence speed of a repair-based system, but that too much knowledge can overwhelm a system and result in degraded performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145963824"
                        ],
                        "name": "M. Spong",
                        "slug": "M.-Spong",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Spong",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Spong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 2
                            }
                        ],
                        "text": "4 Sutton (1988) proved convergence of linear TD(0) in the mean to the minimal MSVE solution for the case in which the feature vectors, {\u03c6(s) : s \u2208 S}, are linearly independent."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9545500,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "00920ddc2a231c54dfbd4f4f57f4ce35d00b6128",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Investigates the problem of swing up control of the Acrobot, a two-link, underactuated robot that is a useful vehicle to study problems in nonlinear control. The author develops a swing up strategy based on partial feedback linearization. The algorithm works by creating \"unstable zero dynamics\" which drives the first link of the Acrobot away from its open loop stable equilibrium toward the inverted position. Control is switched to a linear controller, designed to balance the arm about the inverted configuration, whenever the swing up controller moves the Acrobot into the near vertical position. Simulation results are presented showing the performance of the system.<<ETX>>"
            },
            "slug": "Swing-up-control-of-the-Acrobot-Spong",
            "title": {
                "fragments": [],
                "text": "Swing up control of the Acrobot"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A swing up strategy based on partial feedback linearization is developed which works by creating \"unstable zero dynamics\" which drives the first link of the Acrobot away from its open loop stable equilibrium toward the inverted position."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1994 IEEE International Conference on Robotics and Automation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737243"
                        ],
                        "name": "L. Fogel",
                        "slug": "L.-Fogel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Fogel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46400209"
                        ],
                        "name": "A. J. Owens",
                        "slug": "A.-J.-Owens",
                        "structuredName": {
                            "firstName": "Alvin",
                            "lastName": "Owens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1446900976"
                        ],
                        "name": "M. J. Walsh",
                        "slug": "M.-J.-Walsh",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Walsh",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Walsh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 137
                            }
                        ],
                        "text": "RPE-signaling neurons may belong to one among multiple populations of dopamine neurons having different targets and subserving different functions. Berns, McClure, Pagnoni, and Montague (2001), Breiter, Aharon, Kahneman, Dale, and Shizgal (2001), Pagnoni, Zink, Montague, and Berns (2002), and O\u2019Doherty, Dayan, Friston, Critchley, and Dolan (2003) described functional brain imaging studies supporting the existence of signals like TD errors in the human brain."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62252283,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a9e41a611b3b57b828775a45a7d74a1c75ed3f20",
            "isKey": false,
            "numCitedBy": 3229,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: References Artificial Intelligence through a Simulation of Evolution Natural Automata and Prosthetic Devices"
            },
            "slug": "Artificial-Intelligence-through-Simulated-Evolution-Fogel-Owens",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence through Simulated Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This chapter contains sections titled: References Artificial Intelligence through a Simulation of Evolution Natural Automata and Prosthetic Devices and Artificial intelligence through a simulation of Evolution natural automata and prosthetic devices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144495105"
                        ],
                        "name": "E. Thorp",
                        "slug": "E.-Thorp",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Thorp",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Thorp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61372441,
            "fieldsOfStudy": [
                "Law"
            ],
            "id": "a4da338cff8261a9853bc8007a5257ca33b0cd38",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "New York Times Bestseller \u0412 Edward O. Thorp is the father of card counting, and in Beat the Dealer he reveals the revolutionary point system that has been successfully used by professional and amateur card players for two generations. From Las Vegas to Monte Carlo, the tables have been turned and the house no longer has the advantage at blackjack. \u0412 \u0412 \u0412 \u0412 \u0412 \u0412 \u0412 \u0412 \u0412 Containing the basic rules of the game, proven winning strategies, how to overcome casino counter measures and spot cheating. Beat the Dealer is the bible for players of this game of chance. Perforated cards included in the book are a convenient way to bring the strategies into the casino.\u0412 A winning strategy for the game of 21. The essentials, consolidated in simple charts, can be understood and memorized by the average player.From the Paperback edition."
            },
            "slug": "Beat-the-Dealer:-A-Winning-Strategy-for-the-Game-of-Thorp",
            "title": {
                "fragments": [],
                "text": "Beat the Dealer: A Winning Strategy for the Game of Twenty-One"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Beat the Dealer is the bible for players of this game of chance, containing the basic rules of the game, proven winning strategies, how to overcome casino counter measures and spot cheating."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734004"
                        ],
                        "name": "G. Goodwin",
                        "slug": "G.-Goodwin",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Goodwin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Goodwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20808839"
                        ],
                        "name": "K. Sin",
                        "slug": "K.-Sin",
                        "structuredName": {
                            "firstName": "Kwai",
                            "lastName": "Sin",
                            "middleNames": [
                                "Sang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 2
                            }
                        ],
                        "text": ", Goodwin and Sin, 1984), where they are used to make the same kind of distinction. The term system identification is used in adaptive control for what we call model-learning (e.g., Goodwin and Sin, 1984; Ljung and S\u00f6derstrom, 1983; Young, 1984). The Dyna architecture is due to Sutton (1990), and the results in this and the next section are based on results reported there."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60452885,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "ff9da0b0a16634f76422d8533e1996aac9b871bc",
            "isKey": false,
            "numCitedBy": 4266,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This unified survey focuses on linear discrete-time systems and explores the natural extensions to nonlinear systems. In keeping with the importance of computers to practical applications, the authors emphasize discrete-time systems. Their approach summarizes the theoretical and practical aspects of a large class of adaptive algorithms.1984 edition."
            },
            "slug": "Adaptive-filtering-prediction-and-control-Goodwin-Sin",
            "title": {
                "fragments": [],
                "text": "Adaptive filtering prediction and control"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This unified survey focuses on linear discrete-time systems and explores the natural extensions to nonlinear systems and summarizes the theoretical and practical aspects of a large class of adaptive algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39933800"
                        ],
                        "name": "C. Gibbs",
                        "slug": "C.-Gibbs",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gibbs",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gibbs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5320989"
                        ],
                        "name": "V. Cool",
                        "slug": "V.-Cool",
                        "structuredName": {
                            "firstName": "Valerie",
                            "lastName": "Cool",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Cool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060265532"
                        ],
                        "name": "T. Land",
                        "slug": "T.-Land",
                        "structuredName": {
                            "firstName": "Tamio",
                            "lastName": "Land",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Land"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144964201"
                        ],
                        "name": "E. Kehoe",
                        "slug": "E.-Kehoe",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Kehoe",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kehoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3571380"
                        ],
                        "name": "I. Gormezano",
                        "slug": "I.-Gormezano",
                        "structuredName": {
                            "firstName": "Isidore",
                            "lastName": "Gormezano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Gormezano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12260351,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "dccf7d93a08366e3970faa5ce5321225b243200f",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Second-order conditioning of the rabbit\u2019s nictitating membrane response (NMR) was investigated when second-order trials (CS1-CS2) were intermixed with first-order trials (CS2-US) from the outset of training. Experiment 1 showed that CR acquisition to CS1 was inversely related to the CS1-CS2 interval but nevertheless extended to an interval of 8,400 ms. Experiment 2 revealed that CR acquisition of CS1 was an inverted-U function of the number of CS1-CS2 trials relative to a fixed number of CS2-US trials. Experiment 3 directly contrasted second-order conditioning with a reinforced serial compound procedure (CS1-CS2-US) and a mixed procedure in which second-order trials were intermixed with the reinforced serial compound. Second-order conditioning was about half the strength of either the reinforced serial compound or the mixed procedure, which were similar. The present results are discussed with respect to the relative strength of excitatory and inhibitory processes in second-order conditioning."
            },
            "slug": "Second-order-conditioning-of-the-rabbit\u2019s-membrane-Gibbs-Cool",
            "title": {
                "fragments": [],
                "text": "Second-order conditioning of the rabbit\u2019s nictitating membrane response"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Second-order conditioning of the rabbit\u2019s nictitating membrane response (NMR) was investigated when second-order trials (CS1-CS2) were intermixed with first-order Trials (CS2-US) from the outset of training and the present results are discussed with respect to the relative strength of excitatory and inhibitory processes in second- order conditioning."
            },
            "venue": {
                "fragments": [],
                "text": "Integrative physiological and behavioral science : the official journal of the Pavlovian Society"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696575"
                        ],
                        "name": "J. Bentley",
                        "slug": "J.-Bentley",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Bentley",
                            "middleNames": [
                                "Louis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bentley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 89
                            }
                        ],
                        "text": "The first demonstrations of multi-headed learning in reinforcement learning were by Jaderberg et al. (2017). Bellemare, Dabney and Munos (2017) showed that predicting more things about the distribution of reward could significantly speed learning to optimize its expectation, an instance of auxiliary tasks."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13091446,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc73bacd6a00442570d15e122604ad6862b8663d",
            "isKey": false,
            "numCitedBy": 6701,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops the multidimensional binary search tree (or <italic>k</italic>-d tree, where <italic>k</italic> is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The <italic>k</italic>-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an <italic>n</italic> record file are: insertion, <italic>O</italic>(log <italic>n</italic>); deletion of the root, <italic>O</italic>(<italic>n</italic><supscrpt>(<italic>k</italic>-1)/<italic>k</italic></supscrpt>); deletion of a random node, <italic>O</italic>(log <italic>n</italic>); and optimization (guarantees logarithmic performance of searches), <italic>O</italic>(<italic>n</italic> log <italic>n</italic>). Search algorithms are given for partial match queries with <italic>t</italic> keys specified [proven maximum running time of <italic>O</italic>(<italic>n</italic><supscrpt>(<italic>k</italic>-<italic>t</italic>)/<italic>k</italic></supscrpt>)] and for nearest neighbor queries [empirically observed average running time of <italic>O</italic>(log <italic>n</italic>).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that <italic>k</italic>-d trees could be quite useful in many applications, and examples of potential uses are given."
            },
            "slug": "Multidimensional-binary-search-trees-used-for-Bentley",
            "title": {
                "fragments": [],
                "text": "Multidimensional binary search trees used for associative searching"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The multidimensional binary search tree (or <italic>k-d tree) as a data structure for storage of information to be retrieved by associative searches is developed and it is shown to be quite efficient in its storage requirements."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144223805"
                        ],
                        "name": "JOHN F. Young",
                        "slug": "JOHN-F.-Young",
                        "structuredName": {
                            "firstName": "JOHN F.",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JOHN F. Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 120
                            }
                        ],
                        "text": "Michie consistently emphasized the role of trial and error and learning as essential aspects of artificial intelligence (Michie, 1974)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4290129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6cea23b4a07b66e3122252f47f9cc715b469cf9",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Progress of Cybernetics. Vol. 1: Main Papers, The Meaning of Cybernetics, Neuro- and Biocybernetics. Edited by J. Rose. (Proceedings of the First International Congress of Cybernetics, London, 1969.) Pp. xiv + 521. (Gordon and Breach: London and New York, September 1970.) $24.50; \u00a310.00."
            },
            "slug": "Machine-Intelligence-Young",
            "title": {
                "fragments": [],
                "text": "Machine Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The meaning of cybernetics, Neuro- and Biocybernetics and its applications to medicine, science and society are still being investigated."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157819406"
                        ],
                        "name": "P. Kumar",
                        "slug": "P.-Kumar",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713762"
                        ],
                        "name": "P. Varaiya",
                        "slug": "P.-Varaiya",
                        "structuredName": {
                            "firstName": "Pravin",
                            "lastName": "Varaiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Varaiya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57169308,
            "fieldsOfStudy": [
                "Mathematics",
                "Engineering"
            ],
            "id": "8ae83806465bb89763a6ec4c1f87add1d5ee1841",
            "isKey": false,
            "numCitedBy": 1072,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "stochastic systems estimation identification and adaptive stochastic adaptive control eolss stochastic systems estimation identification and adaptive stochastic systems estimation identification and adaptive control of stochastic systems eolss stochastic systems estimation identification and adaptive stochastic systems: estimation, identification and adaptation in stochastic dynamic systems survey and new identification and stochastic adaptive control (systems identification and adaptive control methods for some robust stochastic adaptive control dspace@mit: home adaptation in stochastic dynamic systems survey and new chapter 1: introduction to adaptive control stochastic systems estimation identification and adaptive stochastic adaptive nash certainty equivalence control coefficient estimation in adaptive control systems maximum likelihood identification and realization of (size 44,85mb) download ebook stable adaptive systems optimal adaptive control of uncertain stochastic discrete 19,42mb file download system identification adaptive on-line identification and adaptive trajectory tracking ece686: filtering and control of stochastic linear systems robustness and convergence of least-squares identification 68,58mb file system identification adaptive control bahram adaptation in stochastic dynamic systems survey and new identification and system parameter estimation 1991 gbv stochastic adaptive control via consistent parameter adaptive control of stochastic sage pub stochastic delay estimation and adaptive control of eece 574 adaptive control basics of system identification robust identification of stochastic linear systems with robust adaptive els-qr algorithm for linear discrete time stochastic systems: the mathematics of filtering and ee/ise 556: stochastic systems fall 2013 usc search identification and system parameter estimation 1991 gbv"
            },
            "slug": "Stochastic-Systems:-Estimation,-Identification,-and-Kumar-Varaiya",
            "title": {
                "fragments": [],
                "text": "Stochastic Systems: Estimation, Identification, and Adaptive Control"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The mathematics of filtering and ee/ise 556: stochastic systems fall 2013 usc search identification and system parameter estimation 1991 gbv is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754307"
                        ],
                        "name": "H. Markram",
                        "slug": "H.-Markram",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Markram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Markram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40139468"
                        ],
                        "name": "J. L\u00fcbke",
                        "slug": "J.-L\u00fcbke",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "L\u00fcbke",
                            "middleNames": [
                                "H.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. L\u00fcbke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4947775"
                        ],
                        "name": "M. Frotscher",
                        "slug": "M.-Frotscher",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Frotscher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Frotscher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3204511"
                        ],
                        "name": "B. Sakmann",
                        "slug": "B.-Sakmann",
                        "structuredName": {
                            "firstName": "Bert",
                            "lastName": "Sakmann",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sakmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46640132,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "679f48306b47634b3ad6b64d8505d36e09dadfe3",
            "isKey": false,
            "numCitedBy": 2889,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials (APs) and unitary excitatory postsynaptic potentials (EPSPs) was found to induce changes in EPSPs. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic APs relative to EPSPs. These observations suggest that APs propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons."
            },
            "slug": "Regulation-of-Synaptic-Efficacy-by-Coincidence-of-Markram-L\u00fcbke",
            "title": {
                "fragments": [],
                "text": "Regulation of Synaptic Efficacy by Coincidence of Postsynaptic APs and EPSPs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of post Synaptic action potentials and unitary excitatory postsynaptic potentials was found to induce changes in EPSPs."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2291243"
                        ],
                        "name": "Gary N. Boone",
                        "slug": "Gary-N.-Boone",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Boone",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary N. Boone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 48
                            }
                        ],
                        "text": ", Spong, 1994) and machine-learning researchers (e.g., Dejong and Spong, 1994; Boone, 1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15189788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6992b208f65b90ae38c39549bce799a8254e8b5",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The Acrobot, a two-link arm that is unactuated at the first joint, is challenging to control because it is an underactuated, nonlinear, continuous system. We describe a direct search algorithm for finding swingup trajectories for the Acrobot. The algorithm uses a lookahead search that maximizes the Acrobot's total energy in an N-step window. Because the controls are extremal and the number of switches in the window limited, the algorithm is fast. Nevertheless, the resulting trajectories are near optimal."
            },
            "slug": "Minimum-time-control-of-the-Acrobot-Boone",
            "title": {
                "fragments": [],
                "text": "Minimum-time control of the Acrobot"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A direct search algorithm for finding swingup trajectories for the Acrobot is described, which uses a lookahead search that maximizes theAcrobot's total energy in an N-step window."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Robotics and Automation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703847"
                        ],
                        "name": "Boris Polyak",
                        "slug": "Boris-Polyak",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Polyak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Polyak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754887"
                        ],
                        "name": "A. Juditsky",
                        "slug": "A.-Juditsky",
                        "structuredName": {
                            "firstName": "Anatoli",
                            "lastName": "Juditsky",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Juditsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 70
                            }
                        ],
                        "text": "The notions of momentum (Derthick, 1984), of Polyak-Ruppert averaging (Polyak, 1990; Ruppert, 1988; Polyak and Juditsky, 1992), or further extensions of these ideas may significantly help."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3548228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dc61f37ecc552413606d8c89ffbc46ec98ed887",
            "isKey": false,
            "numCitedBy": 1536,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence."
            },
            "slug": "Acceleration-of-stochastic-approximation-by-Polyak-Juditsky",
            "title": {
                "fragments": [],
                "text": "Acceleration of stochastic approximation by averaging"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Convergence with probability one is proved for a variety of classical optimization and identification problems and it is demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5856803,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "dd491e855313f954536ab809a69545c24c8d8703",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matters-temporal-Dayan",
            "title": {
                "fragments": [],
                "text": "Matters temporal"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105387808"
                        ],
                        "name": "D. Shepard",
                        "slug": "D.-Shepard",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Shepard",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shepard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 150
                            }
                        ],
                        "text": "Predating widespread interest in kernel regression in machine learning, these authors did not use the term kernel, but referred to \u201cShepard\u2019s method\u201d (Shepard, 1968)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 42723195,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "dd03f2ba2e166143a173aa4dba432df302ae898f",
            "isKey": false,
            "numCitedBy": 4387,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In many fields using empirical areal data there arises a need for interpolating from irregularly-spaced data to produce a continuous surface. These irregularly-spaced locations, hence referred to as \u201cdata points,\u201d may have diverse meanings: in meterology, weather observation stations; in geography, surveyed locations; in city and regional planning, centers of data-collection zones; in biology, observation locations. It is assumed that a unique number (such as rainfall in meteorology, or altitude in geography) is associated with each data point. In order to display these data in some type of contour map or perspective view, to compare them with data for the same region based on other data points, or to analyze them for extremes, gradients, or other purposes, it is extremely useful, if not essential, to define a continuous function fitting the given values exactly. Interpolated values over a fine grid may then be evaluated. In using such a function it is assumed that the original data are without error, or that compensation for error will be made after interpolation."
            },
            "slug": "A-two-dimensional-interpolation-function-for-data-Shepard",
            "title": {
                "fragments": [],
                "text": "A two-dimensional interpolation function for irregularly-spaced data"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "In many fields using empirical areal data there arises a need for interpolating from irregularly-spaced data to produce a continuous surface, and it is extremely useful, if not essential, to define a continuous function fitting the given values exactly."
            },
            "venue": {
                "fragments": [],
                "text": "ACM National Conference"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143664817"
                        ],
                        "name": "D. White",
                        "slug": "D.-White",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "White",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62587712,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2cf6ef148f43b4df12acb2abe72af67eae54a09b",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first few years of an ongoing survey of applications of Markov decision processes where the results have been implemented or have had some influence on decisions, few applications have been identified where the results have been implemented but there appears to be an increasing effort to model many phenomena as Markov decision processes."
            },
            "slug": "Real-Applications-of-Markov-Decision-Processes-White",
            "title": {
                "fragments": [],
                "text": "Real Applications of Markov Decision Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "In the first few years of an ongoing survey of applications of Markov decision processes where the results have been implemented or have had some influence on decisions, few applications have been identified but there appears to be an increasing effort to model many phenomena as Markov decisions processes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144964201"
                        ],
                        "name": "E. Kehoe",
                        "slug": "E.-Kehoe",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Kehoe",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kehoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5376085"
                        ],
                        "name": "B. Schreurs",
                        "slug": "B.-Schreurs",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Schreurs",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schreurs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112067008"
                        ],
                        "name": "Peita Graham",
                        "slug": "Peita-Graham",
                        "structuredName": {
                            "firstName": "Peita",
                            "lastName": "Graham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peita Graham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143793990,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "eac84fd3ff37fdeb793efe1ac5ed31fc77a3c7b1",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditioning of the rabbit\u2019s nictitating membrane response was conducted with a serial compound CSA-CSB-US. In the present experiments, prior training of CSB was pitted against the temporal primacy of CSA. Prior training of CSB was able to only weakly block CR acquisition to the added CSA, but CSA caused a pronounced decline in responding to the pretrained CSB. By the end of training, high levels of responding were sustained only in the final portion of the serial compound in which CSA or its traces coincided with CSB. These results provide support for real-time models as exemplified by Sutton and Barto (1981)."
            },
            "slug": "Temporal-primacy-overrides-prior-training-in-serial-Kehoe-Schreurs",
            "title": {
                "fragments": [],
                "text": "Temporal primacy overrides prior training in serial compound conditioning of the rabbit\u2019s nictitating membrane response"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "The results of the present experiments provide support for real-time models as exemplified by Sutton and Barto (1981)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42241754,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "e8bf97dbdb88e60476cd6124ccae6e5accccd0b4",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithmic model for distributed computation of fixed points whereby several processors participate simultaneously in the calculations while exchanging information via communication links. We place essentially no assumptions on the ordering of computation and communication between processors thereby allowing for completely uncoordinated execution. We provide a general convergence theorem for algorithms of this type, and demonstrate its applicability to several classes of problems including the calculation of fixed points of contraction and monotone mappings arising in linear and nonlinear systems of equations, optimization problems, shortest path problems, and dynamic programming."
            },
            "slug": "Distributed-asynchronous-computation-of-fixed-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Distributed asynchronous computation of fixed points"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A general convergence theorem is provided for algorithms of this type including the calculation of fixed points of contraction and monotone mappings arising in linear and nonlinear systems of equations, optimization problems, shortest path problems, and dynamic programming."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026553"
                        ],
                        "name": "O. Selfridge",
                        "slug": "O.-Selfridge",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Selfridge",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Selfridge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116586237,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e601ad411e21f7545ea26a5edc4d9f0f8b9f5b24",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "To say that something is ill-defined is more to describe us than it. That is, for some system we are dealing with, we are more or less ignorant of its working, and that means that we have special problems in trying to control it. Nevertheless, of course, most of the real systems we deal with are ill-defined in that sense \u2014 like other people. But we do find ways to exercise some degree of control."
            },
            "slug": "Some-Themes-and-Primitives-in-Ill-Defined-Systems-Selfridge",
            "title": {
                "fragments": [],
                "text": "Some Themes and Primitives in Ill-Defined Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "For some system the authors are dealing with, they are more or less ignorant of its working, and that means that they have special problems in trying to control it."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34099369"
                        ],
                        "name": "B. Farley",
                        "slug": "B.-Farley",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Farley",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113802893"
                        ],
                        "name": "W. A. Clark",
                        "slug": "W.-A.-Clark",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Clark",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. A. Clark"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28327238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "879650714f3dce59b666346ccf63fd73250259d6",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A general discussion of ideas and definitions relating to self-organizing systems and their synthesis is given, together with remarks concerning their simulation by digital computer. Synthesis and simulation of an actual system is then described. This system, initially randomly organized within wide limits, organizes itself to perform a simple prescribed task."
            },
            "slug": "Simulation-of-self-organizing-systems-by-digital-Farley-Clark",
            "title": {
                "fragments": [],
                "text": "Simulation of self-organizing systems by digital computer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A general discussion of ideas and definitions relating to self-organizing systems and their synthesis is given, together with remarks concerning their simulation by digital computer."
            },
            "venue": {
                "fragments": [],
                "text": "Trans. IRE Prof. Group Inf. Theory"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145495871"
                        ],
                        "name": "A. Karlsen",
                        "slug": "A.-Karlsen",
                        "structuredName": {
                            "firstName": "A",
                            "lastName": "Karlsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karlsen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8055364,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f350c2c887c9a78a217dc429297799c5d738b7f5",
            "isKey": false,
            "numCitedBy": 347,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Selection made on the basis of consequences is a causal mode found only in \"living\" things. Selection happens at the level of species, individual and culture. Human behaviour is the joint product of the contingencies of existence responsible for natural selection and of contingencies by which the behaviour of individuals are selected, including the contingencies maintained by an evolved social environment. Behaviour is not caused by immaterial processes inside the organism. It is the contingencies for behaviour and the behaviour itself that have to be analysed, and possibly changed. The implications for treatment may be great."
            },
            "slug": "[Selection-by-consequences].-Karlsen",
            "title": {
                "fragments": [],
                "text": "[Selection by consequences]."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Human behaviour is the joint product of the contingencies of existence responsible for natural selection and of contingencies by which the behaviour of individuals are selected, including the contingency maintained by an evolved social environment."
            },
            "venue": {
                "fragments": [],
                "text": "Tidsskrift for den Norske laegeforening : tidsskrift for praktisk medicin, ny raekke"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49528487"
                        ],
                        "name": "Hong Wang",
                        "slug": "Hong-Wang",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Wang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 91
                            }
                        ],
                        "text": "The term system identification is used in adaptive control for what we call model-learning (e.g., Goodwin and Sin, 1984; Ljung and S\u00f6derstrom, 1983; Young, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28031941,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "673a33fd73d1c10ba615c703f51154e419cec034",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "recursive estimation and time series analysis What to say and what to do when mostly your friends love reading? Are you the one that don't have such hobby? So, it's important for you to start having that hobby. You know, reading is not the force. We're sure that reading will lead you to join in better concept of life. Reading will be a positive activity to do every time. And do you know our friends become fans of recursive estimation and time series analysis as the best book to read? Yeah, it's neither an obligation nor order. It is the referred book that will not make you feel disappointed."
            },
            "slug": "Recursive-estimation-and-time-series-analysis-Wang",
            "title": {
                "fragments": [],
                "text": "Recursive estimation and time-series analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It's important for you to start having that hobby that will lead you to join in better concept of life and reading will be a positive activity to do every time."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069089245"
                        ],
                        "name": "Ryan",
                        "slug": "Ryan",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Ryan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120534422"
                        ],
                        "name": "Deci",
                        "slug": "Deci",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Deci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1098145,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b55987b4cfff292dd121ee03c46b41f4f696136e",
            "isKey": false,
            "numCitedBy": 12876,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Intrinsic and extrinsic types of motivation have been widely studied, and the distinction between them has shed important light on both developmental and educational practices. In this review we revisit the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory. Intrinsic motivation remains an important construct, reflecting the natural human propensity to learn and assimilate. However, extrinsic motivation is argued to vary considerably in its relative autonomy and thus can either reflect external control or true self-regulation. The relations of both classes of motives to basic human needs for autonomy, competence and relatedness are discussed. Copyright 2000 Academic Press."
            },
            "slug": "Intrinsic-and-Extrinsic-Motivations:-Classic-and-Ryan-Deci",
            "title": {
                "fragments": [],
                "text": "Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This review revisits the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory and discusses the relations of both classes of motives to basic human needs for autonomy, competence and relatedness."
            },
            "venue": {
                "fragments": [],
                "text": "Contemporary educational psychology"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096776749"
                        ],
                        "name": "Peter Redgrave",
                        "slug": "Peter-Redgrave",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Redgrave",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Redgrave"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37274,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "1ff995ae6646015e7f9ae1697299d99881a4ec93",
            "isKey": false,
            "numCitedBy": 1238,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "eprints@whiterose.ac.uk https://eprints.whiterose.ac.uk/ Reuse Unless indicated otherwise, fulltext items are protected by copyright with all rights reserved. The copyright exception in section 29 of the Copyright, Designs and Patents Act 1988 allows the making of a single copy solely for the purpose of non-commercial research or private study within the limits of fair dealing. The publisher or other rights-holder may allow further reproduction and re-use of this version refer to the White Rose Research Online record for this item. Where records identify the publisher as the copyright holder, users can verify any specific terms of use on the publisher\u2019s website."
            },
            "slug": "Basal-ganglia-Redgrave",
            "title": {
                "fragments": [],
                "text": "Basal ganglia"
            },
            "venue": {
                "fragments": [],
                "text": "Scholarpedia"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790459"
                        ],
                        "name": "E. Barnard",
                        "slug": "E.-Barnard",
                        "structuredName": {
                            "firstName": "Etienne",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Barnard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 874,
                                "start": 0
                            }
                        ],
                        "text": "Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981) experimented with associative stochastic learning automata in single-layer artificial neural networks to which a global reinforcement signal was broadcast. They called neuron-like elements implementing this kind of learning associative search elements (ASEs). Barto and Anandan (1985) introduced a more sophisticated associative reinforcement learning algorithm called the associative reward-penalty (AR\u2212P) algorithm. They proved a convergence result by combining theory of stochastic learning automata with theory of pattern classification. Barto (1985, 1986) and Barto and Jordan (1987) described results with teams of AR\u2212P units connected into multi-layer neural networks, showing that they could learn nonlinear functions, such as XOR and others, with a globally-broadcast reinforcement signal. Barto (1985) extensively discussed this approach to artificial neural networks and how this type of learning rule is related to others in the literature at that time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 28
                            }
                        ],
                        "text": "Illuminating this result is Barnard\u2019s (1993) derivation of the TD algorithm as a combination of one step of an incremental method for learning a model of the Markov chain and one step of a method for computing predictions from the model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1044,
                                "start": 0
                            }
                        ],
                        "text": "Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981) experimented with associative stochastic learning automata in single-layer artificial neural networks to which a global reinforcement signal was broadcast. They called neuron-like elements implementing this kind of learning associative search elements (ASEs). Barto and Anandan (1985) introduced a more sophisticated associative reinforcement learning algorithm called the associative reward-penalty (AR\u2212P) algorithm. They proved a convergence result by combining theory of stochastic learning automata with theory of pattern classification. Barto (1985, 1986) and Barto and Jordan (1987) described results with teams of AR\u2212P units connected into multi-layer neural networks, showing that they could learn nonlinear functions, such as XOR and others, with a globally-broadcast reinforcement signal. Barto (1985) extensively discussed this approach to artificial neural networks and how this type of learning rule is related to others in the literature at that time. Williams (1992) mathematically analyzed and broadened this class of learning rules and related their use to the error backpropagation method for training multilayer artificial neural networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 651,
                                "start": 0
                            }
                        ],
                        "text": "Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981) experimented with associative stochastic learning automata in single-layer artificial neural networks to which a global reinforcement signal was broadcast. They called neuron-like elements implementing this kind of learning associative search elements (ASEs). Barto and Anandan (1985) introduced a more sophisticated associative reinforcement learning algorithm called the associative reward-penalty (AR\u2212P) algorithm. They proved a convergence result by combining theory of stochastic learning automata with theory of pattern classification. Barto (1985, 1986) and Barto and Jordan (1987) described results with teams of AR\u2212P units connected into multi-layer neural networks, showing that they could learn nonlinear functions, such as XOR and others, with a globally-broadcast reinforcement signal."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 73
                            }
                        ],
                        "text": "Bootstrapping methods are not in fact instances of true gradient descent (Barnard, 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1237,
                                "start": 0
                            }
                        ],
                        "text": "Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981) experimented with associative stochastic learning automata in single-layer artificial neural networks to which a global reinforcement signal was broadcast. They called neuron-like elements implementing this kind of learning associative search elements (ASEs). Barto and Anandan (1985) introduced a more sophisticated associative reinforcement learning algorithm called the associative reward-penalty (AR\u2212P) algorithm. They proved a convergence result by combining theory of stochastic learning automata with theory of pattern classification. Barto (1985, 1986) and Barto and Jordan (1987) described results with teams of AR\u2212P units connected into multi-layer neural networks, showing that they could learn nonlinear functions, such as XOR and others, with a globally-broadcast reinforcement signal. Barto (1985) extensively discussed this approach to artificial neural networks and how this type of learning rule is related to others in the literature at that time. Williams (1992) mathematically analyzed and broadened this class of learning rules and related their use to the error backpropagation method for training multilayer artificial neural networks. Williams (1988) described several ways that backpropagation and reinforcement learning can be combined for training artificial neural networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1381,
                                "start": 0
                            }
                        ],
                        "text": "Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981) experimented with associative stochastic learning automata in single-layer artificial neural networks to which a global reinforcement signal was broadcast. They called neuron-like elements implementing this kind of learning associative search elements (ASEs). Barto and Anandan (1985) introduced a more sophisticated associative reinforcement learning algorithm called the associative reward-penalty (AR\u2212P) algorithm. They proved a convergence result by combining theory of stochastic learning automata with theory of pattern classification. Barto (1985, 1986) and Barto and Jordan (1987) described results with teams of AR\u2212P units connected into multi-layer neural networks, showing that they could learn nonlinear functions, such as XOR and others, with a globally-broadcast reinforcement signal. Barto (1985) extensively discussed this approach to artificial neural networks and how this type of learning rule is related to others in the literature at that time. Williams (1992) mathematically analyzed and broadened this class of learning rules and related their use to the error backpropagation method for training multilayer artificial neural networks. Williams (1988) described several ways that backpropagation and reinforcement learning can be combined for training artificial neural networks. Williams (1992) showed that a special case of the AR\u2212P algorithm is a REINFORCE algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 347,
                                "start": 0
                            }
                        ],
                        "text": "Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981) experimented with associative stochastic learning automata in single-layer artificial neural networks to which a global reinforcement signal was broadcast. They called neuron-like elements implementing this kind of learning associative search elements (ASEs). Barto and Anandan (1985) introduced a more sophisticated associative reinforcement learning algorithm called the associative reward-penalty (AR\u2212P) algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 0
                            }
                        ],
                        "text": "Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981) experimented with associative stochastic learning automata in single-layer artificial neural networks to which a global reinforcement signal was broadcast."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6834759,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e05ebaa7725ce7da3fbd16d113e71c88822aeaa",
            "isKey": true,
            "numCitedBy": 52,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The relation between temporal-difference training methods and Markov models is explored. This relation is derived from a new perspective, and in this way the particular association between conventional temporal-difference methods and first-order Markov models is explained. The authors then derive a generalization of temporal-difference methods that is suitable for Markov models of higher order. Several issues related to the performance of mismatched temporal-difference methods (i.e., the performance when the temporal-difference method is not specifically designed to match the order of the Markov model) are investigated numerically. >"
            },
            "slug": "Temporal-difference-methods-and-Markov-models-Barnard",
            "title": {
                "fragments": [],
                "text": "Temporal-difference methods and Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A generalization of temporal-difference methods is derived that is suitable for Markov models of higher order, and several issues related to the performance of mismatched temporal-Difference methods are investigated numerically."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11017566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffa94bba647817fa5e8f8d3250fc977435b5ca76",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a generic method for iteratively approximating various second-order gradient steps-Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD."
            },
            "slug": "Fast-Curvature-Matrix-Vector-Products-for-Gradient-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A generic method for iteratively approximating various second-order gradient steps-Newton, Gauss- newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143664817"
                        ],
                        "name": "D. White",
                        "slug": "D.-White",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "White",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62200958,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "000fbd970e7ff72107fdf806027f4ba597637571",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper extends an earlier paper [White 1985] on real applications of Markov decision processes in which the results of the studies have been implemented, have had some influence on the actual decisions, or in which the analyses are based on real data."
            },
            "slug": "Further-Real-Applications-of-Markov-Decision-White",
            "title": {
                "fragments": [],
                "text": "Further Real Applications of Markov Decision Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper extends an earlier paper on real applications of Markov decision processes in which the results of the studies have been implemented, have had some influence on the actual decisions, or on which the analyses are based on real data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35064932"
                        ],
                        "name": "R. Rubinstein",
                        "slug": "R.-Rubinstein",
                        "structuredName": {
                            "firstName": "Reuven",
                            "lastName": "Rubinstein",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rubinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 80
                            }
                        ],
                        "text": "Coverage of Monte Carlo methods in this sense can be found in several textbooks (e.g., Kalos and Whitlock, 1986; Rubinstein, 1981)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39230485,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0367b5c4eca7bef6245ec183648f9aec18da91a6",
            "isKey": false,
            "numCitedBy": 2476,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProvides the first simultaneous coverage of the statistical aspects of simulation and Monte Carlo methods, their commonalities and their differences for the solution of a wide spectrum of engineering and scientific problems. Contains standard material usually considered in Monte Carlo simulation as well as new material such as variance reduction techniques, regenerative simulation, and Monte Carlo optimization."
            },
            "slug": "Simulation-and-the-Monte-Carlo-method-Rubinstein",
            "title": {
                "fragments": [],
                "text": "Simulation and the Monte Carlo method"
            },
            "venue": {
                "fragments": [],
                "text": "Wiley series in probability and mathematical statistics"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698345"
                        ],
                        "name": "C. Page",
                        "slug": "C.-Page",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Page",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Page"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 140
                            }
                        ],
                        "text": "In addition to linear function approximation, Samuel experimented with lookup tables and hierarchical lookup tables called signature tables (Griffith, 1966, 1974; Page, 1977; Biermann, Fairfield, and Beres, 1982)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6558384,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d9f746acd675adbddbab4d81ecefdbb59f4bc22",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The signature table method is a hierarchical approach for the recognition of binary patterns which are described by means of many features. The method was first applied by A. L. Samuel and his students for prediction of whether a given checker position was master quality or not. The variants of the methods discussed herein consider it as a general pattern recognition technique. Examples are presented to suggest both the potential and hazards of using the method for recognition of patterns. Advantages of the method are the extraction of nonlinear Boolean relationships among the variables, the use of incomplete data in a routine way, and that it sometimes may provide better prediction at less cost than multiple regression. The principle disadvantage, the lack of a theory to guide the choice of key parameters in the method, can sometimes be overcome by systematic computer search."
            },
            "slug": "Heuristics-for-Signature-Table-Analysis-as-a-Page",
            "title": {
                "fragments": [],
                "text": "Heuristics for Signature Table Analysis as a Pattern Recognition Technique"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Advantages of the signature table method are the extraction of nonlinear Boolean relationships among the variables, the use of incomplete data in a routine way, and that it sometimes may provide better prediction at less cost than multiple regression."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10023329,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "81a15463a09b9555a78b755e43f9a1c278321ce3",
            "isKey": false,
            "numCitedBy": 1887,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The primary aim of this paper is to show how graphical models can be used as a mathematical language for integrating statistical and subject-matter information. In particular, the paper develops a principled, nonparametric framework for causal inference, in which diagrams are queried to determine if the assumptions available are sufficient for identifying causal effects from nonexperimental data. If so the diagrams can be queried to produce mathematical expressions for causal effects in terms of observed distributions; otherwise, the diagrams can be queried to suggest additional observations or auxiliary experiments from which the desired inferences can be obtained."
            },
            "slug": "Causal-diagrams-for-empirical-research-Pearl",
            "title": {
                "fragments": [],
                "text": "Causal diagrams for empirical research"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": false,
            "numCitedBy": 4043,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143664817"
                        ],
                        "name": "D. White",
                        "slug": "D.-White",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "White",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60633492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6c3243e2f7da9ded57246493e565cd1f7853204",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "A collection of papers on the application of Markov decision processes is surveyed and classified according to the use of real life data, structural results and special computational schemes. Observations are made about various features of the applications."
            },
            "slug": "A-Survey-of-Applications-of-Markov-Decision-White",
            "title": {
                "fragments": [],
                "text": "A Survey of Applications of Markov Decision Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A collection of papers on the application of Markov decision processes is surveyed and classified according to the use of real life data, structural results and special computational schemes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34196655,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "id": "b05b67aca720d0bc39bc9afad02a19f522c7a1bc",
            "isKey": false,
            "numCitedBy": 2414,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Objective\u2014To evaluate the pharmacokinetics of a novel commercial formulation of ivermectin after administration to goats.\r\n\r\nAnimals\u20146 healthy adult goats.\r\n\r\nProcedure\u2014Ivermectin (200 \u03bcg/kg) was initially administered IV to each goat, and plasma samples were obtained for 36 days. After a washout period of 3 weeks, each goat received a novel commercial formulation of ivermectin (200 \u03bcg/kg) by SC injection. Plasma samples were then obtained for 42 days. Drug concentrations were quantified by use of high-performance liquid chromatography with fluorescence detection.\r\n\r\nResults\u2014Pharmacokinetics of ivermectin after IV administration were best described by a 2-compartment open model; values for main compartmental variables included volume of distribution at a steady state (9.94 L/kg), clearance (1.54 L/kg/d), and area under the plasma concentration-time curve (AUC; 143 [ng\u2022d]/mL). Values for the noncompartmental variables included mean residence time (7.37 days), AUC (153 [ng\u2022d]/mL), and clearance (1.43 L/kg/d). After SC administration, noncompartmental pharmacokinetic analysis was conducted. Values of the variables calculated by use of this method included maximum plasma concentration (Cmax; 21.8 ng/mL), time to reach Cmax (3 days), and bioavailability (F; 91.8%).\r\n\r\nConclusions and Clinical Relevance\u2014The commercial formulation used in this study is a good option to consider when administering ivermectin to goats because of the high absorption, which is characterized by high values of F. In addition, the values of Cmax and time to reach Cmax are higher than those reported by other investigators who used other routes of administration."
            },
            "slug": "Pharmacokinetics-of-a-novel-formulation-of-after-to-Ng-Russell",
            "title": {
                "fragments": [],
                "text": "Pharmacokinetics of a novel formulation of ivermectin after administration to goats"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Pharmacokinetics of ivermectin after IV administration were best described by a 2-compartment open model; values for main compartmental variables included volume of distribution at a steady state, area under the plasma concentration-time curve, and area underThe AUC curve."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3329205"
                        ],
                        "name": "D. Dennett",
                        "slug": "D.-Dennett",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Dennett",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dennett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 153
                            }
                        ],
                        "text": "The authors were also strongly influenced by psychological studies of latent learning (Tolman, 1932) and by psychological views of the nature of thought (e.g., Galanter and Gerstenhaber, 1956; Craik, 1943; Campbell, 1960; Dennett, 1978)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 120
                            }
                        ],
                        "text": "Despite this, the Law of Effect\u2014in one form or another\u2014is widely regarded as a basic principle underlying much behavior (e.g., Hilgard and Bower, 1975; Dennett, 1978; Campbell, 1960; Cziko, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 143907418,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "821c66f8321580adbafa380f4456f191fbf70628",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Why-the-Law-of-Effect-will-not-Go-Away-Dennett",
            "title": {
                "fragments": [],
                "text": "Why the Law of Effect will not Go Away"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35126179"
                        ],
                        "name": "P. Agre",
                        "slug": "P.-Agre",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Agre",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Agre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055155252"
                        ],
                        "name": "David Chapman",
                        "slug": "David-Chapman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chapman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chapman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43169899,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d88a427b464c432210b2e7d9770ea18de69d9898",
            "isKey": false,
            "numCitedBy": 473,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-are-plans-for-Agre-Chapman",
            "title": {
                "fragments": [],
                "text": "What are plans for?"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48801723"
                        ],
                        "name": "F. Downton",
                        "slug": "F.-Downton",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Downton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Downton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 20
                            }
                        ],
                        "text": "The E3 algorithm of Kearns and Singh (2002) and the R-max algorithm of Brafman and Tennenholtz (2003) are guaranteed to find a near-optimal solution in time polynomial in the number of states and actions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4224834,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "555a7921231b86464a8db34ada9edade765bf8f0",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic ApproximationBy M. T. Wasan. (Cambridge Tracts in Mathematics and Mathematical Physics, No. 58.) Pp. x + 202. (Cambridge University Press: London, June 1969.) 70s; $9.50."
            },
            "slug": "Stochastic-Approximation-Downton",
            "title": {
                "fragments": [],
                "text": "Stochastic Approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111120931"
                        ],
                        "name": "Jorge Nuno Silva",
                        "slug": "Jorge-Nuno-Silva",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Silva",
                            "middleNames": [
                                "Nuno"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge Nuno Silva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37388925,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "b8ec3ae36f26e78f9a87a4edc5993d1a73a07cb7",
            "isKey": false,
            "numCitedBy": 709,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we survey most of the games that, throughout history, can be classified as mathematical games. This paper is based on a talk given at the conference \u2018Numeracy: historical, philosophical, and educational perspectives\u2019 at St Anne's College, Oxford, December 2009."
            },
            "slug": "Mathematical-Games-Silva",
            "title": {
                "fragments": [],
                "text": "Mathematical Games"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722620"
                        ],
                        "name": "K. Sivarajan",
                        "slug": "K.-Sivarajan",
                        "structuredName": {
                            "firstName": "Kumar",
                            "lastName": "Sivarajan",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sivarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38533359"
                        ],
                        "name": "J. Ketchum",
                        "slug": "J.-Ketchum",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Ketchum",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ketchum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61943508,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "dfd2f0963c2b024594bd991a4f09267e17e01862",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Dynamic channel assignment algorithms for cellular systems are developed. The algorithms are compared with an easily simulated bound. Using this bound, it is demonstrated that in the case of homogeneous spatial traffic distribution, some of these algorithms are virtually unbeatable by any channel assignment algorithm. These algorithms are shown to be feasible for implementation in current cellular systems. For the examples considered, in the interesting range of blocking probabilities (2-4%), the dynamic channel assignment algorithms yielded an increase of 60-80% in the carried traffic over the best-known fixed channel assignment.<<ETX>>"
            },
            "slug": "Dynamic-channel-assignment-in-cellular-radio-Sivarajan-McEliece",
            "title": {
                "fragments": [],
                "text": "Dynamic channel assignment in cellular radio"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that in the case of homogeneous spatial traffic distribution, some of these algorithms are virtually unbeatable by any channel assignment algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "40th IEEE Conference on Vehicular Technology"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38193578,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2b0fdeb9f6b0297afc8b071ee83004e1259cd91f",
            "isKey": false,
            "numCitedBy": 2011,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book presents, characterizes and analyzes problem solving strategies that are guided by heuristic information"
            },
            "slug": "Heuristics-intelligent-search-strategies-for-Pearl",
            "title": {
                "fragments": [],
                "text": "Heuristics - intelligent search strategies for computer problem solving"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This book presents, characterizes and analyzes problem solving strategies that are guided by heuristic information and provides examples of how these strategies have changed over time."
            },
            "venue": {
                "fragments": [],
                "text": "Addison-Wesley series in artificial intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727680"
                        ],
                        "name": "A. Biermann",
                        "slug": "A.-Biermann",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Biermann",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Biermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1514043144"
                        ],
                        "name": "John R. Fairfield",
                        "slug": "John-R.-Fairfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Fairfield",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John R. Fairfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395353470"
                        ],
                        "name": "Thomas R. Beres",
                        "slug": "Thomas-R.-Beres",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Beres",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas R. Beres"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206788517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea42c0d1bd38db3da1e2b7276c8e215b23493895",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A characterization theorem is given for the classes of functions which are representable by signature table systems. The usefulness of the theorem is demonstrated in the analysis and synthesis of such systems. The limitations on the power of these systems come from the restrictions on the table alphabet sizes, and a technique is given for evaluating these limitations. A practical learning system is proposed and analyzed in terms of the theoretical model of this paper. Then an improved method is described and results are presented from a series of experiments."
            },
            "slug": "Signature-Table-Systems-and-Learning-Biermann-Fairfield",
            "title": {
                "fragments": [],
                "text": "Signature Table Systems and Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "A characterization theorem is given for the classes of functions which are representable by signature table systems and the usefulness of the theorem is demonstrated in the analysis and synthesis of such systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36485660"
                        ],
                        "name": "J. Adler",
                        "slug": "J.-Adler",
                        "structuredName": {
                            "firstName": "Julius",
                            "lastName": "Adler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Adler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 179
                            }
                        ],
                        "text": "The experiments mentioned that involve optogenetic activation of dopamine neurons were conducted by Tsai, Zhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg, Keiflin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge-Chang, Roorda, Vrontou, Sjulson, Li, Hirsh, and Miesenb\u00f6ck (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 107
                            }
                        ],
                        "text": "Models of classical conditioning other than Rescorla and Wagner\u2019s include the models of Klopf (1988), Grossberg (1975), Mackintosh (1975), Moore and Stickney (1980), Pearce and Hall (1980), and Courville, Daw, and Touretzky (2006). Schmajuk (2008) reviews models of classical conditioning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "Models of classical conditioning other than Rescorla and Wagner\u2019s include the models of Klopf (1988), Grossberg (1975), Mackintosh (1975), Moore and Stickney (1980), Pearce and Hall (1980), and Courville, Daw, and Touretzky (2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 179
                            }
                        ],
                        "text": "The experiments mentioned that involve optogenetic activation of dopamine neurons were conducted by Tsai, Zhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg, Keiflin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge-Chang, Roorda, Vrontou, Sjulson, Li, Hirsh, and Miesenb\u00f6ck (2009). Fiorillo, Yun, and Song (2013), Lammel, Lim, and Malenka (2014), and Saddoris, Cacciapaglia, Wightmman, and Carelli (2015) are among studies showing that the signaling properties of dopamine neurons are specialized for different target regions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 25
                            }
                        ],
                        "text": "In another example, Steinberg et al. (2013) used optogenetic activation of dopamine neurons to create artificial bursts of dopamine neuron activity in rats at the times when rewarding stimuli were expected but omitted\u2014times when dopamine neuron activity normally pauses."
                    },
                    "intents": []
                }
            ],
            "corpusId": 24915550,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "508dd432a33cb7c3d02d8185b6741c139efa0b60",
            "isKey": true,
            "numCitedBy": 1087,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Motile Escherichia coli placed at one end of a capillary tube containing an energy source and oxygen migrate out into the tube in one or two bands, which are clearly visible to the naked eye and can also be demonstrated by photography, microscopy, and densitometry and by assaying for bacteria throughout the tube. The formation of two bands is not due to heterogeneity among the bacteria, since the bacteria in each band, when reused, will form two more bands. If an anaerobically utilizable energy source such as galactose is present in excess over the oxygen, the first band consumes all the oxygen and a part of the sugar and the second band uses the residual sugar anaerobically. On the other hand, if oxygen is present in excess over the sugar, the first band oxidizes all the sugar and leaves behind unused oxygen, and the second band uses up the residual oxygen to oxidize an endogenous energy source. The essence of the matter is that the bacteria create a gradient of oxygen or of an energy source, and then they move preferentially in the direction of the higher concentration of the chemical. As a consequence, bands of bacteria (or rings of bacteria in the case of agar plates) form and move out. These results show that E. coli is chemotactic toward oxygen and energy sources such as galactose, glucose, aspartic acid, threonine, or serine. The full repertoire of chemotactic responses by E. coli is no doubt greater than this, and a more complete list remains to be compiled. The studies reported here demonstrate that chemotaxis allows bacteria to find that environment which provides them with the greatest supply of energy. It is clearly an advantage for bacteria to be able to carry out chemotaxis, since by this means they can avoid unfavorable conditions and seek optimum surroundings. Finally, it is necessary to acknowledge the pioneering work of Englemann, Pfeffer, and the other late-19thcentury biologists who discovered chemotaxis in bacteria, and to point out that the studies reported here fully confirm the earlier reports of Beijerinck (4) and Sherris and his collaborators (5,6) on a band of bacteria chemotactic toward oxygen. By using a chemically defined medium instead of a complex broth, it has been possible to study this band more closely and to demonstrate in addition the occurrence of a second band of bacteria chemotactic toward an energy source. Beijerinck (4) did, in fact, sometimes observe a second band, but he did not offer an explanation for it."
            },
            "slug": "Chemotaxis-in-Bacteria-Adler",
            "title": {
                "fragments": [],
                "text": "Chemotaxis in Bacteria"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "These results show that E. coli is chemotactic toward oxygen and energy sources such as galactose, glucose, aspartic acid, threonine, or serine and that chemotaxis allows bacteria to find that environment which provides them with the greatest supply of energy."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9380569"
                        ],
                        "name": "G. Klir",
                        "slug": "G.-Klir",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Klir",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Klir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121090074,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "283471d3a4d1dcc02c9ceea3f1bc674fe573e5f4",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The aim of the paper is to challenge the claims (as described by Lindley38). \u201cthat probability is the only sensible description of uncertainty and is adequate for all problems involving uncertainty. All other methods are inadequate\u201d. The paper concentrates primarily on the codification of the concept of uncertainty and the discussion of adequate principles of maximum and minimum uncertainty."
            },
            "slug": "IS-THERE-MORE-TO-UNCERTAINTY-THAN-SOME-PROBABILITY-Klir",
            "title": {
                "fragments": [],
                "text": "IS THERE MORE TO UNCERTAINTY THAN SOME PROBABILITY THEORISTS MIGHT HAVE US BELIEVE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805158"
                        ],
                        "name": "S. Rixner",
                        "slug": "S.-Rixner",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Rixner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rixner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 138
                            }
                        ],
                        "text": "FR-FCFS was shown to outperform other scheduling policies in terms of average memory-access latency under conditions commonly encountered (Rixner, 2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 635945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "548aaa1aafdf25ad10b7e8ea9dd4904803502979",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes memory access scheduling and virtual channels as mechanisms to reduce the latency of main memory accesses by the CPU and peripherals in web servers. Despite the address filtering effects of the CPU's cache hierarchy, there is significant locality and bank parallelism in the DRAM access stream of a web server, which includes traffic from the operating system, application, and peripherals. However, a sequential memory controller leaves much of this locality and parallelism unexploited, as serialization and bank conflicts affect the realizable latency. Aggressive scheduling within the memory controller to exploit the available parallelism and locality can reduce the average read latency of the SDRAM. However, bank conflicts and the limited ability of the SDRAM's internal row buffers to act as a cache hinder further latency reduction. Virtual channel SDRAMovercomes these limitations by providing a set of channel buffers that can hold segments from rows of any internal SDRAM bank. This paper presents memory controller policies that can make effective use of these channel buffers to further reduce the average read latency of the SDRAM."
            },
            "slug": "Memory-Controller-Optimizations-for-Web-Servers-Rixner",
            "title": {
                "fragments": [],
                "text": "Memory Controller Optimizations for Web Servers"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Memory access scheduling and virtual channels are analyzed as mechanisms to reduce the latency of main memory accesses by the CPU and peripherals in web servers and policies are presented that can make effective use of these channel buffers to further reduce the average read latency of the SDRAM."
            },
            "venue": {
                "fragments": [],
                "text": "37th International Symposium on Microarchitecture (MICRO-37'04)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145478723"
                        ],
                        "name": "R. Luce",
                        "slug": "R.-Luce",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Luce",
                            "middleNames": [
                                "Duncan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Luce"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 145
                            }
                        ],
                        "text": "\u2019 Like the presence representation, the CSC representation is unrealistic as a hypothesis about how the brain internally represents stimuli, but Ludvig et al. (2012) call it a \u201cuseful fiction\u201d because it can reveal details of how the TD model works when relatively unconstrained by the stimulus representation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Mackintosh (1983) proposed using the term reinforcement to refer to either strengthening or weakening a pattern of behavior."
                    },
                    "intents": []
                }
            ],
            "corpusId": 155930383,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "6fd04fb2f6408868a7103b78d4c464478af5a9be",
            "isKey": false,
            "numCitedBy": 2448,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Individual-Choice-Behavior-Luce",
            "title": {
                "fragments": [],
                "text": "Individual Choice Behavior"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928927"
                        ],
                        "name": "C. Stanfill",
                        "slug": "C.-Stanfill",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Stanfill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stanfill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16624499,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "123a726f6feb2bce29708b68ab2db5cdf9fcdaf4",
            "isKey": false,
            "numCitedBy": 1436,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "slug": "Toward-memory-based-reasoning-Stanfill-Waltz",
            "title": {
                "fragments": [],
                "text": "Toward memory-based reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49140741"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641506"
                        ],
                        "name": "P. Vit\u00e1nyi",
                        "slug": "P.-Vit\u00e1nyi",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Vit\u00e1nyi",
                            "middleNames": [
                                "M.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Vit\u00e1nyi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 35
                            }
                        ],
                        "text": "3 The term coarse coding is due to Hinton (1984), and our Figure 9."
                    },
                    "intents": []
                }
            ],
            "corpusId": 63405852,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "063d43c5a12a3a4ecfa205b826f5799c5454c550",
            "isKey": false,
            "numCitedBy": 454,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The doctoral program in Educational Policy and Leadership is designed to foster the development of scholar-practitioners. It asks students not only to inquire deeply into the process of teaching and learning, but also how the organization of schools shapes the process. In addition, the program asks students to acquire adjacent disciplinary strengths that provide contexts for considering what knowledge is of most worth, how forms of knowledge are socially distributed, and what educational measures might help bring about a more just society. Students are expected to gain expertise in research that will enable them to contribute to the ways we think about education and to develop technological and other practical skills that will enable them to implement strategies for change."
            },
            "slug": "Theories-of-learning-Li-Vit\u00e1nyi",
            "title": {
                "fragments": [],
                "text": "Theories of learning"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The doctoral program in Educational Policy and Leadership is designed to foster the development of scholar-practitioners by asking students not only to inquire deeply into the process of teaching and learning, but also how the organization of schools shapes the process."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262347"
                        ],
                        "name": "A. Turing",
                        "slug": "A.-Turing",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Turing",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Turing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5213112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "650d4e7c4fbd68b05f87a46985f1da6f2f4f0ed8",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 193,
            "paperAbstract": {
                "fragments": [],
                "text": "The analysis of interrupts is a structured quagmire. In this position paper, we demonstrate the investigation of simulated annealing. KaliNil, our new methodology for DNS, is the solution to all of these problems."
            },
            "slug": "Intelligent-Machinery,-A-Heretical-Theory*-Turing",
            "title": {
                "fragments": [],
                "text": "Intelligent Machinery, A Heretical Theory*"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "KaliNil, the new methodology for DNS, is the solution to all of the problems of simulated annealing, and is demonstrated in this position paper."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818025"
                        ],
                        "name": "T. Lai",
                        "slug": "T.-Lai",
                        "structuredName": {
                            "firstName": "Tze",
                            "lastName": "Lai",
                            "middleNames": [
                                "Leung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120034176,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "52f72a347c92ad10079efc89f6f537affe9d2341",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "On propose une classe de regles d'allocation adaptative simples qui sont basees sur certaines limites de confiance superieures developpees a partir de la theorie du passage a la frontiere"
            },
            "slug": "Adaptive-treatment-allocation-and-the-multi-armed-Lai",
            "title": {
                "fragments": [],
                "text": "Adaptive treatment allocation and the multi-armed bandit problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1234937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e08d090d1e586610d636a46004876e9f3ded8209",
            "isKey": false,
            "numCitedBy": 640,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-time-delay-neural-network-architecture-for-word-Lang-Waibel",
            "title": {
                "fragments": [],
                "text": "A time-delay neural network architecture for isolated word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50568602"
                        ],
                        "name": "J. Stevens",
                        "slug": "J.-Stevens",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Stevens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stevens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1394313360"
                        ],
                        "name": "S. Gostage",
                        "slug": "S.-Gostage",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Gostage",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gostage"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4037230,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "7daaf2a9fbb52db57f1e79bfeae213baf92df108",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "As NATURE frequently contains notices of intelligence in animals, I have ventured to send you the inclosed note from the Reading local paper, as containing a remarkable fact regarding intelligence in a blind horse. The writer, Mr. Gostage, is quite trustworthy, and I have taken pains to verify the truth of his statements."
            },
            "slug": "Animal-Intelligence-Stevens-Gostage",
            "title": {
                "fragments": [],
                "text": "Animal Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1883
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2171948744"
                        ],
                        "name": "M. Zhang",
                        "slug": "M.-Zhang",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219733"
                        ],
                        "name": "T. Yum",
                        "slug": "T.-Yum",
                        "structuredName": {
                            "firstName": "Tak-Shing",
                            "lastName": "Yum",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62027438,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "c04308d15500b69a9e67bb1ae7ccf67fa19c2d87",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Two new channel assignment strategies are proposed. They are the locally optimized dynamic assignment (LODA) strategy and the borrowing with directional channel locking (BDCL) strategy. Their performance is compared with the fixed assignment (FA) strategy (currently used on certain systems) and the borrowing with channel ordering (BCO) strategy (the stability that has given the lowest blocking probability in previous research). Computer simulations on a 49-cell network for both uniform and nonuniform traffic show that the average call-blocking probability of the BDCL strategy is always the lowest. The LODA performance is comparable with BCO under nonuniform traffic conditions but is inferior under uniform traffic conditions.<<ETX>>"
            },
            "slug": "Comparisons-of-channel-assignment-strategies-in-Zhang-Yum",
            "title": {
                "fragments": [],
                "text": "Comparisons of channel assignment strategies in cellular mobile telephone systems"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The locally optimized dynamic assignment (LODA) strategy and the borrowing with directional channel locking (BDCL) strategy are proposed and computer simulations show that the average call-blocking probability of the BDCL strategy is always the lowest."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Communications, World Prosperity Through Communications,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144054003"
                        ],
                        "name": "Martin M\u00fcller",
                        "slug": "Martin-M\u00fcller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15132510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49ca0eda8e224507d341d16a8c3fdb4d566cefe3",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Go-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Computer Go"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 66
                            }
                        ],
                        "text": "Details of this and related algorithms are provided in many texts (e.g., Widrow and Stearns, 1985; Bishop, 1995; Duda and Hart, 1973)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057376208"
                        ],
                        "name": "Thomas Ross",
                        "slug": "Thomas-Ross",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Ross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27337846,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "093f53afc2f012ea6146194bfc9193e296998149",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We may not be able to make you love reading, but machines who think will lead you to love reading starting from now. Book is the window to open the new world. The world that you want is in the better stage and level. World will always guide you to even the prestige stage of the life. You know, this is some of how reading will give you the kindness. In this case, more books you read more knowledge you know, but it can mean also the bore is full."
            },
            "slug": "Machines-who-think.-Ross",
            "title": {
                "fragments": [],
                "text": "Machines who think."
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The authors may not be able to make you love reading, but machines who think will lead you to love reading starting from now, because book is the window to open the new world."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696575"
                        ],
                        "name": "J. Bentley",
                        "slug": "J.-Bentley",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Bentley",
                            "middleNames": [
                                "Louis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bentley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236311"
                        ],
                        "name": "R. Finkel",
                        "slug": "R.-Finkel",
                        "structuredName": {
                            "firstName": "Raphael",
                            "lastName": "Finkel",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Finkel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10811510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cab3c73f1b2140231b98944c720100b356d91b28",
            "isKey": false,
            "numCitedBy": 2965,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record. The computation required to organize the file is proportional to kNlogN. The expected number of records examined in each search is independent of the file size. The expected computation to perform each search is proportional to logN. Empirical evidence suggests that except for very small files, this algorithm is considerably faster than other methods."
            },
            "slug": "An-Algorithm-for-Finding-Best-Matches-in-Expected-Friedman-Bentley",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Finding Best Matches in Logarithmic Expected Time"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49074950"
                        ],
                        "name": "L. Kamin",
                        "slug": "L.-Kamin",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Kamin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kamin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 218
                            }
                        ],
                        "text": "3 The idea built into the Rescorla-Wagner model that learning occurs when animals are surprised is derived from Kamin (1969), who first reported blocking\u2014 now commonly known as Kamin blocking\u2014in classical conditioning (Kamin, 1968)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 112
                            }
                        ],
                        "text": "3 The idea built into the Rescorla-Wagner model that learning occurs when animals are surprised is derived from Kamin (1969), who first reported blocking\u2014 now commonly known as Kamin blocking\u2014in classical conditioning (Kamin, 1968). Moore and Schmajuk (2008) provide an excellent summary of the blocking phenomenon, the research it stimulated, and its lasting influence on"
                    },
                    "intents": []
                }
            ],
            "corpusId": 140471913,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "de02c70882108516629bb867ba186fab17ef8350",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditioned emotional response studies using rats already trained to press bar for food supply"
            },
            "slug": "Attention-like-processes-in-classical-conditioning-Kamin",
            "title": {
                "fragments": [],
                "text": "Attention-like processes in classical conditioning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15282189"
                        ],
                        "name": "J. Daniel",
                        "slug": "J.-Daniel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Daniel",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daniel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ") There is now a fairly extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121201738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d20ef4c1d8a10b8ad570c2290923a1dd8d0f8425",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Splines-and-efficiency-in-dynamic-programming-Daniel",
            "title": {
                "fragments": [],
                "text": "Splines and efficiency in dynamic programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699388"
                        ],
                        "name": "L. Ljung",
                        "slug": "L.-Ljung",
                        "structuredName": {
                            "firstName": "Lennart",
                            "lastName": "Ljung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ljung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15487656"
                        ],
                        "name": "T. S\u00f6derstr\u00f6m",
                        "slug": "T.-S\u00f6derstr\u00f6m",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "S\u00f6derstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. S\u00f6derstr\u00f6m"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 90
                            }
                        ],
                        "text": "The term system identification is used in adaptive control for what we call modellearning (e.g., Goodwin and Sin, 1984; Ljung and S\u00f6derstrom, 1983; Young, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60920727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9df0be142d96fb454d6d863a76dc9e0448cc8428",
            "isKey": false,
            "numCitedBy": 2418,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods of recursive identification deal with the problem of building mathematical models of signals and systems on-line, at the same time as data is being collected. Such methods, which are also k ..."
            },
            "slug": "Theory-and-Practice-of-Recursive-Identification-Ljung-S\u00f6derstr\u00f6m",
            "title": {
                "fragments": [],
                "text": "Theory and Practice of Recursive Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Methods of recursive identification deal with the problem of building mathematical models of signals and systems on-line, at the same time as data is being collected."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145119134"
                        ],
                        "name": "R. Varga",
                        "slug": "R.-Varga",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Varga",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Varga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144272344"
                        ],
                        "name": "J. Gillis",
                        "slug": "J.-Gillis",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Gillis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gillis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 123318011,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e726f00657c2d82e5ff0ef011910f2d197c4b010",
            "isKey": false,
            "numCitedBy": 6010,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Matrix Properties and Concepts.- Nonnegative Matrices.- Basic Iterative Methods and Comparison Theorems.- Successive Overrelaxation Iterative Methods.- Semi-Iterative Methods.- Derivation and Solution of Elliptic Difference Equations.- Alternating-Direction Implicit Iterative Methods.- Matrix Methods for Parabolic Partial Differential Equations.- Estimation of Acceleration Parameters."
            },
            "slug": "Matrix-Iterative-Analysis-Varga-Gillis",
            "title": {
                "fragments": [],
                "text": "Matrix Iterative Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711777"
                        ],
                        "name": "C. Breazeal",
                        "slug": "C.-Breazeal",
                        "structuredName": {
                            "firstName": "Cynthia",
                            "lastName": "Breazeal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Breazeal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63144895,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "98155159aa2e0632c9592bf36faca9dec800becd",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Infant-Caregiver Interaction, Lessons from Ethology, Organization of Kismet's Behavior System, Kismet's Proto-Social Responses, Overview of the Motor Systems, Playful Interactions with Kismet, Summary"
            },
            "slug": "The-Behavior-System-Breazeal",
            "title": {
                "fragments": [],
                "text": "The Behavior System"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 40
                            }
                        ],
                        "text": "Prioritized sweeping was developed simultaneously and independently by Moore and Atkeson (1993) and Peng and Williams (1993). The results in Figure 9."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 40
                            }
                        ],
                        "text": "Prioritized sweeping was developed simultaneously and independently by Moore and Atkeson (1993) and Peng and Williams (1993). The results in Figure 9.10 are due to Peng and Williams (1993). The results in Figure 9."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 435,
                                "start": 49
                            }
                        ],
                        "text": "The theory of MDPs evolved from efforts to understand the problem of making sequences of decisions under uncertainty, where each decision can depend on the previous decisions and their outcomes. It is sometimes called the theory of multi-stage decision processes, or sequential decision processes, and has roots in the statistical literature on sequential sampling beginning with the papers by Thompson (1933, 1934) and Robbins (1952) that we cited in Chapter 2 in connection with bandit problems (which are prototypical MDPs if formulated as multiple-situation problems)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 19
                            }
                        ],
                        "text": "82 The earliest instance of which we are aware in which reinforcement learning was discussed using the MDP formalism is Andreae's (1969b) description of a unified view of learning machines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 135
                            }
                        ],
                        "text": "Feldbaum (1965) called it the dual control problem, referring to the need to solve the two problems of identification and control simultaneously when trying to control a system under uncertainty. In discussing aspects of genetic algorithms, Holland (1975) emphasized the importance of this conflict, referring to it as the conflict between exploitation and new information."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 19
                            }
                        ],
                        "text": "82 The earliest instance of which we are aware in which reinforcement learning was discussed using the MDP formalism is Andreae's (1969b) description of a unified view of learning machines. Witten and Corbin (1973) experimented with a reinforcement learning system later analyzed by Witten (1977) using the MDP formalism."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 882,
                                "start": 19
                            }
                        ],
                        "text": "82 The earliest instance of which we are aware in which reinforcement learning was discussed using the MDP formalism is Andreae's (1969b) description of a unified view of learning machines. Witten and Corbin (1973) experimented with a reinforcement learning system later analyzed by Witten (1977) using the MDP formalism. Although he did not explicitly mention MDPs, Werbos (1977) suggested approximate solution methods for stochastic optimal control problems that are related to modern reinforcement learning methods (see also Werbos, 1982, 1987, 1988, 1989,1992). Although Werbos' ideas were not widely recognized at the time, they were prescient in emphasizing the importance of approximately solving optimal control problems in a variety of domains, including artificial intelligence. The most influential integration of reinforcement learning and MDPs is due to Watkins (1989). His treatment of reinforcement learning using the MDP formalism has been widely adopted."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 40
                            }
                        ],
                        "text": "Prioritized sweeping was developed simultaneously and independently by Moore and Atkeson (1993) and Peng and Williams (1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 381,
                                "start": 19
                            }
                        ],
                        "text": "82 The earliest instance of which we are aware in which reinforcement learning was discussed using the MDP formalism is Andreae's (1969b) description of a unified view of learning machines. Witten and Corbin (1973) experimented with a reinforcement learning system later analyzed by Witten (1977) using the MDP formalism. Although he did not explicitly mention MDPs, Werbos (1977) suggested approximate solution methods for stochastic optimal control problems that are related to modern reinforcement learning methods (see also Werbos, 1982, 1987, 1988, 1989,1992)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-agent reinforcement learning: Independent vs"
            },
            "venue": {
                "fragments": [],
                "text": "cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, pages 330--337. Morgan Kaufmann."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 136
                            }
                        ],
                        "text": "We follow researchers in the computational and engineering communities in using the phrase reinforcement learning, influenced mostly by Minsky\u2019s (1961) use of the phrase in his computational studies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 13
                            }
                        ],
                        "text": "dissertation (Minsky, 1954), where he called the synapse-like learning element a SNARC (Stochastic Neural-Analog Reinforcement Calculator)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 28
                            }
                        ],
                        "text": "Samuel made no reference to Minsky\u2019s work or to possible connections to animal learning. His inspiration apparently came from Claude Shannon\u2019s (1950) suggestion"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 13
                            }
                        ],
                        "text": "dissertation (Minsky, 1954), Marvin Minsky discussed computational models of reinforcement learning and described his construction of an analog machine composed of components he called SNARCs (Stochastic Neural-Analog Reinforcement Calculators) meant to resemble modifiable synaptic connections in the brain (Chapter 15) The fascinating web site cyberneticzoo."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain-Model Problem"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. thesis, Princeton University."
            },
            "year": 1954
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "An idea developed by computational neuroscientists Daw, Niv, and Dayan (2005) is that animals use both model-free and model-based processes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "Tabular TD(0) was proved to converge in the mean by Sutton (1988) and with probability 1 by Dayan (1992), based on the work of Watkins and Dayan (1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 92
                            }
                        ],
                        "text": "Tabular TD(0) was proved to converge in the mean by Sutton (1988) and with probability 1 by Dayan (1992), based on the work of Watkins and Dayan (1992). These results were extended and strengthened by Jaakkola, Jordan, and Singh (1994) and Tsitsiklis (1994) by using extensions of the powerful existing theory of stochastic approximation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 117
                            }
                        ],
                        "text": "5 Q-learning was introduced by Watkins (1989), whose outline of a convergence proof was made rigorous by Watkins and Dayan (1992). More general"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 627,
                                "start": 101
                            }
                        ],
                        "text": "Convergence with probability 1 was proved by several researchers at about the same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis, 1994; Gurvits, Lin, and Hanson, 1994). In addition, Jaakkola, Jordan, and Singh (1994) proved convergence under on-line updating. All of these results assumed linearly independent feature vectors, which implies at least as many components to \u03b8t as there are states. Convergence for the more important case of general (dependent) feature vectors was first shown by Dayan (1992). A significant generalization and strengthening of Dayan\u2019s result was proved by Tsitsiklis and Van Roy (1997). They proved the main result presented in this section, the bound on the asymptotic error of linear bootstrapping methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 517,
                                "start": 101
                            }
                        ],
                        "text": "Convergence with probability 1 was proved by several researchers at about the same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis, 1994; Gurvits, Lin, and Hanson, 1994). In addition, Jaakkola, Jordan, and Singh (1994) proved convergence under on-line updating. All of these results assumed linearly independent feature vectors, which implies at least as many components to \u03b8t as there are states. Convergence for the more important case of general (dependent) feature vectors was first shown by Dayan (1992). A significant generalization and strengthening of Dayan\u2019s result was proved by Tsitsiklis and Van Roy (1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 91
                            }
                        ],
                        "text": "This hypothesis (though not in these exact words) was first explicitly stated by Montague, Dayan, and Sejnowski (1996), who showed how the TD error concept from reinforcement learning accounts for many features of the phasic activity of dopamine neurons in the brains of mammals."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 101
                            }
                        ],
                        "text": "Convergence with probability 1 was proved by several researchers at about the same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis, 1994; Gurvits, Lin, and Hanson, 1994). In addition, Jaakkola, Jordan, and Singh (1994) proved convergence under on-line updating."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement comparison"
            },
            "venue": {
                "fragments": [],
                "text": "D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E. Hinton (eds.), Connectionist Models: Proceedings of the 1990 Summer School, pp. 45\u201351. Morgan Kaufmann, San Mateo, CA."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110627749"
                        ],
                        "name": "Robert Miller",
                        "slug": "Robert-Miller",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 380,
                                "start": 159
                            }
                        ],
                        "text": "Extensive studies have been made of the effect of different displacement vectors on the generalization of tile coding (Parks and Militzer, 1991; An, 1991; An, Miller and Parks, 1991; Miller, Glanz and Carter, 1991), assessing their homegeneity and tendency toward diagonal artifacts like those seen for the (1, 1) displacement vectors. Based on this work, Miller and Glanz (1996) recommend using displacement vectors consisting of the first odd integers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 34
                            }
                        ],
                        "text": "In another farsighted hypothesis, Miller (1981) proposed a Law-of-Effect-like learning rule that includes synaptically-local contingent eligibility traces:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 143
                            }
                        ],
                        "text": "A well-known demonstration of the effects on conditioning of temporal relationships among stimuli within a trial is an experiment by Egger and Miller (1962) that involved two overlapping CSs in a delay configuration as shown in the top panel of Figure 14."
                    },
                    "intents": []
                }
            ],
            "corpusId": 142063235,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5e2749bedef00c053e9f793554ba5938ed9e1185",
            "isKey": true,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Meaning-and-Purpose-in-the-Intact-Brain:-A-and-of-Miller",
            "title": {
                "fragments": [],
                "text": "Meaning and Purpose in the Intact Brain: A Philosophical, Psychological, and Biological Account of Conscious Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49809354"
                        ],
                        "name": "E. Fischer",
                        "slug": "E.-Fischer",
                        "structuredName": {
                            "firstName": "Ernst",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fischer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Pawlak and Kerr (2008) showed that dopamine is necessary to induce STDP at the corticostriatal synapses of medium spiny neurons."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 0
                            }
                        ],
                        "text": "Pawlak and Kerr (2008) showed that dopamine is necessary to induce STDP at the corticostriatal synapses of medium spiny neurons. See also Pawlak, Wickens, Kirkwood, and Kerr (2010). Yagishita, HayashiTakagi, Ellis-Davies, Urakubo, Ishii, and Kasai (2014) found that dopamine promotes spine enlargement in mice medium spiny neurons only during a time window of from 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 158
                            }
                        ],
                        "text": "1 To the best of our knowledge, the term reinforcement in the context of animal learning first appeared in the 1927 English translation of Pavlov\u2019s monograph (Pavlov, 1927), where it was used in regard to the non-action-contingent case."
                    },
                    "intents": []
                }
            ],
            "corpusId": 36661596,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "61fb6eaac8fd1b65142a45d89b3b7ed6f27e204f",
            "isKey": true,
            "numCitedBy": 1559,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Conditioned-Reflexes-Fischer",
            "title": {
                "fragments": [],
                "text": "Conditioned Reflexes"
            },
            "venue": {
                "fragments": [],
                "text": "American journal of physical medicine"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 105
                            }
                        ],
                        "text": "Some of the earliest uses of eligibility traces were in the actor\u2013critic methods discussed in Chapter 13 (Barto, Sutton, and Anderson, 1983; Sutton, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 96
                            }
                        ],
                        "text": "It influenced much later work in reinforcement learning, beginning with some of our own studies (Barto, Sutton, and Anderson, 1983; Sutton, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 130
                            }
                        ],
                        "text": "Methods that we now see as related to policy gradients were actually some of the earliest to be studied in reinforcement learning (Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984; Williams, 1987, 1992) and in predecessor fields (Phansalkar and Thathachar, 1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 55
                            }
                        ],
                        "text": "Our use of eligibility traces is based on Klopf\u2019s work (Sutton, 1978a, 1978b, 1978c; Barto and Sutton, 1981a, 1981b; Sutton and Barto, 1981a; Barto, Sutton, and Anderson, 1983; Sutton, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 94
                            }
                        ],
                        "text": "5\u20136 Actor\u2013critic methods were among the earliest to be investigated in reinforcement learning (Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60564875,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "22069cd4504656d3bb85748a4d43be7a4d7d5545",
            "isKey": false,
            "numCitedBy": 862,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-credit-assignment-in-reinforcement-Sutton",
            "title": {
                "fragments": [],
                "text": "Temporal credit assignment in reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "147865273"
                        ],
                        "name": "Quartz",
                        "slug": "Quartz",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Quartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67024781"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Sejnowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 229634021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83ace3ecedcfab917e2af4bda3bbbe198ceacef6",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Expectation-learning-in-the-brain-using-diffuse-Quartz-Dayan",
            "title": {
                "fragments": [],
                "text": "Expectation learning in the brain using diffuse ascending projections"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145708111"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59809750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c8bb027eb65b6d250a22e9b6db22853a552ac81",
            "isKey": false,
            "numCitedBy": 2946,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-from-delayed-rewards-Watkins",
            "title": {
                "fragments": [],
                "text": "Learning from delayed rewards"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093637206"
                        ],
                        "name": "Nancy Forbes",
                        "slug": "Nancy-Forbes",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Forbes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nancy Forbes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 57
                            }
                        ],
                        "text": "Grey Walter, already known for his \u201cmechanical tortoise\u201d (Walter, 1950), built a version capable of a simple form of learning (Walter, 1951)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 64785328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a31134b294ab0c6a42e9ad5c5d7190497027aef",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Imitation-of-Life-Forbes",
            "title": {
                "fragments": [],
                "text": "Imitation of Life"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083088273"
                        ],
                        "name": "E. C. O. N. Ometrica",
                        "slug": "E.-C.-O.-N.-Ometrica",
                        "structuredName": {
                            "firstName": "E",
                            "lastName": "Ometrica",
                            "middleNames": [
                                "C",
                                "O",
                                "N"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. C. O. N. Ometrica"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207912280,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "05abdc87bcaf2963fd511672e64ab39d02239aaf",
            "isKey": false,
            "numCitedBy": 25202,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Prospect-Theory-:-An-Analysis-of-Decision-under-Ometrica",
            "title": {
                "fragments": [],
                "text": "Prospect Theory : An Analysis of Decision under Risk"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707626"
                        ],
                        "name": "N. Wiener",
                        "slug": "N.-Wiener",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Wiener",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiener"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 181
                            }
                        ],
                        "text": "In the engineering context, Norbert Wiener, the founder of cybernetics, warned of this problem more than half a century ago by relating the supernatural story of \u201cThe Monkey\u2019s Paw\u201d (Wiener, 1964): \u201c."
                    },
                    "intents": []
                }
            ],
            "corpusId": 170276881,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "5673375315295948b44c8f4dde496609819a97af",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "God-and-Golem,-inc.-:-a-comment-on-certain-points-Wiener",
            "title": {
                "fragments": [],
                "text": "God and Golem, inc. : a comment on certain points where cybernetics impinges on religion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33258940"
                        ],
                        "name": "G. Finch",
                        "slug": "G.-Finch",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Finch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Finch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4288545"
                        ],
                        "name": "E. Culler",
                        "slug": "E.-Culler",
                        "structuredName": {
                            "firstName": "Elmer",
                            "lastName": "Culler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Culler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 147446689,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9c3014a83c52355a85323bb5cad4479c362c7df1",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Higher-Order-Conditioning-with-Constant-Motivation-Finch-Culler",
            "title": {
                "fragments": [],
                "text": "Higher Order Conditioning with Constant Motivation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1934
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 145488447,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a892210e032446b9da81177cd2d18b89a7b58f77",
            "isKey": false,
            "numCitedBy": 618,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Time-Derivative-Models-of-Pavlovian-Reinforcement-Sutton-Barto",
            "title": {
                "fragments": [],
                "text": "Time-Derivative Models of Pavlovian Reinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40358971"
                        ],
                        "name": "E. Tolman",
                        "slug": "E.-Tolman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Tolman",
                            "middleNames": [
                                "Chace"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tolman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 86
                            }
                        ],
                        "text": "The authors were also strongly influenced by psychological studies of latent learning (Tolman, 1932) and by psychological views of the nature of thought (e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 145706818,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "37ef75da654b92a5f31f854fa565d2f2ff0dfa79",
            "isKey": false,
            "numCitedBy": 1827,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Purposive-behavior-in-animals-and-men-Tolman",
            "title": {
                "fragments": [],
                "text": "Purposive behavior in animals and men"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1932
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46937836"
                        ],
                        "name": "W. T. Powers",
                        "slug": "W.-T.-Powers",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Powers",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. T. Powers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 143
                            }
                        ],
                        "text": "A view equivalent to ours, and perhaps more illuminating, is that the agent is actually controlling the input it receives from its environment (Powers, 1973)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 145556271,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "86606ccaf060de44ae8c9db77ea19583a935f941",
            "isKey": false,
            "numCitedBy": 1872,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Behavior,-the-control-of-perception-Powers",
            "title": {
                "fragments": [],
                "text": "Behavior, the control of perception"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 110
                            }
                        ],
                        "text": "Some neuroscience models developed at this time are well interpreted in terms of temporal-difference learning (Hawkins and Kandel, 1984; Byrne, Gingrich, and Baxter, 1990; Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986; Friston et al., 1994), although in most cases there was no historical connection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 222288103,
            "fieldsOfStudy": [],
            "id": "1a67aaabaa430d5b90c7b8a5e15ed6a87970c9f6",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Is there a cell-biological alphabet for simple forms of learning?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40255722"
                        ],
                        "name": "C. L. Hull",
                        "slug": "C.-L.-Hull",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Hull",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Hull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 78
                            }
                        ],
                        "text": "The eligibility traces used in the algorithms described in this book are like Hull\u2019s traces: they are decaying traces of past state visitations, or of past state-action pairs. Eligibility traces were introduced by Klopf (1972) in his neuronal theory in which"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 100
                            }
                        ],
                        "text": "To account for goal gradients that extend over longer time periods than spanned by stimulus traces, Hull (1943) proposed that longer gradients result from secondary reinforcement passing backwards from the goal, a process acting in conjunction with his molar stimulus traces."
                    },
                    "intents": []
                }
            ],
            "corpusId": 143577821,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5a6f35e30548fdc0e9f3d7d95bedb96689378498",
            "isKey": false,
            "numCitedBy": 515,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-goal-gradient-hypothesis-and-maze-learning.-Hull",
            "title": {
                "fragments": [],
                "text": "The goal-gradient hypothesis and maze learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1932
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72852227"
                        ],
                        "name": "K. Spence",
                        "slug": "K.-Spence",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Spence",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Spence"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 149
                            }
                        ],
                        "text": "5 Spence, Hull\u2019s student and collaborator at Yale, elaborated the role of secondary reinforcement in addressing the problem of delayed reinforcement (Spence, 1947)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 143741538,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "93f8c9b8483e4915429e4f68eb4b2cceacb68d95",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-role-of-secondary-reinforcement-in-delayed-Spence",
            "title": {
                "fragments": [],
                "text": "The role of secondary reinforcement in delayed reward learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6072745"
                        ],
                        "name": "R. Malott",
                        "slug": "R.-Malott",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Malott",
                            "middleNames": [
                                "W"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Malott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 144149816,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fa2a5abb2cf23d5fba08831c42e6f6b1aa3f2b29",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-of-Behavior-Malott",
            "title": {
                "fragments": [],
                "text": "Principles of Behavior"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98883179"
                        ],
                        "name": "H. Blodgett",
                        "slug": "H.-Blodgett",
                        "structuredName": {
                            "firstName": "Hugh",
                            "lastName": "Blodgett",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Blodgett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 202
                            }
                        ],
                        "text": "It was concluded that \u201cduring the non-reward period, the rats [in the experimental group] were developing a latent learning of the maze which they were able to utilize as soon as reward was introduced\u201d (Blodgett, 1929)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 142869485,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "69bd34fafbe1cfc12caee013533cba39e15a0c35",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-effect-of-the-introduction-of-reward-upon-the-Blodgett",
            "title": {
                "fragments": [],
                "text": "The effect of the introduction of reward upon the maze performance of rats"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1929
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49868364"
                        ],
                        "name": "E. Hilgard",
                        "slug": "E.-Hilgard",
                        "structuredName": {
                            "firstName": "Ernest",
                            "lastName": "Hilgard",
                            "middleNames": [
                                "Ropiequet"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hilgard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38205524"
                        ],
                        "name": "D. Marquis",
                        "slug": "D.-Marquis",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Marquis",
                            "middleNames": [
                                "George."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marquis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5100014"
                        ],
                        "name": "G. Kimble",
                        "slug": "G.-Kimble",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Kimble",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimble"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142574297,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8db8ced348773ba0a3f6af6ec7ab6c1ac619f29b",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hilgard-and-Marquis'-Conditioning-and-learning-Hilgard-Marquis",
            "title": {
                "fragments": [],
                "text": "Hilgard and Marquis' Conditioning and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145215525"
                        ],
                        "name": "R. Romo",
                        "slug": "R.-Romo",
                        "structuredName": {
                            "firstName": "Ranulfo",
                            "lastName": "Romo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Romo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50337944"
                        ],
                        "name": "T. Ljungberg",
                        "slug": "T.-Ljungberg",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Ljungberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ljungberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46533676"
                        ],
                        "name": "J. Mirenowicz",
                        "slug": "J.-Mirenowicz",
                        "structuredName": {
                            "firstName": "Jacques",
                            "lastName": "Mirenowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mirenowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11622133"
                        ],
                        "name": "J. Hollerman",
                        "slug": "J.-Hollerman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Hollerman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hollerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83285099"
                        ],
                        "name": "A. Dickinson",
                        "slug": "A.-Dickinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Dickinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dickinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141275173,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f8271bc1cc4bb42793e45b8adb55fe81d839f0a9",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reward-related-signals-carried-by-dopamine-neurons.-Schultz-Romo",
            "title": {
                "fragments": [],
                "text": "Reward-related signals carried by dopamine neurons."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49074950"
                        ],
                        "name": "L. Kamin",
                        "slug": "L.-Kamin",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Kamin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kamin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 140529520,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "41cfd316679467086ecbe2408f7ed640790541b1",
            "isKey": false,
            "numCitedBy": 1610,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Predictability,-surprise,-attention,-and-Kamin",
            "title": {
                "fragments": [],
                "text": "Predictability, surprise, attention, and conditioning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144329674"
                        ],
                        "name": "J. Wickens",
                        "slug": "J.-Wickens",
                        "structuredName": {
                            "firstName": "Jeffery",
                            "lastName": "Wickens",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wickens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728176"
                        ],
                        "name": "R. K\u00f6tter",
                        "slug": "R.-K\u00f6tter",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "K\u00f6tter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. K\u00f6tter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 140327333,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f923120893ec4c2a1b40a84fe755550327893b58",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cellular-models-of-reinforcement.-Wickens-K\u00f6tter",
            "title": {
                "fragments": [],
                "text": "Cellular models of reinforcement."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143951726"
                        ],
                        "name": "R. Hersh",
                        "slug": "R.-Hersh",
                        "structuredName": {
                            "firstName": "Reuben",
                            "lastName": "Hersh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hersh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48878683"
                        ],
                        "name": "R. Griego",
                        "slug": "R.-Griego",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Griego",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Griego"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(see Hersh and Griego, 1969; Doyle and Snell, 1984). The racetrack exercise is adapted from Barto, Bradtke, and Singh (1995), and from Gardner (1973)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122613160,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7f221d751ee23eafb38dc662f1a182a210403b02",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Brownian-Motion-and-Potential-Theory-Hersh-Griego",
            "title": {
                "fragments": [],
                "text": "Brownian Motion and Potential Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729349"
                        ],
                        "name": "E. Denardo",
                        "slug": "E.-Denardo",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Denardo",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Denardo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 53
                            }
                        ],
                        "text": "Action-value functions also played a central role in Denardo\u2019s (1967) theoretical treatment of DP in terms of contraction mappings."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121356489,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0f5aa175e63d6fd3c0ddcbf30332b3da006a5477",
            "isKey": false,
            "numCitedBy": 444,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "CONTRACTION-MAPPINGS-IN-THE-THEORY-UNDERLYING-Denardo",
            "title": {
                "fragments": [],
                "text": "CONTRACTION MAPPINGS IN THE THEORY UNDERLYING DYNAMIC PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144301316"
                        ],
                        "name": "W. R. Thompson",
                        "slug": "W.-R.-Thompson",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Thompson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. R. Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120462794,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
            "isKey": false,
            "numCitedBy": 2337,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ON-THE-LIKELIHOOD-THAT-ONE-UNKNOWN-PROBABILITY-IN-Thompson",
            "title": {
                "fragments": [],
                "text": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850712"
                        ],
                        "name": "W. Walter",
                        "slug": "W.-Walter",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Walter",
                            "middleNames": [
                                "Grey"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Walter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 126
                            }
                        ],
                        "text": "Grey Walter, already known for his \u201cmechanical tortoise\u201d (Walter, 1950), built a version capable of a simple form of learning (Walter, 1951)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121689183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "885ea2165c9df635712b014093c7f4dd194c37d6",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Machine-that-Learns-Walter",
            "title": {
                "fragments": [],
                "text": "A Machine that Learns"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39828682"
                        ],
                        "name": "R. R. Bush",
                        "slug": "R.-R.-Bush",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bush",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. R. Bush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47976263"
                        ],
                        "name": "F. Mosteller",
                        "slug": "F.-Mosteller",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Mosteller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mosteller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 125223403,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "69a6f10d303cdf58f986dccc5677e9baf2d107a0",
            "isKey": false,
            "numCitedBy": 988,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Models-for-Learning-Bush-Mosteller",
            "title": {
                "fragments": [],
                "text": "Stochastic Models for Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118224933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c71ca26b183025b9f39f940f5e730f2c9a64e414",
            "isKey": false,
            "numCitedBy": 1425,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Radial-basis-functions-for-multivariable-a-review-Powell",
            "title": {
                "fragments": [],
                "text": "Radial basis functions for multivariable interpolation: a review"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728505"
                        ],
                        "name": "J. Mendel",
                        "slug": "J.-Mendel",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Mendel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mendel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117360894,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c6acd0291ad80a515766c2724357209ff9328270",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-artificial-intelligence-techniques-Mendel",
            "title": {
                "fragments": [],
                "text": "Applications of artificial intelligence techniques to a spacecraft control problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144108246"
                        ],
                        "name": "D. Ruppert",
                        "slug": "D.-Ruppert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ruppert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruppert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 70
                            }
                        ],
                        "text": "The notions of momentum (Derthick, 1984), of Polyak-Ruppert averaging (Polyak, 1990; Ruppert, 1988; Polyak and Juditsky, 1992), or further extensions of these ideas may significantly help."
                    },
                    "intents": []
                }
            ],
            "corpusId": 108279905,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2991f9bb677b71c33945e89ac0c7dcf7a36fa198",
            "isKey": false,
            "numCitedBy": 320,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-Estimations-from-a-Slowly-Convergent-Ruppert",
            "title": {
                "fragments": [],
                "text": "Efficient Estimations from a Slowly Convergent Robbins-Monro Process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397487938"
                        ],
                        "name": "J. D. E. Koshland",
                        "slug": "J.-D.-E.-Koshland",
                        "structuredName": {
                            "firstName": "Jr.",
                            "lastName": "Koshland",
                            "middleNames": [
                                "Daniel",
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. E. Koshland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 142
                            }
                        ],
                        "text": "Koshland\u2019s extensive study of bacterial chemotaxis was in part motivated by similarities between features of bacteria and features of neurons (Koshland, 1980)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 89659368,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "fe9bdf273ff004f4f36eed70200a0ff0fa2ed7c3",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bacterial-chemotaxis-as-a-model-behavioral-system-Koshland",
            "title": {
                "fragments": [],
                "text": "Bacterial chemotaxis as a model behavioral system"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31871566"
                        ],
                        "name": "J. Knott",
                        "slug": "J.-Knott",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Knott",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Knott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 294
                            }
                        ],
                        "text": "What do the critic and actor learning rules suggest about how efficacies of corticostriatal synapses change? Both learning rules are related to Donald Hebb\u2019s classic proposal that whenever a presynaptic signal participates in activating the postsynaptic neuron the synapse\u2019s efficacy increases (Hebb, 1949)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 141951152,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b9164335be5808ddd59786869a9f992331af5218",
            "isKey": false,
            "numCitedBy": 3697,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-organization-of-behavior:-A-neuropsychological-Knott",
            "title": {
                "fragments": [],
                "text": "The organization of behavior: A neuropsychological theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 67080635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20d910897b46b969b3e5cf5a0a18d4c2d0608144",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-Chemotaxis-to-cooperativity:-abstract-in-Barto",
            "title": {
                "fragments": [],
                "text": "From Chemotaxis to cooperativity: abstract exercises in neuronal learning strategies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 337,
                                "start": 285
                            }
                        ],
                        "text": "The history of ANNs as learning methods for classification or regression has passed through several stages: roughly, the Perceptron (Rosenblatt, 1962) and ADALINE (ADAptive LINear Element) (Widrow and Hoff, 1960) stage of learning by single-layer ANNs, the error-backpropagation stage (LeCun, 1985; Rumelhart, Hinton, and Williams, 1986) of learning by multi-layer ANNs, and the current deep-learning stage with its emphasis on representation learning (e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 63077747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d007ed936c51a700d8c65d1bbfae7acc83783c31",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Une-procedure-d'apprentissage-pour-reseau-a-seuil-LeCun",
            "title": {
                "fragments": [],
                "text": "Une procedure d'apprentissage pour reseau a seuil asymmetrique (A learning scheme for asymmetric threshold networks)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50472018"
                        ],
                        "name": "W. Miller",
                        "slug": "W.-Miller",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Miller",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63039687,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "2e9faeccd0a9ca31bf87372957c45df0978c045e",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Summary-Comparison-of-CMAC-Neural-Network-and-Miller-Sutton",
            "title": {
                "fragments": [],
                "text": "A Summary Comparison of CMAC Neural Network and Traditional Adaptive Control Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2813699"
                        ],
                        "name": "A. Griffith",
                        "slug": "A.-Griffith",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Griffith",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griffith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 228
                            }
                        ],
                        "text": "A later version (Samuel, 1967) included refinements in its search procedure, such as alpha-beta pruning, extensive use of a supervised learning mode called \u201cbook learning,\u201d and hierarchical lookup tables called signature tables (Griffith, 1966) to represent the value function instead of linear function approximation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62050303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc4421a929dd6a5a1005f1a0a6b67e92e63c77e7",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-New-Machine-Learning-Technique-Applied-to-the-of-Griffith",
            "title": {
                "fragments": [],
                "text": "A New Machine-Learning Technique Applied to the Game of Checkers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50040640"
                        ],
                        "name": "J. S. Edwards",
                        "slug": "J.-S.-Edwards",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Edwards",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S. Edwards"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62179641,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "655135070ffdb6c08d70c74ea7071f8b797764ab",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Hedonistic-Neuron:-A-Theory-of-Memory,-Learning-Edwards",
            "title": {
                "fragments": [],
                "text": "The Hedonistic Neuron: A Theory of Memory, Learning and Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27581584"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Howard",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62124406,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c7d3e9a1dd86f9c96f709d0ddb76972862784231",
            "isKey": false,
            "numCitedBy": 2813,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-Programming-and-Markov-Processes-Howard",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and Markov Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2547681"
                        ],
                        "name": "S. Dreyfus",
                        "slug": "S.-Dreyfus",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Dreyfus",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dreyfus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714607"
                        ],
                        "name": "A. Law",
                        "slug": "A.-Law",
                        "structuredName": {
                            "firstName": "Averill",
                            "lastName": "Law",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Law"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62205854,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "64143c854c1873f8208272b181d0350695d30ec2",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-art-and-theory-of-dynamic-programming-Dreyfus-Law",
            "title": {
                "fragments": [],
                "text": "The art and theory of dynamic programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Montague et al. (1995) pointed out that dopamine likely plays a similar role in the vertebrate brain."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60716402,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5ee38bf9494a91ca8665f9fbe59830464c223b82",
            "isKey": false,
            "numCitedBy": 557,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reinforcement-learning-with-selective-perception-McCallum-Ballard",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning with selective perception and hidden state"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913418"
                        ],
                        "name": "B. Widrow",
                        "slug": "B.-Widrow",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Widrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Widrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976925"
                        ],
                        "name": "M. Hoff",
                        "slug": "M.-Hoff",
                        "structuredName": {
                            "firstName": "Marcian",
                            "lastName": "Hoff",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 177
                            }
                        ],
                        "text": "Sutton and Barto (1981a) contains the earliest recognition of the near identity between the Rescorla\u2013Wagner model and the Least-Mean-Square (LMS), or Widrow-Hoff, learning rule (Widrow and Hoff, 1960)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 89
                            }
                        ],
                        "text": "It is essentially the same as the Least Mean Square (LMS), or Widrow-Hoff, learning rule (Widrow and Hoff, 1960) that finds the weights\u2014 here the associative strengths\u2014that make the average of the squares of all the errors as close to zero as possible."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 189
                            }
                        ],
                        "text": "The history of ANNs as learning methods for classification or regression has passed through several stages: roughly, the Perceptron (Rosenblatt, 1962) and ADALINE (ADAptive LINear Element) (Widrow and Hoff, 1960) stage of learning by single-layer ANNs, the error-backpropagation stage (LeCun, 1985; Rumelhart, Hinton, and Williams, 1986) of learning by multi-layer ANNs, and the current deep-learning stage with its emphasis on representation learning (e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60830585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e14b2ff9dc2234df94fc24d89fc25e797d0e9e7",
            "isKey": false,
            "numCitedBy": 2603,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-switching-circuits-Widrow-Hoff",
            "title": {
                "fragments": [],
                "text": "Adaptive switching circuits"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17044772"
                        ],
                        "name": "M. L. Tsetlin",
                        "slug": "M.-L.-Tsetlin",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Tsetlin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. L. Tsetlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60832129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5cb74a2e0ed05d8298f69d7d24600a583932f49",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automaton-theory-and-modeling-of-biological-systems-Tsetlin",
            "title": {
                "fragments": [],
                "text": "Automaton theory and modeling of biological systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60745539,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "171b9d5f0f29388c1e638de55aed3d19c3524df5",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reinforcement-learning-with-hidden-states-Lin-Mitchell",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning with hidden states"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66517364"
                        ],
                        "name": "A. Butkovskiy",
                        "slug": "A.-Butkovskiy",
                        "structuredName": {
                            "firstName": "Anatoliy",
                            "lastName": "Butkovskiy",
                            "middleNames": [
                                "Grigorjevich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Butkovskiy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 0
                            }
                        ],
                        "text": "Feldbaum (1965) called it the dual control problem, referring to the need to solve the two problems of identification and control simultaneously when trying to control a system under uncertainty. In discussing aspects of genetic algorithms, Holland (1975) emphasized the importance of this conflict, referring to it as the conflict between the need to exploit and the need for new information."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Feldbaum (1965) called it the dual control problem, referring to the need to solve the two problems of identification and control simultaneously when trying to control a system under uncertainty."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60331799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4343b7f3447b8d2f5e48cdd3e542ac3a015380d9",
            "isKey": false,
            "numCitedBy": 400,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-control-of-systems-Butkovskiy",
            "title": {
                "fragments": [],
                "text": "Optimal control of systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5100014"
                        ],
                        "name": "G. Kimble",
                        "slug": "G.-Kimble",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Kimble",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimble"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59998106,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "05be5f2e21b49176750287fff0cc1496b0d93984",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Foundations-of-conditioning-and-learning-Kimble",
            "title": {
                "fragments": [],
                "text": "Foundations of conditioning and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48645107"
                        ],
                        "name": "D. Schultz",
                        "slug": "D.-Schultz",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Schultz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3051300"
                        ],
                        "name": "J. Melsa",
                        "slug": "J.-Melsa",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Melsa",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Melsa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60437212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "796b26e9145ab22db74e09064877992276fe6592",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "State-Functions-and-Linear-Control-Systems-Schultz-Melsa",
            "title": {
                "fragments": [],
                "text": "State Functions and Linear Control Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46914033"
                        ],
                        "name": "D. Abrahamson",
                        "slug": "D.-Abrahamson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Abrahamson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Abrahamson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60224452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1891df4b672b45433e6ccc2f9be4af2d4a98180",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Contemporary-Animal-Learning-Theory-Abrahamson",
            "title": {
                "fragments": [],
                "text": "Contemporary Animal Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412391493"
                        ],
                        "name": "L. Ungar",
                        "slug": "L.-Ungar",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Ungar",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ungar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60167426,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "366901eb4a99a6ebb799701614ccd0ac4601f210",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-bioreactor-benchmark-for-adaptive-network-based-Ungar",
            "title": {
                "fragments": [],
                "text": "A bioreactor benchmark for adaptive network-based process control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143637056"
                        ],
                        "name": "P. Sastry",
                        "slug": "P.-Sastry",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Sastry",
                            "middleNames": [
                                "Shanti"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sastry"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59790242,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0a88882d9dc6d77c3ff099f9ca84d24c91e138d",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimator-Algorithms-for-Learning-Automata-Thathachar-Sastry",
            "title": {
                "fragments": [],
                "text": "Estimator Algorithms for Learning Automata"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3252904"
                        ],
                        "name": "J. Andreae",
                        "slug": "J.-Andreae",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Andreae",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Andreae"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58156254,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "9eacc72402165e573a278089eadc65c23b9d18aa",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Thinking-with-the-teachable-machine-Andreae",
            "title": {
                "fragments": [],
                "text": "Thinking with the teachable machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743978"
                        ],
                        "name": "P. Young",
                        "slug": "P.-Young",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Young",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195644422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e66422ef02bb1997132d2942109e1c97e662da93",
            "isKey": false,
            "numCitedBy": 605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-Estimation-and-Time-Series-Analysis-Young",
            "title": {
                "fragments": [],
                "text": "Recursive Estimation and Time Series Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113469609"
                        ],
                        "name": "B. Skinner",
                        "slug": "B.-Skinner",
                        "structuredName": {
                            "firstName": "Burrhus",
                            "lastName": "Skinner",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Skinner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 149900572,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "216fce085f7112eeec2a9b9252b7f8735d2b846f",
            "isKey": false,
            "numCitedBy": 2292,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Behavior-of-Organisms-Skinner",
            "title": {
                "fragments": [],
                "text": "The Behavior of Organisms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1938
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13240205"
                        ],
                        "name": "James L Olds",
                        "slug": "James-L-Olds",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Olds",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L Olds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717066"
                        ],
                        "name": "P. Milner",
                        "slug": "P.-Milner",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Milner",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Milner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 182
                            }
                        ],
                        "text": "the control exercised over the animal\u2019s behavior by means of this reward is extreme, possibly exceeding that exercised by any other reward previously used in animal experimentation\u201d (Olds and Milner, 1954)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 46686143,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "874ff0cb6e572381a520104ba73fac1e00595eb7",
            "isKey": false,
            "numCitedBy": 2822,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Positive-reinforcement-produced-by-electrical-of-of-Olds-Milner",
            "title": {
                "fragments": [],
                "text": "Positive reinforcement produced by electrical stimulation of septal area and other regions of rat brain."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of comparative and physiological psychology"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49617962"
                        ],
                        "name": "E. Galanter",
                        "slug": "E.-Galanter",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Galanter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Galanter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982544"
                        ],
                        "name": "M. Gerstenhaber",
                        "slug": "M.-Gerstenhaber",
                        "structuredName": {
                            "firstName": "Murray",
                            "lastName": "Gerstenhaber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gerstenhaber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39217311,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "82d7e8efa426f738c86382e9ad2030654800d6fe",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-thought:-the-extrinsic-theory.-Galanter-Gerstenhaber",
            "title": {
                "fragments": [],
                "text": "On thought: the extrinsic theory."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801949"
                        ],
                        "name": "J. Deutsch",
                        "slug": "J.-Deutsch",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Deutsch",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deutsch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 77
                            }
                        ],
                        "text": "Deutsch (1954) described a maze-solving machine based on his behavior theory (Deutsch, 1953) that has some properties in common with model-based reinforcement learning (Chapter 8)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 36581927,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "e7097b535108b1e9938d36075a69f6155fdb164b",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-type-of-behaviour-theory.-Deutsch",
            "title": {
                "fragments": [],
                "text": "A new type of behaviour theory."
            },
            "venue": {
                "fragments": [],
                "text": "British journal of psychology"
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143701657"
                        ],
                        "name": "M. D. Egger",
                        "slug": "M.-D.-Egger",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Egger",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. D. Egger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144451731"
                        ],
                        "name": "N. Miller",
                        "slug": "N.-Miller",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Miller",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "Stochastic learning automata were foreshadowed by earlier work in psychology, beginning with William Estes\u2019 (1950) effort toward a statistical theory of learning and further developed by others, most famously by psychologist Robert Bush and statistician Frederick Mosteller (Bush and Mosteller, 1955)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7497907,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0d59aa88409d15487847768bb8055a41858a9fed",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Secondary-reinforcement-in-rats-as-a-function-of-of-Egger-Miller",
            "title": {
                "fragments": [],
                "text": "Secondary reinforcement in rats as a function of information value and reliability of the stimulus."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46660472"
                        ],
                        "name": "D. Thistlethwaite",
                        "slug": "D.-Thistlethwaite",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Thistlethwaite",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Thistlethwaite"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6583155,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "487907de7c1c8693374f83851db3308b74175a45",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-critical-review-of-latent-learning-and-related-Thistlethwaite",
            "title": {
                "fragments": [],
                "text": "A critical review of latent learning and related experiments."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological bulletin"
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469878"
                        ],
                        "name": "E. Harth",
                        "slug": "E.-Harth",
                        "structuredName": {
                            "firstName": "Erich",
                            "lastName": "Harth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Harth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69049678"
                        ],
                        "name": "E. Tzanakou",
                        "slug": "E.-Tzanakou",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Tzanakou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tzanakou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2575297,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "id": "bd678d09bfebe5d72900153c2041b57c9f4ea55b",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Alopex:-a-stochastic-method-for-determining-visual-Harth-Tzanakou",
            "title": {
                "fragments": [],
                "text": "Alopex: a stochastic method for determining visual receptive fields."
            },
            "venue": {
                "fragments": [],
                "text": "Vision research"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40805799"
                        ],
                        "name": "J. W. Humberston",
                        "slug": "J.-W.-Humberston",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Humberston",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Humberston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5482999,
            "fieldsOfStudy": [],
            "id": "f2dbab1960529561c9432600694607d59d6db694",
            "isKey": false,
            "numCitedBy": 8104,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Classical-mechanics-Humberston",
            "title": {
                "fragments": [],
                "text": "Classical mechanics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44717168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff80b7820fbc54926946c245e139c382266489ae",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-Algorithms-with-Neural-Network-Behavior-Omohundro",
            "title": {
                "fragments": [],
                "text": "Efficient Algorithms with Neural Network Behavior"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 136
                            }
                        ],
                        "text": "5 Function approximation using radial basis functions (RBFs) has received wide attention ever since being related to neural networks by Broomhead and Lowe (1988). Powell (1987) reviewed earlier uses of RBFs, and Poggio and Girosi (1989, 1990) extensively developed and applied this approach."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3686496,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b08ba914037af6d88d16e2657a65cd9dc5cf5da1",
            "isKey": false,
            "numCitedBy": 2307,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariable-Functional-Interpolation-and-Adaptive-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Multivariable Functional Interpolation and Adaptive Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3544623"
                        ],
                        "name": "P. Redgrave",
                        "slug": "P.-Redgrave",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Redgrave",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Redgrave"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38756714"
                        ],
                        "name": "K. Gurney",
                        "slug": "K.-Gurney",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gurney",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gurney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 144673330,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "3bf6af7c8b42a205e4c9efd247317f7c2105a7bc",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "An influential concept in contemporary computational neuroscience is the reward prediction error hypothesis of phasic dopaminergic function. It maintains that midbrain dopaminergic neurons signal the occurrence of unpredicted reward, which is used in appetitive learning to reinforce existing actions that most often lead to reward. However, the availability of limited afferent sensory processing and the precise timing of dopaminergic signals suggest that they might instead have a central role in identifying which aspects of context and behavioural output are crucial in causing unpredicted events."
            },
            "slug": "The-short-latency-dopamine-signal:-a-role-in-novel-Redgrave-Gurney",
            "title": {
                "fragments": [],
                "text": "The short-latency dopamine signal: a role in discovering novel actions?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work has suggested that the availability of limited afferent sensory processing and the precise timing of dopaminergic signals suggest that they might instead have a central role in identifying which aspects of context and behavioural output are crucial in causing unpredicted events."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1825117"
                        ],
                        "name": "M. Hammer",
                        "slug": "M.-Hammer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144020211"
                        ],
                        "name": "R. Menzel",
                        "slug": "R.-Menzel",
                        "structuredName": {
                            "firstName": "Randolf",
                            "lastName": "Menzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Menzel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 65
                            }
                        ],
                        "text": "The model is based on research by Hammer, Menzel, and colleagues (Hammer and Menzel, 1995; Hammer, 1997) showing that the neuromodulator octopamine acts as a reinforcement signal in the honeybee."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15593447,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "bed8538de3e6e988fdf29afa59fef0fa9bf5bad7",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "Insects are favorable subjects for neuroethological studies. Their nervous systems are relatively small and contain many individually identifiable cells. The CNS is highly compartmentalized with clear separations between multisensory higher order neuropiles in the brain and neuropiles serving sensory-motor routines in the ventral cord (Huber, 1974). The rich behavior of insects includes orientation in space and time, visual, chemical, and mechanical communication, and complex motor routines for flying, walking, swimming, nest building, defense, and attack. Learning and memory, though, are not usually considered to be a strong point of insects. Rather, insect behavior is often regarded as highly stereotyped and under tight control of genetically programmed neural circuits. This view, however, does not do justice to the insect order of Hymenoptera (bees, wasps, ants). Most Hymenopteran species care for their brood either as individual females or as a social group of females. Consequently, they regularly return to their nest site to feed, protect, and nurse the larvae, store food, and hide from adverse environmental conditions. Since they search for food (prey; nectar and pollen on flowers) at unpredictable sites, they have to learn the celestial and terrestrial cues that guide their foraging trips over long distances and allow them to find their nest sites (central place foraging; von Frisch, 1967; Seeley, 1985). They learn to relate the sun's position and sky pattern of polarized light to the time of the day (Lindauer, 1959), and landmarks are learned in relationship to the nest site within the framework of the time-compensated sun compass. The honeybee communicates direction and distance of a feeding place to hive mates by performing a ritualized body movement, the waggle dance (von Frisch, 1967). Associative learning is an essential component of the bee's central place foraging behavior and dance communication. Hive mates attending a dance performance learn the odor emanating from the dancing bee and seek it at the indicated food site. The odor, color, and shape of flowers are learned when the bee experiences these stimuli shortly before it finds food (nectar, pollen). This appetitive learning in bees has many characteristics of associative learning well known from mammalian learning studies (Menzel, 1985, 1990; Bitterman, 1988). It follows the rules of classical and operant conditioning, respectively, so that stimuli or behavioral acts are associated with evaluating stimuli. Since associative learning, especially of the classical type, is well described at the phenomenological and operational level (Rescorla, 1988), it provides a favorable approach in the search for the neural substrate underlying learning and memory.(ABSTRACT TRUNCATED AT 400 WORDS)"
            },
            "slug": "Learning-and-memory-in-the-honeybee.-Hammer-Menzel",
            "title": {
                "fragments": [],
                "text": "Learning and memory in the honeybee."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Associative learning is an essential component of the bee's central place foraging behavior and dance communication, and provides a favorable approach in the search for the neural substrate underlying learning and memory."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097739937"
                        ],
                        "name": "T. L. Lai Andherbertrobbins",
                        "slug": "T.-L.-Lai-Andherbertrobbins",
                        "structuredName": {
                            "firstName": "T",
                            "lastName": "Lai Andherbertrobbins",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. L. Lai Andherbertrobbins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18456561,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "572379c1d56e7e19422ae38218ee228c61aefb2f",
            "isKey": false,
            "numCitedBy": 1987,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Asymptotically-Efficient-Adaptive-Allocation-Rules-Andherbertrobbins",
            "title": {
                "fragments": [],
                "text": "Asymptotically Efficient Adaptive Allocation Rules"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067481107"
                        ],
                        "name": "R. Dunn",
                        "slug": "R.-Dunn",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dunn",
                            "middleNames": [
                                "Minta"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dunn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38185336,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ba0c649ff274730ab1f3e21f67ba5aa90080df17",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Brains,-behavior,-and-robotics-Dunn",
            "title": {
                "fragments": [],
                "text": "Brains, behavior, and robotics"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144323963"
                        ],
                        "name": "S. Alexander",
                        "slug": "S.-Alexander",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Alexander",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Alexander"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12374910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7d78b005150b873a1b72423cdc045267e03daa7",
            "isKey": false,
            "numCitedBy": 3372,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Signal-Processing-Alexander",
            "title": {
                "fragments": [],
                "text": "Adaptive Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Texts and Monographs in Computer Science"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2261353"
                        ],
                        "name": "T. Perkins",
                        "slug": "T.-Perkins",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Perkins",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Perkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1862271"
                        ],
                        "name": "Mark D. Pendrith",
                        "slug": "Mark-D.-Pendrith",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Pendrith",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Pendrith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11569095,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f6861db9c45dfe1e0433fe24eda910aac7963ca1",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Existence-of-Fixed-Points-for-Q-Learning-and-Perkins-Pendrith",
            "title": {
                "fragments": [],
                "text": "On the Existence of Fixed Points for Q-Learning and Sarsa in Partially Observable Domains"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990648616"
                        ],
                        "name": "Hector Magno",
                        "slug": "Hector-Magno",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Magno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hector Magno"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17066350,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "dd5197b78aa4fe2bfccc5772f5a88f742f31e715",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Models-of-Learning-Magno",
            "title": {
                "fragments": [],
                "text": "Models of Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50472018"
                        ],
                        "name": "W. Miller",
                        "slug": "W.-Miller",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Miller",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2379041"
                        ],
                        "name": "F. Glanz",
                        "slug": "F.-Glanz",
                        "structuredName": {
                            "firstName": "Filson",
                            "lastName": "Glanz",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Glanz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59671719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a67a842eb60a082cfb8b99d93d8ab2bb52ecdc5b",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UNH_CMAC-Version-2.1-The-University-of-New-of-the-Miller-Glanz",
            "title": {
                "fragments": [],
                "text": "UNH_CMAC Version 2.1 The University of New Hampshire Implementation of the Cerebellar Model Arithmetic Computer - CMAC"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 996637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c6ff9a95c53811930a75538c56c75364280e097",
            "isKey": false,
            "numCitedBy": 337,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(\u03bb) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(\u03bb) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating."
            },
            "slug": "Practical-issues-in-temporal-difference-learning-Tesauro",
            "title": {
                "fragments": [],
                "text": "Practical issues in temporal difference learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which surpasses comparable networks trained on a massive human expert data set."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparisons of channel-assignment strategies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 58
                            }
                        ],
                        "text": "For example, in Goethe\u2019s poem \u201cThe Sorcerer\u2019s Apprentice\u201d (Goethe, 1878), the apprentice uses magic to enchant a broom to do his job of fetching water, but the result is an unintended flood due to the apprentice\u2019s inadequate knowledge of magic."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Sorcerers Apprentice"
            },
            "venue": {
                "fragments": [],
                "text": "The Permanent Goethe, p. 349. The Dial Press, Inc., New York."
            },
            "year": 1878
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probability problem of pattern recognition learning and potential functions method"
            },
            "venue": {
                "fragments": [],
                "text": "Avtomat. i Telemekh, 25 (9):1307\u20131323."
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random Walks and Electric Networks. The Mathematical Association of America"
            },
            "venue": {
                "fragments": [],
                "text": "Carus Mathematical Monograph"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A neural substrate"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 166
                            }
                        ],
                        "text": ") There is now a fairly extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximate solutions of a discounted Markovian decision process"
            },
            "venue": {
                "fragments": [],
                "text": "Bonner Mathematische Schriften, 98:77\u201392."
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "High-performance job-shop scheduling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tracking and trailing: Adaptation in movement strategies"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Bolt Beranek and Newman, Inc. Unpublished report."
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tracking and trailing: Adaptation in movement strategies"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Bolt Beranek and Newman, Inc. Unpublished report."
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic optimization of systems"
            },
            "venue": {
                "fragments": [],
                "text": "Izv. Akad. Nauk SSSR, Tekh. Kibernetika:14\u201319."
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and Optimal Control, Volume 1, third edition"
            },
            "venue": {
                "fragments": [],
                "text": "Athena Scientific, Belmont, MA."
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Explaining temporal differences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning. Kluwer Academic Press. Reprinting of a special double issue on reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 257
                            }
                        ],
                        "text": "Thorndike later modified the law to better account for accumulating data on animal learning (such as differences between the effects of reward and punishment), and the law in its various forms has generated considerable controversy among learning theorists (e.g., see Gallistel, 2005; Herrnstein, 1970; Kimble, 1961, 1967; Mazur, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning and Behavior, 3rd ed"
            },
            "venue": {
                "fragments": [],
                "text": "Prentice-Hall, Englewood Cliffs, NJ."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning in connectionist networks: A mathematical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report ICS 8605,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning for dynamic channel"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving elevator performance using reinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Special triple issue on reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 5
                            }
                        ],
                        "text": "John Holland (1975) outlined a general theory of adaptive systems based on selectional principles."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LTSM can solve hard time lag problems"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems: Proceedings of the 1996 Conference, pp. 473\u2013 479. MIT Press, Cambridge, MA."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 126
                            }
                        ],
                        "text": "There followed several other influential psychological models of classical conditioning based on temporal-difference learning (e.g., Klopf, 1988; Moore et al., 1986; Sutton and Barto, 1987, 1990)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simulation of the classically conditioned nictitating membrane response by a neuronlike adaptive element: I"
            },
            "venue": {
                "fragments": [],
                "text": "Response topography, neuronal firing, and interstimulus intervals. Behavioural Brain Research, 21(2):143\u2013154."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Special triple issue on reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A temporal-difference model of classical conditioning"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Ninth Annual Conference of the Cognitive Science Society,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training and tracking"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Special double issue on reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronlike elements that can solve difficult learning control problems"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic reinforcement algorithm for learning real-valued functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 33
                            }
                        ],
                        "text": "The second version of the system (Zhang and Dietterich, 1995) used a more complicated time-delay neural network (TDNN) borrowed from the field of speech recognition (Lang et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A reinforcement learning approach to job"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning and adaptive critic methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 33
                            }
                        ],
                        "text": "The second version of the system (Zhang and Dietterich, 1995) used a more complicated time-delay neural network (TDNN) borrowed from the field of speech recognition (Lang et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A reinforcement learning approach to job"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 2
                            }
                        ],
                        "text": "4 Graybiel (2000) is a brief primer on the basal ganglia."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 142
                            }
                        ],
                        "text": "We do know that \u03b5-greedification may sometimes result in an inferior policy, as policies may chatter among good policies rather than converge (Gordon, 1996a)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chattering in SARSA(\u03bb)"
            },
            "venue": {
                "fragments": [],
                "text": "CMU learning lab internal report."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A dynamic channel assignment policy through qlearning"
            },
            "venue": {
                "fragments": [],
                "text": "CRL Report 334,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Single channel theory: A neuronal theory of learning"
            },
            "venue": {
                "fragments": [],
                "text": "Brain Theory Newsletter,"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement-learning connectionist systems"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report NU-CCS-87-3,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 50
                            }
                        ],
                        "text": "1 The first semi-gradient method was linear TD(\u03bb) (Sutton, 1988)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to predict by the method of temporal differences"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning, 3(1):9\u201344 (important erratum p. 377)."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Observable operator models and conditioned continuation representations"
            },
            "venue": {
                "fragments": [],
                "text": "Arbeitspapiere der GMD 1043, GMD Forschungszentrum Informationstechnik, Sankt Augustin, Germany."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient learning of multiple degree-of-freedom control problems with quasi-independent Q-agents"
            },
            "venue": {
                "fragments": [],
                "text": "M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. Weigend (eds.), Proceedings of the 1990 Connectionist Models Summer School. Erlbaum, Hillsdale, NJ."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to control a bioreactor using a neural net DynaQ system"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems, pp. 167\u2013172. Center for Systems Science, Dunham Laboratory, Yale University, New Haven."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 44
                            }
                        ],
                        "text": "used in many reinforcement learning systems (e.g., Shewchuk and Dean, 1990; Lin and Kim, 1991; Miller, Scalera, and Kim, 1994; Sofge and White, 1992; Tham, 1994; Sutton, 1996; Watkins, 1989) as well as in other types of learning control systems (e."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applied learning: Optimal control for manufacturing"
            },
            "venue": {
                "fragments": [],
                "text": "D. A. White and D. A. Sofge (eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 259\u2013281. Van Nostrand Reinhold, New York."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gain adaptation beats least squares? Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems, pp"
            },
            "venue": {
                "fragments": [],
                "text": "161\u2013166, Yale University, New Haven, CT."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Incremental learning of evaluation functions for absorbing Markov chains: New methods and theorems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 3
                            }
                        ],
                        "text": "11 Yin and Knowlton (2006) reviewed findings from outcome-devaluation experiments with rodents supporting the view that habitual and goal-directed behavior (as psychologists use the phrase) are respectively most associated with processing in the dorsolateral striatum (DLS) and the dorsomedial striatum (DMS)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ZCS: A zeroth order classifier system"
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation, 2:1\u201318."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A summary appears"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Systems, Man, and Cybernetics,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Incremental learning of evaluation functions for absorbing Markov chains: New methods and theorems"
            },
            "venue": {
                "fragments": [],
                "text": "Preprint."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 351,
                                "start": 179
                            }
                        ],
                        "text": "The definitive demonstration of spike-timing-dependent plasticity (STDP) is attributed to Markram, L\u00fcbke, Frotscher, and Sakmann (1997), with evidence from earlier experiments by Levy and Steward (1983) and others that the relative timing of pre- and postsynaptic spikes is critical for inducing changes in synaptic efficacy. Rao and Sejnowski (2001) suggested how STDP could be the result of a TD-like mechanism at synapses with non-contingent eligibility traces lasting about 10 milliseconds."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 179
                            }
                        ],
                        "text": "The definitive demonstration of spike-timing-dependent plasticity (STDP) is attributed to Markram, L\u00fcbke, Frotscher, and Sakmann (1997), with evidence from earlier experiments by Levy and Steward (1983) and others that the relative timing of pre- and postsynaptic spikes is critical for inducing changes in synaptic efficacy."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Development and application of CMAC neural network-based control"
            },
            "venue": {
                "fragments": [],
                "text": "D. A. White and D. A. Sofge (eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 215\u2013232. Van Nostrand Reinhold, New York."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 70
                            }
                        ],
                        "text": "Many excellent modern treatments of dynamic programming are available (e.g., Bertsekas, 2005, 2012; Puterman, 1994; Ross, 1983; and Whittle, 1982, 1983)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Stochastic Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Escaping brittleness: The possibility of general-purpose learning algorithms applied to rule-based systems"
            },
            "venue": {
                "fragments": [],
                "text": "R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (eds.), Machine Learning: An Artificial Intelligence Approach, vol. 2,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 35
                            }
                        ],
                        "text": "3 The term coarse coding is due to Hinton (1984), and our Figure 9."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 65
                            }
                        ],
                        "text": "Thorndike used the phrase learning by \u201cselecting and connecting\u201d (Hilgard, 1956)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theories of Learning, Second Edition"
            },
            "venue": {
                "fragments": [],
                "text": "Appleton-Century-Cofts, Inc., New York."
            },
            "year": 1956
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learned associations over long delays"
            },
            "venue": {
                "fragments": [],
                "text": "Bower, G., editor, The psychology of learning and motivation, volume 4, pages 1\u201384. Academic Press, Inc., New York."
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization over Time, vol"
            },
            "venue": {
                "fragments": [],
                "text": "1. Wiley, New York."
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 183
                            }
                        ],
                        "text": "In the 1960s the terms \u201creinforcement\u201d and \u201creinforcement learning\u201d were used in the engineering literature for the first time to describe engineering uses of trialand-error learning (e.g., Waltz and Fu, 1965; Mendel, 1966; Fu, 1970; Mendel and McClaren, 1970)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A survey of learning control systems"
            },
            "venue": {
                "fragments": [],
                "text": "ISA Transactions, 5:297\u2013303."
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 2
                            }
                        ],
                        "text": "1 Kamin (1968) first reported blocking, now commonly known as Kamin blocking, in classical conditioning."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Markov processes and the Dirichlet problem"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Japan Academy, 21(310):227\u2013233."
            },
            "year": 1945
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 130
                            }
                        ],
                        "text": "1 The overall view of planning and learning presented here has developed gradually over a number of years, in part by the authors (Sutton, 1990, 1991a, 1991b; Barto, Bradtke, and Singh, 1991, 1995; Sutton and Pinette, 1985; Sutton and Barto, 1981b); it has been strongly influenced by Agre and Chapman (1990; Agre 1988), Bertsekas and Tsitsiklis (1989), Singh (1993), and others."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An adaptive network that constructs and uses an internal model of its world"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition and Brain Theory, 3:217\u2013246."
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 70
                            }
                        ],
                        "text": "The notions of momentum (Derthick, 1984), of Polyak-Ruppert averaging (Polyak, 1990; Ruppert, 1988; Polyak and Juditsky, 1992), or further extensions of these ideas may significantly help."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New stochastic approximation type procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Automat. i Telemekh, 7 (98-107):2 (in Russian)."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 63
                            }
                        ],
                        "text": "Based on these results and analyses by backgammon grandmasters (Robertie, 1992; see Tesauro, 1995), TD-Gammon 3."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Carbon versus silicon: Matching wits with TD-Gammon"
            },
            "venue": {
                "fragments": [],
                "text": "Inside Backgammon, 2:14\u201322."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the complexity of solving Markov decision processes"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Eleventh International Conference on Uncertainty in Artificial Intelligence"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elevator dispatchers for down peak traffic"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report. ECE Department, University of Massachusetts, Amherst."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 126
                            }
                        ],
                        "text": "There followed several other influential psychological models of classical conditioning based on temporal-difference learning (e.g., Klopf, 1988; Moore et al., 1986; Sutton and Barto, 1987, 1990)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simulation of the classically conditioned nictitating membrane response by a neuron-like adaptive element: I"
            },
            "venue": {
                "fragments": [],
                "text": "Response topography, neuronal firing, and interstimulus intervals. Behavioural Brain Research, 21:143\u2013154."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 80
                            }
                        ],
                        "text": "The incremental update of the inverse matrix has been known at least since 1949 (Sherman and Morrison, 1949)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adjustment of an inverse matrix corresponding to changes in the elements of a given column or a given row of the original matrix (abstract)"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematical Statistics 20 :621."
            },
            "year": 1949
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 116
                            }
                        ],
                        "text": "Additional exploration of the TD model and its possible neural implementation was conducted by Moore and colleagues (Moore, Desmond, Berthier, Blazis, Sutton, and Barto, 1986; Moore and Blazis, 1989; Moore, Choi, and Brunzell, 1998; Moore, Marks, Castagna, and Polewan, 2001)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simulation of a classically conditioned response: A cerebellar implementation of the sutton-barto-desmond model"
            },
            "venue": {
                "fragments": [],
                "text": "Byrne, J. H. and Berry, W. O., editors, Neural Models of Plasticity, pages 187\u2013207. Academic Press, San Diego, CA."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Special double issue on reinforcement learning, Machine Learning, 49(2-3)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 130
                            }
                        ],
                        "text": "1 The overall view of planning and learning presented here has developed gradually over a number of years, in part by the authors (Sutton, 1990, 1991a, 1991b; Barto, Bradtke, and Singh, 1991, 1995; Sutton and Pinette, 1985; Sutton and Barto, 1981b); it has been strongly influenced by Agre and Chapman (1990; Agre 1988), Bertsekas and Tsitsiklis (1989), Singh (1993), and others."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The learning of world models by connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh Annual Conference of the Cognitive Science Society, pp. 54\u2013"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theoretical comparison of the efficiencies of two classical methods and a Monte Carlo method for computing one component of the solution of a set of linear algebraic equations"
            },
            "venue": {
                "fragments": [],
                "text": "H. A. Meyer (ed.), Symposium on Monte Carlo Methods, pp. 191\u2013233. Wiley, New York."
            },
            "year": 1954
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Predictive timing under temporal uncertainty: The time derivative model of the conditioned response"
            },
            "venue": {
                "fragments": [],
                "text": "Rosenbaum, D. A. and Collyer, C. E., editors, Timing of Behavior, pages 3\u201334. MIT Press, Cambridge, MA."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Presentation of a maze-solving machine"
            },
            "venue": {
                "fragments": [],
                "text": "Forester, H. V., editor, Cybernetics. Transactions of the Eighth Conference, pages 173\u2013180. Josiah Macy Jr. Foundation."
            },
            "year": 1951
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advanced forecasting methods for global crisis warning and models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning. Kluwer Academic Press. Reprinting of a special double issue on reinforcement learning, Machine Learning, 8(3-4)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Control of self-similar atm call traffic by reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronlike elements that can solve difficult learning control problems"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics, 13:835\u2013846. Reprinted in J. A. Anderson and E. Rosenfeld (eds.), Neurocomputing: Foundations of Research, pp. 535\u2013549. MIT Press, Cambridge, MA, 1988."
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 28
                            }
                        ],
                        "text": "Samuel made no reference to Minsky\u2019s work or to possible connections to animal learning. His inspiration apparently came from Claude Shannon\u2019s (1950) suggestion that a computer could be programmed to use an evaluation function to play chess, and that it might be able to improve its play by modifying this function on-line."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "To the best of our knowledge, the first connection between DP and reinforcement learning was made by Minsky (1961) in commenting on Samuel\u2019s checkers player."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The design of CMAC neural networks for control"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive and Learning Systems 1 :140\u2013145."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Powell (1987) reviewed earlier uses of RBFs, and Poggio and Girosi (1989, 1990) extensively developed and applied this approach."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Radial basis functions for multivariate interpolation: A review"
            },
            "venue": {
                "fragments": [],
                "text": "Mason, J. C. and Cox, M. G., editors, Algorithms for Approximation. Clarendon Press, Oxford."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to control a bioreactor using a neural net dyna-q system"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-time learning and control using asynchronous dynamic programming"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 91-57. Department of Computer and Information Science, University of Massachusetts, Amherst."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learing with"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parameter stability in the TD model of complex CR topographies"
            },
            "venue": {
                "fragments": [],
                "text": "Society for Neuroscience Abstract 642.2."
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern-recognizing control systems"
            },
            "venue": {
                "fragments": [],
                "text": "In Computer and Information Sciences (COINS) Proceedings,"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 189
                            }
                        ],
                        "text": "Other studies showed how reinforcement learning could address important problems in neural network learning, in particular, how it could produce learning algorithms for multilayer networks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto and Anandan, 1985; Barto, 1985, 1986; Barto and Jordan, 1987)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gradient following without back-propagation in layered networks"
            },
            "venue": {
                "fragments": [],
                "text": "M. Caudill and C. Butler (eds.), Proceedings of the IEEE First Annual Conference on Neural Networks, pp. II629\u2013II636. SOS Printing, San Diego, CA."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist Problem Solving: Computational Aspects of Biological Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Birkhauser, Boston."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "The metaphor of a neuron using a learning rule related to bacterial chemotaxis was discussed by Barto (1989). Koshland\u2019s extensive study of bacterial chemotaxis was in part motivated by similarities between features of bacteria and features of neurons (Koshland, 1980)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A biologically plausible and locally optimal learning algorithm for spiking neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Rapport technique, Australian National University."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern-recognizing control systems"
            },
            "venue": {
                "fragments": [],
                "text": "In Computer and Information Sciences (COINS) Proceedings,"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 37
                            }
                        ],
                        "text": "The Gittins index approach is due to Gittins and Jones (1974). Duff (1996) showed how it is possible to learn Gittins indices for bandit problems through reinforcement learning."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A dynamic allocation index for the sequential design of experiments"
            },
            "venue": {
                "fragments": [],
                "text": "Progress in Statistics, pages 241--266."
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Memory-based approaches to approximating continuous functions"
            },
            "venue": {
                "fragments": [],
                "text": "Sante Fe Institute Studies in the Sciences of Complexity, Proceedings Vol. 12, pp. 521\u2013521. Addison-Wesley."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning machines---a unified view"
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Information, Linguistics, and Control,"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elevator dispatchers for down peak traffic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 98
                            }
                        ],
                        "text": "In particular, the offline version of replace-trace TD(1) is formally identical to first-visit MC (Singh and Sutton, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 67
                            }
                        ],
                        "text": "straightforward, but its estimates also converge asymptotically to (Singh and Sutton, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning with replacing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning theory support for a single channel theory of the brain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement today"
            },
            "venue": {
                "fragments": [],
                "text": "American Psychologist, 13(3):94\u201399."
            },
            "year": 1958
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bee foraging in uncertain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Special triple issue on reinforcement learning, Machine Learning, 22(1/2/3)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gradient following without backpropagation in layered networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE First Annual Conference on Neural Networks,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 88
                            }
                        ],
                        "text": "Convergence with probability 1 was proved by several researchers at about the same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis, 1994; Gurvits, Lin, and Hanson, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient Dynamic Programming-Based Learning for Control"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. thesis, Northeastern University, Boston."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A unified theory of expectation in classical and instrumental conditioning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative conditioning"
            },
            "venue": {
                "fragments": [],
                "text": "I. A discriminative property of conditioned anticipation. Journal of Experimental Psychology, 32 (2):150\u2013155."
            },
            "year": 1943
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 76
                            }
                        ],
                        "text": "We call this the problem of delayed reinforcement, which is related to what Minsky (1961) called the \u201ccreditassignment problem for learning systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Minsky (1954) may have been the first to realize that this psychological principle could be important for artificial learning systems."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network control of dynamic balance for a biped walking robot"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, pp. 156\u2013161. Center for Systems Science, Dunham Laboratory, Yale University, New Haven."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 179
                            }
                        ],
                        "text": "The renowned scientist and artificial intelligence pioneer Herbert Simon anticipated the warnings we are hearing today in a presentation at the Earthware Symposium at CMU in 2000 (Simon, 2000)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lecture at the Earthware Symposium, Carnegie Mellon University"
            },
            "venue": {
                "fragments": [],
                "text": "https://www.youtube.com/watch?v=EZhyi8DBjc."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks: A Comprehensive Foundation, Macmillan college publishing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 151
                            }
                        ],
                        "text": "We restrict attention to discrete time to keep things as simple as possible, even though many of the ideas can be extended to the continuous-time case (e.g., see Bertsekas and Tsitsiklis, 1996; Werbos, 1992; Doya, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximate dynamic programming for real-time control and neural modeling"
            },
            "venue": {
                "fragments": [],
                "text": "D. A. White and D. A. Sofge (eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 493\u2013525. Van Nostrand Reinhold, New York."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The role of learning in motivation"
            },
            "venue": {
                "fragments": [],
                "text": "Gallistel, C. R., editor, Stevens handbook of experimental psychology, volume 3, pages 497\u2013533. Wiley, NY."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards learning time-varying functions with"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 326,
                                "start": 190
                            }
                        ],
                        "text": "Other studies showed how reinforcement learning could address important problems in neural-network learning, in particular, how it could produce learning algorithms for multi-layer networks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto and Anandan, 1985; Barto, 1985; Barto, 1986; Barto and Jordan, 1986)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structural learning in connectionist systems"
            },
            "venue": {
                "fragments": [],
                "text": "Program of the Seventh Annual Conference of the Cognitive Science Society, pages 43--54, Irvine, CA."
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments on the mechanisation of game learning"
            },
            "venue": {
                "fragments": [],
                "text": "1. characterization of the model and its parameters. Computer Journal, 1:232\u2013263."
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theseus\u201d maze-solving mouse"
            },
            "venue": {
                "fragments": [],
                "text": "http://cyberneticzoo.com/mazesolvers/1952-theseus-maze-solving-mouse--claude-shannon-american/."
            },
            "year": 1952
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advanced forecasting methods for global crisis warning and models of intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "General Systems Yearbook,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-time learning and control using asynchronous dynamic programming"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 91-57,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "As we outlined in Chapter 1, the idea of TD learning has its early roots in animal learning psychology and artificial intelligence, most notably the work of Samuel (1959) and Klopf (1972)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive linear quadratic control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 153
                            }
                        ],
                        "text": "The authors were also strongly influenced by psychological studies of latent learning (Tolman, 1932) and by psychological views of the nature of thought (e.g., Galanter and Gerstenhaber, 1956; Craik, 1943; Campbell, 1960; Dennett, 1978)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 120
                            }
                        ],
                        "text": "Despite this, the Law of Effect\u2014in one form or another\u2014is widely regarded as a basic principle underlying much behavior (e.g., Hilgard and Bower, 1975; Dennett, 1978; Campbell, 1960; Cziko, 1995)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind variation and selective survival as a general strategy in knowledge-processes"
            },
            "venue": {
                "fragments": [],
                "text": "M. C. Yovits and S. Cameron (eds.), Self-Organizing Systems, pp. 205\u2013231. Pergamon, New York."
            },
            "year": 1960
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 637,
        "totalPages": 64
    },
    "page_url": "https://www.semanticscholar.org/paper/Reinforcement-Learning:-An-Introduction-Sutton-Barto/97efafdb4a3942ab3efba53ded7413199f79c054?sort=total-citations"
}