{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28270470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f54bc3a24a1f01808e6e8479a11e5b0244f5523",
            "isKey": false,
            "numCitedBy": 465,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The problems arising in the modeling and coding of strings for compression purposes are discussed. The notion of an information source that simplifies and sharpens the traditional one is axiomatized, and adaptive and nonadaptive models are defined. With a measure of complexity assigned to the models, a fundamental theorem is proved which states that models that use any kind of alphabet extension are inferior to the best models using no alphabet extensions at all. A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models. Because the coding parameters are the probabilities that define the model, their design is easy, and the application of the code is straightforward even with adaptively changing source models."
            },
            "slug": "Universal-modeling-and-coding-Rissanen-Langdon",
            "title": {
                "fragments": [],
                "text": "Universal modeling and coding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122569569,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ae7beb7920485aca9c252ce3ecc3972c52eb3c37",
            "isKey": false,
            "numCitedBy": 1833,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "of the number of bits required to write down the observed data, has been reformulated to extend the classical maximum likelihood principle. The principle permits estimation of the number of the parameters in statistical models in addition to their values and even of the way the parameters appear in the models; i.e., of the model structures. The principle rests on a new way to interpret and construct a universal prior distribution for the integers, which makes sense even when the parameter is an individual object. Truncated realvalued parameters are converted to integers by dividing them by their precision, and their prior is determined from the universal prior for the integers by optimizing the precision. 1. Introduction. In this paper we study estimation based upon the principle of minimizing the total number of binary digits required to rewrite the observed data, when each observation is given with some precision. Instead of attempting at an absolutely shortest description, which would be futile, we look for the optimum relative to a class of parametrically given distributions. This Minimum Description Length (MDL) principle, which we introduced in a less comprehensive form in [25], turns out to degenerate to the more familiar Maximum Likelihood (ML) principle in case the number of parameters in the models is fixed, so that the description length of the parameters themselves can be ignored. In another extreme case, where the parameters determine the data, it similarly degenerates to Jaynes's principle of maximum entropy, [14]. But the main power of the new criterion is that it permits estimates of the entire model, its parameters, their number, and even the way the parameters appear in the model; i.e., the model structure. Hence, there will be no need to supplement the estimated parameters with a separate hypothesis test to decide whether a model is adequately parameterized or, perhaps, over parameterized."
            },
            "slug": "A-UNIVERSAL-PRIOR-FOR-INTEGERS-AND-ESTIMATION-BY-Rissanen",
            "title": {
                "fragments": [],
                "text": "A UNIVERSAL PRIOR FOR INTEGERS AND ESTIMATION BY MINIMUM DESCRIPTION LENGTH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144123642"
                        ],
                        "name": "J. Maciejowski",
                        "slug": "J.-Maciejowski",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Maciejowski",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Maciejowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 770999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ca402457f05f782c1bad95a19fb8d77dd72bcf7",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Model-discrimination-using-an-algorithmic-criterion-Maciejowski",
            "title": {
                "fragments": [],
                "text": "Model discrimination using an algorithmic information criterion"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727567"
                        ],
                        "name": "R. Solomonoff",
                        "slug": "R.-Solomonoff",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Solomonoff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Solomonoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1898103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bef2ae523cd4447af687fae13bfbb606e4a4a5ca",
            "isKey": false,
            "numCitedBy": 1585,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Formal-Theory-of-Inductive-Inference.-Part-II-Solomonoff",
            "title": {
                "fragments": [],
                "text": "A Formal Theory of Inductive Inference. Part II"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587774"
                        ],
                        "name": "J. Kent",
                        "slug": "J.-Kent",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kent",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kent"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121861651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53c5bbcf12a2dd057c10d1590b141d86fd365487",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Given a parametric model of dependence between two random quantities, X and Y, the notion of information gain can be used to define a measure of correlation. This definition of correlation generalizes both the usual product-moment correlation coeffi- cient for the bivariate normal model and the multiple correlation coefficient in the standard linear regression model. The use of this information-based correlation in a descriptive statistical analysis is examined and several examples are given. If the dependence between two random quantities, X and Y, is modelled parametri- cally, then the concept of information gain can be used to define a measure of correlation. This correlation coefficient can appear in two possible contexts, depending on whether one models the joint distribution of X and Y, or just the conditional distribution of Y given X. An important motivating feature for this information-based correlation coefficient is the fact that it generalizes both the usual product-moment correlation coefficient for the bivariate normal model and the usual multiple correlation coefficient for the standard multiple regression model with normal errors. Our intuition is well developed for these usual correlation coefficients, and hopefully our intuition will still be applicable for-the information-based correlation in more general modelling situations of parametric dependence. Further, since our correlation coefficient is based on information gain, we might hope to extend our intuition to interpret the information gain in any statistical modelling situation where we want to assess how much better a more complicated model is than a simpler model. The concept of information gain for general statistical models is described in ? 2. This concept is then used to define an information-based measure of correlation; the joint case is covered in ? 3 and the conditional case in ? 4. Estimation of the correlation coefficient is carried out by estimating the corresponding information gain; see ?? 5-7. The use of information gain for the purpose of model choice in a descriptive statistical analysis is discussed in ? 8 and a comparison between our approach and Akaike's information criterion is given in ?9. Some examples of the use of this information-based correlation are given in ?? 10 and 11."
            },
            "slug": "Information-gain-and-a-general-measure-of-Kent",
            "title": {
                "fragments": [],
                "text": "Information gain and a general measure of correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "An important motivating feature for this information-based correlation coefficient is the fact that it generalizes both the usual product-moment correlation coefficient for the bivariate normal model and the usual multiple correlation coefficients for the standard multiple regression model with normal errors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 411526,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "50a42ed2f81b9fe150883a6c89194c88a9647106",
            "isKey": false,
            "numCitedBy": 42037,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples."
            },
            "slug": "A-new-look-at-the-statistical-model-identification-Akaike",
            "title": {
                "fragments": [],
                "text": "A new look at the statistical model identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209910"
                        ],
                        "name": "K. Mardia",
                        "slug": "K.-Mardia",
                        "structuredName": {
                            "firstName": "Kanti",
                            "lastName": "Mardia",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mardia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121492535"
                        ],
                        "name": "R. J. Marshall",
                        "slug": "R.-J.-Marshall",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Marshall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J. Marshall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120901690,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "899e863d09b4ab54aa48c8ad366cc40ebb4bc53b",
            "isKey": false,
            "numCitedBy": 823,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the maximum likelihood method for fitting the linear model when residuals are correlated and when the covariance among the residuals is determined by a parametric model containing unknown parameters. Observations are assumed to be Gaussian. We give conditions which ensure consistency and asymptotic normality of the estimators. Our main concern is with the analysis of spatial data and in this context we describe some simulation experiments to assess the small sample behaviour of estimators. We also discuss an application of the spectral approximation to the likelihood for processes on a lattice."
            },
            "slug": "Maximum-likelihood-estimation-of-models-for-in-Mardia-Marshall",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood estimation of models for residual covariance in spatial regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1943223"
                        ],
                        "name": "A. Atkinson",
                        "slug": "A.-Atkinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Atkinson",
                            "middleNames": [
                                "Curtis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atkinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120171442,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dc0bfffe4ea55ac40f2547c843a18d1d72b759c6",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY One way of selecting models is to choose that model for which the maximized log likelihood minus a multiple of the number of parameters estimated is a maximum. This note explores the choice of values for the multiplier, with the value one corresponding to Akaike's information criterion. The relationship with Bayesian procedures is mentioned. Suppose that there are a number of competing models which may be fitted to some data. If the log likelihood of the ith model maximized over q* parameters is Li, the generalized information criterion is to choose the model for which Li - -1cq* is a maximum. In the criterion suggested by Akaike as equals 2. Values of os in the range 1-4 are considered by Bhansali & Downham (1977) for the choice of a time series model. As have many other authors, including Geisser & Eddy (1979) and McClave (1978), Bhansali & Downham assessed their criteria by the frequency of choice of the correct model. One purpose of the present note is to suggest that this. may not be an appropriate basis for choice: the objectives of the analysis need more explicit formulation. A similar point is made by Akaike (1979) who com- pares values of ot on the basis of squared prediction error. His simulation is, however, un- informative about the conditions under which various values of a are optimum. In this note a simple example is simulated to investigate the taxonomy of optimum of values for prediction purposes. For simplicity of structure and ease of simulation we work with linear regression models, for which the information criterion reduces to a generalized Cp statistic. The behaviour of this statistic as a function of ot is investigated in the next section. Significance testing and Bayesian alternatives are discussed in ? 3. Section 4 is concerned with asymptotics. The note closes with some general comments which allude to ridge regression and simulation."
            },
            "slug": "A-note-on-the-generalized-information-criterion-for-Atkinson",
            "title": {
                "fragments": [],
                "text": "A note on the generalized information criterion for choice of a model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680714"
                        ],
                        "name": "J. Conway",
                        "slug": "J.-Conway",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Conway",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Conway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143992833"
                        ],
                        "name": "N. Sloane",
                        "slug": "N.-Sloane",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sloane",
                            "middleNames": [
                                "J.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sloane"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1064224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "399b0319a6b720d4afc953d43341d162b0402ef5",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "If a point is picked at random inside a regular simplex, octahedron, 600 -cell, or other polytope, what is its average squared distance from the centroid? In n -dimensional space, what is the average squared distance of a random point from the closest point of the lattice A_{n} (or D_{n}, E_{n}, A_{n}^{\\ast} or D_{n}^{\\ast})? The answers are given here, together with a description of the Voronoi (or nearest neighbor) regions of these lattices. The results have applications to quantization and to the design of signals for the Gaussian channel. For example, a quantizer based on the eight-dimensional lattice E8 has a mean-squared error per symbol of 0.0717 \\cdots when applied to uniformly distributed data, compared with 0.08333 \\cdots for the best one-dimensional quantizer."
            },
            "slug": "Voronoi-regions-of-lattices,-second-moments-of-and-Conway-Sloane",
            "title": {
                "fragments": [],
                "text": "Voronoi regions of lattices, second moments of polytopes, and quantization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The answers to the squared distance questions and a description of the Voronoi (or nearest neighbor) regions of these lattices have applications to quantization and to the design of signals for the Gaussian channel."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4110643"
                        ],
                        "name": "D. Boulton",
                        "slug": "D.-Boulton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boulton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boulton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61324799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1d76ee9b6fb4d441e31abcd4dadc4e44c576017",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application"
            },
            "slug": "An-Information-Measure-for-Classification-Wallace-Boulton",
            "title": {
                "fragments": [],
                "text": "An Information Measure for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is suggested that the best classification is that which results in the briefest recording of all the attribute information, and the measurements of each thing are regarded as being a message about that thing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64903870,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "400b45a803d642b752a84147ef547af7811e8f3f",
            "isKey": false,
            "numCitedBy": 19578,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper it is shown that the classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion. This observation shows an extension of the principle to provide answers to many practical problems of statistical model fitting."
            },
            "slug": "Information-Theory-and-an-Extension-of-the-Maximum-Akaike",
            "title": {
                "fragments": [],
                "text": "Information Theory and an Extension of the Maximum Likelihood Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion to provide answers to many practical problems of statistical model fitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49413185"
                        ],
                        "name": "E. Hannan",
                        "slug": "E.-Hannan",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Hannan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hannan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123929520,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e45af34134b921c24c719a4642899a2967056dd",
            "isKey": false,
            "numCitedBy": 489,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The order, (p, q), of an autoregressive-moving average sequence, y(t), may be estimated- by minimizing a criterion, log &2 + (p + q) log T/T, with respect to p and q, where ^2 is the maximum likelihood estimate of the variance of the innovations, ?(t). It is suggested that, instead, a2 be estimated from a series of regressions of y(t) on y(t-1),y(t-2),...,y(t-p), 5(t-1),...,?(t-q), where the ?(t) are obtained by fitting a long autoregression to the data. It is shown how the sequence of regressions may, for p = q, be economically recursively calculated by embedding them in a sequence of bivariate autoregressions. Asymptotic properties of the procedure are established under very general conditions."
            },
            "slug": "Recursive-estimation-of-mixed-autoregressive-moving-Hannan-Rissanen",
            "title": {
                "fragments": [],
                "text": "Recursive estimation of mixed autoregressive-moving average order"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206735226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2137f484f04ac73ffca7137f320a4447418703ed",
            "isKey": false,
            "numCitedBy": 668,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A universal data compression algorithm is described which is capable of compressing long strings generated by a \"finitely generated\" source, with a near optimum per symbol length without prior knowledge of the source. This class of sources may be viewed as a generalization of Markov sources to random fields. Moreover, the algorithm does not require a working storage much larger than that needed to describe the source generating parameters."
            },
            "slug": "A-universal-data-compression-system-Rissanen",
            "title": {
                "fragments": [],
                "text": "A universal data compression system"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A universal data compression algorithm is described which is capable of compressing long strings generated by a \"finitely generated\" source, with a near optimum per symbol length without prior knowledge of the source."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121752527,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "92c0dcf47bc774aef0a55cdd27b032db977d24cf",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A theorem is proved which demonstrates that an earlier derived minimum description length estimation criterion is capable of distinguishing between structures in linear models for vector processes. A fairly simple algorithm is described for the estimation of the best model, including its structure and the number of its parameters."
            },
            "slug": "Estimation-of-structure-by-minimum-description-Rissanen",
            "title": {
                "fragments": [],
                "text": "Estimation of structure by minimum description length"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3161644"
                        ],
                        "name": "P. Zador",
                        "slug": "P.-Zador",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Zador",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Zador"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However Zador (1982) has shown that for all p, the best lattice has"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6428815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f15fc5501de049ddafa46a9ed88b80839bf970b5",
            "isKey": false,
            "numCitedBy": 533,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Extensions of the limiting qnanfizafion error formula of Bennet are proved. These are of the form D_{s,k}(N,F)=N^{-\\beta}B , where N is the number of output levels, D_{s,k}(N,F) is the s th moment of the metric distance between quantizer input and output, \\beta,B>0,k=s/\\beta is the signal space dimension, and F is the signal distribution. If a suitably well-behaved k -dimensional signal density f(x) exists, B=b_{s,k}[\\int f^{\\rho}(x)dx]^{1/ \\rho},\\rho=k/(s+k) , and b_{s,k} does not depend on f . For k=1,s=2 this reduces to Bennett's formula. If F is the Cantor distribution on [0,1],0 and this k equals the fractal dimension of the Cantor set [12,13] . Random quantization, optimal quantization in the presence of an output information constraint, and quantization noise in high dimensional spaces are also investigated."
            },
            "slug": "Asymptotic-quantization-error-of-continuous-signals-Zador",
            "title": {
                "fragments": [],
                "text": "Asymptotic quantization error of continuous signals and the quantization dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Extensions of the limiting qnanfizafion error formula of Bennet are proved and random quantization, optimal quantization in the presence of an output information constraint, and quantization noise in high dimensional spaces are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48627980"
                        ],
                        "name": "R. Shibata",
                        "slug": "R.-Shibata",
                        "structuredName": {
                            "firstName": "Ritei",
                            "lastName": "Shibata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shibata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120633978,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4d97dd50206cd0ec23c8e6a1ef001aa76332c679",
            "isKey": false,
            "numCitedBy": 568,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Let $\\{x_t\\}$ be a linear stationary process of the form $x_t + \\Sigma_{1\\leqslant i<\\infty}a_ix_{t-i} = e_t$, where $\\{e_t\\}$ is a sequence of i.i.d. normal random variables with mean 0 and variance $\\sigma^2$. Given observations $x_1, \\cdots, x_n$, least squares estimates $\\hat{a}(k)$ of $a' = (a_1, a_2, \\cdots)$, and $\\hat{\\sigma}^2_k$ of $\\sigma^2$ are obtained if the $k$th order autoregressive model is assumed. By using $\\hat{a}(k)$, we can also estimate coefficients of the best predictor based on $k$ successive realizations. An asymptotic lower bound is obtained for the mean squared error of the estimated predictor when $k$ is selected from the data. If $k$ is selected so as to minimize $S_n(k) = (n + 2k)\\hat{\\sigma}^2_k$, then the bound is attained in the limit. The key assumption is that the order of the autoregression of $\\{x_t\\}$ is infinite."
            },
            "slug": "Asymptotically-Efficient-Selection-of-the-Order-of-Shibata",
            "title": {
                "fragments": [],
                "text": "Asymptotically Efficient Selection of the Order of the Model for Estimating Parameters of a Linear Process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49413185"
                        ],
                        "name": "E. Hannan",
                        "slug": "E.-Hannan",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Hannan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hannan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47718031"
                        ],
                        "name": "L. Kavalieris",
                        "slug": "L.-Kavalieris",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Kavalieris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kavalieris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123066494,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8206d24f67fa61b34624795532b12dd4cb503f23",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The problem considered is that of estimating an autoregressive-moving average system, including estimating the degrees of the autoregressive and moving average lag operators. The basic method is that introduced by Hannan & Rissanen (1982). However, that method may sometimes overestimate the degrees and modifications are here introduced to correct this. The problem is itself due to the use of a long autoregression, of order clog T when T is large, in the first stage of the process. The effect of this is investigated and in particular its effect on the speed of convergence of the estimates."
            },
            "slug": "A-method-for-autoregressive-moving-average-Hannan-Kavalieris",
            "title": {
                "fragments": [],
                "text": "A method for autoregressive-moving average estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748598"
                        ],
                        "name": "S. Kullback",
                        "slug": "S.-Kullback",
                        "structuredName": {
                            "firstName": "Solomon",
                            "lastName": "Kullback",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kullback"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Full details and background may be found in Shannon and Weaver (1959) and Kullback (1959). Consider a discrete random variable X with distribution"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 86412308,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "51739712c9b795f9533131122698cd5d01699f9d",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "information theory and statistics. Book lovers, when you need a new book to read, find the book here. Never worry not to find what you need. Is the information theory and statistics your needed book now? That's true; you are really a good reader. This is a perfect book that comes from great author to share with you. The book offers the best experience and lesson to take, not only take, but also learn."
            },
            "slug": "Information-Theory-and-Statistics-Kullback",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756533"
                        ],
                        "name": "G. Chaitin",
                        "slug": "G.-Chaitin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Chaitin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Chaitin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207698337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b85ae9f4367548a7dac7eae1a2a8d5866d27813a",
            "isKey": false,
            "numCitedBy": 1107,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of Turing machines for calculating finite binary sequences is studied from the point of view of information theory and the theory of recursive functions. Various results are obtained concerning the number of instructions in programs. A modified form of Turing machine is studied from the same point of view. An application to the problem of defining a patternless sequence is proposed in terms of the concepts here developed."
            },
            "slug": "On-the-Length-of-Programs-for-Computing-Finite-Chaitin",
            "title": {
                "fragments": [],
                "text": "On the Length of Programs for Computing Finite Binary Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An application to the problem of defining a patternless sequence is proposed in terms of the concepts here developed to study the use of Turing machines for calculating finite binary sequences."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125327631,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "faa975eaeb6a45031f77d6d7344ac905f74fb962",
            "isKey": false,
            "numCitedBy": 7189,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientific knowledge grows at a phenomenal pace--but few books have had as lasting an impact or played as important a role in our modern world as The Mathematical Theory of Communication, published originally as a paper on communication theory more than fifty years ago. Republished in book form shortly thereafter, it has since gone through four hardcover and sixteen paperback printings. It is a revolutionary work, astounding in its foresight and contemporaneity. The University of Illinois Press is pleased and honored to issue this commemorative reprinting of a classic."
            },
            "slug": "The-mathematical-theory-of-communication-Shannon",
            "title": {
                "fragments": [],
                "text": "The mathematical theory of communication"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This commemorative reprinting of The Mathematical Theory of Communication, published originally as a paper on communication theory more than fifty years ago, is released."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1948
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4052106"
                        ],
                        "name": "S. Geisser",
                        "slug": "S.-Geisser",
                        "structuredName": {
                            "firstName": "Seymour",
                            "lastName": "Geisser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geisser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119364084,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "e6307e1cc2f358f6260aaa8f3ab36d05a22933c8",
            "isKey": false,
            "numCitedBy": 2063,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An account is given of a recently devised method of prediction based on sample reuse techniques. It is most useful in low structure data paradigms that involve minimal assumptions. A series of applications demonstrating the technique is presented."
            },
            "slug": "The-Predictive-Sample-Reuse-Method-with-Geisser",
            "title": {
                "fragments": [],
                "text": "The Predictive Sample Reuse Method with Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A recently devised method of prediction based on sample reuse techniques that is most useful in low structure data paradigms that involve minimal assumptions is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135764"
                        ],
                        "name": "A. Kolmogorov",
                        "slug": "A.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Kolmogorov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kolmogorov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119745517,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "83077865493592cd8ebc5c9a8b900521428491ad",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Three-approaches-to-the-quantitative-definition-of-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Three approaches to the quantitative definition of information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144850958"
                        ],
                        "name": "J. Patrick",
                        "slug": "J.-Patrick",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Patrick",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Patrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 190222497,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0dced67b882a610063ce9de08af102978d1288ec",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Archaeoastronomy-in-the-Old-World:-STONE-CIRCLE-AN-Patrick-Wallace",
            "title": {
                "fragments": [],
                "text": "Archaeoastronomy in the Old World: STONE CIRCLE GEOMETRIES: AN INFORMATION THEORY APPROACH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 146987312,
            "fieldsOfStudy": [],
            "id": "bde36e4caf466d9afdba5d657cce9bd91e516c8c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Mathematical Theory of Communication"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1943223"
                        ],
                        "name": "A. Atkinson",
                        "slug": "A.-Atkinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Atkinson",
                            "middleNames": [
                                "Curtis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atkinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121976782,
            "fieldsOfStudy": [
                "Mathematics",
                "Psychology"
            ],
            "id": "689f763662572d0af3b32f04c51c0a58cd728f96",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Likelihood-ratios,-posterior-odds-and-information-Atkinson",
            "title": {
                "fragments": [],
                "text": "Likelihood ratios, posterior odds and information criteria"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102914571"
                        ],
                        "name": "Hieotugu Akaike",
                        "slug": "Hieotugu-Akaike",
                        "structuredName": {
                            "firstName": "Hieotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieotugu Akaike"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122530979,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e389294c3d86d067341d2a86ddebb36b5e7c15ad",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Comments-on-\u2018-On-model-structure-testing-in-system-Akaike",
            "title": {
                "fragments": [],
                "text": "Comments on \u2018 On model structure testing in system identification\u2019"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102164325"
                        ],
                        "name": "Rissanen",
                        "slug": "Rissanen",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116909576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9540cecb6618d6f506518fe6bea930f6d029ce51",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Parameter-estimation-by-shortest-description-of-Rissanen",
            "title": {
                "fragments": [],
                "text": "Parameter estimation by shortest description of data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Full details and background may be found in Shannon and Weaver (1959) and Kullback (1959). Consider a discrete random variable X with distribution"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125523249,
            "fieldsOfStudy": [],
            "id": "11fbf06e4c1c4eddc91a68e434433a4fc5f7cfc4",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694809"
                        ],
                        "name": "M. Georgeff",
                        "slug": "M.-Georgeff",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Georgeff",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Georgeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8748989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d816314835d23e31ab7fc3f59491f872fd877d",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-General-Selection-Criterion-for-Inductive-Georgeff-Wallace",
            "title": {
                "fragments": [],
                "text": "A General Selection Criterion for Inductive Inference"
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4110643"
                        ],
                        "name": "D. Boulton",
                        "slug": "D.-Boulton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boulton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boulton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 41661355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0396beddfbba4c20825019fd84c5fccb8a69c115",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Information-Measure-for-Hierarchic-Boulton-Wallace",
            "title": {
                "fragments": [],
                "text": "An Information Measure for Hierarchic Classification"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4110643"
                        ],
                        "name": "D. Boulton",
                        "slug": "D.-Boulton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boulton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boulton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37908979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a6712be6e3e46b547ecb2b975dfa612a3f83164",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Program-for-Numerical-Classification-Boulton-Wallace",
            "title": {
                "fragments": [],
                "text": "A Program for Numerical Classification"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052516042"
                        ],
                        "name": "G. Schwarz",
                        "slug": "G.-Schwarz",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Schwarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schwarz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Full details and background may be found in Shannon and Weaver (1959) and Kullback (1959)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123722079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37e44d1de8003d8394d158ec6afd1ff0e87e595b",
            "isKey": false,
            "numCitedBy": 39573,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-the-Dimension-of-a-Model-Schwarz",
            "title": {
                "fragments": [],
                "text": "Estimating the Dimension of a Model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Estimation-and-Inference-by-Compact-Coding-Wallace-Freeman/04eb446825da7a4c2ab3fa6df7ebd377baa66ebe?sort=total-citations"
}